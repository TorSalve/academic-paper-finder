{
  "papers": [
    {
      "title": "A desktop VR prototype for industrial training applications",
      "authors": ["Q. H. Wang", "J. R. Li"],
      "abstract": "The recent advances in computer graphics has spurred interest from both academics and industries in virtual reality (VR) enabled training applications. This paper presents a desktop VR prototype for industrial training applications. It is designed and implemented as a general shell by providing the data interface to import both the virtual environment models and specific domain knowledge. The geometric models of the virtual environment are constructed using feature-based modelling and assembly function by external CAD tools, and then transferred into the prototype through a conversion module. A hierarchical structure is proposed to partition and organise these imported virtual environment models. Based on this structure, a visibility culling approach is developed for fast rendering and user interaction. The case study has demonstrated the functionality of the proposed prototype system by applying it to a maintenance training application for a refinery bump system, which, in general, has a large number of polygons and a certain depth complexity. Significant speedup in both context rendering and response to user manipulations has been achieved to provide the user with a fast system response within the desktop virtual environment. Compared with the immersive VR system, the proposed system has offered an affordable and portable training media for industrial applications.",
      "keywords": [
        "Virtual reality",
        "Desktop virtual environment",
        "Computer-based training",
        "Visibility culling"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "20 May 2004",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0127-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0127-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0127-z"
    },
    {
      "title": "A comparison of two-dimensional prediction tracing and a virtual reality patient methods for diagnosis and treatment planning of orthognathic cases in dental students: a randomized preliminary study",
      "authors": [
        "Scott M. Sakowitz",
        "Marita R. Inglehart",
        "Vidya Ramaswamy",
        "Sean Edwards",
        "Brandon Shoukri",
        "Stephen Sachs",
        "Hera Kim-Berman"
      ],
      "abstract": "Virtual reality patient (VR patient), a simulated patient module in a virtual reality environment allowing manipulation of the upper and lower jaws and chin in three planes of space, was developed to help students understand diagnosis and treatment planning of orthognathic surgical procedures. The objective was to compare student understanding in diagnosing and treatment planning complex orthognathic cases using the VR patient versus a conventional 2D prediction tracing method and to determine feasibility of utilizing VR methods. Thirty third year dental students were assigned randomly to an experimental (VR patient) or control (2D tracing) group. The dependent variables were a multiple choice question (MCQ) examination, baseline and exit surveys, and written case analysis of two cases. Student\u2013teacher interactions were recorded for both length and type of interaction. Data were evaluated using descriptive and inferential statistics. The students\u2019 performance on the MCQ examinations improved immediately following the educational intervention (p\u2009&lt;\u2009.05). However, no significant difference was found between the 2 groups on the written case analysis and pre-test, post-test, and follow-up MCQ examinations. The effect size of the intervention ranged from .14 to .90 and differed greatly between the written responses to the two cases. Intra- and inter-rater reliability of the written response scoring was found to be reliable and reproducible (&gt;\u2009.928). Dental students were able to improve their understanding of diagnosis and treatment planning of orthognathic cases using both 2D prediction tracing and the VR patient methods. The method of scoring the written responses was reliable and reproducible and should be used for future full-scale studies.",
      "keywords": [
        "Virtual reality",
        "Dental education",
        "Orthognathic surgical prediction",
        "Simulated patient",
        "Orthodontics",
        "Oral surgery"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "04 December 2019",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-019-00413-w",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-019-00413-w.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-019-00413-w"
    },
    {
      "title": "Virtual realia: maneuverable computer 3D models and their use in learning assembly skills",
      "authors": ["William A. Kealy", "Chitra P. Subramaniam"],
      "abstract": "Two experiments compared real and virtual models as aids for learning assembly skills. In Experiment 1, ten participants individually studied either a fully assembled model, or a computer-generated one, in exploded view, that could be spatially manipulated in any direction. Participants then assembled the object in front of a video camera. ANOVA indicated virtual model are studied significantly longer but yield faster assembly than a real model. Experiment 2 used the same treatments plus a fully assembled virtual mode, randomly assigned to 28 participants who studied the aid, assembled the model, and then repeated the task from memory 3\u00a0days later. ANOVA indicated no differences between the three groups in assembly speed or accuracy. However, participants studied the exploded virtual model significantly longer than the two intact views of the model suggesting the former may impose a greater cognitive load due to the additional visual information it provides.",
      "keywords": [
        "Virtual Reality",
        "Real Model",
        "Augmented Reality",
        "Instructional Material",
        "Virtual Object"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "17 October 2006",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0054-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0054-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0054-2"
    },
    {
      "title": "The effect of a virtual reality learning environment on learners\u2019 spatial ability",
      "authors": ["Rui Sun", "Yenchun Jim Wu", "Qian Cai"],
      "abstract": "This study employed electroencephalography to record event-related potentials to investigate the difference in learning performance between learners with different levels of spatial ability in a traditional learning environment that utilizes presentation slides and a learning environment that incorporates virtual reality (VR). Thirty-two university students participated in the experiment. The N1 and P2 components were results that indicated selective attention at an early stage; their amplitudes were proportional to the cognitive loads. The experiment results revealed that the main effect of learning environment was significant. The N1 and P2 components had larger amplitudes and indicated higher cognitive loads in the presentation slides-based environment than in the VR-based environment. The main effect of spatial ability was also significant. The N1 and P2 amplitudes evoked in the high spatial ability (HSA) learners were smaller than those evoked in the low spatial ability (LSA) learners, indicating that the LSA learners possessed fewer cognitive resources and bore relatively high cognitive loads. The interaction effect of learning environment and spatial ability was significant. Larger P2 amplitude was observed in LSA learners in the presentation slides-based environment than in the VR-based environment, implying that VR facilitates the reduction of cognitive loads in LSA learners. The P2 amplitude detected in HSA learners did not show any significant difference in both learning environments, indicating that the VR-based learning environment did not enhance their learning. This result supports the ability-as-compensator hypothesis to a certain extent.",
      "keywords": [
        "Virtual reality (VR)",
        "Event-related potential (ERP)",
        "Spatial ability",
        "Cognitive load",
        "Education"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "03 July 2018",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-018-0355-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-018-0355-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-018-0355-2"
    },
    {
      "title": "The virtual playground: an educational virtual reality environment for evaluating interactivity and conceptual learning",
      "authors": ["Maria Roussou", "Martin Oliver", "Mel Slater"],
      "abstract": "The research presented in this paper aims at investigating user interaction in immersive virtual learning environments, focusing on the role and the effect of interactivity on conceptual learning. The goal has been to examine if the learning of young users improves through interacting in (i.e. exploring, reacting to, and acting upon) an immersive virtual environment (VE) compared to non-interactive or non-immersive environments. Empirical work was carried out with more than 55 primary school students between the ages of 8 and 12, in different between-group experiments: an exploratory study, a pilot study, and a large-scale experiment. The latter was conducted in a virtual environment designed to simulate a playground. In this \u201cVirtual Playground,\u201d each participant was asked to complete a set of tasks designed to address arithmetical \u201cfractions\u201d problems. Three different conditions, two experimental virtual reality (VR) conditions and a non-VR condition, that varied the levels of activity and interactivity, were designed to evaluate how children accomplish the various tasks. Pre-tests, post-tests, interviews, video, audio, and log files were collected for each participant, and analysed both quantitatively and qualitatively. This paper presents a selection of case studies extracted from the qualitative analysis, which illustrate the variety of approaches taken by children in the VEs in response to visual cues and system feedback. Results suggest that the fully interactive VE aided children in problem solving but did not provide a strong evidence of conceptual change as expected; rather, it was the passive VR environment, where activity was guided by a virtual robot, that seemed to support student reflection and recall, leading to indications of conceptual change.",
      "keywords": [
        "Virtual learning environments",
        "Interactivity",
        "Conceptual learning",
        "Evaluation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "05 October 2006",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0035-5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0035-5.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0035-5"
    },
    {
      "title": "Assessing hands-free interactions for VR using eye gaze and electromyography",
      "authors": ["Yun Suen Pai", "Tilman Dingler", "Kai Kunze"],
      "abstract": "With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel techniques, such as eye tracking, are now commercially available. In our work, we investigate the use of physiological signals as input to enhance VR experiences. We present a system using gaze tracking and electromyography on a user\u2019s forearm to make selection tasks in virtual spaces more efficient. In a study with 16 participants, we compared five different input techniques using a Fitts\u2019 law task: Using gaze tracking for cursor movement in combination with forearm contractions for making selections was superior to using an HTC Vive controller, Xbox gamepad, dwelling time, and eye-gaze dwelling time. To explore application scenarios and collect qualitative feedback, we further developed and evaluated a game with our input technique. Our findings inform the design of applications that use eye-gaze tracking and forearm muscle movements for effective user input in VR.",
      "keywords": [
        "Virtual reality",
        "Physiological sensing",
        "Eye gaze",
        "Electromyography"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "20 November 2018",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-018-0371-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-018-0371-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-018-0371-2"
    },
    {
      "title": "Body size illusions influence perceived size of objects: a validation of previous research in virtual reality",
      "authors": ["Stefan Weber", "Fred W. Mast", "David Weibel"],
      "abstract": "Previous research indicates that the size of the own body affects the judgment of objects\u2019 size, depending on the amount of subjective ownership toward the body (Van der Hoort et al. in PLOS ONE 6(5):e20195, 2011). We are the first to transfer this own-body-size effect into a virtual environment. In a series of three experiments, participants (N\u2009=\u200968) had to embody small, medium, and large avatars and judge the size of objects. Body ownership was manipulated using synchronous and asynchronous touch. We also included a new paradigm with an additional change of perspective to induce stronger ownership (Experiment 2). Additionally, we assessed whether the visibility of the body during the judgment phase influenced the results (Experiment 3). In all three experiments, we found an overestimation in a small and an underestimation in a large body compared to a medium body. However, size estimation did not depend on the degree of ownership despite clear differences in self-reported ownership. Our results show that a virtual reality scenario does not require a visuotactile manipulation of ownership in order to evoke the own-body-size effect. Our validation of the effect in a virtual setting may be helpful for the design of clinical applications.",
      "keywords": [
        "Own-body-size effect",
        "Ownership",
        "Virtual reality",
        "Size perception",
        "Embodiment",
        "Avatars"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "31 August 2019",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-019-00402-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-019-00402-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-019-00402-z"
    },
    {
      "title": "A comparative evaluation of viewing metaphors on psychophysical skills education in an interactive virtual environment",
      "authors": [
        "Dhaval Parmar",
        "Jeffrey Bertrand",
        "Sabarish V. Babu",
        "Kapil Madathil",
        "Melissa Zelaya",
        "Tianwei Wang",
        "John Wagner",
        "Anand K. Gramopadhye",
        "Kristin Frady"
      ],
      "abstract": "In an empirical evaluation, we examined the effect of viewing condition on psychophysical skills education in an interactive 3D simulation to train users in electrical circuitry. We compared an immersive head-mounted display (HMD)-based viewing metaphor versus a limited, desktop-based virtual reality (DVR) viewing metaphor with interaction using a spatial user interface. Psychophysical skills education involves the association of cognitive functions with motor functions to make the task autonomous with repeated practice. In electrical circuitry, this is demonstrated by the fine movements involved in handling and manipulating components on the electrical circuit, particularly while measuring electrical parameters. We created an interactive circuitry simulation (IBAS) where participants could learn about electrical measurement instruments such as the ammeter, voltmeter and multimeter, in a simulated breadboard VR system. Twenty-four participants utilized the simulation (12 in each condition), and the quantitative and qualitative aspects of psychophysical skills education with respect to the viewing metaphor were examined. Each viewing condition in IBAS was head-tracked and non-stereoscopic. Perspective correction was coupled with head-tracking in the DVR condition. The key quantitative measures were cognitive questionnaires addressing different levels of Bloom\u2019s cognitive taxonomy and a real-world psychophysical task addressing various levels of Dave\u2019s psychomotor taxonomy. The qualitative measures were the Witmer\u2013Singer sense of presence questionnaire and self-report. Results suggest that there was a significant increase in cognition post-experiment in both DVR and HMD viewing conditions in levels of knowledge, application, analysis and evaluation. Results also revealed a significant learning benefit with respect to the higher level concepts pertaining to evaluation in the HMD condition as compared to DVR. Participants seem to have enjoyed a greater level of affordance in task performance and spent a larger amount of time to complete the simulated exercises as well as manually maneuvered to further distances in the HMD viewing condition as compared to DVR viewing.",
      "keywords": [
        "HMD",
        "Human factors",
        "Education",
        "3D human\u2013computer interaction"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "14 May 2016",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-016-0287-7",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-016-0287-7.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-016-0287-7"
    },
    {
      "title": "Control mapping in virtual reality: effects on spatial presence and controller naturalness",
      "authors": ["Jonmichael Seibert", "Daniel M. Shafer"],
      "abstract": "This study explores how a video game player\u2019s sense of being in a game world (i.e., spatial presence) is impacted by the use of a virtual reality head-mounted display (VR HMD). Research focused on VR (as realized with the use of HMDs) has fallen by the wayside since the early 1990s due to the limitations in the technology. With modern reimagining of VR HMDs, there is now an opportunity to reexamine the impact it has on gaming experience. This article explores the results of an experiment in which university students played video games using either a VR HMD or a standard monitor while playing a first-person shooter video game. Control interface was also manipulated between incomplete tangible mapped devices (Razer Hydra) and directionally mapped devices (mouse and keyboard). Results indicated that VR HMDs have a positive impact on a players\u2019 level of spatial presence and feelings of controller naturalness. Controller naturalness also impacted spatial presence regardless of display condition.",
      "keywords": [
        "Virtual reality",
        "Spatial presence",
        "Video games",
        "Controller naturalness",
        "Natural mapping"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "06 June 2017",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-017-0316-1",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-017-0316-1.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-017-0316-1"
    },
    {
      "title": "A framework to design 3D interaction assistance in constraints-based virtual environments",
      "authors": [
        "Mouna Essabbah",
        "Guillaume Bouyer",
        "Samir Otmane",
        "Malik Mallem"
      ],
      "abstract": "The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D interactions. In this paper, we propose a generic framework for designing assistance to 3D user interaction in constraints-based virtual environment that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs). This framework is applied to design assistance tools for molecular biology analysis. Evaluation shows that VF designed using our framework improve effectiveness of the manipulation task.",
      "keywords": [
        "Virtual reality",
        "3D interaction",
        "Framework",
        "Complex environments",
        "Constraints",
        "Assistance model",
        "Virtual fixtures"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "25 June 2014",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-014-0247-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-014-0247-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-014-0247-z"
    },
    {
      "title": "Multisensory VR interaction for protein-docking in the CoRSAIRe project",
      "authors": [
        "N. F\u00e9rey",
        "J. Nelson",
        "C. Martin",
        "L. Picinali",
        "G. Bouyer",
        "A. Tek",
        "P. Bourdot",
        "J. M. Burkhardt",
        "B. F. G. Katz",
        "M. Ammi",
        "C. Etchebest",
        "L. Autin"
      ],
      "abstract": "Proteins take on their function in the cell by interacting with other proteins or biomolecular complexes. To study this process, computational methods, collectively named protein docking, are used to predict the position and orientation of a protein ligand when it is bound to a protein receptor or enzyme, taking into account chemical or physical criteria. This process is intensively studied to discover new biological functions for proteins and to better understand how these macromolecules take on these functions at the molecular scale. Pharmaceutical research also employs docking techniques for a variety of purposes, most notably in the virtual screening of large databases of available chemicals to select likely molecular candidates for drug design. The basic hypothesis of our work is that Virtual Reality (VR) and multimodal interaction can increase efficiency in reaching and analysing docking solutions, in addition to fully a computational docking approach. To this end, we conducted an ergonomic analysis of the protein\u2013protein current docking task as it is carried out today. Using these results, we designed an immersive and multimodal application where VR devices, such as the three-dimensional mouse and haptic devices, are used to interactively manipulate two proteins to explore possible docking solutions. During this exploration, visual, audio, and haptic feedbacks are combined to render and evaluate chemical or physical properties of the current docking configuration.",
      "keywords": [
        "Protein docking",
        "User-centered design",
        "Virtual reality",
        "Multimodal rendering"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "01 October 2009",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-009-0136-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-009-0136-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-009-0136-z"
    },
    {
      "title": "Shopping in virtual reality: a study on consumers\u2019 shopping experience in a stereoscopic virtual reality",
      "authors": ["Kung Wong Lau", "Pui Yuen Lee"],
      "abstract": "The popularity of home-based stereoscopic television provides researchers and practitioners with possibilities of bringing stereoscopic virtual reality (StereoVR) at consumers\u2019 home. To further the investigation on the potential development of applying StereoVR in retailing, this research focuses on understanding consumers\u2019 shopping experiences in this new platform. The research team believes that the use of StereoVR has potentials to become a new arena for interactive business. To explore these potential uses of technology in retailing, the team designed and built a StereoVR, called \u201cFutureShop,\u201d for implementing a virtual fashion retailing practices as well as collecting consumers\u2019 responses for further development. Participants are asked to complete a shopping process from product selection to purchase in FutureShop. The factors examined in this research included the consumers\u2019 purchase intention, interactive shopping and hedonic shopping experience. The findings and implications suggest that the StereoVR can make a significant contribution in creating more interactive experiences for apparel retailing by enhancing consumers\u2019 hedonic shopping experiences in the StereoVR.",
      "keywords": [
        "Virtual reality",
        "Stereoscopic displays",
        "Consumer research",
        "Retailing",
        "Shopping experiences"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "14 August 2018",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-018-0362-3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-018-0362-3.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-018-0362-3"
    },
    {
      "title": "Beyond user experimentation: notational-based systematic evaluation of interaction techniques in virtual reality environments",
      "authors": [
        "Emmanuel Dubois",
        "Luciana P. Nedel",
        "Carla M. Dal Sasso. Freitas",
        "Liliane Jacon"
      ],
      "abstract": "Despite the increasing number of interaction devices for virtual reality (VR) applications (e.g. data-gloves, space balls, data-suits and so on), surprisingly very little attention has been given to the evaluation of VR interaction techniques or more generally to the usability of virtual reality environments (VRE). The main reasons for these limited efforts are probably that empirical user testing with VREs is difficult and time-consuming and ergonomic rules or criteria and traditional HCI tools and methods are not well suited for VRE. Alternatively, the specification of interaction based on a formal method or notation provides a precise and unambiguous description that can be used to reason the user\u2019s actions while interacting with a VRE. In this paper, we propose a new approach to design interaction techniques in VRE, based on the use of a formal specification language: the ASUR notation. In the early stages of system design, time and effort are reduced by assisting the designers in considering alternative solutions and anticipating usability issues. To better explain the proposed methodology, we report an evaluation of selection and manipulation techniques in a virtual environment based on a chess game. The evaluation has been carried out in two ways: predictively, with the help of the ASUR notation, and empirically via user experiments. We present the outcomes of the empirical studies and demonstrate that the reasoning with the ASUR notation leads to similar but also results complementary to those obtained with the experiments.",
      "keywords": [
        "Mixed reality",
        "Virtual reality",
        "3D interaction",
        "Interaction design notation",
        "User experimentation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "23 February 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-005-0151-7",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-005-0151-7.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-005-0151-7"
    },
    {
      "title": "Virtual reality for assembly methods prototyping: a review",
      "authors": ["Abhishek Seth", "Judy M. Vance", "James H. Oliver"],
      "abstract": "Assembly planning and evaluation is an important component of the product design process in which details about how parts of a new product will be put together are formalized. A well designed assembly process should take into account various factors such as optimum assembly time and sequence, tooling and fixture requirements, ergonomics, operator safety, and accessibility, among others. Existing computer-based tools to support virtual assembly either concentrate solely on representation of the geometry of parts and fixtures and evaluation of clearances and tolerances or use simulated human mannequins to approximate human interaction in the assembly process. Virtual reality technology has the potential to support integration of natural human motions into the computer aided assembly planning environment (Ritchie et al. in Proc I MECH E Part B J Eng 213(5):461\u2013474, 1999). This would allow evaluations of an assembler\u2019s ability to manipulate and assemble parts and result in reduced time and cost for product design. This paper provides a review of the research in virtual assembly and categorizes the different approaches. Finally, critical requirements and directions for future research are presented.",
      "keywords": [
        "Virtual assembly",
        "Collision detection",
        "Physics-based modeling",
        "Constraint-based modeling",
        "Virtual reality",
        "Haptics",
        "Human\u2013computer interaction"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "22 January 2010",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-009-0153-y",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-009-0153-y.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-009-0153-y"
    },
    {
      "title": "Cable harness design, assembly and installation planning using immersive virtual reality",
      "authors": [
        "James M. Ritchie",
        "Graham Robinson",
        "Philip N. Day",
        "Richard G. Dewar",
        "Raymond C. W. Sung",
        "John E. L. Simmons"
      ],
      "abstract": "Earlier research work using immersive virtual reality (VR) in the domain of cable harness design has shown conclusively that this technology had provided substantial productivity gains over traditional computer-aided design (CAD) systems. The follow-on work in this paper was aimed at understanding the degree to which various aspects of the immersive VR system were contributing to these benefits and how engineering design and planning processes could be analysed in detail as they are being carried out; the nature of this technology being such that the user\u2019s activities can be non-intrusively monitored and logged without interrupting a creative design process or a manufacturing planning task. This current research involved the creation of a more robust CAD-equivalent VR system for cable harness routing design, harness assembly and installation planning which could be functionally evaluated using a set of creative design-task experiments to provide detail about the system and users\u2019 performance. A design task categorisation scheme was developed which allowed both a general and detailed breakdown of the design engineer\u2019s cable harness design process and associated activities. This showed that substantial amounts of time were spend by the designer in navigation (41%), sequence breaks (28%) and carrying out design-related activities (27%). The subsequent statistical analysis of the data also allowed cause and effect relationships between categories to be examined and showed statistically significant results in harness design, harness design modification and menu/model interaction. This insight demonstrated that poorly designed interfaces can have adverse affects on the productivity of the designer and that 3D direct manipulation interfaces have advantages. Indeed, the categorisation scheme provided a valuable tool for understanding design behaviour and could be used for comparing different design platforms as well as examining other aspects of the design function, such as the acquisition of design decision intent. The system also demonstrated the successful automatic generation of cable harness assembly and cable harness installation plans from non-intrusive user-system interaction logging, which further demonstrates the potential for concurrent design and manufacturing planning to be carried out.",
      "keywords": [
        "Virtual Reality",
        "Design Task",
        "Virtual Reality System",
        "Creative Design",
        "Assembly Planning Sequence"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "03 May 2007",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-007-0073-7",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-007-0073-7.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-007-0073-7"
    },
    {
      "title": "Two-handed assembly with immersive task planning in virtual reality",
      "authors": ["H. Sun", "B. Hujun"],
      "abstract": "Assembly modelling is the process of capturing entities and activity information related to assembling and assembly. Currently, most CAD systems have been developed to ease the design of individual components, but are limited in their support for assembly designs and planning capability, which are crucial for reducing the cost and processing time in complex design, constraint analysis and assembly task planning. This paper presents a framework of a two-handed virtual assembly (VA) planner for assembly tasks, which coordinates two hands jointly for feature-based manipulation, assembly analysis and constraint-based task planning. Feature-based manipulation highlights the important assembling features (e.g. dynamic reference frames, moving arrow, mating features) to guide users for the ease of assembly and in an efficient and fluid manner. The users can freely navigate and move the mating pair along the collision-free path. The free motion of two-handed input in assembly is further restricted to the allowable motion guided by the constraints recognised on-line. The allowable motion in assembly is planned by the logic steps derived from the analysis of constraints and their translation in the progress of assembly. No preprocessing or predefined assembly sequence is necessary since the planning is produced in real-time upon the two-handed interactions. Mating features and constraints in databases are automatically updated after each assembly to simplify the planning process. The two-handed task planner has been developed and experimented for several assembly examples including a drill (12-parts) and a robot (17-parts). The system can be generally applied for the interactive task planning of assembly-type applications.",
      "keywords": [
        "Two-handed interface",
        "User interaction",
        "Virtual assembly",
        "Virtual reality"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "March 2002",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/BF01408565",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/BF01408565.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/BF01408565"
    },
    {
      "title": "Auditory and visual 3D virtual reality therapy for chronic subjective tinnitus: theoretical framework",
      "authors": [
        "Alain Londero",
        "Isabelle Viaud-Delmon",
        "Alexis Baskind",
        "Olivier Delerue",
        "St\u00e9phanie Bertet",
        "Pierre Bonfils",
        "Olivier Warusfel"
      ],
      "abstract": "It is estimated that ~10% of the adult population in developed countries is affected by subjective tinnitus. Physiopathology of subjective tinnitus remains incompletely explained. Nevertheless, subjective tinnitus is thought to result from hyperactivity and neuroplastic reorganization of cortical and subcortical networks following acoustic deafferentation induced by cochlear or auditory nerve damage. Involvement of both auditory and non-auditory central nervous pathways explains the conscious perception of tinnitus and also the potentially incapacitating discomfort experienced by some patients (sound hypersensitivity, sleep disorders, attention deficit, anxiety or depression). These clinical patterns are similar to those observed in chronic pain following amputation where conditioning techniques using virtual reality have been shown both to be theoretically interesting and effectively useful. This analogy led us to develop an innovative setup with dedicated auditory and visual 3D virtual reality environments in which unilateral subjective tinnitus sufferers are given the possibility to voluntarily manipulate an auditory and visual image of their tinnitus (tinnitus avatar). By doing so, the patients will be able to transfer their subjective auditory perception to the tinnitus avatar and to gain agency on this multimodal virtual percept they hear, see and spatially control. Repeated sessions of such virtual reality immersions are then supposed to contribute to tinnitus treatment by promoting cerebral plasticity. This paper describes the theoretical framework and setup adjustments required by this very first attempt to adapt virtual reality techniques to subjective tinnitus treatment. Therapeutic usefulness will be validated by a further controlled clinical trial.",
      "keywords": ["Tinnitus", "Virtual reality", "Neuroplasticity"],
      "published_in": "Virtual Reality",
      "publication_date": "25 September 2009",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-009-0135-0",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-009-0135-0.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-009-0135-0"
    },
    {
      "title": "Simulator sickness in patients with neck pain and vestibular pathology during virtual reality tasks",
      "authors": [
        "Ryan Tyrrell",
        "Hilla Sarig-Bahat",
        "Katrina Williams",
        "Grace Williams",
        "Julia Treleaven"
      ],
      "abstract": "Immersion in virtual environments can cause simulator sickness (SS). Further, head and neck movement in interactive virtual reality (VR) assessment and training stimulates the vestibular and cervical afferent systems that can cause dizziness in subjects with neck pain and vestibular pathology. This cross-sectional, observational, study investigated SS and factors that may influence this between 20 neck pain, 14 vestibular pathology and 20 asymptomatic control subjects. Pre-VR questionnaires included a visual symptom scale and dizziness intensity. SS measures included the simulator sickness visual analogue scale and the simulator sickness questionnaire. Significantly greater incidence of any SS and higher values were found in the vestibular and neck pain groups compared to the control group in selected SS measures. No significant differences were found when comparing SS measures between the vestibular and neck pain groups. Significant mild-to-moderate correlations for the entire population were found between both SS measures to pre-VR visual symptoms and dizziness intensity. SS levels in neck pain and vestibular populations are comparable and higher than asymptomatic individuals. Dizziness and visual disturbances may be associated with an increase in severity of SS in these clinical populations.",
      "keywords": [
        "Simulator sickness",
        "Dizziness",
        "Virtual reality",
        "Vestibular"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "22 September 2017",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-017-0324-1",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-017-0324-1.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-017-0324-1"
    },
    {
      "title": "Perceiving interpersonally-mediated risk in virtual environments",
      "authors": ["David B. Portnoy", "Natalie D. Smoak"],
      "abstract": "Using virtual reality (VR) to examine risky behavior that is mediated by interpersonal contact, such as agreeing to have sex, drink, or smoke with someone, offers particular promise and challenges. Social contextual stimuli that might trigger impulsive responses can be carefully controlled in virtual environments (VE), and yet manipulations of risk might be implausible to participants if they do not feel sufficiently immersed in the environment. The current study examined whether individuals can display adequate evidence of presence in a VE that involved potential interpersonally-induced risk: meeting a potential dating partner. Results offered some evidence for the potential of VR for the study of such interpersonal risk situations. Participants\u2019 reaction to the scenario and risk-associated responses to the situation suggested that the embodied nature of virtual reality override the reality of the risk\u2019s impossibility, allowing participants to experience adequate situational embedding, or presence.",
      "keywords": [
        "Presence",
        "Sexual risk",
        "Embodiment",
        "Interpersonal risk"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "24 May 2009",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-009-0120-7",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-009-0120-7.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-009-0120-7"
    },
    {
      "title": "Context-driven interaction in immersive virtual environments",
      "authors": ["Scott Frees"],
      "abstract": "There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they are often performed under different contexts. For each task and context, specialized interaction techniques can be developed. We present the context-driven interaction model: a design pattern that represents contextual information as a first-class, quantifiable component within a user interface and supports the development of context-sensitive applications by decoupling context recognition, context representation, and interaction technique development. As a primary contribution, this model provides an enumeration of important representations of contextual information gathered from across the literature and describes how these representations can effect the selection of an appropriate interaction technique. We also identify how several popular 3D interaction techniques adhere to this design pattern and describe how the pattern itself can lead to a more focused development of effective interfaces. We have constructed a formalized programming toolkit and runtime system that serves as a reference implementation of the context-driven model and a discussion is provided explaining how the toolkit can be used to implement a collection of representative 3D interaction interfaces.",
      "keywords": [
        "Human\u2013computer interaction",
        "Context-sensitive interaction",
        "Virtual reality",
        "Virtual environments, 3DUI, interaction techniques"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "16 November 2010",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-010-0178-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-010-0178-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-010-0178-2"
    },
    {
      "title": "Haptic Cooperative Virtual Workspace: Architecture and evaluation",
      "authors": ["M. O. Alhalabi", "S. Horiguchi"],
      "abstract": "The Haptic Cooperative Virtual Workspace (HCVW), where users can simultaneously manipulate and haptically feel the same object, is beneficial and in some cases indispensable for training a team of surgeons, or in application areas in telerobotics and entertainment. In this paper we propose an architecture for the haptic cooperative workspace where the participants can kinesthetically interact, feel and push each other simultaneously while moving in the simulation. This involves the ability to manipulate the same virtual object at the same time. A set of experiments carried out to investigate the haptic cooperative workspace is reported. A new approach to quantitatively evaluate the cooperative haptic system is proposed, which can be extended to evaluate haptic systems in general.",
      "keywords": [
        "Cooperative system architecture",
        "Haptic cooperative applications",
        "Haptic cooperative virtual workspace",
        "Haptic"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "September 2000",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/BF01409421",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/BF01409421.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/BF01409421"
    },
    {
      "title": "The affect of contact force sensations on user performance in virtual assembly tasks",
      "authors": ["Samir Garbaya", "U. Zaldivar-Colado"],
      "abstract": "The development of a realistic virtual assembly environment is challenging because of the complexity of the physical processes and the limitation of available VR technology. Many research activities in this domain primarily focused on particular aspects of the assembly task such as the feasibility of assembly operations in terms of interference between the manipulated parts. The virtual assembly environment reported in this research is focused on mechanical part assembly. The approach presented addresses the problem of part-to-part contacts during the mating phase of assembly tasks. The system described calculates contact force sensations by making their intensity dependent on the depth of penetration. However the penetration is not visible to the user who sees a separate model, which does not intersect the mating part model. The two 3D models of the part, the off-screen rendered model and the on-screen rendered model are connected by a spring-dumper arrangement. The force calculated is felt by the operator through the haptic interface when parts come in contact during the mating phase of the assembly task. An evaluation study investigating the effect of contact force sensation on user performance during part-to-part interface was conducted. The results showed statistically significant effect of contact force sensation on user performance in terms of task completion time. The subjective evaluation based on feedback from users confirmed that contact force sensation is a useful cue for the operator to find the relative positions of components in the final assembly state.",
      "keywords": [
        "Virtual assembly",
        "Spring-damper model",
        "Haptic interface",
        "Human performance"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "09 May 2007",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-007-0075-5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-007-0075-5.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-007-0075-5"
    },
    {
      "title": "Development and validation of a simulation workload measure: the simulation task load index (SIM-TLX)",
      "authors": ["David Harris", "Mark Wilson", "Samuel Vine"],
      "abstract": "Virtual reality (VR) simulation offers significant potential for human factors training as it provides a novel approach which enables training in environments that are otherwise dangerous, impractical or expensive to simulate. While VR training has been adopted in many environments, such as heavy industry, surgery and aviation, there remains an inadequate understanding of how virtual simulations impact cognitive factors. One such factor, which needs careful consideration during the design of VR simulations, is the degree of mental or cognitive load experienced during training. This study aimed to validate a newly developed measure of workload, based on existing instruments (e.g. the NASA-TLX), but tailored to the specific demands placed on users of simulated environments. While participants completed a VR puzzle game, a series of experimental manipulations of workload were used to assess the sensitivity of the new instrument. The manipulations affected the questionnaire subscales (mental demands; physical demands; temporal demands; frustration; task complexity; situational stress; distraction; perceptual strain; task control; presence) as predicted in all cases (ps\u2009&lt;\u2009.05), except for presence, which displayed little relationship with other aspects of task load. The scale was also found to have good convergent validity with an alternate index of task load. The findings support the sensitivity of the new instrument for assessing task load in virtual reality. Overall, this study contributes to the understanding of mental workload in simulated environments and provides a practical tool for use in both future research and applications in the field.",
      "keywords": [
        "Workload",
        "Cognitive load",
        "Learning",
        "Virtual reality",
        "Training"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "21 December 2019",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-019-00422-9",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-019-00422-9.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-019-00422-9"
    },
    {
      "title": "Towards adaptive Web scriptable user interfaces for virtual environments",
      "authors": ["Emad Barsoum", "Falko Kuester"],
      "abstract": "The vast majority of Web-based technology, its ability to visualize static and time varying information and the pervasive nature of its content have lead the development of applications and user interfaces that port between a broad range of operating systems, databases and devices. However the integration of this immense resource in virtual environments (VEs) remains elusive. In this paper we present a Web scriptable user interface that utilizes Web browser technology to enable the user to search the internet for arbitrary information and to seamlessly augment this information into the VE. WebVRI provides access to the standard data input and query mechanisms offered by conventional Web browsers, with the difference that it generates active texture-skins of the Web contents that can be mapped onto arbitrary surfaces within the environment. Once mapped, the corresponding texture functions as a fully integrated Web-browser that will respond to traditional events such as the selection of links or text input. As a result, any surface within the environment can be turned into a Web-enabled resource that provides access to user-definable data. Using WebVRI, users can merge Web contents into their VE, control its behavior and collaborate with other users inside and outside the VE. This provides a completely new mechanism to access readily available Web-based data, documents, images, animations, simulations and visualization. WebVRI also enables game-based education by providing the ability to create contents rich, pervasive VR-based edutainment environments.",
      "keywords": [
        "Virtual reality",
        "3D Web browser",
        "WWW",
        "Navigation",
        "2D/3D user interface"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "20 February 2008",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-008-0087-9",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-008-0087-9.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-008-0087-9"
    },
    {
      "title": "New wireless connection between user and VE using speech processing",
      "authors": ["M. Ali Mirzaei", "Frederic Merienne", "James H. Oliver"],
      "abstract": "This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of the process through a Wi-Fi network. The proposed VRPN server provides a handy, portable and wireless human machine interface that can facilitate interaction in a variety interfaces and application domains including HMD- and CAVE-based virtual reality systems, flight and driving simulators and many others. The VRPN server is based on a speech processing software development kits and VRPN library in C++. Speak-to-VR VRPN works well even in the presence of background noise or the voices of other users in the vicinity. The speech processing algorithm is not sensitive to the user\u2019s accent because it is trained while it is operating. Speech recognition parameters are trained by hidden Markov model in real time. The advantages and disadvantages of the speak-to-VR server are studied under different configurations. Then, the efficiency and the precision of the speak-to-VR server for a real application are validated via a formal user study with ten participants. Two experimental test setups are implemented on a CAVE system by using either Kinect Xbox or array microphone as input device. Each participant is asked to navigate in a virtual environment and manipulate an object. The experimental data analysis shows promising results and motivates additional research opportunities.",
      "keywords": [
        "Speak-to-VR",
        "Wi-Fi network",
        "Speech processing",
        "VRPN server"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "20 July 2014",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-014-0248-y",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-014-0248-y.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-014-0248-y"
    },
    {
      "title": "Grasp programming by demonstration in virtual reality with automatic environment reconstruction",
      "authors": ["Jacopo Aleotti", "Stefano Caselli"],
      "abstract": "A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer.",
      "keywords": [
        "Virtual reality",
        "Environment modeling",
        "Grasp programming",
        "Glove interaction"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "20 November 2010",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-010-0172-8",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-010-0172-8.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-010-0172-8"
    },
    {
      "title": "Sensory-motor enhancement in a virtual therapeutic environment",
      "authors": [
        "Richard A. Foulds",
        "David M. Saxe",
        "Arthur W. Joyce Iii",
        "Sergei Adamovich"
      ],
      "abstract": "The sensory-motor skills of persons with neuromuscular disabilities have been shown to be enhanced by intensive and repetitive therapeutic interventions. This paper describes a form of low immersion virtual reality and a prototype, open source system that allow a user with significant physical disability to actively interact with computer-generated objects whose behaviors promote a game-like interaction. Unlike fully immersive and haptic virtual reality, this approach frees the user from head-mounted displays and gloves. It extracts the user\u2019s real-time silhouette from the output of a remote video camera and uses that two-dimensional outline to interact with graphical objects on screen. In contrast to video games that have been modified with specialized interfaces, this virtual interaction system promotes the repetitive use of goal directed movements of the arms and body, which are essential to promote cortical reorganization, as well as discourage unwanted changes in muscle tissue that result in contracture. A prototype system demonstrates the potential of low immersion technology to motivate users and encourage participation in therapy. It also offers the potential of accommodating the sensory-motor skills of individuals with very significant impairment. The behaviors of the computer-generated graphics can be altered to allow use by those with very limited range of motion and/or motor control. These behaviors can be adjusted to provide a continuing challenge as the user\u2019s skills improve. This prototype is described in terms of functional capabilities that include a silhouette extraction from a video image, and generation of graphical objects that interact with the silhouette. The work is extended with a discussion of a more sophisticated region of interest detection algorithm that can select specific parts of the body.",
      "keywords": [
        "Low-immersion",
        "Rehabilitation",
        "Cortical reorganization",
        "Therapy",
        "Sensory-motor skills",
        "Biomedical engineering"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "28 March 2007",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-007-0067-5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-007-0067-5.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-007-0067-5"
    },
    {
      "title": "Physics-based virtual reality for task learning and intelligent disassembly planning",
      "authors": ["Jacopo Aleotti", "Stefano Caselli"],
      "abstract": "Physics-based simulation is increasingly important in virtual manufacturing for product assembly and disassembly operations. This work explores potential benefits of physics-based modeling for automatic learning of assembly tasks and for intelligent disassembly planning in desktop virtual reality. The paper shows how realistic physical animation of manipulation tasks can be exploited for learning sequential constraints from user demonstrations. In particular, a method is proposed where information about physical interaction is used to discover task precedences and to reason about task similarities. A second contribution of the paper is the application of physics-based modeling to the problem of disassembly sequence planning. A novel approach is described to find all physically admissible subassemblies in which a set of rigid objects can be disassembled. Moreover, efficient strategies are presented aimed at reducing the computational time required for automatic disassembly planning. The proposed strategies take into account precedence relations arising from user assembly demonstrations as well as geometrical clustering. A motion planning technique has also been developed to generate non-destructive disassembly paths in a query-based approach. Experiments have been performed in an interactive virtual environment including a dataglove and motion tracker that allows realistic object manipulation and grasping.",
      "keywords": [
        "Virtual reality",
        "Disassembly planning",
        "Precedence graphs",
        "Physics-based animation",
        "Programming by demonstration"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "02 February 2010",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-009-0145-y",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-009-0145-y.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-009-0145-y"
    },
    {
      "title": "Handling of Virtual Contact in Immersive Virtual Environments: Beyond Visuals",
      "authors": [
        "R. W. Lindeman",
        "J. N. Templeman",
        "J. L. Sibert",
        "J. R. Cutler"
      ],
      "abstract": "This paper addresses the issue of improving the perception of contact that users make with purely virtual objects in virtual environments. Because these objects have no physical component, the user's perceptual understanding of the material properties of the object, and of the nature of the contact, is hindered, often limited solely to visual feedback. Many techniques for providing haptic feedback to compensate for the lack of touch in virtual environments have been proposed. These systems have increased our understanding of the nature of how humans perceive contact. However, providing effective, general-purpose haptic feedback solutions has proven elusive. We propose a more-holistic approach, incorporating feedback to several modalities in concert. This paper describes a prototype system we have developed for delivering vibrotactile feedback to the user. The system provides a low-cost, distributed, portable solution for incorporating vibrotactile feedback into various types of systems. We discuss different parameters that can be manipulated to provide different sensations, propose ways in which this feedback can be combined with feedback of other modalities to create a better understanding of virtual contact, and describe possible applications.",
      "keywords": ["Haptic; Multimodal; Vibrotactile"],
      "published_in": "Virtual Reality",
      "publication_date": "October 2002",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s100550200014",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s100550200014.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s100550200014"
    },
    {
      "title": "Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube",
      "authors": [
        "Emmanuelle Chapoulie",
        "Maud Marchal",
        "Evanthia Dimara",
        "Maria Roussou",
        "Jean-Christophe Lombardo",
        "George Drettakis"
      ],
      "abstract": "A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate objects with fingers of both hands in a close-to-natural manner for moderately complex, general purpose tasks. Our solution couples finger tracking with a real-time physics engine, combined with a heuristic approach for hand manipulation, which is robust to tracker noise and simulation instabilities. A first study has been performed to evaluate our interface, with tasks involving complex manipulations, such as balancing objects while walking in the cube. The user\u2019s finger-tracked manipulation was compared to manipulation with a 6 degree-of-freedom wand (or flystick), as well as with carrying out the same task in the real world. Users were also asked to perform a free task, allowing us to observe their perceived level of presence in the scene. Our results show that our approach provides a feasible interface for immersive cube environments and is perceived by users as being closer to the real experience compared to the wand. However, the wand outperforms direct manipulation in terms of speed and precision. We conclude with a discussion of the results and implications for further research.",
      "keywords": [
        "Virtual reality",
        "Direct manipulation",
        "Immersive cube",
        "Finger tracking"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "13 February 2014",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-014-0246-0",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-014-0246-0.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-014-0246-0"
    },
    {
      "title": "A two visual systems approach to understanding voice and gestural interaction",
      "authors": ["Barry A. Po", "Brian D. Fisher", "Kellogg S. Booth"],
      "abstract": "It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a neuroanatomical model of perception and action known as the two visual systems hypothesis, we conducted a study with two controlled experiments to compare four different kinds of spatial interaction: (1) voice-based input, (2) pointing with a visual cursor, (3) pointing without a visual cursor, and (4) pointing with a time-lagged visual cursor. Consistent with the two visual systems hypothesis, we found that voice-based input and pointing with a cursor were less robust to a display illusion known as the induced Roelofs effect than pointing without a cursor or even pointing with a lagged cursor. The implications of these findings are discussed, with an emphasis on how the two visual systems model can be used to understand the basis for voice and gestural interactions that support spatial target selection in large screen and immersive environments.",
      "keywords": [
        "Two visual systems",
        "Pointing",
        "Cursors",
        "Visual feedback",
        "Voice input",
        "Visual illusions"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "14 June 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-005-0156-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-005-0156-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-005-0156-2"
    },
    {
      "title": "Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment",
      "authors": [
        "Ang\u00e9lica De Antonio",
        "Ricardo Imbert",
        "Jaime Ram\u00edrez",
        "Xavier Ferr\u00e9"
      ],
      "abstract": "This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants). The general aim was to integrate environmental scanning technologies with human modelling and radiological dose estimation tools, and to deliver an intuitive and cost-effective system for use by operators involved with interventions in radiologically controlled areas. The usability of the resulting products was one of the main success criteria. This paper describes the general approach and design mechanisms used in the HeSPI (HeSPI stands for the Spanish for Herramienta para la Simulaci\u00f3n y Planificaci\u00f3n de Intervenciones, or tool for the simulation and planning of interventions) tool that has been developed by one of the teams. The tool provides the designer of an intervention with a humanoid 3D model, or mannequin, that can be loaded into the desired environment and will be used by the designer as if he was manipulating a puppet, making it move around the environment and perform different kinds of actions, adopting varied postures, interacting with the objects in the environment and manipulating tools and equipment. A combination of a graphical user interface (GUI) and a voice recognition system, together with the selected design mechanisms, has proven to offer good enough interaction possibilities for this kind of desktop virtual environment.",
      "keywords": [
        "Planning and design of interventions",
        "Object manipulation",
        "Voice recognition",
        "Generic actions",
        "Usability of desktop virtual environments"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "25 May 2004",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0129-x",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0129-x.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0129-x"
    },
    {
      "title": "A seamless solution for 3D real-time interaction: design and evaluation",
      "authors": ["Franck Hernoux", "Olivier Christmann"],
      "abstract": "This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are not enough for interacting in 3D VE; current motion capture systems are expensive and require wearing equipment. We developed a solution to allow more natural interactions with objects and VE for navigation and manipulation tasks. We conducted an experimental study involving 20 participants. The goal was to realize object manipulation (moving, orientation, scaling) and navigation tasks in VE. We compared our solution (Microsoft Kinect-based) with data gloves and magnetic sensors (3DGloves) regarding two criteria: performance and acceptability. Results demonstrate similar performance (precision, execution time) but a better overall acceptability for our solution. Preferences of participants are mostly in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. Our solution can be considered as a real alternative to conventional systems for object manipulation in virtual reality.",
      "keywords": [
        "Virtual reality",
        "Seamless solution",
        "Hand tracking",
        "Real time",
        "3D interaction",
        "3D camera"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "05 November 2014",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-014-0255-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-014-0255-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-014-0255-z"
    },
    {
      "title": "Interaction styles in tools for developing virtual environments",
      "authors": ["Jesper Kjeldskov", "Jan Stage"],
      "abstract": "This article discusses and compares interaction styles in development tools for virtual environments (VE). The comparison relies on a qualitative empirical study of two development processes where a command language and a direct manipulation based tool were used to develop the same virtual environment application. The command language tool proved very flexible and facilitated an even distribution of effort and progress over time, but debugging and identification of errors was very difficult. Contrasting this, the direct manipulation tool enabled faster implementation of a first prototype but did not facilitate a shorter implementation process as a whole. On the basis of these findings, the strength and weaknesses of direct manipulation for developing virtual environment applications are explored further through a comparison with a successful direct manipulation tool for developing interactive multimedia applications. The comparisons are used to identify and emphasize key requirements for virtual environment development tool interface design.",
      "keywords": [
        "Virtual environments",
        "Development tools",
        "Interaction styles",
        "Empirical study"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "04 March 2008",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-008-0091-0",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-008-0091-0.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-008-0091-0"
    },
    {
      "title": "Immersive manipulation of virtual objects through glove-based hand gesture interaction",
      "authors": ["Gan Lu", "Lik-Kwan Shark", "Geoff Hall", "Ulrike Zeshan"],
      "abstract": "Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are therefore presented to provide natural user interaction to enable effortless virtual object manipulation. Presented in this paper is the development and evaluation of an immersive human\u2013computer interaction system based on stereoscopic viewing and natural hand gestures. For the development, it is based on the integration of a back-projection stereoscopic system for object and hand display, a hybrid inertial and ultrasonic tracking system to provide the absolute positions and orientations of the user\u2019s head and hands, as well as a pair of high degrees-of-freedom data gloves to provide the relative positions and orientations of digit joints and tips on both hands. For the evaluation, it is based on a two-object scene with a virtual cube and a CT (computed tomography) volume created for demonstration of real-time immersive object manipulation. The system is shown to provide a correct user view of objects and hands in 3D with depth, as well as to enable a user to use a number of simple hand gestures to perform basic object manipulation tasks involving selection, release, translation, rotation and scaling. Also included in the evaluation are some quantitative tests of the system performance in terms of speed and latency.",
      "keywords": [
        "Hand gesture tracking and recognition",
        "Immersive stereoscopic visualisation",
        "Virtual object manipulation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "05 August 2011",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-011-0195-9",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-011-0195-9.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-011-0195-9"
    },
    {
      "title": "Untethered gesture acquisition and recognition for virtual world manipulation",
      "authors": ["David Demirdjian", "Teresa Ko", "Trevor Darrell"],
      "abstract": "Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a system that incorporates user body-pose estimation, gesture recognition and speech recognition for interaction in virtual reality environments. We describe a vision-based method for tracking the pose of a user in real time and introduce a technique that provides parameterized gesture recognition. More precisely, we train a support vector classifier to model the boundary of the space of possible gestures, and train Hidden Markov Models (HMM) on specific gestures. Given a sequence, we can find the start and end of various gestures using a support vector classifier, and find gesture likelihoods and parameters with a HMM. A multimodal recognition process is performed using rank-order fusion to merge speech and vision hypotheses. Finally we describe the use of our multimodal framework in a virtual world application that allows users to interact using gestures and speech.",
      "keywords": [
        "Support Vector Machine",
        "Hide Markov Model",
        "Virtual World",
        "Support Vector Machine Classifier",
        "Gesture Recognition"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "12 July 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-005-0155-3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-005-0155-3.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-005-0155-3"
    },
    {
      "title": "Workspace analysis for haptic feedback manipulator in virtual cockpit system",
      "authors": ["Shiyu Zhang", "Shuling Dai"],
      "abstract": "To obtain natural space experience of haptic interaction for users in virtual cockpit systems (VCS), a haptic feedback system and a workspace analysis framework for haptic feedback manipulator (HFM) are presented in this paper. Firstly, improving the classical three-dimensional workspace obtained by the Monte Carlo method, a novel workspace representation method, oriented workspace, is presented, which can indicate both the position and the orientation of the end-effector. Then, aimed at the characters of HFMs, the oriented workspace is divided into the effective workspace and the prohibited area by extracting the control panel area. At last, the effective workspace volume and the control panel area are calculated by the double-directed extremum method, with the accuracy improved by repeatedly adding and extracting boundary points. By simulation, the area in which interactions between the manipulator and users hand performed is determined and accordingly the effective workspace along with its boundary and volume are obtained in a relative high precision, which lay a basis for haptic interaction in VCS.",
      "keywords": [
        "Virtual cockpit system (VCS)",
        "Haptic feedback",
        "Workspace division",
        "The Monte Carlo method",
        "Boundary extraction",
        "Volume calculation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "30 January 2018",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-017-0327-y",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-017-0327-y.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-017-0327-y"
    },
    {
      "title": "A system for desktop conceptual 3D design",
      "authors": ["Ji-Young Oh", "Wolfgang Stuerzlinger"],
      "abstract": "In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configuration until they are satisfied with the general layout. In this activity, one of the main operations is the rearrangement of single and composite parts of a scene. With current desktop virtual reality (VR) systems, the selection and manipulation of arbitrary objects in 3D is still difficult. In this work, we present new and efficient techniques that allow even novice users to perform meaningful rearrangement tasks with traditional input devices. The results of our work show that the presented techniques can be mastered quickly and enable users to perform complex tasks on composite objects. Moreover, the system is easy to learn, supports creativity, and is fun to use.",
      "keywords": [
        "Conceptual 3D design",
        "Interactive 3D environment",
        "3D manipulation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "27 May 2004",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0128-y",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0128-y.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0128-y"
    },
    {
      "title": "A hybrid reality environment and its application to the study of earthquake engineering",
      "authors": [
        "Tara C. Hutchinson",
        "Falko Kuester",
        "Tung-Ju Hsieh",
        "Rebecca Chadwick"
      ],
      "abstract": "Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. In this paper, an interactive 2D and 3D (hybrid) environment is described, which facilitates collaborative learning and research and utilizes techniques in visualization and VR, therefore enhancing the interpretation of physical problems within these fields. The environment described, termed VizClass, incorporates a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. The environment is particularly appealing for studying critical, complex engineering problems, for example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed evaluation survey was also conducted to determine the merits of the presented environment and the techniques implemented. Results substantiate the plausibility of using these techniques for more general, everyday users. Over 70% of the survey participants believed that the techniques implemented were valuable for engineers.",
      "keywords": [
        "Virtual reality",
        "Visualization",
        "Multimodal interaction",
        "Human\u2013computer interaction",
        "Display technologies"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "08 October 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-005-0001-7",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-005-0001-7.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-005-0001-7"
    },
    {
      "title": "A Novel Seven Degree of Freedom Haptic Device for Engineering Design",
      "authors": ["S. Kim", "J. J. Berkley", "M. Sato"],
      "abstract": "In this paper, the authors intend to demonstrate a new intuitive force-feedback device that is ideally suited for engineering design. Force feedback for the device is tension-based and is characterised by 7 degrees of freedom (3\u2008DOF for translation, 3\u2008DOF for rotation, and 1\u2008DOF for grasp). The SPIDAR-G (SPace Interface Device for Artificial Reality with Grip) allows users to interact with virtual objects naturally by manipulating two hemispherical grips located in the centre of a device frame. Force feedback is achieved by controlling tension in cables that are connected between a grip and motors located at the corners of a frame. Methodologies will be discussed for displaying force and calculating translation, orientation and grasp using the length of 8 connecting cables. The SPIDAR-G is characterised by smooth force feedback, minimised inertia, no backlash, scalability and safety. Such features are attributed to strategic cable arrangement and control that results in stable haptic rendering. Experimental results validate the feasibility of the proposed device and example applications are described.",
      "keywords": ["CAD", "Force Feedback", "Haptics", "Virtual Design"],
      "published_in": "Virtual Reality",
      "publication_date": "August 2003",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-003-0105-x",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-003-0105-x.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-003-0105-x"
    },
    {
      "title": "MagicMeeting: A Collaborative Tangible Augmented Reality System",
      "authors": ["H.T. Regenbrecht", "M. Wagner", "G. Baratoff"],
      "abstract": "We describe an augmented reality (AR) system that allows multiple participants to interact with 2D and 3D data using tangible user interfaces. The system features face-to-face communication, collaborative viewing and manipulation of 3D models, and seamless access to 2D desktop applications within the shared 3D space. All virtual content, including 3D models and 2D desktop windows, is attached to tracked physical objects in order to leverage the efficiencies of natural two-handed manipulation. The presence of 2D desktop space within 3D facilitates data exchange between the two realms, enables control of 3D information by 2D applications, and generally increases productivity by providing access to familiar tools. We present a general concept for a collaborative tangible AR system, including a comprehensive set of interaction techniques, a distributed hardware setup, and a component-based software architecture that can be flexibly configured using XML. We show the validity of our concept with an implementation of an application scenario from the automotive industry.",
      "keywords": [
        "Augmented reality; Collaboration; CSCW; Tangible user interfaces; 3D user interfaces"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "October 2002",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s100550200016",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s100550200016.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s100550200016"
    },
    {
      "title": "The impact of haptic augmentation on middle school students\u2019 conceptions of the animal cell",
      "authors": [
        "James Minogue",
        "M. Gail Jones",
        "Bethany Broadwell",
        "Tom Oppewall"
      ],
      "abstract": "Of the five sensory channels\u2014sight, sound, taste, smell, and touch, it is only our sense of touch that enables us to modify and manipulate the world around us. This article reports the preliminary findings of a systematic study investigating the efficacy of adding haptic feedback to a desktop virtual reality program for use in middle school science instruction. Current technology allows for the simulation of tactile and kinesthetic sensations via sophisticated haptic devices and a computer interface. This research, conducted with 80 middle school students, examined the cognitive and affective impact of this technology on students\u2019 understandings of the structure and function of an animal cell. The results of this work offer valuable insights into the theoretical and practical considerations involved in the development and implementation of haptically augmented virtual reality instructional programs.",
      "keywords": ["Haptics", "Virtual reality", "Science instruction"],
      "published_in": "Virtual Reality",
      "publication_date": "26 September 2006",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0052-4",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0052-4.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0052-4"
    },
    {
      "title": "Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload",
      "authors": [
        "Lauriane Pouliquen-Lardy",
        "Isabelle Milleville-Pennel",
        "Fran\u00e7ois Guillaume",
        "Franck Mars"
      ],
      "abstract": "In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in spatial dialogs. Pairs of distant participants with specific roles (a guide and a manipulator) had to collaboratively move a virtual object in a plane factory mock-up. The displays allowed the participants to be immersed together in the virtual environment. We analyzed the dialogs that took place according to the frames of reference and the mental transformations required to produce the spatial statements. We also measured the associated mental workload. Results showed that when participants took a perspective, the manipulator\u2019s point of view was preferred. Perspective-taking only yielded a moderate increase in mental rotations, which may explain a specifically high mental demand score for the guides\u2019 NASA-TLX. Overall, this is in accordance with the least collaborative effort principle. This study reinforces the idea that, in collaboration, operators do not need the same aids as each other. Thus, it is not necessary to develop symmetrical tools, i.e., the same tools for all co-workers; instead, the needs of each operator should be taken into account, according to the task he has to perform. In our case, the guides would be helped with perspective-taking aids, while the manipulators would be helped with action-oriented tools.",
      "keywords": [
        "Remote collaboration",
        "Spatial common frame of reference",
        "Spatial cognition",
        "Virtual reality",
        "Mental workload"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "06 September 2016",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-016-0294-8",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-016-0294-8.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-016-0294-8"
    },
    {
      "title": "The use of force feedback and auditory cues for performance of an assembly task in an immersive virtual environment",
      "authors": [
        "Gregory W. Edwards",
        "Woodrow Barfield",
        "Maury A. Nussbaum"
      ],
      "abstract": "Using an immersive virtual environment, this study investigated whether the inclusion of force feedback or auditory cues improved manipulation performance and subjective reports of usability for an assembly task. Twenty-four volunteers (12 males and 12 females) were required to assemble and then disassemble five interconnecting virtual parts with either auditory, force, or no feedback cues provided. Performance for the assembly task was measured using completion time and number of collisions between parts, while the users\u2019 preferences across conditions were evaluated using subjective reports of usability. The results indicated that the addition of force feedback slowed completion time and led to more collisions between parts for males. In contrast, females exhibited no change in the mean completion time for the assembly task but did show an increase in collision counts. Despite these negative performance findings when adding force feedback, users did report perceived increases in realism, helpfulness and utility towards the assembly task when force feedback was provided. Unlike force feedback, the results showed that auditory feedback, indicating that parts had collided during the assembly task, had no negative performance effects on the objective measures while still increasing perceived realism and overall user satisfaction. When auditory cues and force feedback were presented together, performance times, number of collisions, and usability were not improved compared to conditions containing just auditory cues or force feedback alone. Based on these results, and given the task and display devices used in the present study, the less costly option of excluding auditory and force feedback cues would produce the best performance when measured by the number of collisions and completion time. However, if increased ratings of usability for an assembly task are desired while maintaining objective performance levels and reduced cost, then the inclusion of auditory feedback cues is best.",
      "keywords": [
        "Force Feedback",
        "Virtual environment",
        "Auditory cues",
        "Haptics",
        "Gender differences"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "25 March 2004",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0120-6",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0120-6.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0120-6"
    },
    {
      "title": "Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments",
      "authors": ["Wendy A. Schafer", "Doug A. Bowman"],
      "abstract": "Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange furniture together or exchange directions with one another. Collaborative virtual environments using desktop PCs are particularly useful for spatial activities when the participants are distributed. This work investigates ways to enhance distributed, collaborative spatial activities. This paper explores how different frames of reference affect spatial collaboration. Specifically, it reports on an experiment that examines different combinations of exocentric and egocentric frames of reference with two users. Tasks involve manipulating an object, where one participant knows the objective (director) and the other performs the interactions (actor). It discusses the advantages and disadvantages of the different combinations for a spatial collaboration task. Findings from this study demonstrate that frames of reference affect collaboration in a variety of ways and simple exocentric-egocentric combinations do not always provide the most usable solution.",
      "keywords": [
        "Awareness",
        "Collaborative virtual environment (CVE)",
        "Multiple perspectives"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "04 May 2004",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0123-3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0123-3.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0123-3"
    },
    {
      "title": "Touch-enabled haptic modeling of deformable multi-resolution surfaces",
      "authors": ["Hanqiu Sun", "Huawei Wang", "Hui Chen", "Kaihuai Qin"],
      "abstract": "Currently, interactive data exploration in virtual environments is mainly focused on vision-based and non-contact sensory channels such as visual/auditory displays. The lack of tactile sensation in virtual environments removes an important source of information to be delivered to the users. In this paper, we propose the touch-enabled haptic modeling of deformable multi-resolution surfaces in real time. The 6-DOF haptic manipulation is based on a dynamic model of Loop surfaces, where the dynamic parameters are computed easily without subdividing the control mesh recursively. A local deforming scheme is developed to approximate the solution of the dynamics equations, thus the order of the linear equations is reduced greatly. During each of the haptic interaction loop, the contact point is traced and reflected to the rendering of updated graphics and haptics. The sense of touch against the deforming surface is calculated according to the surface properties and the damping-spring force profile. Our haptic system supports the dynamic modeling of deformable Loop surfaces intuitively through the touch-enabled interactive manipulation.",
      "keywords": [
        "Haptics interface",
        "Physics-based modeling",
        "Subdivision surfaces"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "10 January 2007",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0065-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0065-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0065-z"
    },
    {
      "title": "Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments",
      "authors": [
        "Amine Chellali",
        "Isabelle Milleville-Pennel",
        "C\u00e9dric Dumas"
      ],
      "abstract": "Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, users must develop a common representation of their shared virtual space. In this work, we investigated spatial communication in virtual environments. In order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. We conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. Forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. The participants were asked to perform in dyads an object co-manipulation task. The results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. These results allow us to provide some design recommendations for CVEs for object manipulation tasks.",
      "keywords": [
        "Spatial communication",
        "Virtual environment",
        "Collaboration",
        "Common frame of reference",
        "Visual landmarks"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "12 September 2012",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-012-0214-5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-012-0214-5.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-012-0214-5"
    },
    {
      "title": "Efficient virtual reality design of quiet underwater shells",
      "authors": ["W. Akl", "A. Baz"],
      "abstract": "AbstractEfficient computational tools are developed to model, visualize, and feel the structural-acoustics of shells in a virtual reality environment. These tools aim at building the structural-acoustic models of shells from an array of basic building blocks including: beams, shells, and stiffeners. The concepts of finite element analysis, sub-structuring, model reduction, meta-modeling, and parallel computations form the main steps to be followed for building simplified computational models of complex shell systems. The resulting models are particularly suitable for the efficient application of multi-criteria optimization techniques in order to select the optimal design parameters of these complex shell systems. The developed integrated analysis tools enable the engineers to design complex systems in a cost effective and a timely manner. Furthermore, engineers will be immersed in an audio-visually coupled tele-operated environment whereby direct interaction and control of the design process can be achieved. In this manner, the behavior of synthetic models of shells can be monitored by literally walking through the shell and adjusting its design parameters as needed to ensure optimal performance while satisfying design and operational requirements. For example, engineers can move electronic wands to vary the number, size, type, and location of stiffeners in the shell, monitor the resulting structural-acoustic visually or by haptic feedback and simultaneously listen to the radiated sound pressure field. Such manipulations of the virtual shells in the scene are carried out while the engineer is navigating through and around the shell to ensure that the vibration and sound levels, at any critical locations, are within the acceptable limits. The developed integrated approach also serves as a means for virtual training of students and engineers on designing and operating complex smart structures on the site as well as through collaborative efforts with other virtual reality sites. Such unique capability will enable engineers to design prototypes of expensive vehicles without building them. Examples of these vehicles include aircraft, submersibles, torpedoes, and others that can share this virtual experience and can be profoundly impacted upon by the proposed approach. The presented optimal design approach is implemented in the Virtual Reality CAVE Laboratory at the University of Maryland that is controlled by an eight parallel processor Silicon Graphics Infinite Reality (ONYX2) computer.",
      "keywords": [
        "Finite element analysis",
        "Sub-structural analysis",
        "Reduced order models",
        "Meta-models",
        "Multi-criteria design optimization",
        "Design in virtual reality environment"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "30 September 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-005-0004-4",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-005-0004-4.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-005-0004-4"
    },
    {
      "title": "A usability study of multimodal input in an augmented reality environment",
      "authors": [
        "Minkyung Lee",
        "Mark Billinghurst",
        "Woonhyuk Baek",
        "Richard Green",
        "Woontack Woo"
      ],
      "abstract": "In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech input in a natural way using a multimodal fusion architecture. We describe the system architecture and present a study exploring the usability of the AR MMI compared with speech-only and 3D-hand-gesture-only interaction conditions. The interface was used in an AR application for selecting 3D virtual objects and changing their shape and color. For each interface condition, we measured task completion time, the number of user and system errors, and user satisfactions. We found that the MMI was more usable than the gesture-only interface conditions, and users felt that the MMI was more satisfying to use than the speech-only interface conditions; however, it was neither more effective nor more efficient than the speech-only interface. We discuss the implications of this research for designing AR MMI and outline directions for future work. The findings could also be used to help develop MMIs for a wider range of AR applications, for example, in AR navigation tasks, mobile AR interfaces, or AR game applications.",
      "keywords": [
        "Multimodal interface",
        "Augmented reality",
        "Usability",
        "Efficiency",
        "Effectiveness",
        "Satisfaction"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "21 September 2013",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-013-0230-0",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-013-0230-0.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-013-0230-0"
    },
    {
      "title": "Carpeno: interfacing remote collaborative virtual environments with table-top interaction",
      "authors": [
        "Holger Regenbrecht",
        "Michael Haller",
        "Joerg Hauber",
        "Mark Billinghurst"
      ],
      "abstract": "Creativity is enhanced by communication and collaboration. Thus, the increasing number of distributed creative tasks requires better support from computer-mediated communication and collaborative tools. In this paper we introduce \u201cCarpeno\u201d, a new system for facilitating intuitive face-to-face and remote collaboration on creative tasks. Normally the most popular and efficient way for people to collaborate is face-to-face, sitting around a table. Computer augmented surface environments, in particular interactive table-top environments, are increasingly used to support face-to-face meetings. They help co-located teams to develop new ideas by facilitating the presentation, manipulation, and exchange of shared digital documents displayed on the table-top surface. Users can see each other at the same time as the information they are talking about. In this way the task space and communication space can be brought together in a more natural and intuitive way. The discussion of digital content is redirected from a computer screen, back to a table where people can gather around. In contrast, collaborative virtual environments (CVE) are used to support remote collaboration. They frequently create familiar discussion scenarios for remote interlocutors by utilizing room metaphors. Here, virtual avatars and table metaphors are used, where the participants can get together and communicate with each other in a way that allows behaviour that is as close to face-to-face collaboration as possible. The Carpeno system described here combines table-top interaction with a CVE to support intuitive face-to-face and remote collaboration. This allows for simultaneous co-located and remote collaboration around a common, interactive table.",
      "keywords": [
        "Collaborative work",
        "CSCW",
        "Virtual environments",
        "Table-top interfaces",
        "Teleconferencing"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "07 September 2006",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0045-3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0045-3.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0045-3"
    },
    {
      "title": "A computational model of perceptual saliency for 3D objects in virtual environments",
      "authors": [
        "Graciela Lara",
        "Ang\u00e9lica De Antonio",
        "Adriana Pe\u00f1a"
      ],
      "abstract": "When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful for locating a target object within a virtual environment (VE), a computational model to identify perceptual saliency is presented. Based on the object\u2019s features with the major stimulus for the human visual system, three basic features of a 3D object (i.e., color, size, and shape) are individually evaluated and then combined to get a degree of saliency for each 3D object in a virtual scenario. An experiment was conducted to evaluate the extent to which the proposed measure of saliency matches with the people\u2019s subjective perception of saliency; the results showed a good performance of this computational model.",
      "keywords": [
        "Reference object",
        "Perceptual salience",
        "Virtual environment",
        "3D object\u2019s features extraction"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "06 October 2017",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-017-0326-z",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-017-0326-z.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-017-0326-z"
    },
    {
      "title": "A cross-cultural comparison of salient perceptual characteristics of height channels for a virtual auditory environment",
      "authors": ["Sungyoung Kim", "Richard King", "Toru Kamekawa"],
      "abstract": "Perceptual characteristics of virtual auditory environments from three listener groups were compared. To generate convincing and pleasing virtual auditory environments, acoustic impulse responses were measured in two venues using an innovative microphone array and convolved with two anechoic recordings. Subsequently, the convolved sound sources were assigned to loudspeakers (five horizontal channels and four height channels), and inter-channel level balances were optimized. The authors conducted a controlled listening test with two variables: height-channel configurations (eight conditions) and stimuli (four conditions\u2014two musical selections times and two target venues) to determine the influence of (1) two control variables on the perceived appropriateness of virtual auditory environments and (2) the cultural background of three listener groups composed of participants from Canada (group\u00a01, 11 subjects), the USA (group\u00a02, 12 subjects), and Japan (group\u00a03, 14 subjects). The data analysis revealed that the configuration variable (the height position of the loudspeakers) has a greater influence on perceived appropriateness than the stimulus variable for all three groups. In addition, the results showed that although group 1 data had a similar listening response pattern to group\u00a02, the response of group\u00a03 was different. A subsequent analysis of reported descriptors found that groups\u00a01 and 2 chose height configurations that generated a \u201cfrontal\u201d and \u201cnarrow\u201d impression as a more appropriate virtual auditory environment, while group\u00a03 chose the same characteristics but as a less appropriate environment. Groups\u00a01 and 2 also described a less appropriate auditory environment with \u201cwide, spacious, and surrounding\u201d images that again were described by group\u00a03 as more appropriate. While room acoustics and loudspeaker size also contributed to the overall modulation of listeners\u2019 judgment, the findings support the idea that cultural background affects perceptual responses to spatial sound and is therefore important in rendering a homogeneous experience of a virtual auditory environment for listeners in remote spaces.",
      "keywords": [
        "Height-channel perception",
        "Multichannel-reproduced virtual auditory environment",
        "Cross-cultural comparison"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "12 August 2015",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-015-0269-1",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-015-0269-1.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-015-0269-1"
    },
    {
      "title": "Factors affecting user performance in haptic assembly",
      "authors": [
        "T. Lim",
        "J. M. Ritchie",
        "R. G. Dewar",
        "J. R. Corney",
        "P. Wilkinson",
        "M. Calis",
        "M. Desmulliez",
        "J.-J. Fang"
      ],
      "abstract": "Current computer-aided assembly systems provide engineers with a variety of spatial snapping and alignment techniques for interactively defining the positions and attachments of components. With the advent of haptics and its integration into virtual assembly systems, users now have the potential advantage of tactile information. This paper reports research that aims to quantify how the provision of haptic feedback in an assembly system can affect user performance. To investigate human\u2013computer interaction processes in assembly modeling, performance of a peg-in-hole manipulation was studied to determine the extent to which haptics and stereovision may impact on task completion time. The results support two important conclusions: first, it is apparent that small (i.e. visually insignificant) assembly features (e.g. chamfers) affect the overall task completion at times only when haptic feedback is provided; and second, that the difference is approximately similar to the values reported for equivalent real world peg-in-hole assembly tasks.",
      "keywords": [
        "Haptic assembly",
        "Human\u2013computer Interaction",
        "Human factors",
        "Design for assembly",
        "Peg-in-hole metrics",
        "Virtual reality"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "24 April 2007",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-007-0072-8",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-007-0072-8.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-007-0072-8"
    },
    {
      "title": "Cognitive ability and information retrieval: When less is more",
      "authors": ["S. J. Westerman", "T. Cribbin"],
      "abstract": "This paper is concerned with the use of virtual environments as a means of conveying semantic information relating to the contents of computerised textual databases. two empirical studies are reported that investigated the influence of individual differences in cognitive ability on search task performance. In the first experiment, objects (each representing a type of animal) were placed ordinally in a three-dimensional cube arrangement based on ratings of semantic similarity. Participants were required to locate a series of randomly selected objects. Contrary to prediction, participants with high associative memory were comparatively poorer performers. In a second experiment \u2018true\u2019 rating distances were used to locate objects in virtual space. High spatial ability was associated with better performance and, in contrast with the results of Experiment 1, this pattern also was replicated for associative memory. Implications are discussed.",
      "keywords": [
        "Individual Differences",
        "Information Retrieval",
        "Visualisation"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "March 2000",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/BF01418971",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/BF01418971.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/BF01418971"
    },
    {
      "title": "Conceptualising mixed spaces of interaction for designing continuous interaction",
      "authors": [
        "Daniela Gorski Trevisan",
        "Jean Vanderdonckt",
        "Beno\u00eet Macq"
      ],
      "abstract": "Recent progress in the overlay and registration of digital information on the user\u2019s workspace in a spatially meaningful way has allowed mixed reality (MR) to become a more effective operational medium. However, research in software structures, design methods and design support tools for MR systems is still in its infancy. In this paper, we propose a conceptual classification of the design space to support the development of MR systems. The proposed design space (DeSMiR) is an abstract tool for systematically exploring several design alternatives at an early stage of interaction design, without being biassed towards a particular modality or technology. Once the abstract design possibilities have been identified and a concrete design decision has been taken (i.e. a specific modality has been selected), a concrete MR application can be considered in order to analyse the interaction techniques in terms of continuous interaction properties. We suggest that our design space can be applied to the design of several kinds of MR applications, especially those in which very little user focus distraction can be tolerated, and where smooth connections and interactions between real and virtual worlds is critical for the system development. An image-guided surgery system (IGS) is used as a case study.",
      "keywords": [
        "Design space",
        "Mixed reality",
        "Continuous interaction",
        "Image-guided surgery"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "14 January 2005",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-004-0140-2",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-004-0140-2.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-004-0140-2"
    },
    {
      "title": "Can\u2019t touch this: the impact of augmented reality versus touch and non-touch interfaces on perceived ownership",
      "authors": ["Malaika Brengman", "Kim Willems", "Helena Van Kerrebroeck"],
      "abstract": "The rise of augmented reality (AR) technology presents e-retailers with new opportunities. According to previous research, it is a technology that can positively affect engagement, brand recall and purchase confidence. Mobile-enabled augmented reality differs from regular mobile phone use as the technology virtually overlays images or information to the real environment. As the use of a touch screen device (i.e. smartphone vs. laptop) has previously been found to positively affect feelings of perceived ownership, the current study examines whether the possibility to virtually manipulate a product on a mobile AR application would have an even stronger effect. This is examined for products with either material properties (i.e. products that require the examination of sensory information) or geometric properties (i.e. products that can be examined via written and/or visual information). The findings reveal that AR does indeed result in higher levels of perceived ownership, particularly in case of material products.",
      "keywords": [
        "Augmented reality",
        "Perceived ownership",
        "Touch",
        "Virtual product interaction",
        "Mobile commerce"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "15 February 2018",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-018-0335-6",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-018-0335-6.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-018-0335-6"
    },
    {
      "title": "Aerial full spherical HDR imaging and display",
      "authors": ["Fumio Okura", "Masayuki Kanbara", "Naokazu Yokoya"],
      "abstract": "This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic superimposition of virtual objects using image-based lighting. We propose a complete and practical system to acquire full spherical HDR images from the sky, using two omnidirectional cameras mounted above and below an unmanned aircraft. The HDR images are generated by combining multiple omnidirectional images captured with different exposures controlled automatically. Our system consists of methods for image completion, alignment, and color correction, as well as a novel approach for automatic exposure control, which selects optimal exposure so as to avoid banding artifacts. Experimental results indicated that our system generated better spherical images compared to an ordinary spherical image completion system in terms of naturalness and accuracy. In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed system. The experiment demonstrated HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses of HDR images.",
      "keywords": [
        "Omnidirectional camera",
        "High dynamic range image",
        "Immersive panorama",
        "Image-based lighting",
        "Tone-mapping"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "27 August 2014",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-014-0249-x",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-014-0249-x.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-014-0249-x"
    },
    {
      "title": "NAVIG: augmented reality guidance system for the visually impaired",
      "authors": [
        "Brian F. G. Katz",
        "Slim Kammoun",
        "Ga\u00ebtan Parseihian",
        "Olivier Gutierrez",
        "Adrien Brilhault",
        "Malika Auvray",
        "Philippe Truillet",
        "Michel Denis",
        "Simon Thorpe",
        "Christophe Jouffrais"
      ],
      "abstract": "Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.",
      "keywords": [
        "Assisted navigation",
        "Guidance",
        "Spatial audio",
        "Visually impaired assistive device",
        "Need analysis"
      ],
      "published_in": "Virtual Reality",
      "publication_date": "12 June 2012",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-012-0213-6",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-012-0213-6.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-012-0213-6"
    },
    {
      "title": "Analysis of expression in simple musical gestures to enhance audio in interfaces",
      "authors": ["Luca Mion", "Gianluca D\u2019Inc\u00e0"],
      "abstract": "Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers\u2019 strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human\u2013computer interfaces.",
      "keywords": ["Expression", "Audio interfaces", "Sonification"],
      "published_in": "Virtual Reality",
      "publication_date": "03 May 2006",
      "citations": "",
      "isbn": "",
      "doi": "10.1007/s10055-006-0029-3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10055-006-0029-3.pdf",
      "paper_url": "https://link.springer.com/article/10.1007/s10055-006-0029-3"
    }
  ],
  "total_results": 415,
  "total_filtered_results": 59,
  "total_pages": 21
}
