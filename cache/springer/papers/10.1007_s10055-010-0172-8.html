<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Grasp programming by demonstration in virtual reality with automatic e"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Grasp programming by demonstration in virtual reality with automatic environment reconstruction"/>

    <meta name="dc.source" content="Virtual Reality 2010 16:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-11-20"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-11-20"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="87"/>

    <meta name="prism.endingPage" content="104"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0172-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0172-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0172-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0172-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Grasp programming by demonstration in virtual reality with automatic environment reconstruction"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2012/06"/>

    <meta name="citation_online_date" content="2010/11/20"/>

    <meta name="citation_firstpage" content="87"/>

    <meta name="citation_lastpage" content="104"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0172-8"/>

    <meta name="DOI" content="10.1007/s10055-010-0172-8"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0172-8"/>

    <meta name="description" content="A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD)"/>

    <meta name="dc.creator" content="Jacopo Aleotti"/>

    <meta name="dc.creator" content="Stefano Caselli"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Aleotti J, Caselli S (2005) Trajectory clustering and stochastic approximation for robot programming by demonstration. In: IEEE international conference on intelligent robots and systems, (IROS) Edmonton, Canada"/>

    <meta name="citation_reference" content="Aleotti J, Caselli S (2006) Grasp recognition in virtual reality for Robot Pregrasp planning by demonstration. In: IEEE International conference on robotics and automation, (ICRA), Orlando, USA"/>

    <meta name="citation_reference" content="Aleotti J, Caselli S (2007) Robot grasp synthesis from virtual demonstration and topology-preserving environment reconstruction. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) San Diego, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Rob Auton Syst; citation_title=Leveraging on a virtual environment for robot programming by demonstration; citation_author=J Aleotti, S Caselli, M Reggiani; citation_volume=47; citation_issue=2&#8211;3; citation_publication_date=2004; citation_pages=153-161; citation_doi=10.1016/j.robot.2004.03.009; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res Suppl; citation_title=Coordinated control programs for control of the hands. Hand function and the neocortex; citation_author=MA Arbib, T Iberall, D Lyons; citation_volume=10; citation_publication_date=1985; citation_pages=111-29; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot; citation_title=A sensor fusion approach for recognizing continuous human grasping sequences using hidden Markov models; citation_author=K Bernardin, K Ogawara, K Ikeuchi, R Dillmann; citation_volume=21; citation_issue=1; citation_publication_date=2005; citation_pages=47-57; citation_doi=10.1109/TRO.2004.833816; citation_id=CR6"/>

    <meta name="citation_reference" content="Calinon S, Billard A (2004) Stochastic gesture production and recognition model for a humanoid robot. In: IEEE/RSJ Intl conference on intelligent robots and systems (IROS), pp 2769&#8211;2744, Sendai, Japan"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=On grasp choice, grasp models, and the design of hands for manufacturing tasks; citation_author=MR Cutkosky; citation_volume=5; citation_issue=3; citation_publication_date=1989; citation_pages=269-279; citation_doi=10.1109/70.34763; citation_id=CR8"/>

    <meta name="citation_reference" content="Cutkosky MR, Howe RD (1990) Human grasp choice and robotic grasp analysis. Dextrous Robot Hands, chap 1, pp 111&#8211;29. Springer"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Feature extraction from boundary models of three-dimensional objects; citation_author=L Floriani; citation_volume=11; citation_issue=8; citation_publication_date=1989; citation_pages=785-798; citation_id=CR14"/>

    <meta name="citation_reference" content="Ehrennmann M, Ambela D, Steinaus P, Dillmann R (2000) A comparison of four fast vision based object recognition methods for programming by demonstration applications. In: IEEE international conference on robotics and automation, (ICRA), San Francisco, USA"/>

    <meta name="citation_reference" content="Ekvall S, Kragi&#263; D (2004) Interactive grasp learning based on human demonstration. In IEEE international conference on robotics and automation, (ICRA) New Orleans, USA"/>

    <meta name="citation_reference" content="Ekvall S, Kragi&#263; D (2005) Grasp recognition for programming by demonstration. In: IEEE Intl conference on robotics and automation, (ICRA) Barcelona, Spain"/>

    <meta name="citation_reference" content="Ferrari C, Canny J (1992) Planning optimal grasps. In: IEEE international conference on robotics and automation, (ICRA) pp 2290&#8211;2295, Nice, France"/>

    <meta name="citation_reference" content="Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary operators: grasp classification using neural network classifiers. In: IASTED international conference on intelligent systems and control, Santa Barbara, USA"/>

    <meta name="citation_reference" content="Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of classification methods. In: IEEE virtual reality, pp 19&#8211;26, Charlotte, USA"/>

    <meta name="citation_reference" content="Hirano Y, Kitahama K, Yoshizawa S (2005) Image-based object recognition and dexterous hand/arm motion planning using RRTs for grasping in cluttered scene. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)"/>

    <meta name="citation_reference" content="Hsiao K, Lozano-P&#233;rez T (2006) Imitation learning of whole-body grasps. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China"/>

    <meta name="citation_reference" content="Iberall T (1987) The nature of human prehension: Three dextrous hands in one. In: IEEE international conference on robotics and automation, (ICRA) pp 396&#8211;401"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Toward an assembly plan from observation, Part I: task recognition with polyhedral objects; citation_author=K Ikeuchi, T Suehiro; citation_volume=10; citation_issue=3; citation_publication_date=1994; citation_pages=368-385; citation_doi=10.1109/70.294211; citation_id=CR20"/>

    <meta name="citation_reference" content="Jang H, Moradi H, Lee S, Han J (2005) A visibility-based accessibility analysis of the grasp points for real-time manipulation. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Toward automatic robot instruction from perception-mapping human grasps to manipulator grasps; citation_author=SB Kang, K Ikeuchi; citation_volume=13; citation_issue=1; citation_publication_date=1997; citation_pages=81-95; citation_doi=10.1109/70.554349; citation_id=CR22"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality (IWAR)"/>

    <meta name="citation_reference" content="Kitahama K, Tsukada A, Galpin F, Matsubara T, Hirano Y (2006) Vision-based scene representation for 3D interaction of service robots. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China"/>

    <meta name="citation_reference" content="Lee S, Jang D, Kim E, Hong S, Han J (2005) A real-time 3D workspace modeling with stereo camera. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Programming contact tasks using a reality-based virtual environment integrated with vision; citation_author=E Lloyd, JS Beis, DK Pai, DG Lowe; citation_volume=15; citation_issue=3; citation_publication_date=1999; citation_pages=423-434; citation_doi=10.1109/70.768176; citation_id=CR26"/>

    <meta name="citation_reference" content="Matas J, Soh LM, Kittler J (1997) Object recognition using a tag. In: International conference on image processing"/>

    <meta name="citation_reference" content="Miller AT, Allen PK (2000) Graspit!: a versatile simulator for grasp analysis. In: ASME international mechanical engineering congress, pp 1251&#8211;1258, Orlando, USA"/>

    <meta name="citation_reference" content="Morales A, Asfour T, Azad P, Knoop S, Dillmann R (2006) Integrated grasp planning and visual object localization for a humanoid robot with five-fingered hands. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS), Beijing, China"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Robotic assembly operation teaching in a virtual environment; citation_author=H Ogata, T Takahashi; citation_volume=10; citation_issue=3; citation_publication_date=1994; citation_pages=391-399; citation_doi=10.1109/70.294213; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=On NURBS: a survey; citation_author=L Piegl; citation_volume=11; citation_issue=1; citation_publication_date=1991; citation_pages=55-71; citation_doi=10.1109/38.67702; citation_id=CR31"/>

    <meta name="citation_reference" content="Takahashi T, Sakai T (1991) Teaching robot&#8217;s movement in virtual reality. In: IEEE/RSJ international workshop on intelligent robots and systems, (IROS)"/>

    <meta name="citation_reference" content="citation_title=Dextrous robot hands; citation_publication_date=1990; citation_id=CR33; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Wojtara T, Nonami K (2004) Hand posture detection by neural network and grasp mapping for a master slave hand system. In: IEEE/RSJ international conference on intelligent robots and systems (IROS) pp 866&#8211;871, Sendai, Japan"/>

    <meta name="citation_reference" content="Wong AKC, Rong L, Liang X (1998) Robotic vision: 3D object recognition and pose determination. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Victoria, Canada"/>

    <meta name="citation_reference" content="Z&#246;llner R, Rogalla O, Dillmann R (2001) Integration of tactile sensors in a programming by demonstration system. In: IEEE international conference on robotics and automation, (ICRA)"/>

    <meta name="citation_reference" content="Z&#246;llner R, Rogalla O, Dillmann R, Z&#246;llner M (2002) Understanding users intention: programming fine manipulation tasks by demonstration. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)"/>

    <meta name="citation_author" content="Jacopo Aleotti"/>

    <meta name="citation_author_email" content="aleotti@ce.unipr.it"/>

    <meta name="citation_author_institution" content="Dipartimento di Ingegneria dell&#8217;Informazione, University of Parma, Parma, Italy"/>

    <meta name="citation_author" content="Stefano Caselli"/>

    <meta name="citation_author_email" content="caselli@ce.unipr.it"/>

    <meta name="citation_author_institution" content="Dipartimento di Ingegneria dell&#8217;Informazione, University of Parma, Parma, Italy"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0172-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0172-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Grasp programming by demonstration in virtual reality with automatic environment reconstruction"/>
        <meta property="og:description" content="A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Grasp programming by demonstration in virtual reality with automatic environment reconstruction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0172-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Environment modeling, Grasp programming, Glove interaction","kwrd":["Virtual_reality","Environment_modeling","Grasp_programming","Glove_interaction"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0172-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0172-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=172;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0172-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Grasp programming by demonstration in virtual reality with automatic environment reconstruction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0172-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0172-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-11-20" itemprop="datePublished">20 November 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Grasp programming by demonstration in virtual reality with automatic environment reconstruction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jacopo-Aleotti" data-author-popup="auth-Jacopo-Aleotti" data-corresp-id="c1">Jacopo Aleotti<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Parma" /><meta itemprop="address" content="grid.10383.39, 0000000417580937, Dipartimento di Ingegneria dell’Informazione, University of Parma, Parma, Italy" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stefano-Caselli" data-author-popup="auth-Stefano-Caselli">Stefano Caselli</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Parma" /><meta itemprop="address" content="grid.10383.39, 0000000417580937, Dipartimento di Ingegneria dell’Informazione, University of Parma, Parma, Italy" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">87</span>–<span itemprop="pageEnd">104</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">457 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">7 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0172-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Programming by demonstration (PbD) (Takahashi and Sakai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Takahashi T, Sakai T (1991) Teaching robot’s movement in virtual reality. In: IEEE/RSJ international workshop on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR32" id="ref-link-section-d4478e296">1991</a>; Ogata and Takahashi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ogata H, Takahashi T (1994) Robotic assembly operation teaching in a virtual environment. IEEE Trans Robot Autom 10(3):391–399" href="/article/10.1007/s10055-010-0172-8#ref-CR30" id="ref-link-section-d4478e299">1994</a>; Ikeuchi and Suehiro <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385" href="/article/10.1007/s10055-010-0172-8#ref-CR20" id="ref-link-section-d4478e302">1994</a>) is a form of human–robot interaction intended to reduce the complexity incurred in programming robot applications by letting the user teach new tasks to the robot. Traditional techniques for robot programming are based on handheld teach pendants (portable devices including a display and a keypad to impart motions and store robot configurations) and textual programming. However, using these techniques is often difficult for unexperienced or unskilled users. Hence, PbD provides user-friendly interfaces and intuitive strategies for robot programming by letting the user act as a teacher and the robot act as a learner. Since humans are familiar with teaching and demonstration concepts, the complexity of task programming is essentially charged to the robot system itself. PbD systems can be classified into two main categories depending on the way demonstration is carried out. The most general way relies on one or more demonstrations performed in a physical environment, either the same where the task must be carried out by the robot or a similar one, e.g. (Ikeuchi and Suehiro <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385" href="/article/10.1007/s10055-010-0172-8#ref-CR20" id="ref-link-section-d4478e305">1994</a>; Zöllner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Zöllner R, Rogalla O, Dillmann R, Zöllner M (2002) Understanding users intention: programming fine manipulation tasks by demonstration. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR37" id="ref-link-section-d4478e308">2002</a>; Calinon and Billard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Calinon S, Billard A (2004) Stochastic gesture production and recognition model for a humanoid robot. In: IEEE/RSJ Intl conference on intelligent robots and systems (IROS), pp 2769–2744, Sendai, Japan" href="/article/10.1007/s10055-010-0172-8#ref-CR7" id="ref-link-section-d4478e312">2004</a>). An alternative approach involves performing the demonstration in a virtual environment (VE) reproducing with some fidelity the target environment (Takahashi and Sakai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Takahashi T, Sakai T (1991) Teaching robot’s movement in virtual reality. In: IEEE/RSJ international workshop on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR32" id="ref-link-section-d4478e315">1991</a>; Ogata and Takahashi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ogata H, Takahashi T (1994) Robotic assembly operation teaching in a virtual environment. IEEE Trans Robot Autom 10(3):391–399" href="/article/10.1007/s10055-010-0172-8#ref-CR30" id="ref-link-section-d4478e318">1994</a>; Lloyd et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lloyd E, Beis JS, Pai DK, Lowe DG (1999) Programming contact tasks using a reality-based virtual environment integrated with vision. IEEE Trans Robot Autom 15(3):423–434" href="/article/10.1007/s10055-010-0172-8#ref-CR26" id="ref-link-section-d4478e321">1999</a>; Aleotti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Aleotti J, Caselli S, Reggiani M (2004) Leveraging on a virtual environment for robot programming by demonstration. Rob Auton Syst 47(2–3):153–161" href="/article/10.1007/s10055-010-0172-8#ref-CR4" id="ref-link-section-d4478e324">2004</a>). When applicable, the latter approach avoids disrupting actual robot usage and provides several functional advantages. Indeed, tracking of user actions and computation of object pose (both automatically performed by the system) are easier within a simulated environment than in a real one. The required physical environment may be unavailable for teaching. Moreover, the virtual environment can be augmented with operator aids (sensory information called virtual fixtures) that help the user while demonstrating the task.</p><p>In this work, a virtual reality-based PbD system for programming robotic manipulation tasks is presented. The system supports essential manipulation skills like automatic grasp recognition and grasp planning. The novelty of the system stems mainly from its end-to-end reliance upon a virtual environment. The system learns and classifies human grasps off-line by observing a number of grasps performed on exemplar objects in the VE. Likewise, the system learns the grasps afforded by a robotic end-effector during an interactive session where a 3D model of the end-effector is operated in the VE by an expert instructor. When a task must be programmed, the system first automatically builds a VE matching the real one. VE construction is done by acquiring images of the scene from one or more viewpoints using a monocular camera and generating a topology-preserving 3D representation. Then, task demonstration occurs in the VE and may be repeated by the user one or more times based on convenience and effectiveness. During this phase, the PbD system observes and analyzes human motions and grasping operations. Analysis entails segmentation of the demonstrated task, automatic recognition and classification of the grasping operations, and recording of the human hand and objects motions which have occurred in the scene. Next, the system synthesizes a robotic task based on the human demonstration. This phase involves filtering noisy input data and fitting trajectories and grasping data to the robotic manipulation system, taking into account its kinematic limitations and grasping capabilities. The synthesized task can be simulated in 3D in the VE for safety check, confirmation and refinement and is available for execution on the real robotic system where appropriate.</p><p>The paper is organized as follows. The rest of this section outlines the main research contributions of the paper and illustrates the architecture of the virtual reality-based PbD system. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec2">2</a> reviews the state of the art concerning grasp classification, robot grasp synthesis and environment modeling. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec4">3</a> briefly illustrates the components and technologies of the PbD system. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec5">4</a> illustrates the problem of automatic grasp recognition in virtual reality and the proposed algorithm for grasp classification. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec8">5</a> describes the automatic approach for workspace modeling, while Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec9">6</a> describes the grasp synthesis phase and shows the results obtained in simulation. The paper closes in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec13">7</a> summarizing the work.</p><h3 class="c-article__sub-heading" id="Sec2">Research contributions</h3><p>This paper describes a unique desktop virtual reality system that integrates human grasp recognition, automated environment modeling and robot grasp planning. Even though the overall PbD system incorporates a number of components (Aleotti and Caselli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Aleotti J, Caselli S (2005) Trajectory clustering and stochastic approximation for robot programming by demonstration. In: IEEE international conference on intelligent robots and systems, (IROS) Edmonton, Canada" href="/article/10.1007/s10055-010-0172-8#ref-CR1" id="ref-link-section-d4478e358">2005</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Aleotti J, Caselli S (2006) Grasp recognition in virtual reality for Robot Pregrasp planning by demonstration. In: IEEE International conference on robotics and automation, (ICRA), Orlando, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR2" id="ref-link-section-d4478e361">2006</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Aleotti J, Caselli S (2007) Robot grasp synthesis from virtual demonstration and topology-preserving environment reconstruction. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) San Diego, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR3" id="ref-link-section-d4478e364">2007</a>), in this paper we focus on the three aforementioned research contributions.</p><p>The first contribution is the proposal of a technique for classification of anthropomorphic grasps in virtual reality. Previous research in grasp classification has never addressed the problem of grasp recognition in a virtual environment, as will be pointed out in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec2">2</a>. The approach takes advantage from the fact that virtual grasping provides useful information about the contact points and the contact normals, which can be exploited for grasp classification and segmentation of user’s actions. The same information cannot be easily measured if grasping is performed in a real environment. While providing significant advantages in terms of contact information, automatic grasp recognition in desktop virtual reality raises some problems that do not occur in a real environment. Grasping interaction may be influenced by a limited view of the scene, as the system does not offer full immersion. Moreover, tactile feedback for anthropomorphic grasping is usually limited due to the cost of high-end devices. Indeed, in this work the user is assumed to receive only a visual feedback from the environment. Furthermore, a training session is required for the user to get acquainted with the system and to achieve an adequate rate of correct grasp classifications. The grasp classification algorithm has been experimentally evaluated both by skilled operators and by a group of untrained users, novices to the VE setup.</p><p>A second contribution of the paper is the proposal of a workspace modeling technique that preserves the topology of the environment by computing adjacency relations between the objects in the scene. Workspace modeling is a crucial component for automating the construction of a virtual environment suitable for 3D interaction. As pointed out in (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lee S, Jang D, Kim E, Hong S, Han J (2005) A real-time 3D workspace modeling with stereo camera. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR25" id="ref-link-section-d4478e376">2005</a>), the problem of environment reconstruction has often been considered in the context of map building for robot navigation, whereas it has received less attention for manipulation tasks, where it demands even more accuracy. Moreover, fast response time and stability are important requirements for environment modeling. In particular, object recognition must be stable in spite of cluttered environments, rotations, scaling transformations and partial occlusions (Ehrennmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Ehrennmann M, Ambela D, Steinaus P, Dillmann R (2000) A comparison of four fast vision based object recognition methods for programming by demonstration applications. In: IEEE international conference on robotics and automation, (ICRA), San Francisco, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR10" id="ref-link-section-d4478e379">2000</a>). Traditionally, the problem of object recognition has been investigated by following two main strategies. The first strategy includes feature-based methods that try to identify objects using local features such as points, lines and regions. The second strategy includes global or template-based methods. Template matching is sometimes considered as a simplification for the general problem of object detection; however, it is an efficient method that is also robust to a wide range of illumination and background conditions. Feature-based approaches can be more robust to occlusion but require long time to extract features and need a larger amount of data to be stored. The modeling technique adopted in this paper is based on a real-time pattern matching algorithm for object recognition which takes less than 300 ms per scene and allows data integration from multiple views. In addition, topology-preserving environment reconstruction is extremely useful for recognition and planning of manipulation tasks.</p><p>A third contribution of this work is a method for planning robot grasps from demonstration. Given the kinematic dissimilarities between human and robot hands, how can a suitable robot grasp be derived? Departing from previous approaches, we propose to perform such association exploiting once again the interactive nature of the VE. In an off-line phase, a user demonstrates the prototype robot grasps corresponding to the catalog of human grasps by driving a virtual instance of the grasping device. In this way, a database of robot grasps afforded by the specific device is built. When an anthropomorphic manipulation task is demonstrated, a corresponding robot grasp prototype is immediately available from the database. The planning phase adapts this robot grasp to the task based on the specific user-demonstrated data. The proposed solution for grasp planning ensures accurate positioning of the robot end-effector relative to the object to be grasped.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig1">1</a> shows the overall architecture of the PbD system for manipulation tasks. Prototype human grasps are learned off-line from demonstrations. Human grasps are demonstrated and classified in a VE comprising an anthropomorphic hand driven by a glove input device. Prototype robot grasps corresponding to human grasps are also taught off-line in the VE by driving a 3D model of the specific robotic hand. Teaching of human grasps and teaching of robot grasps are not contingent upon a specific task being demonstrated. Rather, these efforts are shared and amortized across multiple tasks and therefore can be conveniently carried out by an expert operator. When a new manipulation task must be programmed, the workspace model is generated using the vision-based technique described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec8">5</a>. The user then demonstrates in the VE the needed manipulation task that involves some grasp operation. The chosen anthropomorphic grasp is interpreted with the help of the human grasp database and mapped onto a suitable robot grasp. Finally, the grasping task is synthesized in a simulation environment where the robot hand is mounted on a robot manipulator.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The manipulation and grasping PbD system architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Robot grasp teaching is required to translate the recognized human hand posture into a configuration suitable for the available robot hand. Interactive teaching of robot grasps speeds up the synthesis phase, as an exhaustive procedure for grasp planning and optimization is computationally expensive. Grasp planning exploits exact contact conditions between the robot hand and the grasped object computed in the VE as well as a trajectory generator based on NURBS (Non-Uniform Rational B-Spline). The overall method is quite general, as only task simulation and teaching of robot grasps are directly tied to the specific robotic setup.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Related work</h2><div class="c-article-section__content" id="Sec3-content"><p>This section reviews prior work on grasp classification, robot grasp planning, and automatic environment modeling.</p><p>Understanding the grasps adopted by a human operator has been a major goal in robotics for over two decades (Cutkosky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Trans Robot Autom 5(3):269–279" href="/article/10.1007/s10055-010-0172-8#ref-CR8" id="ref-link-section-d4478e425">1989</a>; Venkataraman and Iberall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Venkataraman ST, Iberall T (eds) (1990) Dextrous robot hands. Springer, New York" href="/article/10.1007/s10055-010-0172-8#ref-CR33" id="ref-link-section-d4478e428">1990</a>). However, anthropomorphic grasp learning has never been investigated in virtual reality. Two different approaches have been proposed for grasp recognition. The first approach is based on static classification, while the second approach relies on dynamic classification of grasp sequences. In Friedrich et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary operators: grasp classification using neural network classifiers. In: IASTED international conference on intelligent systems and control, Santa Barbara, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR15" id="ref-link-section-d4478e431">1999</a>), static hand posture classification has been investigated using neural networks and relying only on angular data collected by gloves. This work used Cutkosky’s taxonomy (Cutkosky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Trans Robot Autom 5(3):269–279" href="/article/10.1007/s10055-010-0172-8#ref-CR8" id="ref-link-section-d4478e434">1989</a>) as the basis for grasp classification and obtained about 90% in overall recognition accuracy. A similar approach has been adopted in (Wojtara and Nonami <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wojtara T, Nonami K (2004) Hand posture detection by neural network and grasp mapping for a master slave hand system. In: IEEE/RSJ international conference on intelligent robots and systems (IROS) pp 866–871, Sendai, Japan" href="/article/10.1007/s10055-010-0172-8#ref-CR34" id="ref-link-section-d4478e437">2004</a>). Recently, Heumer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of classification methods. In: IEEE virtual reality, pp 19–26, Charlotte, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR16" id="ref-link-section-d4478e441">2007</a>) conducted a systematic evaluation of classification techniques for recognizing six grasps with an uncalibrated data glove. An interesting result was that calibration-free classification is possible with reasonable accuracy. Moreover, the authors found that minimum distance classifiers outperform most other algorithms in this domain.</p><p>Dynamic grasp classification has been proposed by Bernardin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human grasping sequences using hidden Markov models. IEEE Trans Robot 21(1):47–57" href="/article/10.1007/s10055-010-0172-8#ref-CR6" id="ref-link-section-d4478e447">2005</a>) and by Ekvall and Kragić (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ekvall S, Kragić D (2004) Interactive grasp learning based on human demonstration. In IEEE international conference on robotics and automation, (ICRA) New Orleans, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR11" id="ref-link-section-d4478e450">2004</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385" href="/article/10.1007/s10055-010-0172-8#ref-CR20" id="ref-link-section-d4478e453">2005</a>). In Bernardin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human grasping sequences using hidden Markov models. IEEE Trans Robot 21(1):47–57" href="/article/10.1007/s10055-010-0172-8#ref-CR6" id="ref-link-section-d4478e456">2005</a>), the authors proposed a sensor fusion approach for dynamic grasp recognition using composite hidden Markov models (HMM). The system used both hand shape and contact information obtained from tactile sensors. Grasp recognition referred to twelve patterns according to Kamakura’s taxonomy and achieved an accuracy of about 90%. Post-processing was required after dynamic gesture recognition to avoid misclassifications. In Ekvall and Kragić (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ekvall S, Kragić D (2005) Grasp recognition for programming by demonstration. In: IEEE Intl conference on robotics and automation, (ICRA) Barcelona, Spain" href="/article/10.1007/s10055-010-0172-8#ref-CR12" id="ref-link-section-d4478e459">2005</a>), a hybrid method for dynamic classification was presented which combined both HMM classification and hand trajectory classification. Ten grasps from Cutkosky’s taxonomy were considered, yielding a recognition ability of about 70% for a multiple user setting.</p><p>A key issue in human grasps classification for PbD is understanding when a grasp actually occurs during manipulation. Some of the previous contributions neglect the problem as they only focus on static recognition of individual grasps (Friedrich et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary operators: grasp classification using neural network classifiers. In: IASTED international conference on intelligent systems and control, Santa Barbara, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR15" id="ref-link-section-d4478e465">1999</a>; Heumer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of classification methods. In: IEEE virtual reality, pp 19–26, Charlotte, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR16" id="ref-link-section-d4478e468">2007</a>). The use of tactile information to help segmenting grasps is pursued in Zöllner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Zöllner R, Rogalla O, Dillmann R (2001) Integration of tactile sensors in a programming by demonstration system. In: IEEE international conference on robotics and automation, (ICRA)" href="/article/10.1007/s10055-010-0172-8#ref-CR36" id="ref-link-section-d4478e471">2001</a>) and Bernardin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human grasping sequences using hidden Markov models. IEEE Trans Robot 21(1):47–57" href="/article/10.1007/s10055-010-0172-8#ref-CR6" id="ref-link-section-d4478e474">2005</a>). Ekvall and Kragić (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ekvall S, Kragić D (2004) Interactive grasp learning based on human demonstration. In IEEE international conference on robotics and automation, (ICRA) New Orleans, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR11" id="ref-link-section-d4478e477">2004</a>) attempt to identify start and end timepoints of a grasp sequence with a computationally expensive motion analysis procedure. However, this approach appears to be suitable only for coarse hand pose representations. In all these approaches, isolating grasp segments or grasp poses is a major source of complexity and uncertainty in the overall grasp recognition process. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec5">4</a> will describe how a virtual environment enables simpler grasp segmentation.</p><p>The present work aims also at integrating grasp recognition with robot grasp planning, as these two problems have often been decoupled in previous research. Only a few works have addressed both problems. Among them, the early work of Kang and Ikeuchi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Kang SB, Ikeuchi K (1997) Toward automatic robot instruction from perception-mapping human grasps to manipulator grasps. IEEE Trans Robot Autom 13(1):81–95" href="/article/10.1007/s10055-010-0172-8#ref-CR22" id="ref-link-section-d4478e488">1997</a>) proposed a complete PbD system combining static grasp classification, based on the analytical computation of the contact-web, with grasp synthesis on a real robot manipulator. Ikeuchi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385" href="/article/10.1007/s10055-010-0172-8#ref-CR20" id="ref-link-section-d4478e491">1994</a>) have also proposed a method for recognizing an assembly plan from the observation of changes in the contact states of the objects. Few free robotic simulators allowing grasp simulation are currently available to the research community. One of the most promising grasp simulation tool is Graspit! (Miller and Allen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Miller AT, Allen PK (2000) Graspit!: a versatile simulator for grasp analysis. In: ASME international mechanical engineering congress, pp 1251–1258, Orlando, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR28" id="ref-link-section-d4478e494">2000</a>). Graspit! is focused on grasp analysis, dynamic simulation, and trajectory generation. It also provides a 3D user interface which, however, does not support automatic environment modeling and human guidance with haptic devices.</p><p>Some of the major contributions that have considered both problems of environment modeling and robot grasp synthesis, although without addressing the issue of grasp classification, are reviewed next. Lloyd et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lloyd E, Beis JS, Pai DK, Lowe DG (1999) Programming contact tasks using a reality-based virtual environment integrated with vision. IEEE Trans Robot Autom 15(3):423–434" href="/article/10.1007/s10055-010-0172-8#ref-CR26" id="ref-link-section-d4478e500">1999</a>) developed a virtual reality-based system to program part-mating and contact tasks executed with a parallel gripper using a fast and occlusion-tolerant vision system in a strongly structured environment. Morales et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Morales A, Asfour T, Azad P, Knoop S, Dillmann R (2006) Integrated grasp planning and visual object localization for a humanoid robot with five-fingered hands. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS), Beijing, China" href="/article/10.1007/s10055-010-0172-8#ref-CR29" id="ref-link-section-d4478e503">2006</a>) presented an integrated system for grasp planning and visual object localization for a humanoid robot with five-fingered hands. In this system, grasp analysis was not driven by a human operator but was performed off-line. Hirano et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hirano Y, Kitahama K, Yoshizawa S (2005) Image-based object recognition and dexterous hand/arm motion planning using RRTs for grasping in cluttered scene. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR17" id="ref-link-section-d4478e506">2005</a>) described a method for object grasping in cluttered environments including image-based 3D reconstruction and multi-goal path planning. Jang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Jang H, Moradi H, Lee S, Han J (2005) A visibility-based accessibility analysis of the grasp points for real-time manipulation. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR21" id="ref-link-section-d4478e509">2005</a>) presented a method for accessibility analysis of grasp points where the workspace is modeled with a mixture of complete solid models and point clouds. Experiments were performed with a parallel gripper. Hsiao and Lozano-Perez (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hsiao K, Lozano-Pérez T (2006) Imitation learning of whole-body grasps. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China" href="/article/10.1007/s10055-010-0172-8#ref-CR18" id="ref-link-section-d4478e512">2006</a>) proposed a system for imitating whole body grasps by teleoperation of a simulated robot. However, the system does not include automatic environment reconstruction and objects in the workspace are approximated as a combination of solid primitives.</p><p>Many works have addressed the particular problem of automatic environment modeling without explicitly relating it to human interaction and grasping. Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lee S, Jang D, Kim E, Hong S, Han J (2005) A real-time 3D workspace modeling with stereo camera. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)" href="/article/10.1007/s10055-010-0172-8#ref-CR25" id="ref-link-section-d4478e518">2005</a>) proposed a modeling technique based on the scale-invariant feature transform (SIFT) algorithm combined with 3D point clouds from stereopsis. The algorithm requires about 1.5 s for image processing in real time, but allows image registration from different views, a rarely considered yet useful functionality. Kitahama et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kitahama K, Tsukada A, Galpin F, Matsubara T, Hirano Y (2006) Vision-based scene representation for 3D interaction of service robots. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China" href="/article/10.1007/s10055-010-0172-8#ref-CR24" id="ref-link-section-d4478e521">2006</a>) described another SIFT-based method for real-time 3D scene representation with the introduction of a confidence map to remove inaccurate and wrong input data. The algorithm works in real time, taking from 100 ms up to few seconds for computation. Wong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Wong AKC, Rong L, Liang X (1998) Robotic vision: 3D object recognition and pose determination. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Victoria, Canada" href="/article/10.1007/s10055-010-0172-8#ref-CR35" id="ref-link-section-d4478e524">1998</a>) developed a robust and efficient method for pose determination using a monocular vision system and an algorithm for feature detection and grouping. Matas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Matas J, Soh LM, Kittler J (1997) Object recognition using a tag. In: International conference on image processing" href="/article/10.1007/s10055-010-0172-8#ref-CR27" id="ref-link-section-d4478e527">1997</a>) proposed a template-based algorithm for object recognition using markers on the objects.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">PbD system setup</h2><div class="c-article-section__content" id="Sec4-content"><p>The PbD system described in this paper comprises a CyberTouch glove (by Immersion Corporation, Inc.) and a FasTrak 3D motion tracking device (by Polhemus, Inc.). These devices, along with the software components described later and a visual rendering of the scene, allow an operator to perform manipulation tasks in a desktop virtual environment. The FasTrak is an electro-magnetic sensor that tracks the position and orientation of a small receiver that is mounted on the wrist of the CyberTouch. The CyberTouch used in the experiments is an instrumented glove with 18 sensors for bend and abduction measurements. Joint angles are returned by the device as a 22-dimensional vector where the angles of the distal joints of each of the four fingers are estimated. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab1">1</a> lists the CyberTouch joints. The CyberTouch also includes 6 programmable vibrotactile actuators, which however were not used in this research. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig2">2</a> shows a view of the desktop VR environment and of the devices involved.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 CyberTouch returned joint angles (<i>j</i> joint, <i>abd</i>. abduction)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Desktop virtual reality workspace (<i>left image</i>) and VR devices (<i>right image</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>A preliminary analysis of the variance of the joint angles across a large number of grasps showed a very limited range of motion used for three metacarpophalangeal joints (middle, ring, and pinkie finger), with a standard deviation lower than 0.1 rad (joints 8, 12 and 16 in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab1">1</a>). Therefore, these joints were not considered in the grasp classification algorithm, and the data vector representing the hand posture was restricted to 19 values. For demonstration purposes, the operator’s hand pose was directly mapped to an anthropomorphic 3D model of the hand, shown in the simulated workspace along with the objects.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Grasp recognition in virtual reality</h2><div class="c-article-section__content" id="Sec5-content"><p>This section describes a technique for recognizing human grasps that fully leverages upon the potential of VR. A more detailed presentation of the grasp recognition technique is given in (Aleotti and Caselli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Aleotti J, Caselli S (2006) Grasp recognition in virtual reality for Robot Pregrasp planning by demonstration. In: IEEE International conference on robotics and automation, (ICRA), Orlando, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR2" id="ref-link-section-d4478e871">2006</a>). It should be emphasized that in all previous research discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec2">2</a>, the human operator executed grasps by physically manipulating material objects while wearing one or more tracking devices, such as gloves, electromagnetic trackers, and contact sensors. In all approaches, hand motion was traced, segmented either by motion analysis or with the help of contact sensors and then analyzed to infer the grasp adopted by the operator. Even though it is impossible to assess the relative merit of the various approaches due to the different experimental conditions, reported results show that recognition rates between 70 and 90% can be attained by the various methods that rely on tracing and analyzing human grasps exerted on physical objects (Bernardin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human grasping sequences using hidden Markov models. IEEE Trans Robot 21(1):47–57" href="/article/10.1007/s10055-010-0172-8#ref-CR6" id="ref-link-section-d4478e877">2005</a>; Ekvall and Kragić <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ekvall S, Kragić D (2004) Interactive grasp learning based on human demonstration. In IEEE international conference on robotics and automation, (ICRA) New Orleans, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR11" id="ref-link-section-d4478e880">2004</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385" href="/article/10.1007/s10055-010-0172-8#ref-CR20" id="ref-link-section-d4478e883">2005</a>; Friedrich et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary operators: grasp classification using neural network classifiers. In: IASTED international conference on intelligent systems and control, Santa Barbara, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR15" id="ref-link-section-d4478e887">1999</a>; Heumer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of classification methods. In: IEEE virtual reality, pp 19–26, Charlotte, USA" href="/article/10.1007/s10055-010-0172-8#ref-CR16" id="ref-link-section-d4478e890">2007</a>; Wojtara and Nonami <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wojtara T, Nonami K (2004) Hand posture detection by neural network and grasp mapping for a master slave hand system. In: IEEE/RSJ international conference on intelligent robots and systems (IROS) pp 866–871, Sendai, Japan" href="/article/10.1007/s10055-010-0172-8#ref-CR34" id="ref-link-section-d4478e893">2004</a>).</p><p>There are several drawbacks in analyzing such grasps. First, all grasp recognition approaches require multiple, repeated grasping trials. VR gloves are not designed for physical work nor physical manipulation: they are rather delicate devices and can be easily broken with the repeated contacts involved in physical grasps. Second, there are several applications for which the physical objects to be grasped are not immediately available or are dangerous for human manipulation. Hazardous material manipulation and remote maintenance are examples of such applications. Third, physical manipulation requires motion segmentation to extract the actual grasp applied by the operator. Segmentation is performed either with additional contact sensors or with complex motion analysis techniques or manually. In any case, additional complexity is incurred. Performing a PbD task using purely virtual grasps, as proposed in this paper, brings several advantages, including increased robustness, possibility for unlimited task repetitions with minimal device wear, operator safety, and easier and more robust grasp segmentation. The drawback of the virtual environment is the need for a prior representation, which must be obtained with an authoring tool or with an automatic acquisition method like the one described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec8">5</a>.</p><p>For the proposed PbD system, the virtual environment is built upon the Virtual Hand Toolkit (VHT) library provided by Immersion Corporation. A human hand is drawn in the virtual scene and driven based on the data captured with the VR glove and the tracker (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig2">2</a>). Collision information between the hand and the objects, including the coordinates of the contact points and the surface normals at the contacts, is computed with the V-Clip collision detection algorithm. A virtual grasping algorithm thus exploits this information to determine whether the object has been grasped. Grasp stability is evaluated using the <i>L</i>
                <sub>1</sub> quality metric (Ferrari and Canny <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Ferrari C, Canny J (1992) Planning optimal grasps. In: IEEE international conference on robotics and automation, (ICRA) pp 2290–2295, Nice, France" href="/article/10.1007/s10055-010-0172-8#ref-CR13" id="ref-link-section-d4478e913">1992</a>) based on the grasp wrench space. This procedure overcomes the segmentation problems incurred by grasps on physical objects, as a grasp is identified whenever a stable grasping condition exists.</p><p>The PbD system recognizes eleven grasping classes. These classes are a subset of Cutkosky’s taxonomy (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Trans Robot Autom 5(3):269–279" href="/article/10.1007/s10055-010-0172-8#ref-CR8" id="ref-link-section-d4478e919">1989</a>), the most widely used grasp classification scheme. Five classes of the original taxonomy were not included because they differ mainly on object shape rather than on hand shape (such as the circular disk grasps and the small diameter grasps). The resulting grasp tree is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig3">3</a>, along with the labels and with example images of the grasps. The set of objects used in the experiments includes standard geometrical primitives such as boxes, spheres, and cylinders of different sizes, and a few daily life objects such as a classical teapot. A subset of the objects is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The grasp set for learning of human grasps</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A scene with some of the objects proposed for grasp teaching</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec6">Grasp classification</h3><p>The first step in grasp classification is building the human grasp database mentioned in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig1">1</a>. In assessing the interaction with any VR system, users’ skill levels, ranging from novice to expert, must be taken into account. Since the purpose of PbD is to simplify robot programming for standard users, a high degree of VR experience should not be assumed in the user demonstrating the task. However, the universe of grasps that can be exerted on objects is a static knowledge base that can be built off-line by an expert operator. This approach relieves the generic user from the burden of supplying him/herself such knowledge. Thus, in the proposed system a classifier is initially built using grasps demonstrated by an experienced users. To assess the validity of this concept, two human grasp databases have been developed by experienced users (a man and a woman) who provided multiple examples for each grasp in the set freely choosing among available virtual objects. The two databases have led to very similar classification performance.</p><p>When a generic user demonstrates a manipulation task in the VE, grasps are automatically segmented and classified with the help of the human grasp database. Segmentation takes advantage from the contact information available in the VE. As previously discussed, each grasp posture is represented by a 19-valued vector. The adopted classification algorithm consists of two steps. Firstly, a nearest neighbor algorithm computes the distance between the current hand posture to be classified and each of the 11 patterns representing the recognizable grasps, which were collected in the training session. The algorithm then sorts the grasp patterns starting from the nearest candidate in descending order. The distance between a pattern and the current hand configuration is computed in the joint space as Euclidean distance between the two vectors of joint angles.</p><p>After the scoring phase, if the two best candidate patterns belong to a predefined set of pairs of possibly ambiguous grasp patterns, then a heuristic decision process is applied. The heuristic decision process has the purpose of decreasing misclassifications in ambiguous cases. The predefined heuristic rules exploit information about the contacts and the normals at the virtual contact points. This information is immediately available in the VE, whereas it is often noisy or unavailable when grasping physical objects. The proposed method for grasp classification is summarized in Algorithm <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab2">1</a>. Some examples are provided in the following.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Algorithm 1  </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To disambiguate between a circular power grasp and a circular precision grasp, the system checks the total number of contacts (assuming one contact per hand element colliding with the grasped object, e.g. phalange). If this number is larger than 10, the system heuristically classifies the grasp as a power grasp, otherwise the system classifies it as a precision grasp. This threshold has been determined by minimizing the classification error over a sample of training data. To disambiguate between two apparently similar grasps belonging to the same side of the grasp tree, the system mainly looks for differences in orientation of the contact normals. This strategy is applied to disambiguate between the medium wrap and the lateral pinch grasp and between the precision tripod grasp and the thumb–two finger grasp. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab3">2</a> provides the complete set of predefined heuristics.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 2 Applied heuristics for ambiguous grasps</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The above heuristic rules, based on orientation of the contact normals, can be interpreted in terms of virtual fingers (Arbib et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Arbib MA, Iberall T, Lyons D (1985) Coordinated control programs for control of the hands. Hand function and the neocortex. Exp Brain Res Suppl 10:111–29. Springer" href="/article/10.1007/s10055-010-0172-8#ref-CR5" id="ref-link-section-d4478e1340">1985</a>). A virtual finger is a group of real fingers acting as a single functional unit. This concept can be used to formally characterize different types of grasps in an abstract way. Iberall showed that hand postures of different taxonomies, including Cutkosky’s classification, can be described in terms of oppositions between virtual fingers (Iberall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Iberall T (1987) The nature of human prehension: Three dextrous hands in one. In: IEEE international conference on robotics and automation, (ICRA) pp 396–401" href="/article/10.1007/s10055-010-0172-8#ref-CR19" id="ref-link-section-d4478e1343">1987</a>). The medium wrap and the lateral pinch grasp, previously cited, are easily identifiable by different types of oppositions. The same consideration holds for the precision tripod grasp and the thumb–two finger grasp.</p><p>The medium wrap grasp exhibits a palm opposition between two virtual fingers (VF): the palm (VF1) and the digits (VF2). An example of virtual grasp with palm opposition is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig5">5</a>a. In palm opposition, ”the object is fixed along an axis roughly normal to the palm of the hand” (Iberall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Iberall T (1987) The nature of human prehension: Three dextrous hands in one. In: IEEE international conference on robotics and automation, (ICRA) pp 396–401" href="/article/10.1007/s10055-010-0172-8#ref-CR19" id="ref-link-section-d4478e1352">1987</a>). The lateral pinch grasp establishes two types of oppositions concurrently: a palm opposition and a side opposition between the thumb (VF1) and the side of the index finger (VF2), which makes it different from the medium wrap as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig5">5</a>b. Opposition occurs primarily along an axis transverse to the palm. Both the tripod grasp and the thumb–two finger grasp are precision grasps that exhibit a pad opposition (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig6">6</a>, left image). Opposition occurs along an axis roughly parallel to the palm. However, the thumb–two finger grasp can be interpreted as a two virtual finger grasp (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig6">6</a>, central image), whereas the tripod grasp can be classified as a three virtual finger grasp (Cutkosky and Howe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Cutkosky MR, Howe RD (1990) Human grasp choice and robotic grasp analysis. Dextrous Robot Hands, chap 1, pp 111–29. Springer" href="/article/10.1007/s10055-010-0172-8#ref-CR9" id="ref-link-section-d4478e1365">1990</a>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig6">6</a>, right image).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Palm and side opposition grasps</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Pad opposition grasp (<i>left image</i>) with two examples: two finger prismatic precision grasp (<i>central image</i>) and tripod grasp (<i>right image</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The given interpretation of grasps in terms of a combination of oppositions between virtual fingers suggests that the contact normals, expressed in a reference frame relative to the hand, can be exploited to disambiguate between pairs of grasps. From the information about the contact normals, which is provided by the collision detection engine, it is indeed possible to determine the type of opposition, the number of virtual fingers and therefore the class of the grasp. In particular, the heuristic algorithm defines cones of acceptance for the orientation of the contact normals for each type of recognizable grasp, which provide some degree of tolerance between slightly different grasps.</p><h3 class="c-article__sub-heading" id="Sec7">Experiment</h3><p>A recognition experiment was organized involving two experienced subjects and ten unexperienced subjects (five men and five women, mean age 23 years). Subjects were recruited among students and staff of the University of Parma. The CyberTouch was calibrated for each user at the beginning of the session. Each subject performed a short training session before the experiment. During the actual experiment, subjects were asked to reproduce each of the 11 grasp classes 4 times in a random order, for a total of 44 grasp demonstrations per user. Subjects were free to choose among multiple cylinders, spheres, and boxes of different sizes in the VE. They were also allowed to change orientation of the virtual camera in the environment. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab4">3</a> reports the mean recognition rate of the grasp classification experiment for the two groups of users (experienced users and unexperienced users) and provides the recognition statistics for individual grasps considering unexperienced subjects only.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 3 Results of grasp classification</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The average recognition rate for unexperienced users was 82.8%, a value comparable with the recognition performance of methods relying on physical grasps. The recognition rate for the two expert users was 94%, noticeably better than the performance of the unexperienced ones. Grasp classification for the skilled users was carried out with their own training data, while classification for the unskilled ones was carried out with the human grasp database generated by one of the expert users, so as to simulate a practical scenario where ordinary users cannot be asked to spend too much time for a training session. The low variance of recognition rate across individual grasps and between power and precision grasps suggests a robust behavior of the classification algorithm. The algorithm has also proven rather robust to varying object sizes.</p><p>A data analysis was then carried out removing the heuristic rules from the grasp classification process. In this case, the recognition rate decreased of about 20%, confirming the importance of the second step of the classification algorithm. Interestingly enough, though, the worst recognition rates in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab4">3</a> were obtained for the medium wrap and the circular sphere power grasps (with 67.5 and 72.5%, respectively), two grasps involved in the heuristic disambiguation rules. This remark suggests that additional investigation of the heuristic rules could lead to further improvement in grasp recognition performance.</p><p>Results reported in this section show that a simple nearest neighbor learning algorithm can be successfully adopted for grasp classification, provided that information such as virtual fingers and contact states is exploited. This information is indeed easily available in the VE. We argue that if the VE were supplemented with additional immersive features, like haptic information or virtual fixtures, its potential would be further emphasized. For example, one of the drawbacks of a purely graphical environment in demonstrating a grasping task is the lack of tactile feedback. However, an advanced device like the CyberTouch offers programmable vibratory actuators that could be exploited to provide such feedback. This feature was not used in the research described in this paper, which was rather focused on assessing the performance attainable using standard tracking devices and gloves.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Environment modeling</h2><div class="c-article-section__content" id="Sec8-content"><p>In this section, a method for visual workspace modeling based on pattern matching is introduced. The input of the algorithm is a database of recognizable objects. Each object is uniquely identified by a flat marker that is attached to a face of its surface orthogonal to the ground of the workspace. Entries of the database consist of a label, a sample image of its associated marker, the relative transformation between the center of the marker and the centroid of the object, and a 3D VRML model of the object used for environment modeling.</p><p>Modeling of a workspace comprising some objects in the database starts with acquisition of one or more images of the workspace with a camera. Based on recognized markers, VRML object models are instantiated in the VE. The marker recognition phase is implemented exploiting ARToolKit (Kato and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality (IWAR)" href="/article/10.1007/s10055-010-0172-8#ref-CR23" id="ref-link-section-d4478e1694">1999</a>), an open-source multiplatform software library designed for augmented reality applications. The library exploits a pattern recognition algorithm searching for square regions containing known markers. In particular, the basic algorithm of ARToolKit computes the real camera position and orientation relative to the markers, which have a known size (4 × 4 cm in the proposed experiments). One snapshot of each marker must be provided to the system in advance as training pattern. The recognition stage is a loop comprising several steps. First, the live video image is binarized using a lighting threshold. Then, the algorithm searches for square regions. The squares containing real markers are then identified and the corresponding transformation matrix is computed. ARToolKit is able to identify and track multiple markers concurrently. As additional functionality, the library allows overlaying of computer graphics images on the video stream captured by a monocular camera in real time.</p><p>The automatic environment modeling procedure proposed in this paper computes for each recognized marker <i>i</i> its relative transformation <span class="mathjax-tex">\({}^{R}_{i}T\)</span> with respect to a reference marker <i>R</i>, placed on the reference plane. The location of the reference marker is then mapped to the reference frame of the reconstructed virtual environment. The basic recognition phase is extremely fast, since it requires a single image of the environment. To improve the robustness of the pattern matching procedure, the recognition phase has been extended to a window of ten frames, which takes about 300 ms, thereby filtering out false-positive and false-negative detections. The algorithm also supports data integration from multiple views of the scene to overcome face occlusion.</p><p>After the object recognition phase, the algorithm computes the edge-face graph of the workspace. The edge-face graph is derived from the <i>generalized edge-face graph</i> (GEFG) (de Floriani <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="de Floriani L (1989) Feature extraction from boundary models of three-dimensional objects. IEEE Trans Pattern Anal Mach Intell 11(8):785–798" href="/article/10.1007/s10055-010-0172-8#ref-CR14" id="ref-link-section-d4478e1723">1989</a>). The GEFG of an object <i>S</i> is a 4-tuple <i>H</i> = (<i>F</i>, <i>L</i>, <i>FL</i>, <i>LL</i>), where <i>F</i> is the set of nodes of <i>H</i> of type <i>face</i>, <i>L</i> the set of nodes of <i>H</i> of type <i>loop</i>, <i>FL</i> the set of <i>face-loop</i> arcs of <i>H</i> (which connect a node in <i>F</i> to a node in <i>L</i>), and <i>LL</i> is the set of the <i>loop-loop</i> arcs of <i>H</i> (which connect pairs of nodes in <i>L</i>). A loop is any closed chain of edges bounding an object face. Given the GEFG <i>H</i> of an object <i>S</i>, the <i>edge-face graph</i> (EFG) associated with <i>H</i> is defined as the graph <i>G</i> = (<i>N</i>, <i>A</i>), such that <i>N</i> = <i>F</i> and <span class="mathjax-tex">\(A=\left\{(f,f^{'},v,v^{'})|(f,l),(f^{'},l^{'})\,{\in}\,FL\land (l,l^{'},v,v^{'}) {\in}LL \right\}\)</span>, where (<i>l</i>, <i>l</i>
                <sup>'</sup>, <i>v</i>, <i>v</i>
                <sup>'</sup>) is an arc (<i>l</i>, <i>l</i>
                <sup>'</sup>) in <i>LL</i> joining the two vertices (<i>v</i>, <i>v</i>
                <sup>'</sup>). Hence, the EFG graph can be interpreted as the boundary model of a solid object defined such that <i>N</i> is the set of the faces and <i>A</i> is the set of the arcs connecting adjacent faces. The EFG of the workspace is given by the union of the edge-face graphs of the composing objects. Therefore, the resulting graph provides a topological description of the partition of the boundary of the workspace into a finite set of shells, where each shell is a maximal connected set of faces forming the boundary.</p><p>In the proposed environment modeling system, the EFG is computed incrementally by fusing the information collected from each view of the scene. Knowledge of the EFG allows reconstruction of the workspace in a simulated 3D environment by preserving the topology of the contacts and hence by reducing the effect of sensor noise. In particular, the simulated environment is rendered from a depth-first traversal of the EFG. Visual virtual fixtures are applied to the 3D VRML models that are automatically aligned according to the topology extracted from the graph. The virtual environment enables 3D interaction and is used for demonstration of manipulation tasks within the PbD system by letting objects to be graspable with the virtual hand driven by the dataglove.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig7">7</a> shows the reconstruction of two scenes with boxes of different size. The reference marker is visible at the center of the workspace. The figure also reports images of the graphically augmented video where artificial colored panels are superimposed to the markers and a reference frame is superimposed on the reference marker. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig7">7</a> finally shows the reconstructed virtual environment with the proper topology. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig8">8</a>, the edge-face graphs relative to the previous environments are displayed. The complete EFG is obtained by composing subgraphs having the form of an octahedron that describe the single boxes. Clusters of nodes, corresponding to different shells of the workspace, are easily identifiable.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Two environment modeling experiments. Real workspace (<i>first row</i>), graphically augmented video output (<i>second row</i>) and reconstructed environment (<i>third row</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>EFG for the two experiments of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig7">7</a>
                      </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig9">9</a> shows the reconstruction of a cluttered environment from the registration of data acquired from two different points of view of the scene. Occluded objects are correctly recognized after data fusion. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig10">10</a> reports a final experiment where the objects are not properly aligned with each others. The system intelligently detects the vertical adjacency relations as well as the misalignment along the <i>y</i> axis and does not apply the virtual fixtures; hence, the original orientation of the objects is preserved. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0172-8#Tab5">4</a> reports statistics about the visual recognition system. A set of 100 experiments was conducted changing the number of objects, their position and their relative orientation. The rotation of the markers relative to the camera has been kept within 60°. Beyond this value, performance of the recognition algorithm significantly degrades.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Environment modeling from image registration of two different views</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Environment modeling for misaligned objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 4 Performance of the environment modeling system</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Despite the use of a low-cost camera, the marker tracking algorithm has proven remarkably robust against a variable range of distances between the camera and the markers. When the distance is below 50 cm the recognition rate is indeed very good. Moreover, the mean error per axis is quite low, and markers can be correctly identified up to the specified limit yaw angle. In general, objects are not constrained to lie orthogonal to the ground, i.e. the algorithm can recognize objects lying on a slope plane. The workspace modeling algorithm has been tested in environments with boxes with arbitrary dimensions. In principle, it could be generalized to handle prismatic objects with an arbitrary number of faces with any orientation, as long as their VRML model is available and the attached marker is flat.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Grasp synthesis</h2><div class="c-article-section__content" id="Sec9-content"><p>The grasp synthesis stage allows simulation of grasping tasks. Simulation of the learned manipulation task improves operator’s confidence in the system, allows task refinements to be specified, and increases overall safety. As shown by the system architecture in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig1">1</a>, after the environment modeling phase an operator performs manipulation tasks wearing the CyberGlove and the FasTrak. The operator’s hand pose is mapped to the anthropomorphic 3D hand model that is shown in the simulated workspace along with the reconstructed objects. Collision detection is enabled to compute contact information between the hand and the objects, and a virtual grasping algorithm determines whether the objects can be grasped. The grasp classification algorithm is invoked whenever an object is grasped. The simulator then plans the grasp to be synthesized by exploiting information about the classified grasp as well as a robot grasp database which is built off-line.</p><p>The user is allowed to demonstrate multiple times the same manipulation task. If multiple demonstrations are provided, the system plans a set of smooth pregrasp trajectories by exploiting a geometric approach for trajectory clustering and a stochastic procedure for trajectory evaluation based on hidden Markov models (Aleotti and Caselli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Aleotti J, Caselli S (2005) Trajectory clustering and stochastic approximation for robot programming by demonstration. In: IEEE international conference on intelligent robots and systems, (IROS) Edmonton, Canada" href="/article/10.1007/s10055-010-0172-8#ref-CR1" id="ref-link-section-d4478e2110">2005</a>). The stochastic algorithm allows noise reduction and filtering of involuntary movements.</p><h3 class="c-article__sub-heading" id="Sec10">Teaching of robot grasps</h3><p>The purpose of the off-line teaching phase is to build a database of suitable robot grasps and speed up simulation. The teaching phase is performed by an experienced user who demonstrates typical robot grasps by guiding the robot hand in real time in a virtual environment. A grasp mapping procedure is also required to translate the sensor readings of the glove and the tracker to the robot hand available in the simulated setup.</p><p>Grasp mapping allows kinematics dissimilarities between the human hand and the robot hand to be overcome under control from an expert operator. Translation of the chosen hand pose to the robot hand is achieved at the joint level. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig11">11</a> shows examples of grasp mapping implemented in the system described in this paper. The gripper used in the current simulation setup is the Barrett hand, which has three fingers and four degrees of freedom. Each finger has two phalanges that are driven by a single motor, thus resulting in a single flexion degree of freedom per finger. The fourth degree of freedom controls abduction of the two lateral fingers that are symmetrically rotated on both sides of the third finger. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig11">11</a>, the proximal interphalangeal joints of three fingers of the human hand (thumb, index, and middle finger) were mapped to the flexion joints of Barrett hand fingers, while the thumb abduction joint was mapped to the rotating angle of the lateral fingers of the robot hand. This default mapping function provides a natural way for manipulating with the Barrett hand, as the flexion motion of the three robot fingers is controlled by the flexion of three human phalanges, while the spreading motion of the two lateral fingers of the robot hand is controlled by thumb abduction. In the VE, the joint mapping function can be easily customized according to the preferences of each user by taking advantage of the large number of sensed degrees of freedom in the CyberTouch.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Examples of grasp mapping</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As the Barrett hand cannot replicate all the eleven recognizable human grasps, similar grasps are grouped together after the demonstration phase. For example, all prismatic precision grasps that can be demonstrated with the anthropomorphic hand are grouped into a single thumb–two finger class, which is the only prismatic precision grasp achievable with the Barrett hand. Once the three robot fingers collide with the object to be grasped, the algorithm computes the relative transformation of a reference frame local to the Barrett hand with respect to the object: <span class="mathjax-tex">\({}^{G}_{B}T={}^{O}_{G}T^{-1}{}^{O}_{B}T\)</span> where <span class="mathjax-tex">\({}^{O}_{B}T\)</span> and <span class="mathjax-tex">\({}^{O}_{G}T\)</span> represent the Barrett hand and the object transformations with respect to the reference frame of the teaching environment.</p><p>For each demonstrated robot grasp, the teacher is asked to specify the corresponding grasp class. Multiple demonstrations of the same grasp class can be provided. Hence, a dataset <i>C</i>
                  <sub>
                    <i>i</i>
                  </sub> is built for each of the eleven recognizable classes containing the relative transformation matrices. A synthetic virtual fixture is also applied to those relative transformations between the hand and the object that resemble peculiar rotations around the coordinate axes. The visual virtual fixture is aimed at compensating small drifts between the actual configuration of the robot hand and the desired one. Drift can arise naturally from the user-controlled positioning of the robot hand and from sensor noise. In particular, a set <i>M</i> of peculiar transformation matrices is built describing rotations of multiple of <span class="mathjax-tex">\(\frac{\pi}{2}\)</span> radians around the coordinate axes. Then, a matrix nearness problem is solved to find the closest matrix <i>A</i>
                  <sup>*</sup> in <i>M</i> to <span class="mathjax-tex">\({}^{G}_{B}T\)</span>:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$A^{*}=\mathop{\hbox{argmin}}\limits_{A \in M}d(A,^{G}_{B}T)=\mathop{\hbox{argmin}}\limits_{A \in M}\left(\sqrt{\sum_{i,j}(a_{ij}-t_{ij})^{2}}\right) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>d</i> is the Euclidean matrix distance function, while <i>a</i>
                  <sub>
                    <i>ij</i>
                  </sub> are elements of matrix <i>A</i> and <i>t</i>
                  <sub>
                    <i>ij</i>
                  </sub> are elements of matrix <span class="mathjax-tex">\({}^{G}_{B}T\)</span>. If <i>d</i>(<i>A</i>
                  <sup>*</sup>, <span class="mathjax-tex">\({}^{G}_{B}T\)</span>) &lt; ε, given a small ε &gt; 0, then matrix <span class="mathjax-tex">\({}^{G}_{B}T\)</span> is replaced with matrix <i>A</i>
                  <sup>*</sup> in the robot grasp database. The database of robot grasps is queried after the demonstration phase of each manipulation task. Data collected from the database, along with the samples that describe the pregrasping trajectory, are exploited by the grasp planner that generates the robot commands.</p><h3 class="c-article__sub-heading" id="Sec11">Task simulation</h3><p>The goal of the PbD system is the execution of the learned grasping task in a simulation environment comprising a robot manipulator equipped with a robot hand. The proposed solution for grasp synthesis is a general approach exploiting both a pregrasp planner, which generates the approach trajectory for the robot to the object, and a grasp mapping algorithm that selects an appropriate posture for the robot hand. Let <i>C</i>
                  <sub>
                    <i>i</i>
                  </sub> be the class of the robot grasp database corresponding to the demonstrated anthropomorphic grasp, and let <span class="mathjax-tex">\({}^{G}_{H}T\)</span> be the transformation of the virtual human hand, relative to the grasped object in the demonstration environment. Then the robot hand posture <span class="mathjax-tex">\({}^{G}_{B}T\)</span>, relative to the object, to be synthesized is selected within <i>C</i>
                  <sub>
                    <i>i</i>
                  </sub> as the one that minimizes the distance <span class="mathjax-tex">\(d({}^{G}_{B}R,{}^{G}_{H}R)\)</span> between the normalized approach vectors, yielding another matrix nearness problem:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\mathop{\hbox{argmin}}\limits_{^{G}_{B}T \in C_{i}}d(^{G}_{B}R,^{G}_{H}R) $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where the approach vector of the robot hand is given by the rotational component <span class="mathjax-tex">\({}^{G}_{B}R\)</span> of the transformation matrix <span class="mathjax-tex">\({}^{G}_{B}T\)</span>, while the approach vector of the demonstrated grasping task is given by the rotational component <span class="mathjax-tex">\({}^{G}_{H}R\)</span> of the transformation matrix <span class="mathjax-tex">\({}^{G}_{H}T\)</span>.</p><p>The task simulator can be applied to any robot hand mounted on a serial manipulator. In particular, it has been tested with a Puma 560 robot arm and a Barrett hand as its end-effector. The simulation phase works as follows. Initially, the robot manipulator is controlled in the cartesian space by an inverse kinematics algorithm. The tool point of the Puma arm, which is located at the origin of a local reference frame to the Barrett hand, follows a curve that imitates the pregrasp trajectory extracted from the user demonstration. The pregrasp phase is interrupted once the tool point falls close enough to the object. The learned pregrasp trajectory is fit to a parametric NURBS to take advantage from their flexibility, computational efficiency, and ease of manipulation (Piegl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Piegl L (1991) On NURBS: a survey. IEEE Comput Graph Appl 11(1):55–71" href="/article/10.1007/s10055-010-0172-8#ref-CR31" id="ref-link-section-d4478e2434">1991</a>) within the VE.</p><p>After the pregrasp stage, the robot arm is programmed to reach its final pose for grasping. This pose is given by <span class="mathjax-tex">\({}^{S}_{B}T_{f}={}^{S}_{G}T{}^{G}_{B}T\)</span>, where <span class="mathjax-tex">\({}^{S}_{B}T_{f}\)</span> is the final transformation of the Barrett hand with respect to the reference frame of the simulation environment, <span class="mathjax-tex">\({}^{S}_{G}T\)</span> is the object transformation, and <span class="mathjax-tex">\({}^{G}_{B}T\)</span> is again the target transformation of the Barrett hand relative to the object. Let <span class="mathjax-tex">\({}^{S}_{B}T_{i}\)</span> be the transformation matrix of the Barrett hand at the end of the pregrasp phase or, equivalently, at the beginning of the final approach phase to the object. The transformation <span class="mathjax-tex">\({}^{S}_{B}T\)</span> of the robot hand at each step of the simulation in the final approach phase is computed by interpolating the initial and final transformations. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig12">12</a> shows the coordinate frames involved in the grasp synthesis phase. The last stage of the grasping simulation involves the closure of the fingers of the hand. Each flexion joint of the Barrett hand is moved until the simulation engine detects a contact between the finger and the object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Coordinate frames for grasp synthesis</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec12">Experimental evaluation</h3><p>A simulation experiment, which involves the grasp of a tea box, is presented to demonstrate the proposed approach for grasp synthesis. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig13">13</a> shows a picture of the real environment, comprising a tea box and two boxes of different size, as well as the graphically augmented video output. The figure also reports two images of the demonstration phase, consisting of a single trial, where the user grasped the reconstructed model (green box) of the tea box with a prismatic precision grasp (thumb–4 finger). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0172-8#Fig14">14</a> shows the execution of the task in the simulation environment. The pregrasp NURBS trajectory is also displayed (shown in blue or dark). The experiment also shows that the grasp synthesis algorithm works even if the configuration of the object in the simulation environment does not match its configuration in the demonstration environment. Indeed, the position and orientation of the tea box in the simulation environment have been deliberately modified. For slight changes, in the order of some centimeters, the algorithm for pregrasp trajectory following is still applicable. Of course, for large changes the pregrasp trajectory becomes useless and the initial approach phase to the object should be replanned with a different strategy to avoid collisions. Finally, it must be noted that the final grasp posture of the Barrett hand has undergone the visual virtual fixture described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0172-8#Sec10">6.1</a>, resulting in an exact alignment to one of the faces of the tea box.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Grasping of a tea box. Real workspace (<i>top left</i>), graphically augmented video output (<i>top right</i>) and demonstration phase (<i>bottom row</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0172-8/MediaObjects/10055_2010_172_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Simulation phase for grasping the tea box</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0172-8/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Conclusion</h2><div class="c-article-section__content" id="Sec13-content"><p>In this paper, a virtual reality system for programming grasping tasks based on human demonstrations has been presented. Among the main contributions of the paper are the proposal of a robot grasp synthesis strategy combined with a technique for vision-based workspace reconstruction that preserves the topology of the environment. The automatic environment modeling technique works in real-time supporting data integration from multiple views to overcome occlusion problems. Grasp synthesis is achieved by exploiting virtual environments for task demonstration and robot grasp teaching. The system allows learning and classification of virtual human grasps based on Cutkosky’s taxonomy. The proposed approach for grasp recognition exploits information about the contact points and normals between the virtual hand and the objects in the environment. Our current work is addressing integration of an accurate dynamic simulation within the virtual environment.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aleotti J, Caselli S (2005) Trajectory clustering and stochastic approximation for robot programming by demons" /><p class="c-article-references__text" id="ref-CR1">Aleotti J, Caselli S (2005) Trajectory clustering and stochastic approximation for robot programming by demonstration. In: IEEE international conference on intelligent robots and systems, (IROS) Edmonton, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aleotti J, Caselli S (2006) Grasp recognition in virtual reality for Robot Pregrasp planning by demonstration." /><p class="c-article-references__text" id="ref-CR2">Aleotti J, Caselli S (2006) Grasp recognition in virtual reality for Robot Pregrasp planning by demonstration. In: IEEE International conference on robotics and automation, (ICRA), Orlando, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aleotti J, Caselli S (2007) Robot grasp synthesis from virtual demonstration and topology-preserving environme" /><p class="c-article-references__text" id="ref-CR3">Aleotti J, Caselli S (2007) Robot grasp synthesis from virtual demonstration and topology-preserving environment reconstruction. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) San Diego, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Aleotti, S. Caselli, M. Reggiani, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Aleotti J, Caselli S, Reggiani M (2004) Leveraging on a virtual environment for robot programming by demonstra" /><p class="c-article-references__text" id="ref-CR4">Aleotti J, Caselli S, Reggiani M (2004) Leveraging on a virtual environment for robot programming by demonstration. Rob Auton Syst 47(2–3):153–161</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.robot.2004.03.009" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Leveraging%20on%20a%20virtual%20environment%20for%20robot%20programming%20by%20demonstration&amp;journal=Rob%20Auton%20Syst&amp;volume=47&amp;issue=2%E2%80%933&amp;pages=153-161&amp;publication_year=2004&amp;author=Aleotti%2CJ&amp;author=Caselli%2CS&amp;author=Reggiani%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Arbib, T. Iberall, D. Lyons, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Arbib MA, Iberall T, Lyons D (1985) Coordinated control programs for control of the hands. Hand function and t" /><p class="c-article-references__text" id="ref-CR5">Arbib MA, Iberall T, Lyons D (1985) Coordinated control programs for control of the hands. Hand function and the neocortex. Exp Brain Res Suppl 10:111–29. Springer</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Coordinated%20control%20programs%20for%20control%20of%20the%20hands.%20Hand%20function%20and%20the%20neocortex&amp;journal=Exp%20Brain%20Res%20Suppl&amp;volume=10&amp;pages=111-29&amp;publication_year=1985&amp;author=Arbib%2CMA&amp;author=Iberall%2CT&amp;author=Lyons%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Bernardin, K. Ogawara, K. Ikeuchi, R. Dillmann, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human" /><p class="c-article-references__text" id="ref-CR6">Bernardin K, Ogawara K, Ikeuchi K, Dillmann R (2005) A sensor Fusion approach for recognizing continuous human grasping sequences using hidden Markov models. IEEE Trans Robot 21(1):47–57</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTRO.2004.833816" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20sensor%20fusion%20approach%20for%20recognizing%20continuous%20human%20grasping%20sequences%20using%20hidden%20Markov%20models&amp;journal=IEEE%20Trans%20Robot&amp;volume=21&amp;issue=1&amp;pages=47-57&amp;publication_year=2005&amp;author=Bernardin%2CK&amp;author=Ogawara%2CK&amp;author=Ikeuchi%2CK&amp;author=Dillmann%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Calinon S, Billard A (2004) Stochastic gesture production and recognition model for a humanoid robot. In: IEEE" /><p class="c-article-references__text" id="ref-CR7">Calinon S, Billard A (2004) Stochastic gesture production and recognition model for a humanoid robot. In: IEEE/RSJ Intl conference on intelligent robots and systems (IROS), pp 2769–2744, Sendai, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MR. Cutkosky, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Trans " /><p class="c-article-references__text" id="ref-CR8">Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Trans Robot Autom 5(3):269–279</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1002007" aria-label="View reference 8 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.34763" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20grasp%20choice%2C%20grasp%20models%2C%20and%20the%20design%20of%20hands%20for%20manufacturing%20tasks&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=5&amp;issue=3&amp;pages=269-279&amp;publication_year=1989&amp;author=Cutkosky%2CMR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cutkosky MR, Howe RD (1990) Human grasp choice and robotic grasp analysis. Dextrous Robot Hands, chap 1, pp 11" /><p class="c-article-references__text" id="ref-CR9">Cutkosky MR, Howe RD (1990) Human grasp choice and robotic grasp analysis. Dextrous Robot Hands, chap 1, pp 111–29. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Floriani, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="de Floriani L (1989) Feature extraction from boundary models of three-dimensional objects. IEEE Trans Pattern " /><p class="c-article-references__text" id="ref-CR14">de Floriani L (1989) Feature extraction from boundary models of three-dimensional objects. IEEE Trans Pattern Anal Mach Intell 11(8):785–798</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Feature%20extraction%20from%20boundary%20models%20of%20three-dimensional%20objects&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=11&amp;issue=8&amp;pages=785-798&amp;publication_year=1989&amp;author=Floriani%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ehrennmann M, Ambela D, Steinaus P, Dillmann R (2000) A comparison of four fast vision based object recognitio" /><p class="c-article-references__text" id="ref-CR10">Ehrennmann M, Ambela D, Steinaus P, Dillmann R (2000) A comparison of four fast vision based object recognition methods for programming by demonstration applications. In: IEEE international conference on robotics and automation, (ICRA), San Francisco, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekvall S, Kragić D (2004) Interactive grasp learning based on human demonstration. In IEEE international confe" /><p class="c-article-references__text" id="ref-CR11">Ekvall S, Kragić D (2004) Interactive grasp learning based on human demonstration. In IEEE international conference on robotics and automation, (ICRA) New Orleans, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekvall S, Kragić D (2005) Grasp recognition for programming by demonstration. In: IEEE Intl conference on robo" /><p class="c-article-references__text" id="ref-CR12">Ekvall S, Kragić D (2005) Grasp recognition for programming by demonstration. In: IEEE Intl conference on robotics and automation, (ICRA) Barcelona, Spain</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ferrari C, Canny J (1992) Planning optimal grasps. In: IEEE international conference on robotics and automatio" /><p class="c-article-references__text" id="ref-CR13">Ferrari C, Canny J (1992) Planning optimal grasps. In: IEEE international conference on robotics and automation, (ICRA) pp 2290–2295, Nice, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary op" /><p class="c-article-references__text" id="ref-CR15">Friedrich H, Grossmann V, Ehrenmann M, Rogalla O, Zollner R, Dillmann R (1999) Towards cognitive elementary operators: grasp classification using neural network classifiers. In: IASTED international conference on intelligent systems and control, Santa Barbara, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of c" /><p class="c-article-references__text" id="ref-CR16">Heumer G, Ben Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves a comparison of classification methods. In: IEEE virtual reality, pp 19–26, Charlotte, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hirano Y, Kitahama K, Yoshizawa S (2005) Image-based object recognition and dexterous hand/arm motion planning" /><p class="c-article-references__text" id="ref-CR17">Hirano Y, Kitahama K, Yoshizawa S (2005) Image-based object recognition and dexterous hand/arm motion planning using RRTs for grasping in cluttered scene. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hsiao K, Lozano-Pérez T (2006) Imitation learning of whole-body grasps. In: IEEE/RSJ international conference " /><p class="c-article-references__text" id="ref-CR18">Hsiao K, Lozano-Pérez T (2006) Imitation learning of whole-body grasps. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iberall T (1987) The nature of human prehension: Three dextrous hands in one. In: IEEE international conferenc" /><p class="c-article-references__text" id="ref-CR19">Iberall T (1987) The nature of human prehension: Three dextrous hands in one. In: IEEE international conference on robotics and automation, (ICRA) pp 396–401</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Ikeuchi, T. Suehiro, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral" /><p class="c-article-references__text" id="ref-CR20">Ikeuchi K, Suehiro T (1994) Toward an assembly plan from observation, Part I: task recognition with polyhedral objects. IEEE Trans Robot Autom 10(3):368–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.294211" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20an%20assembly%20plan%20from%20observation%2C%20Part%20I%3A%20task%20recognition%20with%20polyhedral%20objects&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=10&amp;issue=3&amp;pages=368-385&amp;publication_year=1994&amp;author=Ikeuchi%2CK&amp;author=Suehiro%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jang H, Moradi H, Lee S, Han J (2005) A visibility-based accessibility analysis of the grasp points for real-t" /><p class="c-article-references__text" id="ref-CR21">Jang H, Moradi H, Lee S, Han J (2005) A visibility-based accessibility analysis of the grasp points for real-time manipulation. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SB. Kang, K. Ikeuchi, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Kang SB, Ikeuchi K (1997) Toward automatic robot instruction from perception-mapping human grasps to manipulat" /><p class="c-article-references__text" id="ref-CR22">Kang SB, Ikeuchi K (1997) Toward automatic robot instruction from perception-mapping human grasps to manipulator grasps. IEEE Trans Robot Autom 13(1):81–95</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.554349" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20automatic%20robot%20instruction%20from%20perception-mapping%20human%20grasps%20to%20manipulator%20grasps&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=13&amp;issue=1&amp;pages=81-95&amp;publication_year=1997&amp;author=Kang%2CSB&amp;author=Ikeuchi%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferen" /><p class="c-article-references__text" id="ref-CR23">Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality (IWAR)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kitahama K, Tsukada A, Galpin F, Matsubara T, Hirano Y (2006) Vision-based scene representation for 3D interac" /><p class="c-article-references__text" id="ref-CR24">Kitahama K, Tsukada A, Galpin F, Matsubara T, Hirano Y (2006) Vision-based scene representation for 3D interaction of service robots. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Beijing, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee S, Jang D, Kim E, Hong S, Han J (2005) A real-time 3D workspace modeling with stereo camera. In: IEEE/RSJ " /><p class="c-article-references__text" id="ref-CR25">Lee S, Jang D, Kim E, Hong S, Han J (2005) A real-time 3D workspace modeling with stereo camera. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Lloyd, JS. Beis, DK. Pai, DG. Lowe, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Lloyd E, Beis JS, Pai DK, Lowe DG (1999) Programming contact tasks using a reality-based virtual environment i" /><p class="c-article-references__text" id="ref-CR26">Lloyd E, Beis JS, Pai DK, Lowe DG (1999) Programming contact tasks using a reality-based virtual environment integrated with vision. IEEE Trans Robot Autom 15(3):423–434</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.768176" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Programming%20contact%20tasks%20using%20a%20reality-based%20virtual%20environment%20integrated%20with%20vision&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=15&amp;issue=3&amp;pages=423-434&amp;publication_year=1999&amp;author=Lloyd%2CE&amp;author=Beis%2CJS&amp;author=Pai%2CDK&amp;author=Lowe%2CDG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Matas J, Soh LM, Kittler J (1997) Object recognition using a tag. In: International conference on image proces" /><p class="c-article-references__text" id="ref-CR27">Matas J, Soh LM, Kittler J (1997) Object recognition using a tag. In: International conference on image processing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Miller AT, Allen PK (2000) Graspit!: a versatile simulator for grasp analysis. In: ASME international mechanic" /><p class="c-article-references__text" id="ref-CR28">Miller AT, Allen PK (2000) Graspit!: a versatile simulator for grasp analysis. In: ASME international mechanical engineering congress, pp 1251–1258, Orlando, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morales A, Asfour T, Azad P, Knoop S, Dillmann R (2006) Integrated grasp planning and visual object localizati" /><p class="c-article-references__text" id="ref-CR29">Morales A, Asfour T, Azad P, Knoop S, Dillmann R (2006) Integrated grasp planning and visual object localization for a humanoid robot with five-fingered hands. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS), Beijing, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Ogata, T. Takahashi, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ogata H, Takahashi T (1994) Robotic assembly operation teaching in a virtual environment. IEEE Trans Robot Aut" /><p class="c-article-references__text" id="ref-CR30">Ogata H, Takahashi T (1994) Robotic assembly operation teaching in a virtual environment. IEEE Trans Robot Autom 10(3):391–399</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.294213" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robotic%20assembly%20operation%20teaching%20in%20a%20virtual%20environment&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=10&amp;issue=3&amp;pages=391-399&amp;publication_year=1994&amp;author=Ogata%2CH&amp;author=Takahashi%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Piegl, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Piegl L (1991) On NURBS: a survey. IEEE Comput Graph Appl 11(1):55–71" /><p class="c-article-references__text" id="ref-CR31">Piegl L (1991) On NURBS: a survey. IEEE Comput Graph Appl 11(1):55–71</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.67702" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20NURBS%3A%20a%20survey&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=11&amp;issue=1&amp;pages=55-71&amp;publication_year=1991&amp;author=Piegl%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Takahashi T, Sakai T (1991) Teaching robot’s movement in virtual reality. In: IEEE/RSJ international workshop " /><p class="c-article-references__text" id="ref-CR32">Takahashi T, Sakai T (1991) Teaching robot’s movement in virtual reality. In: IEEE/RSJ international workshop on intelligent robots and systems, (IROS)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Venkataraman ST, Iberall T (eds) (1990) Dextrous robot hands. Springer, New York" /><p class="c-article-references__text" id="ref-CR33">Venkataraman ST, Iberall T (eds) (1990) Dextrous robot hands. Springer, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dextrous%20robot%20hands&amp;publication_year=1990">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wojtara T, Nonami K (2004) Hand posture detection by neural network and grasp mapping for a master slave hand " /><p class="c-article-references__text" id="ref-CR34">Wojtara T, Nonami K (2004) Hand posture detection by neural network and grasp mapping for a master slave hand system. In: IEEE/RSJ international conference on intelligent robots and systems (IROS) pp 866–871, Sendai, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wong AKC, Rong L, Liang X (1998) Robotic vision: 3D object recognition and pose determination. In: IEEE/RSJ in" /><p class="c-article-references__text" id="ref-CR35">Wong AKC, Rong L, Liang X (1998) Robotic vision: 3D object recognition and pose determination. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS) Victoria, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zöllner R, Rogalla O, Dillmann R (2001) Integration of tactile sensors in a programming by demonstration syste" /><p class="c-article-references__text" id="ref-CR36">Zöllner R, Rogalla O, Dillmann R (2001) Integration of tactile sensors in a programming by demonstration system. In: IEEE international conference on robotics and automation, (ICRA)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zöllner R, Rogalla O, Dillmann R, Zöllner M (2002) Understanding users intention: programming fine manipulatio" /><p class="c-article-references__text" id="ref-CR37">Zöllner R, Rogalla O, Dillmann R, Zöllner M (2002) Understanding users intention: programming fine manipulation tasks by demonstration. In: IEEE/RSJ international conference on intelligent robots and systems, (IROS)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0172-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research has been partially supported by Laboratory AER-TECH of Regione Emilia-Romagna, Italy.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Dipartimento di Ingegneria dell’Informazione, University of Parma, Parma, Italy</p><p class="c-article-author-affiliation__authors-list">Jacopo Aleotti &amp; Stefano Caselli</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jacopo-Aleotti"><span class="c-article-authors-search__title u-h3 js-search-name">Jacopo Aleotti</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jacopo+Aleotti&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jacopo+Aleotti" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jacopo+Aleotti%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Stefano-Caselli"><span class="c-article-authors-search__title u-h3 js-search-name">Stefano Caselli</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Stefano+Caselli&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stefano+Caselli" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stefano+Caselli%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0172-8/email/correspondent/c1/new">Jacopo Aleotti</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Grasp%20programming%20by%20demonstration%20in%20virtual%20reality%20with%20automatic%20environment%20reconstruction&amp;author=Jacopo%20Aleotti%20et%20al&amp;contentID=10.1007%2Fs10055-010-0172-8&amp;publication=1359-4338&amp;publicationDate=2010-11-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Aleotti, J., Caselli, S. Grasp programming by demonstration in virtual reality with automatic environment reconstruction.
                    <i>Virtual Reality</i> <b>16, </b>87–104 (2012). https://doi.org/10.1007/s10055-010-0172-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0172-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-06-12">12 June 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-09-07">07 September 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-20">20 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-06">June 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0172-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0172-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Environment modeling</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Grasp programming</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Glove interaction</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0172-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=172;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

