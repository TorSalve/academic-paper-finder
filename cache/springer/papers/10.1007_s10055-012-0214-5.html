<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Influence of contextual objects on spatial interactions and viewpoints"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, users must develop a common representation..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2012 17:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-09-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, users must develop a common representation of their shared virtual space. In this work, we investigated spatial communication in virtual environments. In order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. We conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. Forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. The participants were asked to perform in dyads an object co-manipulation task. The results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. These results allow us to provide some design recommendations for CVEs for object manipulation tasks."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-09-12"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="15"/>

    <meta name="prism.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-012-0214-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-012-0214-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-012-0214-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-012-0214-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2013/03"/>

    <meta name="citation_online_date" content="2012/09/12"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="15"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-012-0214-5"/>

    <meta name="DOI" content="10.1007/s10055-012-0214-5"/>

    <meta name="citation_doi" content="10.1007/s10055-012-0214-5"/>

    <meta name="description" content="Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, u"/>

    <meta name="dc.creator" content="Amine Chellali"/>

    <meta name="dc.creator" content="Isabelle Milleville-Pennel"/>

    <meta name="dc.creator" content="C&#233;dric Dumas"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bridgeman B (1999) Separate representations of visual space for perception and visually guided behavior. In: Aschersleben G, Bachmann T, M&#252;sseler J (eds) Cognitive contribution to the perception of spatial and temporal events, pp 3&#8211;18"/>

    <meta name="citation_reference" content="Chellali A, Milleville-Pennel I, Dumas C (2008) Elaboration of a common frame of reference in collaborative virtual environments. In: Abascal J, Fajardo I, Oakley I (eds) Proceedings of the 15th European conference on cognitive ergonomics: the ergonomics of cool interaction, Funchal, 16&#8211;19 Sept 2008, vol 369, pp 83&#8211;90"/>

    <meta name="citation_reference" content="Churcher N, Churcher C (1996) A collaborative approach to GIS. In: Proceedings of the 8th annual colloquium of the spatial information research centre, pp 156&#8211;163"/>

    <meta name="citation_reference" content="citation_title=Using language; citation_publication_date=1996; citation_id=CR4; citation_author=HH Clark; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_title=Grounding in communication; citation_inbook_title=Cognition, perspectives on socially shared; citation_publication_date=1991; citation_pages=127-149; citation_id=CR5; citation_author=H Clark; citation_author=S Brennan; citation_publisher=American Psychological Association"/>

    <meta name="citation_reference" content="citation_title=What do you mean by collaborative learning?; citation_inbook_title=Collaborative learning: cognitive and computational approaches; citation_publication_date=1999; citation_pages=1-19; citation_id=CR6; citation_author=P Dillenbourg; citation_publisher=Elsevier"/>

    <meta name="citation_reference" content="Dillenbourg P, Baker M, Blaye A, Malley CO (1996) The evolution of research on collaborative learning. In: Spada E, Reiman P (eds) Learning 117(1):189&#8211;211. Elsevier"/>

    <meta name="citation_reference" content="Erickson T (1993) From interface to interplace: the spatial environment as a medium for interaction. In: Proceedings of the conference on spatial information theory"/>

    <meta name="citation_reference" content="Gaver B (1992) The affordances of media spaces for collaboration. In: Proceedings of the ACM conference on computer support cooperative work (CSCW), Toronto"/>

    <meta name="citation_reference" content="Gaver WW, Sellen A, Heath C, Luff P (1993) One is not enough: multiple views in a media space. In: Proceedings of INTERCHI, pp 335&#8211;341"/>

    <meta name="citation_reference" content="citation_title=The ecological approach to visual perception; citation_publication_date=1979; citation_id=CR11; citation_author=JJ Gibson; citation_publisher=Houghton Mifflin"/>

    <meta name="citation_reference" content="Harrison S, Dourish P (1996) Re-place-ing space: the roles of place and space in collaborative systems. In: Proceedings of the 1996 ACM conference on computer supported cooperative work. ACM, pp 67&#8211;76"/>

    <meta name="citation_reference" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual orientation in virtual environments. In: Proceedings of the ACM 1998 conference on computer-supported cooperative work, pp 217&#8211;226"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Object-focused interaction in collaborative virtual environments; citation_author=J Hindmarsh, M Fraser, C Heath, S Benford, C Greenhalagh; citation_volume=7; citation_issue=4; citation_publication_date=2000; citation_pages=477-509; citation_doi=10.1145/365058.365088; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Towards a cognitive approach to human-machine cooperation in dynamic situations; citation_author=J-M Hoc; citation_volume=54; citation_publication_date=2001; citation_pages=509-540; citation_doi=10.1006/ijhc.2000.0454; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_title=Cerveau d&#8217;homme et cerveau de femme?; citation_publication_date=2001; citation_id=CR16; citation_author=D Kimura; citation_publisher=Odile Jacob"/>

    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Development of a superior frontal-intraparietal network for visuo-spatial working memory; citation_author=T Klingberg; citation_volume=44; citation_publication_date=2006; citation_pages=2171-2177; citation_doi=10.1016/j.neuropsychologia.2005.11.019; citation_id=CR17"/>

    <meta name="citation_reference" content="Kolb B, Whishaw I (2002) Cerveau et comportement. De Boeck Universit&#233;"/>

    <meta name="citation_reference" content="citation_journal_title=Brain Cogn; citation_title=Sex differences in parietal lobe morphology: relationship to mental rotation performance; citation_author=T Koscik, D O&#8217;leary, DJ Moser, NC Andreasen, P Nopoilos; citation_volume=69; citation_publication_date=2009; citation_pages=451-459; citation_doi=10.1016/j.bandc.2008.09.004; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Sex Roles; citation_title=Gender and regional differences in spatial referents used in direction giving; citation_author=CA Lawton; citation_volume=44; citation_publication_date=2001; citation_pages=321-337; citation_doi=10.1023/A:1010981616842; citation_id=CR20"/>

    <meta name="citation_reference" content="Park K, Kapoor A, Scharver C, Leigh J (2000) Exploiting multiple perspectives in tele-immersion. In: Proceedings of the 4th immersive projection technology workshop, Ames"/>

    <meta name="citation_reference" content="citation_journal_title=Child Dev; citation_title=Developmental differences in giving directions: spatial frames of reference and mental rotation; citation_author=R Roberts, C Aman; citation_volume=64; citation_publication_date=1993; citation_pages=1258-1270; citation_doi=10.2307/1131338; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_title=Construction of shared knowledge in collaborative problem solving; citation_inbook_title=Computer-supported collaborative learning; citation_publication_date=1995; citation_pages=69-97; citation_id=CR24; citation_author=J Roschelle; citation_author=SD Teasley; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Spante M, Schroeder R, Axelsson AS (2004) How putting yourself into the other person&#8217;s virtual shoes enhances collaboration. In: Proceedings of the 7th international workshop on presence, Valencia, pp 190&#8211;196"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Inf Syst; citation_title=WYSIWIS revised: early experiences with multiuser interfaces; citation_author=M Stefik, DG Bobrow, S Lanning, D Tatar; citation_volume=5; citation_issue=2; citation_publication_date=1987; citation_pages=147-167; citation_doi=10.1145/27636.28056; citation_id=CR26"/>

    <meta name="citation_reference" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of human factors and computer systems, pp 265&#8211;272"/>

    <meta name="citation_author" content="Amine Chellali"/>

    <meta name="citation_author_email" content="amine.chellali@gmail.com"/>

    <meta name="citation_author_institution" content="Cambridge, USA"/>

    <meta name="citation_author_institution" content="Ecole des Mines de Nantes-IRCCYN, Nantes, France"/>

    <meta name="citation_author" content="Isabelle Milleville-Pennel"/>

    <meta name="citation_author_email" content="Isabelle.Milleville-Pennel@irccyn.ec-nantes.fr"/>

    <meta name="citation_author_institution" content="CNRS-IRCCyN, Nantes, France"/>

    <meta name="citation_author" content="C&#233;dric Dumas"/>

    <meta name="citation_author_email" content="Cedric.Dumas@csiro.au"/>

    <meta name="citation_author_institution" content="
Ecole des Mines de Nantes, Nantes, France"/>

    <meta name="citation_author_institution" content="
CSIRO, Brisbane, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-012-0214-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-012-0214-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments"/>
        <meta property="og:description" content="Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, users must develop a common representation of their shared virtual space. In this work, we investigated spatial communication in virtual environments. In order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. We conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. Forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. The participants were asked to perform in dyads an object co-manipulation task. The results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. These results allow us to provide some design recommendations for CVEs for object manipulation tasks."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-012-0214-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Spatial communication, Virtual environment, Collaboration, Common frame of reference, Visual landmarks","kwrd":["Spatial_communication","Virtual_environment","Collaboration","Common_frame_of_reference","Visual_landmarks"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-012-0214-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-012-0214-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=214;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-012-0214-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0214-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0214-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-09-12" itemprop="datePublished">12 September 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Amine-Chellali" data-author-popup="auth-Amine-Chellali" data-corresp-id="c1">Amine Chellali<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="" /><meta itemprop="address" content="1493 Cambridge Street, Cambridge, MA, 02139, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ecole des Mines de Nantes-IRCCYN" /><meta itemprop="address" content="grid.424465.5, 0000000123230456, Ecole des Mines de Nantes-IRCCYN, Nantes, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Isabelle-Milleville_Pennel" data-author-popup="auth-Isabelle-Milleville_Pennel">Isabelle Milleville-Pennel</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS-IRCCyN" /><meta itemprop="address" content="CNRS-IRCCyN, Nantes, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-C_dric-Dumas" data-author-popup="auth-C_dric-Dumas">Cédric Dumas</a></span><sup class="u-js-hide"><a href="#Aff4">4</a>,<a href="#Aff5">5</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="&#xA;Ecole des Mines de Nantes" /><meta itemprop="address" content="grid.424465.5, 0000000123230456, &#xA;Ecole des Mines de Nantes, Nantes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="&#xA;CSIRO" /><meta itemprop="address" content="grid.1016.6, &#xA;CSIRO, Brisbane, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">1</span>–<span itemprop="pageEnd">15</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">399 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-012-0214-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Collaborative virtual environments (CVEs) are 3D spaces in which users share virtual objects, communicate, and work together. To collaborate efficiently, users must develop a common representation of their shared virtual space. In this work, we investigated spatial communication in virtual environments. In order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. We conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. Forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. The participants were asked to perform in dyads an object co-manipulation task. The results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. These results allow us to provide some design recommendations for CVEs for object manipulation tasks.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In everyday life, spatial interactions are ruled by the operators’ perception of their surrounding space. An operator relies on this knowledge to communicate spatial information to peers. Describing an object’s position, for example, will depend on different parameters: the object position, the speaker’s and the listener’s point of view, etc. However, these kinds of tasks become hard to perform in virtual environments. Indeed, virtual environments change the users’ perception of the surrounding space. This leads to some communication problems when two partners perform a spatial task together (such as co-manipulating a shared object) in a collaborative virtual environment (CVE). From this perspective, the design of virtual environments that support collaborative spatial activities remains an open issue. This paper deals with communication problems during spatial interactions in virtual environments. Our focus is to study communication during synchronous spatial tasks from a human factor point of view. Our goal is to demonstrate that the contents of shared virtual environments can be used to support spatial communication and improve the users’ mutual understanding of the shared space. Before addressing problems arising during collaborative manipulation tasks in virtual environments, we will give some description first, in order to bring out the link between the CVE characteristics and the spatial communication problems during collaboration in these environments.</p><h3 class="c-article__sub-heading" id="Sec2">Common frame of reference</h3><p>To support collaborative interactions in virtual environments, it is important to identify the main components of successful collaboration. When two operators collaborate in the real world, they construct a common frame of reference (COFOR): a common mental representation of the shared situation (Hoc <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hoc J-M (2001) Towards a cognitive approach to human-machine cooperation in dynamic situations. Int J Hum Comput Stud 54:509–540" href="/article/10.1007/s10055-012-0214-5#ref-CR15" id="ref-link-section-d10158e388">2001</a>). It represents a common scale: a language or a system of notions to which one can refer in case of debate. The COFOR is developed through a continuous (explicit and implicit) exchange of information. It allows partners to understand each other and to organize their common work in order to perform different but complementary actions.</p><p>The COFOR is similar to other concepts defined in literature such as the <i>Common ground</i> (Clark and Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark H, Brennan S (1991) Grounding in communication. In: Resnick L, Levine J, Teasley S (eds) Cognition, perspectives on socially shared. American Psychological Association, Washington, pp 127–149" href="/article/10.1007/s10055-012-0214-5#ref-CR5" id="ref-link-section-d10158e397">1991</a>) and the <i>Joint problem space</i> (Roschelle and Teasley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Roschelle J, Teasley SD (1995) Construction of shared knowledge in collaborative problem solving. In: O’Malley C (ed) Computer-supported collaborative learning. Springer, New York, pp 69–97" href="/article/10.1007/s10055-012-0214-5#ref-CR24" id="ref-link-section-d10158e403">1995</a>). These concepts highlight the role of constructing and maintaining a shared comprehension of a problem to ensure efficient collaboration. During collaboration, each participant must (Dillenbourg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dillenbourg P (1999) What do you mean by collaborative learning? In: Dillenbourg P (ed) Collaborative learning: cognitive and computational approaches. Elsevier, Oxford, pp 1–19" href="/article/10.1007/s10055-012-0214-5#ref-CR6" id="ref-link-section-d10158e406">1999</a>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Compare his/her own mental representation to his/her partner’s representation.</p>
                    </li>
                    <li>
                      <p>Exchange understanding cues with his/her partner.</p>
                    </li>
                    <li>
                      <p>Be aware of the possible divergences of opinions or representations.</p>
                    </li>
                    <li>
                      <p>Be aware of the possibility to overcome those divergences.</p>
                    </li>
                  </ul><p>In order to construct and update the COFOR, the partners rely on the grounding process (Clark and Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark H, Brennan S (1991) Grounding in communication. In: Resnick L, Levine J, Teasley S (eds) Cognition, perspectives on socially shared. American Psychological Association, Washington, pp 127–149" href="/article/10.1007/s10055-012-0214-5#ref-CR5" id="ref-link-section-d10158e433">1991</a>): a continuous exchange of information and understanding cues. The grounding process determines the characteristics of the COFOR: it is dynamic and evolves in time. During the grounding process, partners provide collaborative efforts to understand each other (Clark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Clark HH (1996) Using language. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-012-0214-5#ref-CR4" id="ref-link-section-d10158e436">1996</a>). These efforts must be reduced in order to improve the COFOR construction (the least collaborative effort principle; Clark and Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark H, Brennan S (1991) Grounding in communication. In: Resnick L, Levine J, Teasley S (eds) Cognition, perspectives on socially shared. American Psychological Association, Washington, pp 127–149" href="/article/10.1007/s10055-012-0214-5#ref-CR5" id="ref-link-section-d10158e439">1991</a>). To do so, operators must improve their communication by exploiting the features of their environment (Harrison and Dourish <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Harrison S, Dourish P (1996) Re-place-ing space: the roles of place and space in collaborative systems. In: Proceedings of the 1996 ACM conference on computer supported cooperative work. ACM, pp 67–76" href="/article/10.1007/s10055-012-0214-5#ref-CR12" id="ref-link-section-d10158e442">1996</a>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>The common spatial organization and knowledge of the world (e.g., up is toward the sky).</p>
                    </li>
                    <li>
                      <p>Their proximity to the objects and to the persons with which they interact.</p>
                    </li>
                    <li>
                      <p>The sense of others’ presence and the awareness of the partner’s ongoing activities.</p>
                    </li>
                  </ul><p>When partners are face-to-face, the features listed above allow them to organize their behaviors and to improve their collaboration. Hence, they can communicate through two complementary components: an explicit component (e.g., verbal communication) and an implicit component (non-verbal cues including body movements, facial expressions, touch, eye contact, etc.). These forms of communication allow them to reduce their collaborative efforts. But how can these features be exploited in a CVE? This issue is addressed hereafter.</p><h3 class="c-article__sub-heading" id="Sec3">The common frame of reference construction in virtual environments</h3><p>Communication in virtual environments is different from face-to-face communication. Indeed, some of the features listed above become hard to use directly due to some characteristics of the virtual environments:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>The operators do not necessarily share the same viewpoint and are not necessarily aware of their partner’s viewpoint.</p>
                    </li>
                    <li>
                      <p>The partners are usually (physically) distant from each other.</p>
                    </li>
                    <li>
                      <p>The feedback about the partner’s ongoing activities is limited.</p>
                    </li>
                    <li>
                      <p>The users usually interact with distant objects.</p>
                    </li>
                  </ul><p>These characteristics may affect the grounding process and constrain the collaborative activities. Furthermore, communication means in virtual environments are often limited to the explicit component. For instance, body movements, touch, and facial expressions are hard to reproduce faithfully in virtual environments. Operators use a different form of communication, which must take into account the characteristics of the CVE in order to reduce their collaborative effort. In spite of the new interaction possibilities supported by the shared virtual environments (meeting distant collaborators, co-manipulating virtual objects, etc.), the construction of the COFOR in a CVE remains mainly dependent on verbal communication. Therefore, if the virtual environment does not enhance the verbal communications, some ambiguities can occur between the distant collaborators.</p><p>The goal of this work is to highlight communication problems during performing spatial tasks in CVE. Indeed, these tasks require an efficient spatial communication (e.g., referring to the objects in the environment, describing actions in the space, etc.). For that purpose, we improved the CVE design to help the users to develop an efficient spatial COFOR when performing such a task. Before addressing the design concerns, it is important to understand the spatial communication problems in CVE. This is discussed in the following section.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Manipulation tasks in virtual environments</h2><div class="c-article-section__content" id="Sec4-content"><p>When performing an object manipulation task (such as grasping or moving objects; Bridgeman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bridgeman B (1999) Separate representations of visual space for perception and visually guided behavior. In: Aschersleben G, Bachmann T, Müsseler J (eds) Cognitive contribution to the perception of spatial and temporal events, pp 3–18" href="/article/10.1007/s10055-012-0214-5#ref-CR1" id="ref-link-section-d10158e505">1999</a>), an operator has a personal viewpoint of his environment. To perform these tasks correctly, he/she has to continually change this viewpoint (Churcher and Churcher <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Churcher N, Churcher C (1996) A collaborative approach to GIS. In: Proceedings of the 8th annual colloquium of the spatial information research centre, pp 156–163" href="/article/10.1007/s10055-012-0214-5#ref-CR3" id="ref-link-section-d10158e508">1996</a>; Stoakley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of human factors and computer systems, pp 265–272" href="/article/10.1007/s10055-012-0214-5#ref-CR27" id="ref-link-section-d10158e511">1995</a>). Indeed, he/she must be able to explore the surrounding space in order to define his/her personal representation of the environment (this includes objects shapes, positions, etc.). This allows him/her to plan his/her actions in the environment. To make spatial descriptions of the environment, the operator then uses the egocentric reference system (Bridgeman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bridgeman B (1999) Separate representations of visual space for perception and visually guided behavior. In: Aschersleben G, Bachmann T, Müsseler J (eds) Cognitive contribution to the perception of spatial and temporal events, pp 3–18" href="/article/10.1007/s10055-012-0214-5#ref-CR1" id="ref-link-section-d10158e514">1999</a>). In such a reference system, actions and objects positions are described according to the operator’s own perspective (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The problem of using an egocentric reference system in a collaborative virtual environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In a collaborative situation, partners do not necessarily share the same viewpoint of their common environment. Hence, they can misunderstand problems between each other when performing an object manipulation task together. In fact, spatial information like “left/right” depends on each user’s viewpoint and might not be equivalent.</p><p>When partners are face-to-face, they have a direct access to the shared spatial information. Indeed, they are usually aware of their partner’s field of view and gaze direction. Hence, they can adapt all the spatial descriptions according to their partner’s perspective to avoid misunderstandings. Furthermore, they use non-verbal clues (such as pointing gestures and gaze direction) to strengthen the spatial information exchanges.</p><p>As in the real world, operators use egocentric reference systems to perform manipulation tasks in virtual environments (Roberts and Aman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Roberts R, Aman C (1993) Developmental differences in giving directions: spatial frames of reference and mental rotation. Child Dev 64:1258–1270" href="/article/10.1007/s10055-012-0214-5#ref-CR23" id="ref-link-section-d10158e546">1993</a>). However, in virtual environments, distant users are often unaware of their partner’s position in space since they cannot see each other. This leads to misunderstanding when the personal viewpoints are not consistent and the representations of the space are contradictory (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig1">1</a>). To avoid these communication problems, two kinds of paradigms have been described in literature:</p><h3 class="c-article__sub-heading" id="Sec5">Sharing the same viewpoint</h3><p>To avoid misunderstanding, the WYSIWIS (What You See Is What I See) paradigm in which all users have the same viewpoint of the environment (Stefik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Stefik M, Bobrow DG, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. ACM Trans Inf Syst 5(2):147–167" href="/article/10.1007/s10055-012-0214-5#ref-CR26" id="ref-link-section-d10158e560">1987</a>) has been used. Here, the partners must be aware of this viewpoint sharing, allowing them to understand each other while they locate objects according to their own perspective. However, this constrains the task performing in a synchronous collaborative task as users cannot freely choose a personal viewpoint when manipulating objects. Indeed, the partners need to manipulate different objects and to reach personal sub goals (Stefik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Stefik M, Bobrow DG, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. ACM Trans Inf Syst 5(2):147–167" href="/article/10.1007/s10055-012-0214-5#ref-CR26" id="ref-link-section-d10158e563">1987</a>). This solution also becomes useless when one user wants to change the common viewpoint without forewarning his partner (Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Park K, Kapoor A, Scharver C, Leigh J (2000) Exploiting multiple perspectives in tele-immersion. In: Proceedings of the 4th immersive projection technology workshop, Ames" href="/article/10.1007/s10055-012-0214-5#ref-CR21" id="ref-link-section-d10158e566">2000</a>).</p><h3 class="c-article__sub-heading" id="Sec6">The use of different viewpoints</h3><p>An alternative solution is to provide each user with a personal viewpoint of the environment. To avoid misunderstandings in this case, each partner must have information about their partner’s viewpoint. This allows adaption to spatial information according to the partner’s position. In the literature, this can be illustrated through two different paradigms:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Providing feedback about the partner’s viewpoint: In this case, the partner’s visual space is displayed on a separate juxtaposed window. This provides each operator with feedback on his/her partner’s perspective and actions in the virtual environment, and also information on objects present in their partner’s field of view. This allows the collaborators to make a spatial description of the environment and improves their spatial communication (Hindmarsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual orientation in virtual environments. In: Proceedings of the ACM 1998 conference on computer-supported cooperative work, pp 217–226" href="/article/10.1007/s10055-012-0214-5#ref-CR13" id="ref-link-section-d10158e583">1998</a>; Spante et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Spante M, Schroeder R, Axelsson AS (2004) How putting yourself into the other person’s virtual shoes enhances collaboration. In: Proceedings of the 7th international workshop on presence, Valencia, pp 190–196" href="/article/10.1007/s10055-012-0214-5#ref-CR25" id="ref-link-section-d10158e586">2004</a>). However, adding separate windows increases the cognitive workload and slows down the task completion time. Gaver et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Gaver WW, Sellen A, Heath C, Luff P (1993) One is not enough: multiple views in a media space. In: Proceedings of INTERCHI, pp 335–341" href="/article/10.1007/s10055-012-0214-5#ref-CR10" id="ref-link-section-d10158e589">1993</a>) showed that spatial communication and object designation become highly problematic due to the additional windows display. The authors showed that the juxtaposed windows lead to a fragmentation of the visual workspace. This fragmentation leads to users’ difficulties in matching the different views.</p>
                    </li>
                    <li>
                      <p>Using avatars: Embodiments (or avatars) are virtual representations of the users in virtual environments. They can have different forms and are often used in CVE to overcome spatial communication problems. Indeed, representing each user within the virtual environment through an avatar helps the partners to know each other’s position and to be aware of the other’s field of view. The orientation of the avatar’s head allows the partner to determine what the user is looking at and the orientation of the hand can be used for pointing objects. However, in some cases, the presence of avatars can hide one’s view (Hindmarsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual orientation in virtual environments. In: Proceedings of the ACM 1998 conference on computer-supported cooperative work, pp 217–226" href="/article/10.1007/s10055-012-0214-5#ref-CR13" id="ref-link-section-d10158e598">1998</a>). In addition, virtual environments offer a restricted field of view compared with what users can experience in face-to-face situations. Subsequently, the users are rarely in a position to see their partner’s embodiment and objects they are referring to simultaneously (Hindmarsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (2000) Object-focused interaction in collaborative virtual environments. ACM Trans Comput Hum Interact 7(4):477–509" href="/article/10.1007/s10055-012-0214-5#ref-CR14" id="ref-link-section-d10158e601">2000</a>). This leads to misunderstanding problems because the users have problems in relating the referred object and the representation of the partner. Finally, as the avatars continually move during the task, this requires the partner to perform complex mental rotations to assess the objects position according to the new partner’s position. This forces each operator to be continually aware of the other’s viewpoint in the virtual environment whenever the partner modifies his/her viewpoint.</p>
                    </li>
                  </ul><p>These two methods demonstrate that spatial communication based on the egocentric reference system is problematic when performing shared manipulation tasks. These difficulties are mainly due the differences in viewpoints between partners. While users can circumvent these misunderstanding by trying to explain their actions verbally, this constrains the collaborative task because users are distracted by this additional verbal activity. In addition, non-verbal cues are limited in CVEs. For instance, pointing gestures are difficult to reproduce at a distance. Hence, object designation also becomes a complex activity that disturbs the main task achievement (Hindmarsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual orientation in virtual environments. In: Proceedings of the ACM 1998 conference on computer-supported cooperative work, pp 217–226" href="/article/10.1007/s10055-012-0214-5#ref-CR13" id="ref-link-section-d10158e607">1998</a>). This increases the collaborative efforts and constrains collaboration.</p><p>This review suggests that new spatial interaction forms must be investigated to optimize spatial communication when performing collaborative spatial tasks in virtual environments. One alternative can be the use of an exocentric reference system to make spatial descriptions of the environment. This alternative is investigated hereafter.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Proposition : the stable lateralized visual landmark paradigm</h2><div class="c-article-section__content" id="Sec7-content"><p>To avoid the problems described above, the stable lateralized visual landmark (SLVL), a new interaction paradigm, was proposed (Chellali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chellali A, Milleville-Pennel I, Dumas C (2008) Elaboration of a common frame of reference in collaborative virtual environments. In: Abascal J, Fajardo I, Oakley I (eds) Proceedings of the 15th European conference on cognitive ergonomics: the ergonomics of cool interaction, Funchal, 16–19 Sept 2008, vol 369, pp 83–90" href="/article/10.1007/s10055-012-0214-5#ref-CR2" id="ref-link-section-d10158e621">2008</a>). This solution is based on a different approach to the problem. Indeed, in a user-centered design process, we propose to focus the design on the objects of interaction (the user’s environment design) before focusing on the user’s viewpoint (the interaction techniques design). According to the “affordance of space” theory (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Gibson JJ (1979) The ecological approach to visual perception. Houghton Mifflin, New York" href="/article/10.1007/s10055-012-0214-5#ref-CR11" id="ref-link-section-d10158e624">1979</a> cited by Gaver <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Gaver B (1992) The affordances of media spaces for collaboration. In: Proceedings of the ACM conference on computer support cooperative work (CSCW), Toronto" href="/article/10.1007/s10055-012-0214-5#ref-CR9" id="ref-link-section-d10158e627">1992</a>), the space structures the actions and the interactions. The objects, as a common focus between two partners, then become an information carrier. The idea is to include contextual clues in the virtual environment to influence the way partners interact with each other. Indeed, we hypothesize that the presence of contextual clues (i.e., visual landmarks) will encourage operators to use an exocentric reference system when making spatial description rather than using an egocentric system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The spatial descriptions in the exocentric reference system are independent from the users’ viewpoints</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The exocentric system is useful in complex situations (different viewpoints, several manipulations of an object). The objects and actions are located according to the environment landmarks, rather than being located according to the user’s perspective (in the egocentric system). Hence, if the visual landmarks are provided, the operators can use them to make spatial descriptions.</p><p>In addition, if the environment landmarks are fixed, the spatial descriptions become completely independent from the user’s viewpoint (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig2">2</a>) while in the egocentric system, each user must be aware of the partner’s actual viewpoint to make a precise description of space. This facilitates the spatial communication and reduces the ambiguities when describing the environments. This also permits reducing the mental rotations necessary to make spatial description. Indeed, only one mental rotation is necessary to locate an object according to the landmark regardless of the speaker’s and the listener’s viewpoints. Subsequently, the partners can construct a more efficient COFOR and better coordinate their spatial interactions.</p><p>Finally, the exocentric reference system is spontaneously used to make a spatial description of a room or to read a map (Erickson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Erickson T (1993) From interface to interplace: the spatial environment as a medium for interaction. In: Proceedings of the conference on spatial information theory" href="/article/10.1007/s10055-012-0214-5#ref-CR8" id="ref-link-section-d10158e662">1993</a>; Kimura <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kimura D (2001) Cerveau d’homme et cerveau de femme?. Odile Jacob, Paris" href="/article/10.1007/s10055-012-0214-5#ref-CR16" id="ref-link-section-d10158e665">2001</a>). Consequently, the use of visual landmarks to locate objects can be done intuitively. This can help the operators focus on the main task since they are less disturbed by the spatial communication and the coordination problems.</p><h3 class="c-article__sub-heading" id="Sec8">Hypotheses</h3><p>In spite of the importance of reference systems in CVE and their influence on the COFOR development, little research has dealt with this problem. The objective of this work is to focus the CVE design on the use of reference systems when the operators make a spatial description of their environment. By studying the spatial communication and the collaboration strategies adopted by the operators when performing a common spatial task, we hypothesize that:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>The stable and lateralized visual landmarks will have an impact on the preferred reference system (exocentric vs. egocentric): the operators will more frequently use the exocentric system to locate objects and describe actions when the fixed and lateralized visual landmarks are provided.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>The reference system being used will have an impact on spatial communication and on the COFOR. This has direct impacts on:
</p><ul class="u-list-style-bullet">
                            <li>
                              <p>The sharing of the viewpoints and the navigation on the virtual environment: Since the users have a more global perception of the surrounding environment in the exocentric system, they are expected to decrease their viewpoints changing. As the spatial communication is independent from the speaker’s position, the users will not try to frequently share a common viewpoint of the environment.</p>
                            </li>
                            <li>
                              <p>The referenciation of objects, the descriptions of actions, and the collaboration in the shared virtual environment: we hypothesize that a better mutual understanding will increase the users’ feeling of being a member of a collaborative team and will facilitate the descriptions of actions to the partner. We also expect different object referenciation strategies (i.e., how users will make references to the shared objects) depending on the reference system being used.</p>
                            </li>
                          </ul>
                        
                      
                    </li>
                  </ol><p>As a research method to verify these hypotheses, an experimental study on the effects of the exocentric reference system use on COFOR development and on spatial communication in a manipulation task was conducted.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Methods</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10">Participants</h3><p>Forty-four (24 males/20 females) undergraduate engineering students (20–27 years old) from <i>Nantes University</i> and <i>Ecole Centrale de Nantes</i> participated in the experiment. All the participants answered a pre-experimental questionnaire to assess their level of expertise on video games and 3D modeling software (the expertise level was measured by the video games and/or 3D modeling software use frequency estimated by a two 5 degrees scales). Results showed that 10 participants play games at least once a week, 12 participants play at least once a month, and 22 participants play games less than once per month. Subsequently, participants were homogeneously regrouped in 22 dyads according to their level of expertise. The pairs were composed of two participants from the same gender. None of the participants had a prior knowledge, neither of the used virtual environment nor of the objectives of the study. Participants of each pair did not know each other before the experiment and met 5 min before the sessions started. All the students received 10 € for participation in order to motivate them to be more involved in the study.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec11">Apparatus</h3><p>The experimental setup consisted of two personal computers (Intel Pentium 4.2 GHz, 512 MB RAM, and a 3D video card) and two LCD screens (17 inches). The two computers were linked through a direct Ethernet connection. The CVE used for the experiment was developed using the <i>3DVIA Virtools</i> platform from <i>Dassault system</i>. The virtual environment consisted of a simple 3D scene containing a 3D model (yellow) to reproduce, and 6 different white tetramino<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> laying on a table (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The collaborative virtual environment (<i>left</i>: without the visual landmark, <i>right</i>: with the visual landmark)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Each participant could interact with the tetraminos using a 6 DoFs gamepad (3 DoF for translations, 2 DoF for rotations, and 1 DoF to modify the viewpoint). Gamepads allowed each user to select a tetramino, to turn it around the <i>XYZ</i>-axes and to translate it along the <i>XYZ</i>-axes. Two users were allowed to manipulate two different tetraminos at the same time but could not handle the same tetramino at the same time.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec12">The stable lateralized visual landmark</h3><p>According to the experimental conditions (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0214-5#Sec21">5.5</a> below), a virtual character (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig4">4</a>), representing the SLVL, was placed in the scene’s center (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The character representing a stable lateralized visual landmark</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The choice of a humanoid figure as a SLVL was not incidental. Its lateralization (he has one right, one left, one front side, and one backside, cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig4">4</a>) is natural and allows the operators to locate objects according to him. This relative location remains usable and understandable by both operators, even if they do not actually share the same viewpoint. Furthermore, the character was immobile. This requires to use only one mental rotation (to make the user’s point of view consistent with the characters position) in order to make a spatial description, since the users did not have to manage the SLVL displacements.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec13">Experimental setup</h3><p>Each participant was seated in front of a screen and held a gamepad. Partners were separated by a curtain and could not see each other’s screen (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig5">5</a>). This prevented them from using their fingers to point objects on the screen or to have any feedback on their partner’s viewpoint.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Experimental setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Participants were allowed to communicate verbally. This allowed us (through verbalizations analyses) to investigate the use of reference systems for making spatial descriptions. Verbalizations were also used to study the COFOR development. Prior to experimental sessions, participants had some free trials in order to become familiar with the system.</p><p>During the experimental sessions, each pair was asked to reproduce in collaboration 5 different models (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig6">6</a>). To help them rebuild these models, the first tetramino was correctly placed for each model (c.f. the white tetramino for each model, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Models to reproduce</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Moreover, other constraints were introduced to prevent the partners from sharing implicitly the same viewpoint and from having a direct access to their partner’s field of view. In fact, each user’s starting viewpoint was different from his partner’s starting viewpoint for each new model (cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig7">7</a>). Then, users were allowed to freely modify their viewpoints during the task. This helped them to avoid complex mental rotations to manipulate the virtual objects. However, no feedback was given about the other’s viewpoint.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Starting viewpoints</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In addition, several constraints were introduced to prevent the tetraminos from being used as contextual references instead of the stable lateralized visual landmark:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>All the tetraminos had fairly similar geometric shapes,</p>
                    </li>
                    <li>
                      <p>The tetraminos were not lateralized, that is, their right, left, front, or back sides are always dependent on the speakers position,</p>
                    </li>
                    <li>
                      <p>The tetraminos had the same neutral color (white) when they were not handled.</p>
                    </li>
                  </ul><p>A color was assigned to each participant: red for the participant sitting on the left and green for the participant sitting on the right. During the task, when a tetramino was moved, it took the color of the participant who was handling it (red or green). Thus, each participant was aware of his actions and his/her partner’s actions. When the participant dropped the tetramino, it retrieved its original color (white).</p><p>All the constraints described above were introduced in order to isolate the actual effects of the SLVL on collaboration. Finally, although no time limit was imposed, the participants were instructed to perform the task as quick as possible.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec14">Variables and measurements</h3><p>One independent variable (the SLVL) was manipulated (Cf. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab1">1</a>) with two modalities: presence and absence (P-SLVL and A-SLVL, respectively).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Participants’ groups</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Furthermore, participants’ gender was used as a control variable with two modalities: male and female (M and F, respectively; cf. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab1">1</a>). This control variable was introduced to avoid shyness effects within mixed groups.</p><p>The measures for the task performance included task completion time. Furthermore, the mean time of the viewpoints manipulation was calculated in order to investigate the users’ exploration of the VE. In order to investigate the viewpoints sharing, the percentage of time where the partners shared the same viewpoint was calculated. The partners were considered as sharing the same viewpoint when the angle difference between their personal viewpoints was less than twenty degrees (20°).</p><p>Finally, videos of the sessions were recorded. The videos included the verbal discussions and a view of both screens that show all the users actions. All the dialogs between partners were faithfully transcribed. The verbal communications analyses are the main measures to evaluate: the reference systems uses, the COFOR construction, and the objects referenciation.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec15">Statistical analysis</h3><p>According to the small size of the sample, the assumption of normality of the data cannot be verified. Hence, non-parametric statistical tests were used. Both Kruskall–Wallis <i>H</i> test and the Mann–Whitney <i>U</i> test were used to compare groups’ performance and the verbalizations. The level of significance used is <i>p</i> &lt; 0.05. For multiple comparisons, the <i>p</i> values adjustment method with the Bonferroni correction was used. The chi-square tests were used to investigate whether the percentage of use of reference systems, verbalizations, pronouns, and manipulated objects referenciation differ by group.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Results</h2><div class="c-article-section__content" id="Sec16-content"><h3 class="c-article__sub-heading" id="Sec17">Completion time</h3><p>The results show that male subjects in the P-SLVL condition performed the fastest. The Kruskall–Wallis <i>H</i> test shows a significant effect of the group (<i>H</i>
                  <sub>(<i>df</i>=3)</sub> = 8.82, <i>p</i> &lt; 0.05) on the completion time (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig8">8</a>). However, the Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab2">2</a>) show no statistically significant differences between the groups.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Time for task completion</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Paired comparisons using Mann–Whitney tests with Bonferroni correction for completion time, viewpoints manipulation, and viewpoints sharing</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec18">Viewpoints manipulation time</h3><p>The Kruskall–Wallis <i>H</i> test shows a significant effect of the group (<i>H</i>
                  <sub>(<i>df</i>=3)</sub> = 13.7, <i>p</i> &lt; 0.05) on the mean time of viewpoints manipulations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig9">9</a>). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects in the P-SLVL condition spent less time manipulating their point of view compared with male subjects in the A-SLVL condition and female subjects both in the P-SLVL condition and the A-SLVL condition (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab2">2</a>). No significant differences are observed between the other groups (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Viewpoints manipulation time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec19">Viewpoints sharing</h3><p>Kruskall–Wallis <i>H</i> test shows a significant effect of the group (<i>H</i>
                  <sub>(<i>df</i>=3)</sub> = 13.29, <i>p</i> &lt; 0.05) on viewpoints sharing (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig10">10</a>). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects in the P-SLVL condition spent more time on the same point of view compared with male subjects in the A-SLVL condition, female subjects in the P-SLVL condition, and female subjects in the A-SLVL condition (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab2">2</a>). No significant differences are observed between the other groups (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Sharing viewpoints</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec20">Summary of previous verbalization analyses</h3><p>We present in this section a short summary of the detailed analysis of verbalizations previously presented in (Chellali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chellali A, Milleville-Pennel I, Dumas C (2008) Elaboration of a common frame of reference in collaborative virtual environments. In: Abascal J, Fajardo I, Oakley I (eds) Proceedings of the 15th European conference on cognitive ergonomics: the ergonomics of cool interaction, Funchal, 16–19 Sept 2008, vol 369, pp 83–90" href="/article/10.1007/s10055-012-0214-5#ref-CR2" id="ref-link-section-d10158e1632">2008</a>):</p><p>The analysis of verbal communications between the dyads has shown that in the presence of the SLVL, the male groups described their actions in advance, received more positive feedback from their partner, and acted more often in parallel. These results permitted to conclude that the male subjects had a more efficient common frame of reference when they used the SLVL. Indeed, descriptions preceding actions and positive feedback are interpreted as signs of understanding between the partners. Once they established a common plan, they acted in parallel to reduce the task completion time. This was referred to as the Teamwork profile.</p><p>On the other hand, in the absence of the SLVL, the male groups had more negative feedback, alternated their actions, and tried more often to control their partners. These results permitted to conclude that the male subjects had a less efficient common frame of reference when they did not use the SLVL. Indeed, comments and questions about the actions of the partner are interpreted as signs of disagreement and misunderstanding between partners. This encouraged some participants to take the initiative by taking the leadership and guiding their partners to their own solutions. They also had to wait for their partners to finish their actions before they started their own actions. This was referred to as the “Individualistic” profile.</p><p>Finally, no conclusions were taken for the female groups since there are very few differences between the groups.</p><h3 class="c-article__sub-heading" id="Sec21">Spatial communication analysis</h3><p>The objective of the following complementary analysis is to investigate the spatial communications between the partners in order to show how the users projected themselves in the virtual environment and how they exchanged the spatial information during collaboration.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">The use of reference systems</h4><p>The use of reference systems was studied through the verbal references that the participants used to make spatial descriptions during the task. The results show that all the participants employed both the egocentric and the exocentric reference systems to make spatial descriptions. The Kruskall–Wallis <i>H</i> test shows a significant effect of the group on the use of the egocentric reference system (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 10.82, <i>p</i> &lt; 0.05) and the exocentric reference system (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 10.82, <i>p</i> &lt; 0.05; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig11">11</a>). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects in the P-SLVL condition less often employed the egocentric system (<i>U</i>
                    <sub>1</sub> = 0, <i>Z</i> = 2.65, <i>p</i> &lt; 0.05) and more often the exocentric system (<i>U</i>
                    <sub>
                      <i>1</i>
                    </sub> = 30, <i>Z</i> = 2.6, <i>p</i> &lt; 0.05) compared with female subjects in the A-SLVL condition. No significant differences are observed between the other groups (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig11">11</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Reference systems use</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Between-groups comparisons using the chi-square test were used to show which of the two reference systems was mostly utilized by each group. Our chi-square test revealed that (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab3">3</a>):</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Male subjects in the P-SLVL condition employed the exocentric system significantly more often (66 %) than the egocentric system (34 %) compared with the three other groups.</p>
                      </li>
                      <li>
                        <p>Female subjects in the A-SLVL condition employed the egocentric system significantly more often (75 %) than the exocentric system (25 %) compared with the three other groups.</p>
                      </li>
                      <li>
                        <p>Male subjects in the A-SLVL condition as well as female subjects in the P-SLVL condition used equally the egocentric and the exocentric reference systems (54–46 % and 53–47 %, respectively).</p>
                      </li>
                    </ul>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The comparison for the use of the reference systems</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Actions descriptions</h4><p>The Kruskall–Wallis <i>H</i> test shows a significant effect of the group on the actions descriptions (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 8.23, <i>p</i> &lt; 0.05; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig12">12</a>). While the conversations of male subjects’ in the P-SLVL condition tended to involve more actions descriptions than the conversations of male subjects’ in the A-SLVL condition, the conversations of female subjects in the P-SLVL condition, and the conversations of female subjects in the A-SLVL condition (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab4">4</a>), the Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show no significant differences between these groups.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Descriptions of actions, objects referenciations and viewpoints manipulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Paired comparisons using Mann–Whitney tests with Bonferroni correction for actions descriptions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Localization in the virtual environment</h4><p>The results show no significant effect of the group on the viewpoint descriptions (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 1, <i>p</i> &gt; 0.05; cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig12">12</a>) and on the objects referenciation (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 3, <i>p</i> &gt; 0.05; cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig12">12</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Teamwork profile and collaboration</h4><p>The collaboration is measured regarding whether the users considered the performed actions as an individual action (e.g., “I will do that…and you will do…”) or conversely, as a common action (e.g., “we will do…”).</p>
                    <h3 class="c-article__sub-heading">
                      <i>Personal pronouns</i>
                    </h3>
                    <p>The Kruskall–Wallis test shows a significant effect of the group on the use of personal pronouns (<i>H</i>
                      <sub>(<i>df</i>=3)</sub> = 8.85, <i>p</i> &lt; 0.05; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig13">13</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Collaboration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    
                  <p>The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects used the personal pronouns in the A-SLVL condition more often than male subjects in the P-SLVL condition (<i>U</i>
                    <sub>1</sub> = 36, <i>Z</i> = 2.8, <i>p</i> &lt; 0.05; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig13">13</a>) when describing their actions. No other significant differences were observed for the other groups.</p>
                    <h3 class="c-article__sub-heading">
                      <i>Impersonal pronouns</i>
                    </h3>
                    <p>The Kruskall–Wallis <i>H</i> test shows a significant effect of the group on the use of the impersonal pronouns “one” and “we” (<i>H</i>
                      <sub>(<i>df</i>=3)</sub> = 8.85, <i>p</i> &lt; 0.05, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig13">13</a>). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects used the impersonal pronouns in the P-SLVL condition more often than male subjects in the A-SLVL condition (<i>U</i>
                      <sub>1</sub> = 36, <i>Z</i> = −.8, <i>p</i> &lt; 0.05) when describing their actions. No significant differences were observed for the other groups.</p>
                  <p>Between-groups comparisons using the chi-square test were used to show which mode (individualistic vs. group) was mostly employed by each group.</p><p>The results (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab5">5</a>) show that:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Male groups in A-SLVL condition were more in an “individualistic” mode than in a “group” mode compared with the three other groups. Indeed, they used the personal pronouns “I” and “you” (74 %) more than the impersonal pronouns “one” and “we” (26 %, cf. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig13">13</a>). No significant differences were observed between the three other groups that equally used the personal pronouns “I” and “you” (62 % in the MP-SLVL, 57 % in the FP-SLVL, and 62 % in the FA-SLVL) and the impersonal pronouns “one” and “we” (38 % in the MP-SLVL, 43 % in the FP-SLVL, and 38 % in the FA-SLVL, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig13">13</a>).</p>
                      </li>
                    </ul>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Collaboration</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Additional findings: analysis of the users’ projection in the virtual environment</h4><p>The analysis of conversations between the partners shed light on different strategies used by the participant to refer to the objects they were manipulating. This was extracted and considered as an interesting indicator to understand how the users perceived these objects as their own representation in the virtual environment.</p>
                    <h3 class="c-article__sub-heading">
                      <i>Users’ representation in the VE</i>
                    </h3>
                    <p>Avatars are usually used as the virtual representation of users in the virtual space. They can have different forms from simple cursors to very sophisticated 3D humanoid character. They are used to identify the users within the virtual space and also to have a feedback on their actions.</p>
                  <p>The implementation of our system does not include any explicit representation of the users in the virtual environment. However, the system included the representation of the actions performed by the users. This was symbolized by the change of color for the objects being manipulated, that is, when a user manipulated a tetramino, he/she became the owner of this object (the other user cannot manipulate it simultaneously), and the color of this tetramino was changed to the color assigned to the user (red or green) to warn each participant that the object is handled. When the user released the tetramino, it recovered its original color (white) to warn each participant that he/she is allowed to handle this object. Subsequently, we can consider the object being manipulated as the user implicit representation in the virtual environment. The objective here is to analyze how the participant considered these objects as being their own representation in order to understand the way they projected themselves in these virtual objects they were handling.</p>
                    <h3 class="c-article__sub-heading">
                      <i>Users’ projection in the virtual environment</i>
                    </h3>
                    <p>The main measure used to evaluate whether the participants considered the manipulated objects as their own representation was to study the way they refer to these objects during their manipulation. By analyzing the conversations, three cases were observed:</p><ul class="u-list-style-bullet">
                        <li>
                          <p>The participants consider the object they were handling as their own representation in the virtual environment (e.g., “<i>That’s</i>
                            <i><u>me</u></i>
                            <i>you’re trying to move</i>…”). In this situation, the subjects considered themselves as a part of the virtual environment.</p>
                        </li>
                        <li>
                          <p>The participants use the definite articles “the” when talking about the objects being handled (e.g., “<i>I will move</i>
                            <i><u>the tetramino</u></i>”). In this case, we considered that the objects were not perceived as being the continuity of the participant in the virtual environment.</p>
                        </li>
                        <li>
                          <p>The participants use possessive pronouns when talking about the objects being handled (e.g., “<i>This is</i>
                            <i><u>my tetramino</u></i>
                            <i>you’re trying to move…</i>”). In this case, participants perceive the tetramino as being a continuity of their own body but they do not feel themselves completely projected in the virtual object.</p>
                        </li>
                      </ul><p>The results show that all the users used explicitly these three strategies when referring to the objects they were handling but with different occurrences.</p>
                  
                    <h3 class="c-article__sub-heading">
                      <i>Possessive pronouns</i>
                    </h3>
                    <p>The Kruskall–Wallis <i>H</i> test shows no significant effect of the group on the use of the possessive pronoun “my” (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig14">14</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0214-5/MediaObjects/10055_2012_214_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Referenciation of manipulated objects and projection in the virtual environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0214-5/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    
                  
                    <h3 class="c-article__sub-heading">
                      <i>Personal pronouns</i>
                    </h3>
                    <p>The Kruskall–Wallis <i>H</i> test shows a significant effect of the group on the use of the personal pronouns “me” (<i>H</i>
                      <sub>(<i>df</i>=3)</sub> = 7.97, <i>p</i> &lt; 0.05). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that the female subjects utilized the personal pronouns “me” more often than male subjects both in the P-SLVL condition (<i>U</i>
                      <sub>1</sub> = 29.5, <i>Z</i> = 2.56, <i>p</i> &lt; 0.05) and in the A-SLVL condition (<i>U</i>
                      <sub>1</sub> = 26, <i>Z</i> = 1.92, <i>p</i> &lt; 0.05). No significant differences were observed between the other groups.</p>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Definite articles</h4><p>The Kruskall–Wallis test shows a significant effect of the group on the use of the definite articles (<i>H</i>
                    <sub>(<i>df</i>=3)</sub> = 13.0, <i>p</i> &lt; 0.05). The Mann–Whitney <i>U</i> tests in comparison with Bonferroni correction show that male subjects in the P-SLVL condition utilized the definite articles “the” more often when referring the object being handled than female subjects in the P-SLVL condition (<i>U</i>
                    <sub>1</sub> = 1, <i>Z</i> = 2.46, <i>p</i> &lt; 0.05). No significant differences were observed between the other groups.</p><p>In order to show which of the three referenciation strategies was mostly used by each group, a within-group comparison was used.</p><p>The Kruskall–Wallis <i>H</i> test shows a significant effect of the referenciation strategies on both male groups (<i>H</i>
                    <sub>(<i>df</i>=2)</sub> = 11.37, <i>p</i> &lt; 0.05 for the MP-SLSVL group and <i>H</i>
                    <sub>(<i>df</i>=2)</sub> = 11.42, <i>p</i> &lt; 0.05 for the MA-SLVL group; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig14">14</a>). The tests also show a significant effect of the referenciation strategies on the female groups (<i>H</i>
                    <sub>(<i>df</i>=2)</sub> = 9.45, <i>p</i> &lt; 0.05 for the FP-SLSVL condition and <i>H</i>
                    <sub>(<i>df</i>=2)</sub> = 7.04, <i>p</i> &lt; 0.05 for the FA-SLVL condition; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0214-5#Fig14">14</a>).</p><p>Between-groups comparisons using the chi-square test were used to show which of the three referenciation strategies was mostly used by each group. The results highlight a gender difference (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0214-5#Tab6">6</a>):</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Male subjects utilized the definite articles “the” (65 % in the P-SLVL and 62 % in the A-SLVL) than the possessive pronoun “my” (18 % in the P-SLVL and 18 % in the A-SLVL) and the personal pronouns “me” (17 % in the P-SLVL and 20 % in the A-SLVL) more often in both experimental conditions compared with female groups.</p>
                      </li>
                      <li>
                        <p>On the other hand, female subjects utilized the definite articles “the” (42 % in the P-SLVL and 37 % in the A-SLVL) and the personal pronouns “me” (45 % in the P-SLVL and 45 % in the A-SLVL) more often than the possessive pronoun “my” (13 % in the P-SLVL and 19 % in the A-SLVL) in both experimental conditions compared with male groups.</p>
                      </li>
                    </ul>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 The comparison for the referenciations of objects</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  </div></div></section><section aria-labelledby="Sec28"><div class="c-article-section" id="Sec28-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec28">Discussion</h2><div class="c-article-section__content" id="Sec28-content"><p>For this experimental study, we were expecting:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>An impact of the SLVL on the use of reference systems: exocentric versus egocentric.</p>
                  </li>
                  <li>
                    <p>An impact of the reference system being used on the spatial COFOR construction and update. This was expected to have a direct impact on:</p><ol class="u-list-style-none">
                        <li>
                          <span class="u-custom-list-number">1.</span>
                          
                            <p>Viewpoint sharing and navigation in the virtual space: since the users have a more global perception of the surrounding environment in the exocentric system, they were expected to decrease their viewpoints changing. Moreover, the users were expected to share less often a common viewpoint of the environment when using the exocentric system.</p>
                          
                        </li>
                        <li>
                          <span class="u-custom-list-number">2.</span>
                          
                            <p>Object referenciation, actions descriptions, and collaboration in the shared virtual environment: we hypothesized that a better mutual understanding will improve the partners’ collaboration and help them describe their actions. The users were also expected to use different object referenciation strategies depending on the reference system being used.</p>
                          
                        </li>
                      </ol>
                    
                  </li>
                </ul>
              <h3 class="c-article__sub-heading" id="Sec29">Influence of the stable lateralized visual landmark</h3><p>The results show that male subjects were influenced by the stable lateralized visual landmark. Indeed, they used an egocentric reference system in the A-SLVL condition more while they used an exocentric systems in the P-SLVL condition more. These results indicate that male participants tended to use the visual landmark as a reference when it is present. On the other hand, female subjects are less sensitive to the presence of the visual landmark. In fact, they did not use it as a reference. We conclude that our first hypothesis is confirmed only for male participants. This result highlights a gender difference concerning the use of reference systems. This gender difference can be explained by the fact that exocentric system requires to perform mental rotations in order to continuously locate objects according to the visual landmark when the users change their viewpoint. Female participants used this reference system less frequently. This can be linked to some work on inter-gender differences for spatial abilities. Indeed, different studies show a lower performance in mental rotation tests for female participants. For instance, Kimura (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kimura D (2001) Cerveau d’homme et cerveau de femme?. Odile Jacob, Paris" href="/article/10.1007/s10055-012-0214-5#ref-CR16" id="ref-link-section-d10158e2997">2001</a>) argue that “Most of the spatial ability tests show some advantages for men who are better than women, especially in mental rotations.” Lawton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:321–337" href="/article/10.1007/s10055-012-0214-5#ref-CR20" id="ref-link-section-d10158e3000">2001</a>) showed that men are more likely than women to incorporate an overview (or exocentric) perspective into route directions. Kolb and Whishaw (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kolb B, Whishaw I (2002) Cerveau et comportement. De Boeck Université" href="/article/10.1007/s10055-012-0214-5#ref-CR18" id="ref-link-section-d10158e3003">2002</a>) presented a series of cognitive tests to assess differences between men and women. One of their studies involved the assessment of spatial abilities of men and women. The task was to make mental rotations for 3D objects. The results show that men have better results than women in such a task. These results confirm that women have more difficulties to perform mental rotations. On the other hand, women are more effective than men to perform spatial memory tasks as shown in Klingberg’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Klingberg T (2006) Development of a superior frontal-intraparietal network for visuo-spatial working memory. Neuropsychologia 44:2171–2177" href="/article/10.1007/s10055-012-0214-5#ref-CR17" id="ref-link-section-d10158e3006">2006</a>) study. According to Koscik et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Koscik T, O’leary D, Moser DJ, Andreasen NC, Nopoilos P (2009) Sex differences in parietal lobe morphology: relationship to mental rotation performance. Brain Cogn 69:451–459" href="/article/10.1007/s10055-012-0214-5#ref-CR19" id="ref-link-section-d10158e3009">2009</a>) study in the field of neuroscience, these differences are due to structural differences between men and women brains (more specifically, in the parietal lobe). These morphological characteristics involve different performing strategies for spatial tasks, which come at the expense of the mental rotations performances for women (Koscik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Koscik T, O’leary D, Moser DJ, Andreasen NC, Nopoilos P (2009) Sex differences in parietal lobe morphology: relationship to mental rotation performance. Brain Cogn 69:451–459" href="/article/10.1007/s10055-012-0214-5#ref-CR19" id="ref-link-section-d10158e3013">2009</a>). These studies can explain why the female participants preferred to use the egocentric reference system even when the visual landmark was present. They preferred to avoid performing mental rotations by locating objects according to their personal viewpoint. On the other hand, the male subjects’ have superior ability to perform mental rotations that facilitate the use of the exocentric system.</p><h3 class="c-article__sub-heading" id="Sec30">Viewpoint manipulation</h3><p>The results show that the male subjects decreased their viewpoint manipulation when the visual landmark was present. This suggests that the use of the visual landmark as a reference encourages the users to have an exocentric mental representation of the environment. While it is hard to perform the manipulation task efficiently using the exocentric perspective, the use of the visual landmark helped the users during the communication task.</p><p>Also, the results show that the male partners spent more time on shared viewpoints when the spatial landmark was present. This result was unexpected since the use of the exocentric system helps the operators to have a more global perspective of the environment. On the other hand, the verbalizations analyses show that in all the experimental conditions, the conversations between the partners included only few descriptions of the speaker’s position/point of viewpoints. This indicates that the viewpoint sharing in the P-SLVL condition was rather spontaneous. This finding can be linked to the work of Spante et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Spante M, Schroeder R, Axelsson AS (2004) How putting yourself into the other person’s virtual shoes enhances collaboration. In: Proceedings of the 7th international workshop on presence, Valencia, pp 190–196" href="/article/10.1007/s10055-012-0214-5#ref-CR25" id="ref-link-section-d10158e3026">2004</a>) who showed that the users tried to find the best strategy that allowed them to reduce the communication “costs.” In our study, the viewpoints manipulation/sharing were used spontaneously to optimize communication and then reduce the collaborative efforts. Hence, in addition to the use of the visual landmark directly to optimize the spatial COFOR development, it can be considered as a spontaneous tool to facilitate the spatial communication. However, additional investigations on the correlation between the viewpoints manipulation/sharing and the subtasks being preformed (manipulation task/communication) are needed in order to confirm this result.</p><h3 class="c-article__sub-heading" id="Sec31">Spatial communication and common frame of reference</h3><p>It was predicted that the visual landmark will increase the use of expressions for actions descriptions. Although the results showed a trend in this direction for male participants, the difference was not significant. This can suggest that the use of the exocentric reference system improved the spatial communication by helping the partners to easily describe the actions they were performing. However, as the results are statistically not significant, further investigations are needed to confirm this trend.</p><p>While no differences were observed regarding the use of the objects referenciations expressions, a more detailed investigation showed a gender difference in the way the participants perceived the objects they were manipulating as being their own representation in the virtual environment . This additional finding suggests that male and female have different strategies to project themselves in the virtual environment. One possible explanation for this finding is that the female participants used the egocentric reference system much more frequently. The use of this system allows the user to have a personal viewpoint of his/her environment and can consequently help them project themselves in the virtual environment. Indeed, to locate objects according to their personal viewpoint (perspective), the users have to project themselves continually in the virtual space. This continuous projection makes the user’s feel inside the virtual environment. This is another argument to explain why women did not use the visual landmark as a reference.</p><p>On the other hand, male participants have a different perception of the space. They used the definite articles to refer the objects they were manipulating more frequently. This suggests that males have a more global viewpoint of the virtual environment. In this case, the organization of the space becomes independent from the user’s position and does not require a projection in the virtual environment. Consequently, when the visual landmark was present, the male participants used it more naturally. These results are coherent with those claiming that men and women have different mental organizations of the space.</p><p>We also hypothesized that the visual landmark will increase the comprehension between participants and subsequently increase collaboration. Dillenbourg et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Dillenbourg P, Baker M, Blaye A, Malley CO (1996) The evolution of research on collaborative learning. In: Spada E, Reiman P (eds) Learning 117(1):189–211. Elsevier" href="/article/10.1007/s10055-012-0214-5#ref-CR7" id="ref-link-section-d10158e3043">1996</a>) distinguish between the notion of collaboration: “Mutual engagement of participants in a coordinated effort to solve the problem together,” and cooperation: “work accomplished by the division of labor among participants, as an activity where each person is responsible for a portion of the problem solving….” The results show that male participants increased the use of impersonal pronouns when the visual landmark was present. This indicates that in this condition, they considered themselves a member of a team rather than as a single user of the virtual environment. This is consistent with the definition of collaboration and suggests that an efficient COFOR enhances the awareness of working with a teammate. On the other hand, male participants used the personal pronouns when the visual landmark was absent. This is more consistent with the definition of cooperation and indicates that they preferred to work individually than as a team. We conclude that the visual landmark has a positive effect on collaboration: collaboration is increased by enhancing the mutual comprehension between the partners.</p></div></div></section><section aria-labelledby="Sec32"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Conclusion and future work</h2><div class="c-article-section__content" id="Sec32-content"><h3 class="c-article__sub-heading" id="Sec33">Findings and design implications</h3><p>This study is part of a work to define a user-centered design method for CVE with an emphasis on collaboration. The results show that the presence of contextual clues, that is, a stable lateralized visual landmark, in the virtual environment influences the way operators communicate and collaborate in a figure reconstruction task. The results of the experiment also show a difference between women and men in the way they work together in a spatial task. Therefore, the addition of a virtual character as a stable and lateralized visual landmark in the virtual environment has a positive effect on collaboration in the case of male users. The shared landmark improves the construction of a common frame of reference between users, in particular, for actions descriptions. Furthermore, it increases the collaboration in the virtual environment. Finally, the study confirms that men and women have different spatial perceptions of the shared virtual environments.</p><p>According to this work, some recommendations can be given for the design of CVE dedicated to object manipulation tasks: to improve spatial communication between partners in a shared virtual environment, the presence of visual landmarks is useful. To be effective in the construction of a common spatial frame of reference in a virtual scene, the visual landmark must be:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <i>Lateralized</i>, to permit to localize objects easily according to it,</p>
                    </li>
                    <li>
                      <p>
                        <i>Fixed</i>, to avoid direction changing during the task,</p>
                    </li>
                    <li>
                      <p>
                        <i>Easy to name</i> and to <i>identify</i> among the other objects of the scene to avoid ambiguities,</p>
                    </li>
                    <li>
                      <p>
                        <i>Visible to all the users during the task</i> to permit its use as a common reference.</p>
                    </li>
                  </ul>
                <h3 class="c-article__sub-heading" id="Sec34">Limitations and future work</h3><p>Despite the small size of the sample, interesting results were observed in this study. Therefore, additional experiments, involving more subjects, are needed in order to strengthen the actual findings and to explore other experimental conditions.</p><p>The analysis of verbal communications permitted to highlight different strategies for the users to project themselves in the virtual objects they were manipulating. While this is an interesting finding, more investigations are needed to understand this issue. This can be done in a future study by combining the verbalization analysis with other measurements (questionnaires, for example).</p><p>This study shows that the SLVL can improve the common spatial frame of reference for male subjects. On the other hand, other collaborative interaction metaphors can also offer other advantages. For instance, animated avatars can be used to reflect some non-verbal cues (pointing gestures for example). In such a case, the partners can choose the best strategy to communicate spatial information to their partners and improve collaboration. However, the design recommendations must be respected: that is, the SLVL must be identifiable among the other objects and should not be confused with a user’s avatar, for example. In this case, the use of references such as a simple XYZ coordinate system or a compass rose as a SLVL can be more adapted to avoid confusions.</p><p>The next step of our work is to enhance the partners’ mutual comprehension when performing spatial tasks in CVE. Two possible issues can be addressed. The first issue could be to support deictic communications between partners in order to minimize their collaborative efforts. This can be done by combining the use of visual landmarks with existing objects’ pointing techniques. However, the pointing technique must be chosen carefully. Indeed, the designation task must not constrain the main manipulation task performing.</p><p>The second issue could be the use of the stable lateralized visual landmarks in navigation tasks. In this case, several landmarks can be set in a scene. Hence, they can be used continually to help the partners to locate each other while navigating in the virtual environment. In order to make these propositions more effective, other design recommendations have to be defined according to the requirements of this kind of tasks.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>A tetramino is a geometric figure that consists of four cubes, each having at least one side shared with another (Tetris game figures).</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bridgeman B (1999) Separate representations of visual space for perception and visually guided behavior. In: A" /><p class="c-article-references__text" id="ref-CR1">Bridgeman B (1999) Separate representations of visual space for perception and visually guided behavior. In: Aschersleben G, Bachmann T, Müsseler J (eds) Cognitive contribution to the perception of spatial and temporal events, pp 3–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chellali A, Milleville-Pennel I, Dumas C (2008) Elaboration of a common frame of reference in collaborative vi" /><p class="c-article-references__text" id="ref-CR2">Chellali A, Milleville-Pennel I, Dumas C (2008) Elaboration of a common frame of reference in collaborative virtual environments. In: Abascal J, Fajardo I, Oakley I (eds) Proceedings of the 15th European conference on cognitive ergonomics: the ergonomics of cool interaction, Funchal, 16–19 Sept 2008, vol 369, pp 83–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Churcher N, Churcher C (1996) A collaborative approach to GIS. In: Proceedings of the 8th annual colloquium of" /><p class="c-article-references__text" id="ref-CR3">Churcher N, Churcher C (1996) A collaborative approach to GIS. In: Proceedings of the 8th annual colloquium of the spatial information research centre, pp 156–163</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="HH. Clark, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Clark HH (1996) Using language. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR4">Clark HH (1996) Using language. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20language&amp;publication_year=1996&amp;author=Clark%2CHH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Clark, S. Brennan, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Clark H, Brennan S (1991) Grounding in communication. In: Resnick L, Levine J, Teasley S (eds) Cognition, pers" /><p class="c-article-references__text" id="ref-CR5">Clark H, Brennan S (1991) Grounding in communication. In: Resnick L, Levine J, Teasley S (eds) Cognition, perspectives on socially shared. American Psychological Association, Washington, pp 127–149</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognition%2C%20perspectives%20on%20socially%20shared&amp;pages=127-149&amp;publication_year=1991&amp;author=Clark%2CH&amp;author=Brennan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Dillenbourg, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Dillenbourg P (1999) What do you mean by collaborative learning? In: Dillenbourg P (ed) Collaborative learning" /><p class="c-article-references__text" id="ref-CR6">Dillenbourg P (1999) What do you mean by collaborative learning? In: Dillenbourg P (ed) Collaborative learning: cognitive and computational approaches. Elsevier, Oxford, pp 1–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20learning%3A%20cognitive%20and%20computational%20approaches&amp;pages=1-19&amp;publication_year=1999&amp;author=Dillenbourg%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dillenbourg P, Baker M, Blaye A, Malley CO (1996) The evolution of research on collaborative learning. In: Spa" /><p class="c-article-references__text" id="ref-CR7">Dillenbourg P, Baker M, Blaye A, Malley CO (1996) The evolution of research on collaborative learning. In: Spada E, Reiman P (eds) Learning 117(1):189–211. Elsevier</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Erickson T (1993) From interface to interplace: the spatial environment as a medium for interaction. In: Proce" /><p class="c-article-references__text" id="ref-CR8">Erickson T (1993) From interface to interplace: the spatial environment as a medium for interaction. In: Proceedings of the conference on spatial information theory</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gaver B (1992) The affordances of media spaces for collaboration. In: Proceedings of the ACM conference on com" /><p class="c-article-references__text" id="ref-CR9">Gaver B (1992) The affordances of media spaces for collaboration. In: Proceedings of the ACM conference on computer support cooperative work (CSCW), Toronto</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gaver WW, Sellen A, Heath C, Luff P (1993) One is not enough: multiple views in a media space. In: Proceedings" /><p class="c-article-references__text" id="ref-CR10">Gaver WW, Sellen A, Heath C, Luff P (1993) One is not enough: multiple views in a media space. In: Proceedings of INTERCHI, pp 335–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JJ. Gibson, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Gibson JJ (1979) The ecological approach to visual perception. Houghton Mifflin, New York" /><p class="c-article-references__text" id="ref-CR11">Gibson JJ (1979) The ecological approach to visual perception. Houghton Mifflin, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ecological%20approach%20to%20visual%20perception&amp;publication_year=1979&amp;author=Gibson%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harrison S, Dourish P (1996) Re-place-ing space: the roles of place and space in collaborative systems. In: Pr" /><p class="c-article-references__text" id="ref-CR12">Harrison S, Dourish P (1996) Re-place-ing space: the roles of place and space in collaborative systems. In: Proceedings of the 1996 ACM conference on computer supported cooperative work. ACM, pp 67–76</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual or" /><p class="c-article-references__text" id="ref-CR13">Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (1998) Fragmented interaction: establishing mutual orientation in virtual environments. In: Proceedings of the ACM 1998 conference on computer-supported cooperative work, pp 217–226</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hindmarsh, M. Fraser, C. Heath, S. Benford, C. Greenhalagh, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (2000) Object-focused interaction in collaborative vi" /><p class="c-article-references__text" id="ref-CR14">Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalagh C (2000) Object-focused interaction in collaborative virtual environments. ACM Trans Comput Hum Interact 7(4):477–509</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365088" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Object-focused%20interaction%20in%20collaborative%20virtual%20environments&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=7&amp;issue=4&amp;pages=477-509&amp;publication_year=2000&amp;author=Hindmarsh%2CJ&amp;author=Fraser%2CM&amp;author=Heath%2CC&amp;author=Benford%2CS&amp;author=Greenhalagh%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J-M. Hoc, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hoc J-M (2001) Towards a cognitive approach to human-machine cooperation in dynamic situations. Int J Hum Comp" /><p class="c-article-references__text" id="ref-CR15">Hoc J-M (2001) Towards a cognitive approach to human-machine cooperation in dynamic situations. Int J Hum Comput Stud 54:509–540</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.2000.0454" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20a%20cognitive%20approach%20to%20human-machine%20cooperation%20in%20dynamic%20situations&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=54&amp;pages=509-540&amp;publication_year=2001&amp;author=Hoc%2CJ-M">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Kimura, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kimura D (2001) Cerveau d’homme et cerveau de femme?. Odile Jacob, Paris" /><p class="c-article-references__text" id="ref-CR16">Kimura D (2001) Cerveau d’homme et cerveau de femme?. Odile Jacob, Paris</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cerveau%20d%E2%80%99homme%20et%20cerveau%20de%20femme%3F&amp;publication_year=2001&amp;author=Kimura%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Klingberg, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Klingberg T (2006) Development of a superior frontal-intraparietal network for visuo-spatial working memory. N" /><p class="c-article-references__text" id="ref-CR17">Klingberg T (2006) Development of a superior frontal-intraparietal network for visuo-spatial working memory. Neuropsychologia 44:2171–2177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuropsychologia.2005.11.019" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20superior%20frontal-intraparietal%20network%20for%20visuo-spatial%20working%20memory&amp;journal=Neuropsychologia&amp;volume=44&amp;pages=2171-2177&amp;publication_year=2006&amp;author=Klingberg%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kolb B, Whishaw I (2002) Cerveau et comportement. De Boeck Université" /><p class="c-article-references__text" id="ref-CR18">Kolb B, Whishaw I (2002) Cerveau et comportement. De Boeck Université</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Koscik, D. O’leary, DJ. Moser, NC. Andreasen, P. Nopoilos, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Koscik T, O’leary D, Moser DJ, Andreasen NC, Nopoilos P (2009) Sex differences in parietal lobe morphology: re" /><p class="c-article-references__text" id="ref-CR19">Koscik T, O’leary D, Moser DJ, Andreasen NC, Nopoilos P (2009) Sex differences in parietal lobe morphology: relationship to mental rotation performance. Brain Cogn 69:451–459</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.bandc.2008.09.004" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sex%20differences%20in%20parietal%20lobe%20morphology%3A%20relationship%20to%20mental%20rotation%20performance&amp;journal=Brain%20Cogn&amp;volume=69&amp;pages=451-459&amp;publication_year=2009&amp;author=Koscik%2CT&amp;author=O%E2%80%99leary%2CD&amp;author=Moser%2CDJ&amp;author=Andreasen%2CNC&amp;author=Nopoilos%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CA. Lawton, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:3" /><p class="c-article-references__text" id="ref-CR20">Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:321–337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1010981616842" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20and%20regional%20differences%20in%20spatial%20referents%20used%20in%20direction%20giving&amp;journal=Sex%20Roles&amp;volume=44&amp;pages=321-337&amp;publication_year=2001&amp;author=Lawton%2CCA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park K, Kapoor A, Scharver C, Leigh J (2000) Exploiting multiple perspectives in tele-immersion. In: Proceedin" /><p class="c-article-references__text" id="ref-CR21">Park K, Kapoor A, Scharver C, Leigh J (2000) Exploiting multiple perspectives in tele-immersion. In: Proceedings of the 4th immersive projection technology workshop, Ames</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Roberts, C. Aman, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Roberts R, Aman C (1993) Developmental differences in giving directions: spatial frames of reference and menta" /><p class="c-article-references__text" id="ref-CR23">Roberts R, Aman C (1993) Developmental differences in giving directions: spatial frames of reference and mental rotation. Child Dev 64:1258–1270</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F1131338" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developmental%20differences%20in%20giving%20directions%3A%20spatial%20frames%20of%20reference%20and%20mental%20rotation&amp;journal=Child%20Dev&amp;volume=64&amp;pages=1258-1270&amp;publication_year=1993&amp;author=Roberts%2CR&amp;author=Aman%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Roschelle, SD. Teasley, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Roschelle J, Teasley SD (1995) Construction of shared knowledge in collaborative problem solving. In: O’Malley" /><p class="c-article-references__text" id="ref-CR24">Roschelle J, Teasley SD (1995) Construction of shared knowledge in collaborative problem solving. In: O’Malley C (ed) Computer-supported collaborative learning. Springer, New York, pp 69–97</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer-supported%20collaborative%20learning&amp;pages=69-97&amp;publication_year=1995&amp;author=Roschelle%2CJ&amp;author=Teasley%2CSD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spante M, Schroeder R, Axelsson AS (2004) How putting yourself into the other person’s virtual shoes enhances " /><p class="c-article-references__text" id="ref-CR25">Spante M, Schroeder R, Axelsson AS (2004) How putting yourself into the other person’s virtual shoes enhances collaboration. In: Proceedings of the 7th international workshop on presence, Valencia, pp 190–196</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Stefik, DG. Bobrow, S. Lanning, D. Tatar, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Stefik M, Bobrow DG, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. A" /><p class="c-article-references__text" id="ref-CR26">Stefik M, Bobrow DG, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. ACM Trans Inf Syst 5(2):147–167</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F27636.28056" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=WYSIWIS%20revised%3A%20early%20experiences%20with%20multiuser%20interfaces&amp;journal=ACM%20Trans%20Inf%20Syst&amp;volume=5&amp;issue=2&amp;pages=147-167&amp;publication_year=1987&amp;author=Stefik%2CM&amp;author=Bobrow%2CDG&amp;author=Lanning%2CS&amp;author=Tatar%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedi" /><p class="c-article-references__text" id="ref-CR27">Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of human factors and computer systems, pp 265–272</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-012-0214-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We would like to thank students from <i>Ecole Centrale de Nantes</i> and <i>Nantes University</i> who agreed to participate in this experiment. The research was partially funded through <i>InterActeurs</i> project (CRE 43230501) in collaboration with <i>Orange Labs</i>.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">1493 Cambridge Street, Cambridge, MA, 02139, USA</p><p class="c-article-author-affiliation__authors-list">Amine Chellali</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Ecole des Mines de Nantes-IRCCYN, Nantes, France</p><p class="c-article-author-affiliation__authors-list">Amine Chellali</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">CNRS-IRCCyN, Nantes, France</p><p class="c-article-author-affiliation__authors-list">Isabelle Milleville-Pennel</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">
Ecole des Mines de Nantes, Nantes, France</p><p class="c-article-author-affiliation__authors-list">Cédric Dumas</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">
CSIRO, Brisbane, Australia</p><p class="c-article-author-affiliation__authors-list">Cédric Dumas</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Amine-Chellali"><span class="c-article-authors-search__title u-h3 js-search-name">Amine Chellali</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Amine+Chellali&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Amine+Chellali" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Amine+Chellali%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Isabelle-Milleville_Pennel"><span class="c-article-authors-search__title u-h3 js-search-name">Isabelle Milleville-Pennel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Isabelle+Milleville-Pennel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Isabelle+Milleville-Pennel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Isabelle+Milleville-Pennel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-C_dric-Dumas"><span class="c-article-authors-search__title u-h3 js-search-name">Cédric Dumas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;C%C3%A9dric+Dumas&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=C%C3%A9dric+Dumas" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22C%C3%A9dric+Dumas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-012-0214-5/email/correspondent/c1/new">Amine Chellali</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Influence%20of%20contextual%20objects%20on%20spatial%20interactions%20and%20viewpoints%20sharing%20in%20virtual%20environments&amp;author=Amine%20Chellali%20et%20al&amp;contentID=10.1007%2Fs10055-012-0214-5&amp;publication=1359-4338&amp;publicationDate=2012-09-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chellali, A., Milleville-Pennel, I. &amp; Dumas, C. Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments.
                    <i>Virtual Reality</i> <b>17, </b>1–15 (2013). https://doi.org/10.1007/s10055-012-0214-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-012-0214-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-01-19">19 January 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-08-16">16 August 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-09-12">12 September 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-03">March 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-012-0214-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-012-0214-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Spatial communication</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaboration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Common frame of reference</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual landmarks</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0214-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=214;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

