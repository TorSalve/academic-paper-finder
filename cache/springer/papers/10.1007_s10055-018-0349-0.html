<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Haptically enabled simulation system for firearm shooting training"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Firearm shooting training is of importance in military and law enforcement training tasks. Traditional training usually uses actual firearms or modified bullets that are dangerous and expensive and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Haptically enabled simulation system for firearm shooting training"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-06-15"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Firearm shooting training is of importance in military and law enforcement training tasks. Traditional training usually uses actual firearms or modified bullets that are dangerous and expensive and difficult to evaluate the performance. Firearm training simulation systems provide risk-free alternatives. However, most existing simulation is visual-only, which lacks the immersion on the force feedback. In this paper, we proposed a new firearm training simulation system, which can provide more realistic training by incorporating physic effects on recoil and trigger pull weight. Dynamic, immersive, and repeatable training experiences while imposes no danger to trainees are provided in our system. The system consists of haptics, physics engine, and motion capture. These three components are carefully combined by developing the corresponding techniques of haptic force rendering, visuo-haptic integration, and synchronisation, physics-based dynamic simulation and motion analysis. Compared with existing systems, our training system has more complete functionalities that include visual firearm shooting, force generation, shooting reactions, result analysis and evaluation. Moreover, it is adaptable to off-the-shelf hardware and software packages and thus it can provide flexibility to system scalability and budget. To evaluate the proposed system, two demonstrations are conducted for users where the systems accuracy, immersion and usability are analysed. The results show the effectiveness of our physics-based shooting model and the proposed system on simulating different shooting scenarios."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-06-15"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="217"/>

    <meta name="prism.endingPage" content="228"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0349-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0349-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0349-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0349-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Haptically enabled simulation system for firearm shooting training"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2019/09"/>

    <meta name="citation_online_date" content="2018/06/15"/>

    <meta name="citation_firstpage" content="217"/>

    <meta name="citation_lastpage" content="228"/>

    <meta name="citation_article_type" content="S.I. : Virtual Reality, Augmented Reality and Commerce"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0349-0"/>

    <meta name="DOI" content="10.1007/s10055-018-0349-0"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0349-0"/>

    <meta name="description" content="Firearm shooting training is of importance in military and law enforcement training tasks. Traditional training usually uses actual firearms or modified bu"/>

    <meta name="dc.creator" content="Lei Wei"/>

    <meta name="dc.creator" content="Hailing Zhou"/>

    <meta name="dc.creator" content="Saeid Nahavandi"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE/ASME Trans Mechatron; citation_title=Mechatronics, design, and modelling of a motorcycle riding simulator; citation_author=H Ariour, I Nehaoua, S Hima, N Seguy, S Espie; citation_volume=15; citation_publication_date=2010; citation_pages=805-818; citation_doi=10.1109/TMECH.2009.2035499; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Arch Phys Med Rehabil; citation_title=Virtual reality and haptics as a training device for movement rehabilitation after stroke: a single-case study; citation_author=J Broeren, M Rydmark, K Sunnerhagen; citation_volume=85; citation_issue=8; citation_publication_date=2004; citation_pages=1247-1250; citation_doi=10.1016/j.apmr.2003.09.020; citation_id=CR4"/>

    <meta name="citation_reference" content="Buttolo P, Hannaford B (1995) Pen-based force display for precision manipulation in virtual environments. In: Hannaford B (ed) Virtual reality annual international symposium, pp 217&#8211;224"/>

    <meta name="citation_reference" content="citation_title=CHAI 3D: an open-source library for the rapid development of haptic scenes; citation_publication_date=2005; citation_id=CR6; citation_author=F Conti; citation_author=F Barbagli; citation_author=D Morris; citation_author=C Sewell; citation_publisher=IEEE World Haptics"/>

    <meta name="citation_reference" content="citation_journal_title=Force Dimens Lausanne; citation_title=DELTA haptic device: 6-DOF force feedback interface; citation_author=F Dimension; citation_volume=33; citation_issue=3; citation_publication_date=2004; citation_pages=2006-187; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Intell Autom Soft Comput; citation_title=Haptic technology for micro-robotic cell injection training systems&#8212;a review; citation_author=S Faroque, B Horan, H Adam, M Pangestu, M Joordens; citation_volume=22; citation_issue=3; citation_publication_date=2016; citation_pages=509-523; citation_doi=10.1080/10798587.2015.1109200; citation_id=CR8"/>

    <meta name="citation_reference" content="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254&#8211;260"/>

    <meta name="citation_reference" content="Kadlecek P (2011) Overview of current developments in haptic APIs. In: Proceedings of CESCG"/>

    <meta name="citation_reference" content="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19&#8211;20"/>

    <meta name="citation_reference" content="Li S (2009) The design and implementation of shooting system simulation platform for police college. In: 2009 international conference on scalable computing and communications; eighth international conference on embedded computing, Dalian, pp 566&#8211;570"/>

    <meta name="citation_reference" content="Liu G, Lu K (2011) Networked tank gunnery skill training based on haptic interaction. In: Proceedings of international conference on biomedical engineering and informatics, vol 4, pp 2220&#8211;2224"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Haptics-based virtual reality periodontal training simulator; citation_author=C Luciano, P Banerjee, T DeFanti; citation_volume=13; citation_issue=2; citation_publication_date=2009; citation_pages=69-85; citation_doi=10.1007/s10055-009-0112-7; citation_id=CR14"/>

    <meta name="citation_reference" content="Marin F, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image processing (ICIP). IEEE"/>

    <meta name="citation_reference" content="Martin S, Hillier N (2009) Characterisation of the novint falcon haptic device for application as a robot manipulator. In: Australasian Conference on Robotics and Automation (ACRA)"/>

    <meta name="citation_reference" content="citation_title=Distinguishing vibrotactile effects with tactile mouse and trackball; citation_inbook_title=People and computers XIX the bigger picture; citation_publication_date=2006; citation_pages=337-348; citation_id=CR17; citation_author=J Raisamo; citation_author=R Raisamo; citation_author=K Kosonen; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=J Fash Mark Manag; citation_title=The value of textual haptic information in online clothing shopping; citation_author=T Rodrigues, S Silva, P Duarte; citation_volume=21; citation_issue=1; citation_publication_date=2017; citation_pages=88-102; citation_doi=10.1108/JFMM-02-2016-0018; citation_id=CR18"/>

    <meta name="citation_reference" content="Ruspini D, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques. ACM Press, pp 345&#8211;352"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Phantom-based haptic interaction with virtual objects; citation_author=J Salisbury, M Srinivasan; citation_volume=17; citation_issue=5; citation_publication_date=1997; citation_pages=6-10; citation_doi=10.1109/MCG.1997.1626171; citation_id=CR20"/>

    <meta name="citation_reference" content="Sewell C, Blevins NH, Peddamatham S, Tan HZ, Morris D, Salisbury K (2007) The effect of virtual haptic training on real surgical drilling proficiency. In: Second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC 2007), pp. 22&#8211;24"/>

    <meta name="citation_reference" content="Soetedjo A, Ashari M, Mahmudi A, Nakhoda Y (2014) Implementation of sensor on the gun system using embedded camera for shooting training. In: 2014 2nd international conference on technology, informatics, management, engineering &amp; environment, Bandung, pp 69&#8211;74"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Visual immersive haptic mathematics; citation_author=A Sourin, L Wei; citation_volume=13; citation_issue=4; citation_publication_date=2009; citation_pages=221-234; citation_doi=10.1007/s10055-009-0133-2; citation_id=CR23"/>

    <meta name="citation_reference" content="Tong H, Wang J, Duo Y (2010) Combat effectiveness evaluation of firearms system based on MMESE. In: 2010 international conference on information, networking and automation (ICINA), Kunming, pp 342&#8211;345"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=Function-based visualization and haptic rendering in shared virtual spaces; citation_author=L Wei, A Sourin, O Sourina; citation_volume=24; citation_issue=10; citation_publication_date=2008; citation_pages=871-880; citation_doi=10.1007/s00371-008-0285-1; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Analysis of the accuracy and robustness of the leap motion controller; citation_author=F Weichert, D Bachmann, B Rudak, D Fisseler; citation_volume=13; citation_issue=5; citation_publication_date=2013; citation_pages=6380-6393; citation_doi=10.3390/s130506380; citation_id=CR26"/>

    <meta name="citation_reference" content="Wei L, Huynh L, Zhou H, Nahavandi S (2015) Immersive visuo-haptic rendering in optometry training simulation. In: Proceedings of IEEE international conference on systems, man, and cybernetics (SMC), pp 436&#8211;439"/>

    <meta name="citation_reference" content="Wei L, Zhou H, Soe A, Nahavandi S (2013) Integrating kinect and haptics for interactive STEM education in local and distributed environments. In: Proceedings of IEEE/ASME international conference on advanced intelligent mechatronics, pp 1058&#8211;1065"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Manuf Technol; citation_title=A new type haptics-based virtual environment system for assembly training of complex products; citation_author=P Xia, A Lopes, M Restivo, Y Yao; citation_volume=58; citation_issue=1&#8211;4; citation_publication_date=2012; citation_pages=379-396; citation_doi=10.1007/s00170-011-3381-8; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimed; citation_title=Microsoft kinect sensor and its effect; citation_author=Z Zhang; citation_volume=19; citation_issue=2; citation_publication_date=2012; citation_pages=4-10; citation_doi=10.1109/MMUL.2012.24; citation_id=CR30"/>

    <meta name="citation_reference" content="Zilles C, Salisbury J (1995) A constraint-based god-object method for haptic display. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems 95. Human robot interaction and cooperative robots, vol 3, pp 146&#8211;151"/>

    <meta name="citation_author" content="Lei Wei"/>

    <meta name="citation_author_email" content="lei.wei@deakin.edu.au"/>

    <meta name="citation_author_institution" content="Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, Australia"/>

    <meta name="citation_author" content="Hailing Zhou"/>

    <meta name="citation_author_email" content="hailing.zhou@deakin.edu.au"/>

    <meta name="citation_author_institution" content="Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, Australia"/>

    <meta name="citation_author" content="Saeid Nahavandi"/>

    <meta name="citation_author_email" content="saeid.nahavandi@deakin.edu.au"/>

    <meta name="citation_author_institution" content="Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0349-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0349-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Haptically enabled simulation system for firearm shooting training"/>
        <meta property="og:description" content="Firearm shooting training is of importance in military and law enforcement training tasks. Traditional training usually uses actual firearms or modified bullets that are dangerous and expensive and difficult to evaluate the performance. Firearm training simulation systems provide risk-free alternatives. However, most existing simulation is visual-only, which lacks the immersion on the force feedback. In this paper, we proposed a new firearm training simulation system, which can provide more realistic training by incorporating physic effects on recoil and trigger pull weight. Dynamic, immersive, and repeatable training experiences while imposes no danger to trainees are provided in our system. The system consists of haptics, physics engine, and motion capture. These three components are carefully combined by developing the corresponding techniques of haptic force rendering, visuo-haptic integration, and synchronisation, physics-based dynamic simulation and motion analysis. Compared with existing systems, our training system has more complete functionalities that include visual firearm shooting, force generation, shooting reactions, result analysis and evaluation. Moreover, it is adaptable to off-the-shelf hardware and software packages and thus it can provide flexibility to system scalability and budget. To evaluate the proposed system, two demonstrations are conducted for users where the systems accuracy, immersion and usability are analysed. The results show the effectiveness of our physics-based shooting model and the proposed system on simulating different shooting scenarios."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Haptically enabled simulation system for firearm shooting training | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0349-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual training simulation, Haptics, Physics engine, Motion capture","kwrd":["Virtual_training_simulation","Haptics","Physics_engine","Motion_capture"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0349-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0349-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=349;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0349-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Haptically enabled simulation system for firearm shooting training
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0349-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0349-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I. : Virtual Reality, Augmented Reality and Commerce</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-06-15" itemprop="datePublished">15 June 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Haptically enabled simulation system for firearm shooting training</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Lei-Wei" data-author-popup="auth-Lei-Wei">Lei Wei</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Deakin University" /><meta itemprop="address" content="0000 0001 0526 7079, grid.1021.2, Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hailing-Zhou" data-author-popup="auth-Hailing-Zhou" data-corresp-id="c1">Hailing Zhou<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-3859-189X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-3859-189X</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Deakin University" /><meta itemprop="address" content="0000 0001 0526 7079, grid.1021.2, Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Saeid-Nahavandi" data-author-popup="auth-Saeid-Nahavandi">Saeid Nahavandi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Deakin University" /><meta itemprop="address" content="0000 0001 0526 7079, grid.1021.2, Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">217</span>–<span itemprop="pageEnd">228</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">416 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0349-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Firearm shooting training is of importance in military and law enforcement training tasks. Traditional training usually uses actual firearms or modified bullets that are dangerous and expensive and difficult to evaluate the performance. Firearm training simulation systems provide risk-free alternatives. However, most existing simulation is visual-only, which lacks the immersion on the force feedback. In this paper, we proposed a new firearm training simulation system, which can provide more realistic training by incorporating physic effects on recoil and trigger pull weight. Dynamic, immersive, and repeatable training experiences while imposes no danger to trainees are provided in our system. The system consists of haptics, physics engine, and motion capture. These three components are carefully combined by developing the corresponding techniques of haptic force rendering, visuo-haptic integration, and synchronisation, physics-based dynamic simulation and motion analysis. Compared with existing systems, our training system has more complete functionalities that include visual firearm shooting, force generation, shooting reactions, result analysis and evaluation. Moreover, it is adaptable to off-the-shelf hardware and software packages and thus it can provide flexibility to system scalability and budget. To evaluate the proposed system, two demonstrations are conducted for users where the systems accuracy, immersion and usability are analysed. The results show the effectiveness of our physics-based shooting model and the proposed system on simulating different shooting scenarios.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Firearm shooting training is one of the most crucial and most frequently performed procedures for military and law enforcement training tasks. Since the training procedures can be dangerous and have limited exposure to civilians, only a number of research works has been conducted on this. Most previous research works on firearm training can be classified as two approaches: physical firearm training and visual-only firearm training, each with their own advantages and drawbacks. Visual-only firearm training is usually safe and cost-effective and provides reasonably visual clue for shooting. However, they lack the sense of recoil and other impacts happened during shooting and could not provide enough immersion for such training tasks. In Li (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Li S (2009) The design and implementation of shooting system simulation platform for police college. In: 2009 international conference on scalable computing and communications; eighth international conference on embedded computing, Dalian, pp 566–570" href="/article/10.1007/s10055-018-0349-0#ref-CR12" id="ref-link-section-d56783e381">2009</a>), the author proposed a shooting simulation platform for law enforcement, with different training scenarios and result analysis. In Tong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Tong H, Wang J, Duo Y (2010) Combat effectiveness evaluation of firearms system based on MMESE. In: 2010 international conference on information, networking and automation (ICINA), Kunming, pp 342–345" href="/article/10.1007/s10055-018-0349-0#ref-CR24" id="ref-link-section-d56783e384">2010</a>), the authors proposed a firearm training evaluation system based on the concept of man–machine–environment system engineering (MMESE). In Soetedjo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Soetedjo A, Ashari M, Mahmudi A, Nakhoda Y (2014) Implementation of sensor on the gun system using embedded camera for shooting training. In: 2014 2nd international conference on technology, informatics, management, engineering &amp; environment, Bandung, pp 69–74" href="/article/10.1007/s10055-018-0349-0#ref-CR22" id="ref-link-section-d56783e387">2014</a>), the authors proposed to attach a CMUcam4 onto a firearm and accurately track its movement during shooting.</p><p>Physical firearm training, on the other hand, is based on real or modified firearms, providing great immersion but can also be more dangerous and expensive. Fortunately, with the development on haptically enabled simulation techniques and haptic devices, the artificial touch technology is stepping out of the research environment and into commercial training applications. The force feedback (i.e., the physical based model) can be simulated based on sophisticated, high-fidelity haptic devices and software that deliver high precision and a realistic sense of touch. As indicated in Faroque et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Faroque S, Horan B, Adam H, Pangestu M, Joordens M (2016) Haptic technology for micro-robotic cell injection training systems—a review. Intell Autom Soft Comput 22(3):509–523" href="/article/10.1007/s10055-018-0349-0#ref-CR8" id="ref-link-section-d56783e393">2016</a>), the integration of haptics has shown the improvements in training against speed, accuracy and the time to master skills.</p><p>There are many applications developed using haptically enabled simulation such as vehicle manoeuvering (Ariour et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ariour H, Nehaoua I, Hima S, Seguy N, Espie S (2010) Mechatronics, design, and modelling of a motorcycle riding simulator. IEEE/ASME Trans Mechatron 15:805–818" href="/article/10.1007/s10055-018-0349-0#ref-CR2" id="ref-link-section-d56783e399">2010</a>), assembly operations (Xia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Xia P, Lopes A, Restivo M, Yao Y (2012) A new type haptics-based virtual environment system for assembly training of complex products. Int J Adv Manuf Technol 58(1–4):379–396" href="/article/10.1007/s10055-018-0349-0#ref-CR29" id="ref-link-section-d56783e402">2012</a>), weapons handling (Liu and Lu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Liu G, Lu K (2011) Networked tank gunnery skill training based on haptic interaction. In: Proceedings of international conference on biomedical engineering and informatics, vol 4, pp 2220–2224" href="/article/10.1007/s10055-018-0349-0#ref-CR13" id="ref-link-section-d56783e405">2011</a>), education and entertainment (Wei et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Wei L, Zhou H, Soe A, Nahavandi S (2013) Integrating kinect and haptics for interactive STEM education in local and distributed environments. In: Proceedings of IEEE/ASME international conference on advanced intelligent mechatronics, pp 1058–1065" href="/article/10.1007/s10055-018-0349-0#ref-CR28" id="ref-link-section-d56783e408">2013</a>; Krompiec and Park <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20" href="/article/10.1007/s10055-018-0349-0#ref-CR11" id="ref-link-section-d56783e411">2017</a>). The most frequently seen application is the medical simulation and training. A study on the transference of skills learned in a haptical-enabled virtually environment to performance on a surgically relevant task in the real world has been investigated in Sewell et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sewell C, Blevins NH, Peddamatham S, Tan HZ, Morris D, Salisbury K (2007) The effect of virtual haptic training on real surgical drilling proficiency. In: Second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC 2007), pp. 22–24" href="/article/10.1007/s10055-018-0349-0#ref-CR21" id="ref-link-section-d56783e415">2007</a>). The results reflect the improved task proficiency from the haptic training. In Wei et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Wei L, Huynh L, Zhou H, Nahavandi S (2015) Immersive visuo-haptic rendering in optometry training simulation. In: Proceedings of IEEE international conference on systems, man, and cybernetics (SMC), pp 436–439" href="/article/10.1007/s10055-018-0349-0#ref-CR27" id="ref-link-section-d56783e418">2015</a>), haptic is introduced to simulate the optometry surgery. In Faroque et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Faroque S, Horan B, Adam H, Pangestu M, Joordens M (2016) Haptic technology for micro-robotic cell injection training systems—a review. Intell Autom Soft Comput 22(3):509–523" href="/article/10.1007/s10055-018-0349-0#ref-CR8" id="ref-link-section-d56783e421">2016</a>), haptic technology is comprehensively reviewed on the cell injection training. In Luciano et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Luciano C, Banerjee P, DeFanti T (2009) Haptics-based virtual reality periodontal training simulator. Virtual Real 13(2):69–85" href="/article/10.1007/s10055-018-0349-0#ref-CR14" id="ref-link-section-d56783e424">2009</a>), the use of haptically enabled simulation is exploited in the filed of dentistry where tactile sensations are the primary dependence for dentists to perform diagnostic and surgical procedures of periodontics. All the results show that virtual reality-based training with the haptic feedback greatly increases the users’ sense of immersion and interaction, and the sense of touch provides a vital element for realistic training.</p><p>In the literature, the most relevant work to our focus of this paper is Krompiec and Park (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20" href="/article/10.1007/s10055-018-0349-0#ref-CR11" id="ref-link-section-d56783e430">2017</a>) and Junior et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260" href="/article/10.1007/s10055-018-0349-0#ref-CR9" id="ref-link-section-d56783e433">2012</a>). In Krompiec and Park (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20" href="/article/10.1007/s10055-018-0349-0#ref-CR11" id="ref-link-section-d56783e436">2017</a>), they target the creation of a believable interaction, sensory illusion and realistic haptic experience for a first person shooting game. To build an immersive user experience, HMD, motion controllers, a game system and tracking devices are integrated. In their work, the haptic force feedback is simply simulated from motion controllers which are actually rumble forces without any directions and magnitudes. In Junior et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260" href="/article/10.1007/s10055-018-0349-0#ref-CR9" id="ref-link-section-d56783e439">2012</a>), virtual reality-based shooting training is developed to provide a risk-free alternative for training the police force where the user can make errors without dangerous consequences unlike real-world situations. Their training system consists of interactive videos, 3D computer graphics and laser ray capture. However, the physical interactions (i.e., the simulation of recoils) in the training model are ignored. In addition, a commercial shooting simulation, Virtra 300 LE, has been built by Virtra Systems, Inc. where the recoils are simulated by using compressed gas to activate the weapon’s moving parts during a shot.</p><p>In this paper, our main contribution is the development of a new firearm shooting training system that can provide a safety, immersion and low-cost simulation of shooting. The system consists of haptics, physics engine and motion capture techniques, referring to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig1">1</a> for an illustration. In our system, the force feedback is carefully designed using haptics technology where the recoil and trigger pull weight on users hand are rendered to achieve muscle memory training. Moreover, physics engine is integrated to construct a realistic training scene with the aiming and shooting procedures and shooting results. Motion capture is then used to record the users motion during the shooting and provide analysis on shooting results. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig1">1</a>a shows the overview of our solution. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig1">1</a>b shows the technical flow of the system. There are four major technical problems involved: haptic force rendering, visuo-haptic integration and synchronisation, physics-based dynamic simulation and motion analysis. The solutions of these technical problems are presented in details in the next four sections, respectively.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Haptic force rendering</h2><div class="c-article-section__content" id="Sec2-content"><p>The term haptics (Li <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Li S (2009) The design and implementation of shooting system simulation platform for police college. In: 2009 international conference on scalable computing and communications; eighth international conference on embedded computing, Dalian, pp 566–570" href="/article/10.1007/s10055-018-0349-0#ref-CR12" id="ref-link-section-d56783e463">2009</a>) refers to the incorporation of the sense of touch into virtual worlds, by providing active and passive force feedbacks to users. Based on human perception of virtual objects, haptic devices can be classified as tactile devices (Tong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Tong H, Wang J, Duo Y (2010) Combat effectiveness evaluation of firearms system based on MMESE. In: 2010 international conference on information, networking and automation (ICINA), Kunming, pp 342–345" href="/article/10.1007/s10055-018-0349-0#ref-CR24" id="ref-link-section-d56783e466">2010</a>) and kinaesthetic devices (Soetedjo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Soetedjo A, Ashari M, Mahmudi A, Nakhoda Y (2014) Implementation of sensor on the gun system using embedded camera for shooting training. In: 2014 2nd international conference on technology, informatics, management, engineering &amp; environment, Bandung, pp 69–74" href="/article/10.1007/s10055-018-0349-0#ref-CR22" id="ref-link-section-d56783e469">2014</a>). Tactile devices are attached to human skin so as to provide tactile sensations that can be detected by the skin receptors. Kinaesthetic devices are usually manipulated by the human hand and provide sensations that can be perceived by muscles and joints, such as rotation and translation. In this paper, “haptics” refers to kinaesthetic devices. To simulate the firearm shooting, haptic force rendering is used for:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Calculating various forces involved during firearm shooting, such as recoil and trigger pull weight. The forces should be modelled based on mathematical and physical/pseudo-physical formulae;</p>
                  </li>
                  <li>
                    <p>Increasing the immersion of the shooting training experience by reproducing the force feedbacks to users and assist with muscle memory training;</p>
                  </li>
                  <li>
                    <p>Building multiple firearm profiles with respect to their specifications. Firearm profiles include calibre, weight, barrel length, bullet type, rate of fire, capacity, singe-/double-action trigger and trigger pull weight.</p>
                  </li>
                </ul><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="1245" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Haptically enabled simulation system for firearm shooting training. <b>a</b> System overview; <b>b</b> technical pipeline of the shooting training simulation system in (<b>a</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec3">Recoil</h3><p>Recoil is the momentum, and the angular momentum applied to a firearm when a bullet is discharged from it. For momentum, according to the conservation law of momentum, we have</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{{\text{f}}} = - P_{{\text{b}}}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(P_{{\text{f}}}\)</span> and <span class="mathjax-tex">\(P_{{\text{b}}}\)</span> are the momentum of the firearm and the bullet. This implies that</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} m_{{\text{f}}} \cdot v_{{\text{f}}} = - m_{{\text{b}}} \cdot v_{{\text{b}}}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(m_{{\text{f}}}\)</span> (<span class="mathjax-tex">\(m_{{\text{b}}}\)</span>) and <span class="mathjax-tex">\(v_{{\text{f}}}\)</span> (<span class="mathjax-tex">\(v_{{\text{b}}}\)</span>) are the mass and velocity of the firearm (bullet), respectively.</p><p>For haptic rendering of recoil, the haptic device replaces the actual firearm and the recoil force applied to a haptic device should be the same as its real-life counterpart:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{{\text{f}}} = P_{{\text{h}}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Since <span class="mathjax-tex">\(P_{{\text{f}}}\)</span> can also be represented as <span class="mathjax-tex">\(\int _0^{t_{{\text{r}}}} {F_{{\text{r}}}(t) {\text{d}}t}\)</span>, we have</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{{\text{h}}} = \int _0^{t_{{\text{r}}}} {F_{{\text{r}}}(t) {\text{d}}t} = m_{{\text{f}}} \cdot v_{{\text{f}}}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\(m_{{\text{f}}}\)</span> and <span class="mathjax-tex">\(v_{{\text{f}}}\)</span> are known values for a specific firearm. <span class="mathjax-tex">\(F_{{\text{r}}}\)</span> denotes the recoil force. <span class="mathjax-tex">\(t_{{\text{r}}}\)</span> is the recoil time, which is the time elapsed while the bullet remains within the barrel of the firearm, can be directly measured. To be able to achieve maximum immersion of recoil rendering on a haptic device, we need to make sure that:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} v_{{\text{f}}}= v_{{\text{h}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} W_{{\text{f}}}= W_{{\text{h}}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p><span class="mathjax-tex">\(v_{{\text{f}}}\)</span> (<span class="mathjax-tex">\(v_{{\text{h}}}\)</span>) and <span class="mathjax-tex">\(W_{{\text{f}}}\)</span> (<span class="mathjax-tex">\(W_{{\text{h}}}\)</span>) are the velocity and weight of the actual firearm (the haptic handle), respectively.</p><p>To achieve the same velocity of the haptic device and the firearm, we can calculate the force required to push the haptic handle <span class="mathjax-tex">\(m_{{\text{h}}}\)</span> to achieve <span class="mathjax-tex">\(v_{{\text{f}}}\)</span> within <span class="mathjax-tex">\(t_{{\text{r}}}\)</span>:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F_{{\text{r}}}(t) = m_{{\text{h}}} \cdot a_{{\text{h}}} = m_{{\text{h}}} \cdot (v_{{\text{f}}}/t_{{\text{r}}}). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>In order to have the same weight of the haptic handle and the firearm, we need to constantly render a vertical force to simulate a heavier/lighter weight since the weight of a haptic handle is usually fixed:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} W_{{\text{h}}} = \pm (m_{{\text{f}}}-m_{{\text{h}}})\cdot g. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>The actual force rendering duration is based on haptic frame count <span class="mathjax-tex">\(N_{{\text{frame}}}\)</span>. As typical haptic pipeline runs at 1000 Hz, we have:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} N_{{\text{frame}}} = t_{{\text{r}}} \cdot 1000. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>The actual firearm recoil will not only cause the firearm to move forward and backward, but also cause the firearm to rotate around its centre of mass with a deviation angle <span class="mathjax-tex">\(\theta\)</span> and a torque <span class="mathjax-tex">\(\tau\)</span>.</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \tau =I\frac{{\text{d}}^2\theta }{{\text{d}}t^2}=hF_{{\text{r}}}(t) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I = h^2m_{{\text{f}}}=m_{{\text{h}}}/h^2R_{m_{{\text{h}}}/m_{{\text{f}}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \theta =\frac{hm_{{\text{b}}}L}{I}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where <i>I</i> is moment of inertia, <i>h</i> is the distance between the centre of mass for the firearm and the barrel axis, and <i>L</i> is the barrel length. These formulas demonstrate the feasibility of accurate recoil rendering on haptic devices. They also show that the proposed shooting training simulation system has the ability to simulate different types of firearms by applying different <i>h</i>, <i>L</i> and <i>I</i>.</p><h3 class="c-article__sub-heading" id="Sec4">Trigger pull weight</h3><p>Another force that plays an important role during bullet discharge is the trigger pull weight. Different firearms feature distinct trigger pull weights: smaller for rifles and shotguns and larger for pistols. Rendering trigger pull weights on a haptic device is not straightforward, as most haptic grippers do not have motors under the triggers/buttons and the pull weight for the gripper is a constant value. To solve this problem, we propose to render a series of directional forces opposite to the trigger pulling directions during different stages of the trigger pulling procedure. The duration and magnitude of the forces are profiled according to the actual firearms they represent.</p><p>A typical trigger pulling procedure involves three stages: pre-travel, break and over-travel, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig2">2</a>. Before discharge, the trigger rests at position shown in blue. Pre-travel refers to the movement of the trigger before sear action (from the position shown in blue to the position shown in green in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig2">2</a>). It varies depending on the trigger type: single-action triggers and double-action triggers. Single-action triggers perform only one action during the firing procedure, releasing the striker when trigger is pulled, while double-action triggers first cock and then releases the striker. Therefore, a single-action trigger does not move before releasing at the trigger pull weight, while a double-action trigger has a noticeable take up period, followed by a distinct increase in the force required to pull the trigger right before the break. When the two trigger types are implemented on haptic devices, a single-action trigger model will increase the force in proportion with trigger movement as soon as pre-travel starts, and a double-action trigger model will render no force during take up, followed by profiled force increment until break.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig2_HTML.jpg" alt="figure2" loading="lazy" width="685" height="561" /></picture></a><p class="c-article-section__figure-credit text-right c-article-section__figure-credit-right" data-test="figure-credit">(Reproduced with permission from Krompiec and Park (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20" href="/article/10.1007/s10055-018-0349-0#ref-CR11" id="ref-link-section-d56783e2012">2017</a>)) (colour figure online)</p></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Three stages during firearm shooting: pre-travel (blue to green), break (green to red) and over-travel (red to actual trigger position).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Break is a critical stage of firearm shooting simulation. It can be classified as soft break or crisp break, depending on the type of firearms. Soft break refers to a smooth but discernible amount of trigger travel during break, while crisp break refers to a heavier weight and little or even no discernible movement. When implemented on haptic devices, soft break is represented by linear or near-linear force increment profiles over break time, while crisp break is characterised using nonlinear mathematical functions based on firearm specifications or empirical datasets.</p><p>Over-travel refers to the distance a trigger moves after the sear releases, primarily due to the inertia of the trigger (from red position to actual trigger position in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig2">2</a>). During this stage, the bullet has been fired but remains in the barrel (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig3">3</a>). Counter-recoil, which is the counter momentum applied to human body, also starts to take effect at this stage.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig3_HTML.jpg" alt="figure3" loading="lazy" width="685" height="306" /></picture></a><p class="c-article-section__figure-credit text-right c-article-section__figure-credit-right" data-test="figure-credit">(Reproduced with permission from Junior et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260" href="/article/10.1007/s10055-018-0349-0#ref-CR9" id="ref-link-section-d56783e2050">2012</a>))</p></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>X-ray picture showing over-travel stage, bullet fired but still in the barrel.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Counter-recoil has the same magnitude as recoil but in opposite direction. In free recoil and near-free recoil situations where the firearm is not mounted onto the ground or against a heavy mass, human muscle will take much longer time to damp the counter-recoil momentum than the actual recoil time. According to</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P = \int _0^{t}{F(t)}{\text{d}}t, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>the counter-recoil force, which causes human movement, will be much smaller than the recoil force. Although counter-recoil does not contribute to force rendering, the human movement caused by it at this stage will adversely affect the accuracy of shooting. Therefore, counter-recoil is crucial to the motion analysis of firearm shooting training, which will be discussed in details in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0349-0#Sec7">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="175" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Overview of the proposed middleware including an “abstract device”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Visuo-haptic integration and synchronisation</h2><div class="c-article-section__content" id="Sec5-content"><p>Naturally, vision and touch are the two most dominant components for human sensations (Zilles and Salisbury <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Zilles C, Salisbury J (1995) A constraint-based god-object method for haptic display. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems 95. Human robot interaction and cooperative robots, vol 3, pp 146–151" href="/article/10.1007/s10055-018-0349-0#ref-CR31" id="ref-link-section-d56783e2162">1995</a>). For computer-based systems, visual and haptic rendering pipelines are also independent, but they can be interrelated and complement each other. Typical visual rendering has an update rate of 30–60 Hz, while haptic rendering updates at 1000 Hz. This is required due to differences in the way humans perceive the world using their different senses.</p><p>The basic idea of visuo-haptic integration and synchronisation is to seamlessly integrate haptic pipeline into different visual rendering pipelines. There are two approaches for the integration: (1) rewriting both pipelines from scratch or (2) plugging haptic pipeline into an existing visual pipeline. We decided to choose the latter approach because it can be implemented faster, support flexible functionalities and parameters, without considering compatibility issues with existing visual platforms, and provide extensibility to other visualisation platforms.</p><p>There are many haptic SDKs that can be used for the plug-in purpose such as the OpenHaptic Toolkit (OHTK) (Raisamo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Raisamo J, Raisamo R, Kosonen K (2006) Distinguishing vibrotactile effects with tactile mouse and trackball. In: McEwan T, Gulliksen J, Benyon D (eds) People and computers XIX the bigger picture. Springer, London, pp 337–348" href="/article/10.1007/s10055-018-0349-0#ref-CR17" id="ref-link-section-d56783e2171">2006</a>), CHAI3D (Buttolo and Hannaford <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Buttolo P, Hannaford B (1995) Pen-based force display for precision manipulation in virtual environments. In: Hannaford B (ed) Virtual reality annual international symposium, pp 217–224" href="/article/10.1007/s10055-018-0349-0#ref-CR5" id="ref-link-section-d56783e2174">1995</a>), Novint,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> Force Dimension,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> the Reachin API (Wei et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wei L, Sourin A, Sourina O (2008) Function-based visualization and haptic rendering in shared virtual spaces. Vis Comput 24(10):871–880" href="/article/10.1007/s10055-018-0349-0#ref-CR25" id="ref-link-section-d56783e2203">2008</a>) and SenseGraphics (Salisbury and Srinivasan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Salisbury J, Srinivasan M (1997) Phantom-based haptic interaction with virtual objects. IEEE Comput Graph Appl 17(5):6–10" href="/article/10.1007/s10055-018-0349-0#ref-CR20" id="ref-link-section-d56783e2207">1997</a>; Conti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Conti F, Barbagli F, Morris D, Sewell C (2005) CHAI 3D: an open-source library for the rapid development of haptic scenes. IEEE World Haptics, Pisa" href="/article/10.1007/s10055-018-0349-0#ref-CR6" id="ref-link-section-d56783e2210">2005</a>). In this work, we choose the HDAPI and Novint SDKs with their devises the Sensable Omni (Martin and Hillier <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Martin S, Hillier N (2009) Characterisation of the novint falcon haptic device for application as a robot manipulator. In: Australasian Conference on Robotics and Automation (ACRA)" href="/article/10.1007/s10055-018-0349-0#ref-CR16" id="ref-link-section-d56783e2213">2009</a>) and Novint Falcon as they are adaptive to different research and industrial environments, are relatively inexpensive while providing good precision and force fidelity, and have interchangeable grips (Dimension <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Dimension F (2004) DELTA haptic device: 6-DOF force feedback interface. Force Dimens Lausanne 33(3):2006–187" href="/article/10.1007/s10055-018-0349-0#ref-CR7" id="ref-link-section-d56783e2216">2004</a>; Broeren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Broeren J, Rydmark M, Sunnerhagen K (2004) Virtual reality and haptics as a training device for movement rehabilitation after stroke: a single-case study. Arch Phys Med Rehabil 85(8):1247–1250" href="/article/10.1007/s10055-018-0349-0#ref-CR4" id="ref-link-section-d56783e2219">2004</a>). DI-Guy is used for human modelling and simulation. It facilitates modelling of individuals with different social behaviours that change according to situations and environments while reacting to ongoing events and dynamically objects intelligently.</p><p>To provide a generic design for seamless integration between visual and haptic pipelines, it is required to have common communications among haptic devices from different vendors. This integration must consider a number of factors of the haptic devices such as the degrees of freedom (DOF), workspace, accuracy, maximum force output. Hence, a middle layer between the visual and haptic pipelines is proposed as an “abstract device” to handle the unified integration, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig4">4</a>. The “abstract device” is carefully designed to have the following features:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>Device independence</i> Unlike a mouse and keyboard that is a “plug &amp; play” device with the operating systems supporting the driver uniformly, haptic devices often work on different drivers. We introduce the abstract device to enable “plug &amp; play” functionality for most desktop haptic devices. This feature hides tedious details from the end user and can greatly boost the usability of haptic devices.</p>
                  </li>
                  <li>
                    <p><i>Extendible for generic haptic devices</i> The abstract device is extendible for any off-the-shelf and customised haptic devices. The standard interfaces for haptic devices such as the input/output (I/O) of haptic interaction point (HIP) positions, orientations, button statuses, force vectors and torque vectors have all been accommodated.</p>
                  </li>
                  <li>
                    <p><i>Scaling of force magnitude and workspace</i> Different haptic devices have various maximum force magnitudes, workspaces and accuracy. The abstract device internally queries these specifications from each device driver and automatically unify and map these parameters onto a standard haptic model for direct interaction with external components.</p>
                  </li>
                  <li>
                    <p><i>Augmentation of low Degree-of-Freedom (DOF)</i> Certain haptic device has only 3 DOF input (<i>x</i>, <i>y</i>, <i>z</i>) and lacks the ability to perform rotation (roll, pitch, and yaw). The abstract device can utilise unused buttons on these devices and assign them to control rotations. This converts a 3 DOF input device into a 6DOF device and greatly enhances its usability in many cases.</p>
                  </li>
                </ul></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Physics-based dynamic simulation</h2><div class="c-article-section__content" id="Sec6-content"><p>It is important for a firearm shooting training simulation system that can accurately render the bullet trajectory. The simulation of bullet trajectory is affected by two factors. The first one is how the users perform in the shooting procedure through haptic devices, including aiming, hand shaking, counter-recoil, as well as trigger pulling across the three stages, as discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0349-0#Sec2">2</a>. The second factor is how accurate the bullet trajectories are and how close the results resemble to those in the physical world. As we know, there are multiple factors that can affect the trajectory such as gravity, distance, recoil and also the profiles of a gun. Existing techniques often use a simplified model for simulating the bullet trajectory. Like in Krompiec and Park (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20" href="/article/10.1007/s10055-018-0349-0#ref-CR11" id="ref-link-section-d56783e2282">2017</a>), the gravity and recoil are not considered. In Virtra Systems, Inc. and Junior et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260" href="/article/10.1007/s10055-018-0349-0#ref-CR9" id="ref-link-section-d56783e2285">2012</a>), real gun models with compressed gas are used; however, the specific gun models limit the simulation extensibility. To address those problems, we proposed to have a physics-based dynamic simulation technique which calculates and renders virtual scenario interactions based on physical and pseudo-physical laws and properties (such as mass, velocity, friction) derived from the real world. Through these property definitions in the virtual environment, physics engines can simulate many effects and phenomena similar to those happened in real life.</p><p>Existing physics engines include hardware and software based, such as NVidia PhysX, Havok, Bullet and ODE (Kadlecek <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kadlecek P (2011) Overview of current developments in haptic APIs. In: Proceedings of CESCG" href="/article/10.1007/s10055-018-0349-0#ref-CR10" id="ref-link-section-d56783e2291">2011</a>; Ruspini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ruspini D, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques. ACM Press, pp 345–352" href="/article/10.1007/s10055-018-0349-0#ref-CR19" id="ref-link-section-d56783e2294">1997</a>). However, these physics engines are designed as closed-loop systems driven by virtual forces generated from within the visual rendering environment, rather than by actual forces from external sources such as those from haptic devices. To drive the dynamic simulation, we introduce an approach that can feed user-generated haptic forces into the closed physics engine.</p><p>The principle of the proposed approach is Newton's third law concerning force and counter force. Any time a force vector is rendered on a haptic device, the counter force is supplied to the physics engine and drives the resulting dynamics such as shooting a virtual bullet in the virtual environment. We classify environmental objects as either fixed or interactive. Fixed objects refer to those items which have a large mass or are fixed to the ground. Typical examples are walls and shooting targets. Interactive objects are those can be forced to move when collided with a moving object (such as a bullet), and these are our focus in rendering realistic dynamic simulation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="288" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Undetected collision due to fast moving dynamic object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Our dynamic simulation platform is built based on NVidia PhysX due to the popularity, availability and high performance. With the corresponding SDK, parameters set for the firearm shooting training simulation (discussed as Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0349-0#Sec2">2</a>) includes: <i>I</i>—the moment of inertia of a specific firearm, <i>h</i>—the distance between the centre of mass of the firearm and the barrel axis, and <i>L</i>—the barrel length of a specific firearm. Virtual bullets are dynamically generated based on user input from haptic devices. As soon as a break event occurs, information related to the trigger pulling event is sent to the PhysX engine, which then generates a virtual bullet that carries the trigger pulling information. Moreover, the bullet trajectory in our platform can be affected by</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Translation, rotation and velocity of the haptic grip during trigger pulling, due to counter-recoil and body tremors,</p>
                  </li>
                  <li>
                    <p>Firearm profile, including calibre, weight, barrel length, bullet type, rate of fire, capacity, singe-/double-action trigger and trigger pull weight, and</p>
                  </li>
                  <li>
                    <p>Other physical parameters of the scenario, such as wind speed.</p>
                  </li>
                </ul><p>One remaining problem is the collision detection between the fast moving bullet and objects in the scenarios. Take Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig5">5</a> for an illustration. Visual rendering usually updates at only 30–60 Hz, while the collision between the bullet and a thin object (such as the target) may happen between two consecutive visual frames, which often cause the missing collision detection. Although this rarely happens with slow moving objects, for virtual bullets flying at typically 800 m/s, this happens quite frequently. We solved this issue by adopting temporal coherence-based position extrapolation and execute additional collision detection queries.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Motion capture and analysis</h2><div class="c-article-section__content" id="Sec7-content"><p>There are many motion capture devices, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig6">6</a>. Generally, they can fall into two categories: marker-based and markerless systems. Although marker-based systems can produce high-quality tracking and capture results, markerless systems have advantages over them: the location and workspace of markerless systems are not restricted; they are faster to setup and calibration; the participants do not need to wear a special suit or markers; and the costs are much cheaper.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="211" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Hierarchy of various motion capture approaches and devices</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Among them, two of the most popular markerless systems are the Microsoft Kinect<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> and Leap Motion device (Sourin and Wei <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Sourin A, Wei L (2009) Visual immersive haptic mathematics. Virtual Real 13(4):221–234" href="/article/10.1007/s10055-018-0349-0#ref-CR23" id="ref-link-section-d56783e2401">2009</a>). Microsoft Kinect is a series of input devices for Microsoft Xbox 360 and Xbox One. These devices are small in size yet they are able to track 20 human joints around the body (Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19(2):4–10" href="/article/10.1007/s10055-018-0349-0#ref-CR30" id="ref-link-section-d56783e2404">2012</a>). Leap Motion is designed specifically for human hands. It tracks translations and rotations of ten human fingers at over 200 Hz and has high resolution on hand motion details (1/100 of a millimetre) (Weichert et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Weichert F, Bachmann D, Rudak B, Fisseler D (2013) Analysis of the accuracy and robustness of the leap motion controller. Sensors 13(5):6380–6393" href="/article/10.1007/s10055-018-0349-0#ref-CR26" id="ref-link-section-d56783e2407">2013</a>). For our task, related joint motions during firearm shooting procedure include upper limb motions involving shoulders, elbow and wrist, hand and finger motions. To capture all the relevant motions, we utilise both Microsoft Kinect and Leap Motion devices (Marin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin F, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image processing (ICIP). IEEE" href="/article/10.1007/s10055-018-0349-0#ref-CR15" id="ref-link-section-d56783e2410">2014</a>). Kinect captures a relatively large distance for upper limb tracking, and Leap Motion captures at a closer range for hand and finger tracking. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig7">7</a> shows motion capture examples from Kinect and Leap Motion.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="445" /></picture></a><p class="c-article-section__figure-credit text-right c-article-section__figure-credit-right" data-test="figure-credit">(NVIDA, GEFORCE, <a href="http://www.geforce.com/hardware-/technology/physx">http://www.geforce.com/hardware-/technology/physx</a>)</p></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The major joints that Kinect (left) and Leap Motion can capture (right).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>One problem to be solved is the data synchronisation between Kinect and Leap Motion. We use timestamp from the host computer as references for frame synchronisation of the captured data. For Kinect, the transformations of the shoulder, elbow and wrist of the arm holding the firearm are collected, while for Leap Motion, the transformations of the palm, wrist, and index finger are captured. Since both devices capture wrist transformation, we use it at a base reference to combine each pair of captured data frames into a single frame for further processing.</p><p>During the capturing procedure, occlusion is a common problem, especially with Leap Motion. Leap Motion is originally designed to work with open palm and split fingers, while during the firearm training procedure, the hand grasps the firearm grip, which naturally occlude with the capture. To solve this problem, we test a series of positions that a Leap Motion can be set up over the firearm shooting procedure and then adjust thresholds in the API provided by Leap Motion to achieve an optimal recognition of the palm and index finger.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">System evaluation and experimental results</h2><div class="c-article-section__content" id="Sec8-content"><p>To evaluate the proposed simulation system, we implement two demonstrations to show its practicability and capability. Corresponding user studies are also conducted for performance analysis. Five individuals are invited to voluntarily participate for the user study of the demonstrations under different configurations. Their background are from mechatronics, computer science, processing modelling and military, with ages range from 27 to 51. They all have the shooting experience. In addition, two of them have used haptic devices before, two of them have used Leap Motion, and all of them have used Kinect.</p><h3 class="c-article__sub-heading" id="Sec9">Demonstration 1</h3><p>A scenario for fixed target shooting at different distance is set with different firearm profiles such as weight, barrel length, single-action/double-action, trigger pull weight and soft/crisp break. This demonstration aims to evaluate the accuracy and interactivity of our developed system.</p><p>In this demonstration, we implement it using a plug-in to BS Contact through BS SDK. The plug-in is written in C++, while the scenario was created using VRML/X3D and JavaScript, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig8">8</a>. The scenario includes a fixed target and a number of pistols controlled by Phantom Premium from Geomagic. The virtual pistol features 6 DOF transformation and real-life firearm properties, a button on the haptic stylus is assigned to be the trigger with variable trigger pull weights simulated through vertical forces as discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0349-0#Sec2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="568" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Demonstration 1, pistol discharge simulation with force rendering and collision detection. Black dots on the target show the shooting result</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>When a user is holding the handle and pulling the trigger, a force simulating the trigger pull weight is generated across three trigger pulling stages and in accordance with firearm-specific profiles. During the break stage, a virtual bullet is generated in the scene with the same weight and speed as its real-life counterpart. The initial bullet direction will be determined by the users handling of the haptic handle during the over-travel stage. Recoil will also be rendered on the haptic device and felt by the user. Once fired, the moving trajectory of the bullet will be controlled by the PhysX engine through the gravity, wind speed and other factors in the environment. The PhysX engine also constantly detects possible collisions between the bullet and the target. If a collision occurs, the impact point on the target will be recorded and visually rendered as a hole.</p><p>Participants are asked to fire 20 shots towards a standard 800 mm target at a distance of 15, 25, and 50 m. Shooting results are defined by rings range from one to ten. Ten-ring covers the centre of the target while 1-ring is the outmost ring. During the shooting training simulation, Kinect and Leap Motion devices constantly capture the movement of related joints on the user and analyse the movement according to the recorded frames. Data such as the magnitude of trigger pull weight and recoil force, as well as the displacement of trigger and firearm, are plotted with reference to different trigger pulling stages and analysed against other users data. Such analysis, combined with the target shooting results, provide effective validation for the accuracy of the system.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig9">9</a>, shooting results from one participant in this demonstration are shown. The <i>x</i>-axis represents the number of the bullet being shot, the <i>y</i>-axis represents the shooting accuracy, and the three colours of the line charts represent results at three distances from the target. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig9">9</a>b shows the impact of distance over accuracy for all five participants, and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig9">9</a>c shows the impact of accuracy and time taken to finish the 20 shots. It can be observed that the participants take longer time to aim and shoot at further distance, and the average accuracy still drops quite obviously. After the discussion with participants, it is believed that the reason is due to the lack of stereo vision, especially with targets further away.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig9_HTML.png?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig9_HTML.png" alt="figure9" loading="lazy" width="685" height="1612" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Results of Demonstration 1. <b>a</b> Examples of shooting results from a candidate at three distances: 15, 25, and 50 m. Twenty shots were fired at each distance. <b>b</b> Average accuracy of five candidates at three different shooting distances. <b>c</b> The average time taken of five candidates at three different shooting distances</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec10">Demonstration 2</h3><p>A scenario for shooting training simulation against virtual enemies is created in DI-Guy using scenario editor and Lua scripts, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig10">10</a>. Haptic pipeline is plugged into DI-Guy through its reserved interfaces using C++. The scenario consists of three enemies walking some distance away and trying to shoot a soldier who is controlled by the user and supposed to defend himself. A virtual camera is attached to the solider in order to provide a first person view. The solider is armed with an assault rifle and can perform two behaviours: walking and walking while aiming. Initially, the solider is in walking mode, user controls the soldiers movement and view via keyboard and the haptic device, respectively. A side button on the haptic device is used to activate the aiming behaviour, only in this mode can the solider aim and shoot.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig10_HTML.jpg" alt="figure10" loading="lazy" width="685" height="319" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Demonstration 2, rifle shooting with force rendering, artificial intelligence and collision detection. A user has to shoot enemies before they attack (combat scenario removed)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The target of this demonstration is to quickly move, aim and take the enemy down using as few bullets as possible, before the solider was shot. As the assault rifle is an automatic firearm with fixed magazine capacity, the recoil will be constantly rendered on the haptic device while firing until the magazine is empty, which triggers reloading action. Depending on firearm profiles, the firearm can exhibit different rates of fire, bullet type, magazine capacity, etc. The enemies are controlled by AI-based scripts, and they will try to split themselves, distract the user, shoot the user and evade when being aimed. Bullet impact is calculated based on the haptic input as well as the collision detection results between enemy and bullets, shooting distance, wind, etc.</p><p>This demonstration focuses more on the immersion and interactivity in a time-critical manner, and thus the time taken to complete the task and the number of bullets fired were recorded and analysed. To supplement this, questionnaires were also conducted to collect subjective impressions on accuracy, immersion, and practicability of the demonstration, as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0349-0#Tab1">1</a>. Each question can be scored from 1 to 5, representing from strongly disagree to strongly agree. The results are given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0349-0#Tab2">2</a>. We can see that users are satisfied with the simulation of trigger pull weights where they can feel the effects on aiming and shooting results. In addition, the simulation on single- and double-action triggers also contributes to the realism of shooting. The score of Q7 (i.e., the shooting simulation reassembles my previous shooting experience in real life) is low. The main reason is due to simple shooting scenarios with a conventional display. CAVE and headset devices can be used for future to improve the immersion.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Questionnaires for users</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0349-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Statistics on the results of questionnaires from all the users</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0349-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Based on the questionnaire results and feedbacks collected from the users, the impacts of different firearm configurations among the key factors during firearm shooting training including the force factors from haptics (i.e., trigger pulling weight and the recoil force) and the movement factors from motion capture (i.e., trigger movement and firearm displacement) are illustrated in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig11">11</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig12">12</a>. It should be noted that due to the lack of proper measuring devices on force, displacement and time during shooting, the data in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig11">11</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig12">12</a> depict the observations based on users’ experiences that is not statistically conclusive. The curves demonstrate the relative relationships among the involved factors. The corresponding description is given in detail as follows.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig11">11</a>, the horizontal axis represents the different stages of firearm discharge. The four lines to the left of the figure represent trigger pulling force magnitudes, while the four lines on the right represent recoil force magnitudes. For trigger pulling force magnitude, blue lines represent a single-action trigger (SA), while red lines represent a double-action trigger (DA). Double-action triggers generally have a larger trigger pull weight, and therefore, the force threshold is also higher. The double-action trigger exerts virtually no force during pre-travel. The pulling force starts as the sear begins to move, and the pull weight is reached in a short time. The time taken to reach the pull weight also depends on the break type; crisp break (CB) takes a shorter amount of time to reach the pull weight while soft break (SB) takes a little longer. For a single-action trigger, the pulling force begins as soon as pre-travel starts. The force initially increases almost linearly, then increases more rapidly once the sear starts to move. For recoil force magnitudes shown as the four lines to the right of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig11">11</a>, blue lines represent a single-action trigger while red lines represent a double-action trigger. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig11">11</a> also shows that a crisp break will generate a larger recoil force, while a soft break generates a smaller force. Besides, double-action triggers generate a larger recoil force than single-action triggers.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig11_HTML.png?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig11_HTML.png" alt="figure11" loading="lazy" width="685" height="595" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Illustration on the changes in the trigger pulling weight magnitude (i.e., the left four curves) and recoil force magnitude (i.e., the right four curves) with time. The <i>x</i>-axis represents the time in milliseconds, and the <i>y</i>-axis represents the force in Newton. Note that the data depicts users’ observations, not statistically conclusive</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig12">12</a>, the horizontal axis represents the different stages of firearm discharge. The four lines to the left of the figure represent trigger movement magnitudes, while the four lines to the right of the figure represent firearm displacement magnitudes. For trigger movement magnitudes, red lines represent double-action triggers while green lines represent single-action triggers. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig12">12</a> shows that a double-action trigger begins to move as soon as the user starts to pull the trigger. After pre-travel, the trigger moves faster until it reaches the trigger break position. The trigger continues to move backward during over-travel but slows down before reaching the final trigger position. A single-action trigger displays minimal movement during the pre-travel stage. However, once pre-travel is complete, the trigger moves quickly until it reaches the trigger break position. It is also shown that a crisp break takes more time than a soft break for the trigger to slow down and eventually stop. For firearm displacement magnitudes, which are the four lines to the right of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0349-0#Fig12">12</a>, red lines represent double-action triggers while green lines represent single-action triggers. It can be seen that double-action triggers generate slightly larger displacement of the firearm compared to single-action triggers, and soft break triggers generate a smaller firearm displacement compared to crisp break triggers during the over-travel and post-shot stages.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig12_HTML.png?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0349-0/MediaObjects/10055_2018_349_Fig12_HTML.png" alt="figure12" loading="lazy" width="685" height="587" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Illustration on the changes in trigger movement displacement magnitude (i.e., the left four curves) and gun displacement magnitude (i.e., the right four curves) with time. The <i>x</i>-axis represents the time in milliseconds, and the <i>y</i>-axis represents the displacement in millimetres. Note that the data depict users’ observations, not statistically conclusive</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0349-0/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In addition, we also compare our system with existing shooting simulation techniques in terms of user interface, force feedback and factors that affect the shooting trajectory. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0349-0#Tab3">3</a> shows the results. It can be seen that our system can achieve a more realistic simulation of the shooting procedure. In particular, with the haptic technology, recoil and trigger pull weight can be felt; with the physical engine rendering technique, important factors that can affect the bullet trajectory can easily be added into the simulation. For example, in our simulation, the distance, gravity and recoil factors have been considered while calculating the trajectory in our demonstrations. Moreover, compared with Virtra Systems, Inc. (Junior et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260" href="/article/10.1007/s10055-018-0349-0#ref-CR9" id="ref-link-section-d56783e3309">2012</a>), our system can easily simulate firearms with different profiles. The parameters including calibre, weight, barrel length, bullet type, rate of fire, capacity, singe-/double-action trigger, and trigger pull weight are editable to users during the simulation.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Comparison with existing shooting simulation systems</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0349-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Conclusion and future work</h2><div class="c-article-section__content" id="Sec11-content"><p>In this paper, the authors have identified issues in firearm shooting training simulation and proposed a system to enhance the training experience. The system consists of three major components: haptics, physics engine and motion capture, each of which have been discussed in details as how it can contribute to the final system and how it works with other components. Based on the proposed system, two demonstrations were implemented, each showing different aspects and capabilities of the system, including immersion, interaction, accuracy. We also conducted user studies and questionnaires to further analyse the system. In future, the system will be improved by recruiting more participants to explore the factors important for the shooting training and developing a more immersive shooting environment such as the use of CAVE and headset devices.</p><p>Although this work focuses on firearm shooting training, the proposed system and its core components can be adopted to similar research works in related domains, such as for commerce and retailing. One major limitation of contemporary web-based commerce and retailing is that potential customers lack the tactile information of the merchandise. As indicated in Rodrigues et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Rodrigues T, Silva S, Duarte P (2017) The value of textual haptic information in online clothing shopping. J Fash Mark Manag 21(1):88–102" href="/article/10.1007/s10055-018-0349-0#ref-CR18" id="ref-link-section-d56783e3509">2017</a>), the need for touch is important for consumers’ perception, especially in online environments. This can be crucial for some products such as fabrics and leathers. In the future, we plan to apply the proposed system and its core components to the area of commerce and retailing and attempt to solve the above-mentioned issue be by introducing tactile sensation and physics-based interaction to customers and allow them to make more informed purchases. In particular, it is possible to build a virtual store with the haptic technology to feel the texture and gravity, the physical engine-based simulation to render interactions from users, and the motion capture techniques to track the movements of users and browse the store. To allow users to feel the contact of an object, we can add “texture” properties where users can feel different materials once they put their fingers on a touchscreen device.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>KelTec products, PF-9, <a href="http://www.thektog.org/forum/f95/pops-aluminium-trigger-please-help-also-black-anodization-pics-245462">http://www.thektog.org/forum/f95/pops-aluminium-trigger-please-help-also-black-anodization-pics-245462</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>X-Ray gun shots, <a href="https://www.outdoorlife.com/node/1006033110">https://www.outdoorlife.com/node/1006033110</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>3D SYSTEMS, The Touch Haptic Device, <a href="http://www.geomagic.com/en/products/phantom-omni/overview">http://www.geomagic.com/en/products/phantom-omni/overview</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Ariour, I. Nehaoua, S. Hima, N. Seguy, S. Espie, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ariour H, Nehaoua I, Hima S, Seguy N, Espie S (2010) Mechatronics, design, and modelling of a motorcycle ridin" /><p class="c-article-references__text" id="ref-CR2">Ariour H, Nehaoua I, Hima S, Seguy N, Espie S (2010) Mechatronics, design, and modelling of a motorcycle riding simulator. IEEE/ASME Trans Mechatron 15:805–818</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMECH.2009.2035499" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mechatronics%2C%20design%2C%20and%20modelling%20of%20a%20motorcycle%20riding%20simulator&amp;journal=IEEE%2FASME%20Trans%20Mechatron&amp;volume=15&amp;pages=805-818&amp;publication_year=2010&amp;author=Ariour%2CH&amp;author=Nehaoua%2CI&amp;author=Hima%2CS&amp;author=Seguy%2CN&amp;author=Espie%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Broeren, M. Rydmark, K. Sunnerhagen, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Broeren J, Rydmark M, Sunnerhagen K (2004) Virtual reality and haptics as a training device for movement rehab" /><p class="c-article-references__text" id="ref-CR4">Broeren J, Rydmark M, Sunnerhagen K (2004) Virtual reality and haptics as a training device for movement rehabilitation after stroke: a single-case study. Arch Phys Med Rehabil 85(8):1247–1250</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.apmr.2003.09.020" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20and%20haptics%20as%20a%20training%20device%20for%20movement%20rehabilitation%20after%20stroke%3A%20a%20single-case%20study&amp;journal=Arch%20Phys%20Med%20Rehabil&amp;volume=85&amp;issue=8&amp;pages=1247-1250&amp;publication_year=2004&amp;author=Broeren%2CJ&amp;author=Rydmark%2CM&amp;author=Sunnerhagen%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buttolo P, Hannaford B (1995) Pen-based force display for precision manipulation in virtual environments. In: " /><p class="c-article-references__text" id="ref-CR5">Buttolo P, Hannaford B (1995) Pen-based force display for precision manipulation in virtual environments. In: Hannaford B (ed) Virtual reality annual international symposium, pp 217–224</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="F. Conti, F. Barbagli, D. Morris, C. Sewell, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Conti F, Barbagli F, Morris D, Sewell C (2005) CHAI 3D: an open-source library for the rapid development of ha" /><p class="c-article-references__text" id="ref-CR6">Conti F, Barbagli F, Morris D, Sewell C (2005) CHAI 3D: an open-source library for the rapid development of haptic scenes. IEEE World Haptics, Pisa</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=CHAI%203D%3A%20an%20open-source%20library%20for%20the%20rapid%20development%20of%20haptic%20scenes&amp;publication_year=2005&amp;author=Conti%2CF&amp;author=Barbagli%2CF&amp;author=Morris%2CD&amp;author=Sewell%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Dimension, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Dimension F (2004) DELTA haptic device: 6-DOF force feedback interface. Force Dimens Lausanne 33(3):2006–187" /><p class="c-article-references__text" id="ref-CR7">Dimension F (2004) DELTA haptic device: 6-DOF force feedback interface. Force Dimens Lausanne 33(3):2006–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=DELTA%20haptic%20device%3A%206-DOF%20force%20feedback%20interface&amp;journal=Force%20Dimens%20Lausanne&amp;volume=33&amp;issue=3&amp;pages=2006-187&amp;publication_year=2004&amp;author=Dimension%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Faroque, B. Horan, H. Adam, M. Pangestu, M. Joordens, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Faroque S, Horan B, Adam H, Pangestu M, Joordens M (2016) Haptic technology for micro-robotic cell injection t" /><p class="c-article-references__text" id="ref-CR8">Faroque S, Horan B, Adam H, Pangestu M, Joordens M (2016) Haptic technology for micro-robotic cell injection training systems—a review. Intell Autom Soft Comput 22(3):509–523</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10798587.2015.1109200" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20technology%20for%20micro-robotic%20cell%20injection%20training%20systems%E2%80%94a%20review&amp;journal=Intell%20Autom%20Soft%20Comput&amp;volume=22&amp;issue=3&amp;pages=509-523&amp;publication_year=2016&amp;author=Faroque%2CS&amp;author=Horan%2CB&amp;author=Adam%2CH&amp;author=Pangestu%2CM&amp;author=Joordens%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting " /><p class="c-article-references__text" id="ref-CR9">Junior A, Gomes G, Junior N, Santos A, Vidal C, Cavalcante-Neto J, Gattass M (2012) System model for shooting training based on interactive video, three-dimensional computer graphics and laser ray capture. In: 14th symposium on virtual and augmented reality, Rio Janiero, pp 254–260</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kadlecek P (2011) Overview of current developments in haptic APIs. In: Proceedings of CESCG" /><p class="c-article-references__text" id="ref-CR10">Kadlecek P (2011) Overview of current developments in haptic APIs. In: Proceedings of CESCG</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE inter" /><p class="c-article-references__text" id="ref-CR11">Krompiec P, Park K (2017) Enhanced player interaction using motion controllers for VR FPS. In: 2017 IEEE international conference on consumer electronics (ICCE), Las Vegas, NV, pp 19–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li S (2009) The design and implementation of shooting system simulation platform for police college. In: 2009 " /><p class="c-article-references__text" id="ref-CR12">Li S (2009) The design and implementation of shooting system simulation platform for police college. In: 2009 international conference on scalable computing and communications; eighth international conference on embedded computing, Dalian, pp 566–570</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liu G, Lu K (2011) Networked tank gunnery skill training based on haptic interaction. In: Proceedings of inter" /><p class="c-article-references__text" id="ref-CR13">Liu G, Lu K (2011) Networked tank gunnery skill training based on haptic interaction. In: Proceedings of international conference on biomedical engineering and informatics, vol 4, pp 2220–2224</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Luciano, P. Banerjee, T. DeFanti, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Luciano C, Banerjee P, DeFanti T (2009) Haptics-based virtual reality periodontal training simulator. Virtual " /><p class="c-article-references__text" id="ref-CR14">Luciano C, Banerjee P, DeFanti T (2009) Haptics-based virtual reality periodontal training simulator. Virtual Real 13(2):69–85</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0112-7" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptics-based%20virtual%20reality%20periodontal%20training%20simulator&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=2&amp;pages=69-85&amp;publication_year=2009&amp;author=Luciano%2CC&amp;author=Banerjee%2CP&amp;author=DeFanti%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marin F, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 " /><p class="c-article-references__text" id="ref-CR15">Marin F, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image processing (ICIP). IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martin S, Hillier N (2009) Characterisation of the novint falcon haptic device for application as a robot mani" /><p class="c-article-references__text" id="ref-CR16">Martin S, Hillier N (2009) Characterisation of the novint falcon haptic device for application as a robot manipulator. In: Australasian Conference on Robotics and Automation (ACRA)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Raisamo, R. Raisamo, K. Kosonen, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Raisamo J, Raisamo R, Kosonen K (2006) Distinguishing vibrotactile effects with tactile mouse and trackball. I" /><p class="c-article-references__text" id="ref-CR17">Raisamo J, Raisamo R, Kosonen K (2006) Distinguishing vibrotactile effects with tactile mouse and trackball. In: McEwan T, Gulliksen J, Benyon D (eds) People and computers XIX the bigger picture. Springer, London, pp 337–348</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=People%20and%20computers%20XIX%20the%20bigger%20picture&amp;pages=337-348&amp;publication_year=2006&amp;author=Raisamo%2CJ&amp;author=Raisamo%2CR&amp;author=Kosonen%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Rodrigues, S. Silva, P. Duarte, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Rodrigues T, Silva S, Duarte P (2017) The value of textual haptic information in online clothing shopping. J F" /><p class="c-article-references__text" id="ref-CR18">Rodrigues T, Silva S, Duarte P (2017) The value of textual haptic information in online clothing shopping. J Fash Mark Manag 21(1):88–102</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2FJFMM-02-2016-0018" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20value%20of%20textual%20haptic%20information%20in%20online%20clothing%20shopping&amp;journal=J%20Fash%20Mark%20Manag&amp;volume=21&amp;issue=1&amp;pages=88-102&amp;publication_year=2017&amp;author=Rodrigues%2CT&amp;author=Silva%2CS&amp;author=Duarte%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruspini D, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR19">Ruspini D, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques. ACM Press, pp 345–352</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Salisbury, M. Srinivasan, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Salisbury J, Srinivasan M (1997) Phantom-based haptic interaction with virtual objects. IEEE Comput Graph Appl" /><p class="c-article-references__text" id="ref-CR20">Salisbury J, Srinivasan M (1997) Phantom-based haptic interaction with virtual objects. IEEE Comput Graph Appl 17(5):6–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.1997.1626171" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Phantom-based%20haptic%20interaction%20with%20virtual%20objects&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=17&amp;issue=5&amp;pages=6-10&amp;publication_year=1997&amp;author=Salisbury%2CJ&amp;author=Srinivasan%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sewell C, Blevins NH, Peddamatham S, Tan HZ, Morris D, Salisbury K (2007) The effect of virtual haptic trainin" /><p class="c-article-references__text" id="ref-CR21">Sewell C, Blevins NH, Peddamatham S, Tan HZ, Morris D, Salisbury K (2007) The effect of virtual haptic training on real surgical drilling proficiency. In: Second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC 2007), pp. 22–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Soetedjo A, Ashari M, Mahmudi A, Nakhoda Y (2014) Implementation of sensor on the gun system using embedded ca" /><p class="c-article-references__text" id="ref-CR22">Soetedjo A, Ashari M, Mahmudi A, Nakhoda Y (2014) Implementation of sensor on the gun system using embedded camera for shooting training. In: 2014 2nd international conference on technology, informatics, management, engineering &amp; environment, Bandung, pp 69–74</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Sourin, L. Wei, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Sourin A, Wei L (2009) Visual immersive haptic mathematics. Virtual Real 13(4):221–234" /><p class="c-article-references__text" id="ref-CR23">Sourin A, Wei L (2009) Visual immersive haptic mathematics. Virtual Real 13(4):221–234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0133-2" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20immersive%20haptic%20mathematics&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=4&amp;pages=221-234&amp;publication_year=2009&amp;author=Sourin%2CA&amp;author=Wei%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tong H, Wang J, Duo Y (2010) Combat effectiveness evaluation of firearms system based on MMESE. In: 2010 inter" /><p class="c-article-references__text" id="ref-CR24">Tong H, Wang J, Duo Y (2010) Combat effectiveness evaluation of firearms system based on MMESE. In: 2010 international conference on information, networking and automation (ICINA), Kunming, pp 342–345</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wei, A. Sourin, O. Sourina, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Wei L, Sourin A, Sourina O (2008) Function-based visualization and haptic rendering in shared virtual spaces. " /><p class="c-article-references__text" id="ref-CR25">Wei L, Sourin A, Sourina O (2008) Function-based visualization and haptic rendering in shared virtual spaces. Vis Comput 24(10):871–880</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-008-0285-1" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Function-based%20visualization%20and%20haptic%20rendering%20in%20shared%20virtual%20spaces&amp;journal=Vis%20Comput&amp;volume=24&amp;issue=10&amp;pages=871-880&amp;publication_year=2008&amp;author=Wei%2CL&amp;author=Sourin%2CA&amp;author=Sourina%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Weichert, D. Bachmann, B. Rudak, D. Fisseler, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Weichert F, Bachmann D, Rudak B, Fisseler D (2013) Analysis of the accuracy and robustness of the leap motion " /><p class="c-article-references__text" id="ref-CR26">Weichert F, Bachmann D, Rudak B, Fisseler D (2013) Analysis of the accuracy and robustness of the leap motion controller. Sensors 13(5):6380–6393</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs130506380" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20the%20accuracy%20and%20robustness%20of%20the%20leap%20motion%20controller&amp;journal=Sensors&amp;volume=13&amp;issue=5&amp;pages=6380-6393&amp;publication_year=2013&amp;author=Weichert%2CF&amp;author=Bachmann%2CD&amp;author=Rudak%2CB&amp;author=Fisseler%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wei L, Huynh L, Zhou H, Nahavandi S (2015) Immersive visuo-haptic rendering in optometry training simulation. " /><p class="c-article-references__text" id="ref-CR27">Wei L, Huynh L, Zhou H, Nahavandi S (2015) Immersive visuo-haptic rendering in optometry training simulation. In: Proceedings of IEEE international conference on systems, man, and cybernetics (SMC), pp 436–439</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wei L, Zhou H, Soe A, Nahavandi S (2013) Integrating kinect and haptics for interactive STEM education in loca" /><p class="c-article-references__text" id="ref-CR28">Wei L, Zhou H, Soe A, Nahavandi S (2013) Integrating kinect and haptics for interactive STEM education in local and distributed environments. In: Proceedings of IEEE/ASME international conference on advanced intelligent mechatronics, pp 1058–1065</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Xia, A. Lopes, M. Restivo, Y. Yao, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Xia P, Lopes A, Restivo M, Yao Y (2012) A new type haptics-based virtual environment system for assembly train" /><p class="c-article-references__text" id="ref-CR29">Xia P, Lopes A, Restivo M, Yao Y (2012) A new type haptics-based virtual environment system for assembly training of complex products. Int J Adv Manuf Technol 58(1–4):379–396</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00170-011-3381-8" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20type%20haptics-based%20virtual%20environment%20system%20for%20assembly%20training%20of%20complex%20products&amp;journal=Int%20J%20Adv%20Manuf%20Technol&amp;volume=58&amp;issue=1%E2%80%934&amp;pages=379-396&amp;publication_year=2012&amp;author=Xia%2CP&amp;author=Lopes%2CA&amp;author=Restivo%2CM&amp;author=Yao%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19(2):4–10" /><p class="c-article-references__text" id="ref-CR30">Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19(2):4–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMMUL.2012.24" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Microsoft%20kinect%20sensor%20and%20its%20effect&amp;journal=IEEE%20Multimed&amp;volume=19&amp;issue=2&amp;pages=4-10&amp;publication_year=2012&amp;author=Zhang%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zilles C, Salisbury J (1995) A constraint-based god-object method for haptic display. In: Proceedings of IEEE/" /><p class="c-article-references__text" id="ref-CR31">Zilles C, Salisbury J (1995) A constraint-based god-object method for haptic display. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems 95. Human robot interaction and cooperative robots, vol 3, pp 146–151</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0349-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="Fun"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>Funding was provided by Defence Science Institute (Grant No. 50000).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia</p><p class="c-article-author-affiliation__authors-list">Lei Wei, Hailing Zhou &amp; Saeid Nahavandi</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Lei-Wei"><span class="c-article-authors-search__title u-h3 js-search-name">Lei Wei</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Lei+Wei&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lei+Wei" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lei+Wei%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hailing-Zhou"><span class="c-article-authors-search__title u-h3 js-search-name">Hailing Zhou</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hailing+Zhou&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hailing+Zhou" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hailing+Zhou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Saeid-Nahavandi"><span class="c-article-authors-search__title u-h3 js-search-name">Saeid Nahavandi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Saeid+Nahavandi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Saeid+Nahavandi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Saeid+Nahavandi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0349-0/email/correspondent/c1/new">Hailing Zhou</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Haptically%20enabled%20simulation%20system%20for%20firearm%20shooting%20training&amp;author=Lei%20Wei%20et%20al&amp;contentID=10.1007%2Fs10055-018-0349-0&amp;publication=1359-4338&amp;publicationDate=2018-06-15&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0349-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0349-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Wei, L., Zhou, H. &amp; Nahavandi, S. Haptically enabled simulation system for firearm shooting training.
                    <i>Virtual Reality</i> <b>23, </b>217–228 (2019). https://doi.org/10.1007/s10055-018-0349-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0349-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-02-20">20 February 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-05-14">14 May 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-06-15">15 June 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-09-01">01 September 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0349-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0349-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual training simulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Physics engine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Motion capture</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0349-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=349;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

