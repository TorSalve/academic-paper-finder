<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Enabling design and interactive selection of haptic modes"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The ever increasing size and complexity of volumetric data in a wide range of disciplines makes it useful to augment volume visualization tools with alternative modalities. Studies have shown that..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/11/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Enabling design and interactive selection of haptic modes"/>

    <meta name="dc.source" content="Virtual Reality 2006 11:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-07-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The ever increasing size and complexity of volumetric data in a wide range of disciplines makes it useful to augment volume visualization tools with alternative modalities. Studies have shown that introducing haptics can significantly increase both exploration speed and precision. It is also capable of conveying material properties of data and thus has great potential to improve user performance in volume data exploration. In this paper we describe how recent advances in volume haptics can be used to build haptic modes&#8212;building blocks for haptic schemes. These modes have been used as base components of a toolkit allowing for more efficient development of haptic prototypes and applications. This toolkit allows interactive construction, configuration and fine-tuning of both visual and haptic representations of the data. The technology is also used in a pilot study to determine the most important issues and aspects in haptic volume data interaction and exploration, and how the use of haptic modes can facilitate the implementation of effective haptic schemes."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-07-11"/>

    <meta name="prism.volume" content="11"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="13"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0033-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0033-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0033-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0033-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Enabling design and interactive selection of haptic modes"/>

    <meta name="citation_volume" content="11"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2007/03"/>

    <meta name="citation_online_date" content="2006/07/11"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="13"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0033-7"/>

    <meta name="DOI" content="10.1007/s10055-006-0033-7"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0033-7"/>

    <meta name="description" content="The ever increasing size and complexity of volumetric data in a wide range of disciplines makes it useful to augment volume visualization tools with altern"/>

    <meta name="dc.creator" content="Karljohan Lundin"/>

    <meta name="dc.creator" content="Matthew Cooper"/>

    <meta name="dc.creator" content="Anders Persson"/>

    <meta name="dc.creator" content="Daniel Evestedt"/>

    <meta name="dc.creator" content="Anders Ynnerman"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adams RJ, Klowden D, Hannaford B (2001) Virtual training for manual assembly task. Haptics-e. Electron J Haptics Res (
                    http://www.haptics-e.org
                    
                  ), 2(2), October 2001"/>

    <meta name="citation_reference" content="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: Proceedings at IEEE visualization, October 1996, pp 197&#8211;204"/>

    <meta name="citation_reference" content="Aviles W, Ranta J (1999) Haptic interaction with geoscientific data. In: Proceedings at phantom user group workshop&#8217;99, 1999"/>

    <meta name="citation_reference" content="Donald BR, Henle F (2000) Using haptics vector fields for animation motion control. In: Proceedings of IEEE international conference on robotics and automation"/>

    <meta name="citation_reference" content="citation_journal_title=Heart; citation_title=Three dimensional flow in the human left atrium; citation_author=A Fyrenius, L Wigstr&#246;m, T Ebbers, M Karlsson, J Engvall, AF Bolger; citation_volume=86; citation_publication_date=2001; citation_pages=448-455; citation_doi=10.1136/heart.86.4.448; citation_id=CR5"/>

    <meta name="citation_reference" content="Hashimoto W, Iwata H (1997) A versatile software platform for visual/haptic environment. In: Proceedings of ICAT&#39;97, pp 106&#8211;114"/>

    <meta name="citation_reference" content="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization &#8217;03, pp 263&#8211;269"/>

    <meta name="citation_reference" content="Infed F, Brown SV, Lee CD, Lawrence DA, Dougherty AM, Pao LY (1999) Combined visual/haptic rendering modes for scientific visualization. In: Proceedings of 8th annual symposium on haptic interfaces for virtual environment and teleoperator systems"/>

    <meta name="citation_reference" content="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16&#8211;23"/>

    <meta name="citation_reference" content="Kirkpatrick AE, Douglas SA (2002) Application-based evaluation of haptic interfaces. In: Proceedings of the 10th symposium on haptic interfaces for virtual environments and teleoperator systems"/>

    <meta name="citation_reference" content="Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic interface. In: Proceedings of IEEE conference on visualization and computer graphics"/>

    <meta name="citation_reference" content="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104&#8211;109"/>

    <meta name="citation_reference" content="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31&#8211;41"/>

    <meta name="citation_reference" content="Lundin K, Cooper M, Ynnerman A (2005b) The orthogonal constraints problem with the constraint approach to proxy-based volume haptics and a solution. In: Proceedings of SIGRAD conference, Lund, Sweden, November 2005. SIGRAD, pp 45&#8211;49"/>

    <meta name="citation_reference" content="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557&#8211;560"/>

    <meta name="citation_reference" content="Maciejewski R, Choi S, Ebert D, Tan H (2005) Multi-modal perceptualization of volumetric data and its application to molecular docking. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 511&#8211;514"/>

    <meta name="citation_reference" content="Mor A, Gibson S, Samosky J (1996) Interacting with 3-dimensional medical data: haptic feedback for surgical simulation. In: Proceedings of phantom user group workshop&#8217;96, 1996"/>

    <meta name="citation_reference" content="Pao L, Lawrence D (1998) Synergistic visual/haptic computer interfaces. In: Proceedings of Japan/USA/Vietnam workshop on research and education in systems, computation, and control engineering"/>

    <meta name="citation_reference" content="Passmore PJ, Nielsen CF, Cosh WJ, Darzi A (2001) Effects of viewing and orientation on path following in a medical teleoperation environment. In: Proceedings of IEEE virtual reality 2001"/>

    <meta name="citation_reference" content="Wall S, Harwin W (2000) Quantification of the effects of haptic feedback during a motor skills task in a simulated environment. In: Proceedings at phantom user research symposium&#8217;00, 2000"/>

    <meta name="citation_reference" content="Wall SA, Paynter K, Shillito AM, Wright M, Scali S (2002) The effect of haptic feedback and stereo graphics in a 3D target acquisition task. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom"/>

    <meta name="citation_reference" content="citation_journal_title=Magn Reson Med; citation_title=Temporally resolved 3D phase-contrast imaging; citation_author=L Wigstr&#246;m, L Sj&#246;qvist, B Wranne; citation_volume=35; citation_issue=5; citation_publication_date=1996; citation_pages=800-803; citation_doi=10.1002/mrm.1910360521; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Magn Reson Med; citation_title=Particle trace visualization of intracardiac flow using time-resolved 3D phase contrast mri; citation_author=L Wigstr&#246;m, T Ebbers, A Fyrenius, M Karlsson, J Engvall, B Wranne, AF Bolger; citation_volume=41; citation_publication_date=1999; citation_pages=793-799; citation_doi=10.1002/(SICI)1522-2594(199904)41:4&lt;793::AID-MRM19&gt;3.0.CO;2-2; citation_id=CR23"/>

    <meta name="citation_author" content="Karljohan Lundin"/>

    <meta name="citation_author_email" content="karlu@itn.liu.se"/>

    <meta name="citation_author_institution" content="Norrk&#246;ping Visualization and Interaction Studio, Link&#246;ping University, Link&#246;ping, Sweden"/>

    <meta name="citation_author" content="Matthew Cooper"/>

    <meta name="citation_author_email" content="matco@itn.liu.se"/>

    <meta name="citation_author_institution" content="Norrk&#246;ping Visualization and Interaction Studio, Link&#246;ping University, Link&#246;ping, Sweden"/>

    <meta name="citation_author" content="Anders Persson"/>

    <meta name="citation_author_email" content="anders.persson@cmiv.liu.se"/>

    <meta name="citation_author_institution" content="Center for Medical Image Science and Visualization, Link&#246;ping University, Link&#246;ping, Sweden"/>

    <meta name="citation_author" content="Daniel Evestedt"/>

    <meta name="citation_author_email" content="daniel.evestedt@sensegraphics.com"/>

    <meta name="citation_author_institution" content="SenseGraphics AB, Stockholm, Sweden"/>

    <meta name="citation_author" content="Anders Ynnerman"/>

    <meta name="citation_author_email" content="andyn@itn.liu.se"/>

    <meta name="citation_author_institution" content="Norrk&#246;ping Visualization and Interaction Studio, Link&#246;ping University, Link&#246;ping, Sweden"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0033-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2007/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0033-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Enabling design and interactive selection of haptic modes"/>
        <meta property="og:description" content="The ever increasing size and complexity of volumetric data in a wide range of disciplines makes it useful to augment volume visualization tools with alternative modalities. Studies have shown that introducing haptics can significantly increase both exploration speed and precision. It is also capable of conveying material properties of data and thus has great potential to improve user performance in volume data exploration. In this paper we describe how recent advances in volume haptics can be used to build haptic modes—building blocks for haptic schemes. These modes have been used as base components of a toolkit allowing for more efficient development of haptic prototypes and applications. This toolkit allows interactive construction, configuration and fine-tuning of both visual and haptic representations of the data. The technology is also used in a pilot study to determine the most important issues and aspects in haptic volume data interaction and exploration, and how the use of haptic modes can facilitate the implementation of effective haptic schemes."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Enabling design and interactive selection of haptic modes | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0033-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Volume haptics, Haptic modes, Toolkit, User study","kwrd":["Volume_haptics","Haptic_modes","Toolkit","User_study"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0033-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0033-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=33;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0033-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Enabling design and interactive selection of haptic modes
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0033-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0033-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-07-11" itemprop="datePublished">11 July 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Enabling design and interactive selection of haptic modes</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Karljohan-Lundin" data-author-popup="auth-Karljohan-Lundin" data-corresp-id="c1">Karljohan Lundin<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Linköping University" /><meta itemprop="address" content="grid.5640.7, 0000000121629922, Norrköping Visualization and Interaction Studio, Linköping University, Linköping, Sweden" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Matthew-Cooper" data-author-popup="auth-Matthew-Cooper">Matthew Cooper</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Linköping University" /><meta itemprop="address" content="grid.5640.7, 0000000121629922, Norrköping Visualization and Interaction Studio, Linköping University, Linköping, Sweden" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Anders-Persson" data-author-popup="auth-Anders-Persson">Anders Persson</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Linköping University" /><meta itemprop="address" content="grid.5640.7, 0000000121629922, Center for Medical Image Science and Visualization, Linköping University, Linköping, Sweden" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniel-Evestedt" data-author-popup="auth-Daniel-Evestedt">Daniel Evestedt</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="SenseGraphics AB" /><meta itemprop="address" content="grid.451803.b, SenseGraphics AB, Stockholm, Sweden" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Anders-Ynnerman" data-author-popup="auth-Anders-Ynnerman">Anders Ynnerman</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Linköping University" /><meta itemprop="address" content="grid.5640.7, 0000000121629922, Norrköping Visualization and Interaction Studio, Linköping University, Linköping, Sweden" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 11</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">1</span>–<span itemprop="pageEnd">13</span>(<span data-test="article-publication-year">2007</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">167 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">12 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0033-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The ever increasing size and complexity of volumetric data in a wide range of disciplines makes it useful to augment volume visualization tools with alternative modalities. Studies have shown that introducing haptics can significantly increase both exploration speed and precision. It is also capable of conveying material properties of data and thus has great potential to improve user performance in volume data exploration. In this paper we describe how recent advances in volume haptics can be used to build haptic modes—building blocks for haptic schemes. These modes have been used as base components of a toolkit allowing for more efficient development of haptic prototypes and applications. This toolkit allows interactive construction, configuration and fine-tuning of both visual and haptic representations of the data. The technology is also used in a pilot study to determine the most important issues and aspects in haptic volume data interaction and exploration, and how the use of haptic modes can facilitate the implementation of effective haptic schemes.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Volume visualization is rapidly becoming an indispensable tool in the analysis of the vast amount of information contained in volumetric data. The development of efficient tools that will support the analysis and filtering of this data continues to pose research challenges. Evaluations of simple, well defined tasks (Wall and Harwin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Wall S, Harwin W (2000) Quantification of the effects of haptic feedback during a motor skills task in a simulated environment. In: Proceedings at phantom user research symposium’00, 2000" href="/article/10.1007/s10055-006-0033-7#ref-CR20" id="ref-link-section-d44671e373">2000</a>; Kirkpatrick and Douglas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kirkpatrick AE, Douglas SA (2002) Application-based evaluation of haptic interfaces. In: Proceedings of the 10th symposium on haptic interfaces for virtual environments and teleoperator systems" href="/article/10.1007/s10055-006-0033-7#ref-CR10" id="ref-link-section-d44671e376">2002</a>; Passmore et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Passmore PJ, Nielsen CF, Cosh WJ, Darzi A (2001) Effects of viewing and orientation on path following in a medical teleoperation environment. In: Proceedings of IEEE virtual reality 2001" href="/article/10.1007/s10055-006-0033-7#ref-CR19" id="ref-link-section-d44671e379">2001</a>; Iwata and Noma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23" href="/article/10.1007/s10055-006-0033-7#ref-CR9" id="ref-link-section-d44671e382">1993</a>; Adams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Adams RJ, Klowden D, Hannaford B (2001) Virtual training for manual assembly task. Haptics-e. Electron J Haptics Res (&#xA;                    http://www.haptics-e.org&#xA;                    &#xA;                  ), 2(2), October 2001" href="/article/10.1007/s10055-006-0033-7#ref-CR1" id="ref-link-section-d44671e385">2001</a>; Wall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Wall SA, Paynter K, Shillito AM, Wright M, Scali S (2002) The effect of haptic feedback and stereo graphics in a 3D target acquisition task. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom" href="/article/10.1007/s10055-006-0033-7#ref-CR21" id="ref-link-section-d44671e389">2002</a>) have shown that haptics has the potential to significantly increase both speed and accuracy of human–computer interaction. Our sense of touch and kinaesthetics is also capable of providing large amounts of information about the location, structure, stiffness and other material properties of objects, that can be hard to represent visually.</p><p>Implementing effective haptic interaction for volume exploration is a non-trivial task, however. There are no established guidelines, generally little knowledge on how the haptic feedback should be integrated into the exploration process and limited support for volume haptics from the available software packages. We believe that an important reason for this is that the available methods have so far been unsuitable for general handling of volume haptics. Recent methods, introducing haptic primitives (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005c" title="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557–560" href="/article/10.1007/s10055-006-0033-7#ref-CR15" id="ref-link-section-d44671e395">2005c</a>) provide a foundation suitable both for development of general systems and for formalization of haptic interaction with volumetric data.</p><p>In the first part of this article we describe how the haptic primitives are used to construct haptic modes, high-level haptic interaction definitions. To further facilitate the current developments and research, we present a toolkit for haptic volume visualization. In the second part of the article, we use the technology in a formative pilot study executed to motivate the formalization of haptic interaction and identify important aspects and requirements on volume haptics in volume data interaction and exploration.</p><p>The main contributions of this paper are:
</p><ul class="u-list-style-bullet">
                  <li>
                    <p>A formal definition of haptic modes as building blocks for implementing haptic schemes from haptic primitives.</p>
                  </li>
                  <li>
                    <p>The presentation of a toolkit for haptic volume exploration realizing the concept of a multi-modal data flow pipeline using haptic modes as building blocks.</p>
                  </li>
                  <li>
                    <p>The presentation of a real-time environment for interactive selection and configuration of haptic modes and visualization of volumetric data built on top of the toolkit.</p>
                  </li>
                  <li>
                    <p>A pilot study and analysis of how potential users interact with the haptic exploration of volume data, motivating design choices of the toolkit and providing recommendations for the design of haptic interaction.</p>
                  </li>
                </ul>
                     <p>The next section gives an overview of the main approaches used in haptic interaction with volumetric data, their limitations and the haptic primitives approach that overcome these limitations. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec6">3</a> then describes how this method is used to build the haptic modes and in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec7">4</a> we present the toolkit. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec12">5</a> we present the interactive environment and in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec15">6</a> we describe the user study followed by conclusion in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec20">7</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous work on volume haptics</h2><div class="c-article-section__content" id="Sec2-content"><p>Generating haptic feedback from surface data is a well-known field. Volumetric data, on the other hand, has no surface information from which haptic feedback can be generated. One way to overcome this problem is to extract surface data using thresholding in a pre-processing step, or to extract an intermediate local surface in real-time. Surface models for haptics, however, only represent a potential subset of the features in volumetric data. They also suffer from the occlusion of important areas by the use of distinct, impenetrable surfaces. To produce haptic feedback not limited to a predefined subset, a direct volume haptics approach is needed; a method that renders a continuous haptic information field throughout the entire volume, analogous to direct visual volume rendering.</p><p>There are two main approaches for direct volume haptics: force function-based and proxy-based. In most cases neither of these methods attempt to mimic the feedback found in the real world but rather aim at non-realistic perceptualization of the data for better intuitivity and effectiveness. The following two sections give a brief overview of the two approaches and describe the main problems that have limited the use of volume haptics. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec5">2.3</a> then describes the haptic primitives (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005c" title="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557–560" href="/article/10.1007/s10055-006-0033-7#ref-CR15" id="ref-link-section-d44671e458">2005c</a>) that enable the general approach used in our toolkit.</p><h3 class="c-article__sub-heading" id="Sec3">Force function-based volume haptics</h3><p>A simple and popular way to introduce haptic feedback from volumetric data is to define the force feedback as a vector valued function of the data around the haptic probe (Iwata and Noma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23" href="/article/10.1007/s10055-006-0033-7#ref-CR9" id="ref-link-section-d44671e468">1993</a>; Mor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Mor A, Gibson S, Samosky J (1996) Interacting with 3-dimensional medical data: haptic feedback for surgical simulation. In: Proceedings of phantom user group workshop’96, 1996" href="/article/10.1007/s10055-006-0033-7#ref-CR17" id="ref-link-section-d44671e471">1996</a>; Avila and Sobierajski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: Proceedings at IEEE visualization, October 1996, pp 197–204" href="/article/10.1007/s10055-006-0033-7#ref-CR2" id="ref-link-section-d44671e474">1996</a>; Hashimoto and Iwata <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hashimoto W, Iwata H (1997) A versatile software platform for visual/haptic environment. In: Proceedings of ICAT'97, pp 106–114" href="/article/10.1007/s10055-006-0033-7#ref-CR6" id="ref-link-section-d44671e477">1997</a>; Infed et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Infed F, Brown SV, Lee CD, Lawrence DA, Dougherty AM, Pao LY (1999) Combined visual/haptic rendering modes for scientific visualization. In: Proceedings of 8th annual symposium on haptic interfaces for virtual environment and teleoperator systems" href="/article/10.1007/s10055-006-0033-7#ref-CR8" id="ref-link-section-d44671e480">1999</a>; Lawrence et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic interface. In: Proceedings of IEEE conference on visualization and computer graphics" href="/article/10.1007/s10055-006-0033-7#ref-CR11" id="ref-link-section-d44671e484">2000</a>), the active point of the haptic device. Common variables in a force function are the probe velocity, to simulate viscosity, and the gradient vector of the scalar field. The force function is then given as
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{F}} = - C_1 {\mathbf{v}}_{\rm p} -C_2 \varvec{\nabla} V ({\mathbf{x}}_{\rm p}), $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <b>v</b>
                           <sub>p</sub> is the probe velocity, <i>V</i> (<b>x</b>
                           <sub>p</sub>) is the value of the scalar field at the probe position, <b>x</b>
                           <sub>p</sub>, and <i>C</i>
                           <sub>1</sub> and <i>C</i>
                           <sub>2</sub> are positive scalar constants or functions of some property of interest in the data. The viscosity term can be used to convey the scalar value by letting <i>C</i>
                           <sub>1</sub> be a function of the scalar value at the probe position, and the gradient term can convey the orientation of the data surrounding the haptic probe by pushing the instrument towards high or low scalar values, depending on the sign of <i>C</i>
                           <sub>2</sub>.</p><p>In interactions with <i>vector data</i> the force functions can be as simple as using the interpolated vector value at the probe position (Iwata and Noma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23" href="/article/10.1007/s10055-006-0033-7#ref-CR9" id="ref-link-section-d44671e546">1993</a>). A more advanced force function for vector data interaction could pull the haptic probe towards the core of vortex streams in the data (Infed et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Infed F, Brown SV, Lee CD, Lawrence DA, Dougherty AM, Pao LY (1999) Combined visual/haptic rendering modes for scientific visualization. In: Proceedings of 8th annual symposium on haptic interfaces for virtual environment and teleoperator systems" href="/article/10.1007/s10055-006-0033-7#ref-CR8" id="ref-link-section-d44671e549">1999</a>; Lawrence et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic interface. In: Proceedings of IEEE conference on visualization and computer graphics" href="/article/10.1007/s10055-006-0033-7#ref-CR11" id="ref-link-section-d44671e552">2000</a>),
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\mathbf{F}} = - C_{1} \left(\varvec{\nabla} \times {\mathbf{V}} ({\mathbf{x}}_{\rm p})\right) \times {\mathbf{V}}({\mathbf{x}}_{\rm p}).$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                        <p>Force functions are easy to implement, and therefore quite popular. Representing data with a simple “pushing force” may, however, in some cases be too simplistic an approach since only certain data, such as pressure and flow, is convincingly represented by a simple force. In general, force functions provide a limited set of possible feedback. Furthermore, force functions tend to become unstable around areas of rapidly changing force direction.</p><h3 class="c-article__sub-heading" id="Sec4">Proxy-based volume haptics</h3><p>The notion of proxy-based volume haptics was introduced in Lundin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104–109" href="/article/10.1007/s10055-006-0033-7#ref-CR12" id="ref-link-section-d44671e578">2002</a>) as a means to generate surface-like feedback from scalar density data without introducing haptic occlusion or limiting the interaction to a particular iso-value. The method has been used not only with scalar data, but also to render alternative shapes from vector (Ikits et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e581">2003</a>; Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005a" title="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31–41" href="/article/10.1007/s10055-006-0033-7#ref-CR13" id="ref-link-section-d44671e584">2005a</a>) and tensor (Ikits et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e587">2003</a>) fields. Proxy-based approaches use a decoupling scheme where the probe has two representations: the probe, <b>x</b>
                           <sub>p</sub>, the haptic device that can only be directly affected through force feedback, and an internal proxy,
<span class="mathjax-tex">\(\hat{\mathbf{x}}_{\rm p},\)</span> which is fully controlled by the algorithm. The feedback is then obtained, for each time-frame of the haptic loop, through the following three steps (also shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig1">1</a>): (1) collecting data at the local position being probed, (2) using the data to specify proxy movements, and (3) calculating force feedback from the proxy displacement relative to the probe. </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                                       <b>Data properties</b> First the data properties to be rendered by haptic feedback are extracted at the proxy position, for example the gradient vector (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig1">1</a>a). To produce more fine tuned feedback from the volumetric data, a haptic transfer function,
<span class="mathjax-tex">\(\tau:\mathcal{R} \to \mathcal{R},\)</span> can be used to map the data property to the strength of the physical force. This allows for representation of material properties (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104–109" href="/article/10.1007/s10055-006-0033-7#ref-CR12" id="ref-link-section-d44671e637">2002</a>; Avila and Sobierajski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: Proceedings at IEEE visualization, October 1996, pp 197–204" href="/article/10.1007/s10055-006-0033-7#ref-CR2" id="ref-link-section-d44671e640">1996</a>; Aviles and Ranta <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Aviles W, Ranta J (1999) Haptic interaction with geoscientific data. In: Proceedings at phantom user group workshop’99, 1999" href="/article/10.1007/s10055-006-0033-7#ref-CR3" id="ref-link-section-d44671e644">1999</a>) or gives the ability to emphasize specific features in the data in a manner analogous to the way visual transfer functions are used in volume rendering. Some examples of properties that are estimated and affect the haptic feedback are viscosity, friction, stiffness and flow strength.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                                       <b>Moving the proxy</b> Simple one dimensional <i>constraints</i> are defined as functions of the material properties of the local data. Each constraint controls the movement of the proxy in one direction to constrain the movement of the haptic probe in that direction. By combining three independent orthogonal constraints in a local frame of reference, a feeling of surfaces, friction, viscosity or transverse damping can be generated. Thus, since the orientation of the frame and the strength of the constraints are controlled by the local data, the feedback reflects the shape and properties of the data. The proxy movement is calculated separately in each direction and combined linearly to give the new proxy position, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig1">1</a>b.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                                       <b>Calculating feedback</b> After the new proxy position has been determined the force feedback is calculated using a virtual spring-damper system, coupling the probe to the proxy, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig1">1</a>c. Thus the force feedback <b>f</b>
                                       <sub>fb</sub> is evaluated through
</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{f}}_{\rm fb} = k \left(\hat{\mathbf{x}}_{\rm p} - {\mathbf{x}}_{\rm p}\right) + D \left(\hat{\mathbf{v}}_{\rm p} - {\mathbf{v}}_{\rm p} \right), $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> where
<span class="mathjax-tex">\(\hat{\mathbf{x}}_{\rm p}\)</span> and <b>x</b>
                                       <sub>p</sub> are proxy and probe position,
<span class="mathjax-tex">\(\hat{\mathbf{v}}_{\rm p}\)</span> and <b>v</b>
                                       <sub>p</sub> are proxy and probe velocity, and <i>k</i> and <i>D</i> are stiffness and damping parameters.</p>
                      
                    </li>
                  </ol>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Three steps for generating proxy-based haptic feedback, in this example from a virtual surface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The above outlined constraint approach to proxy-based volume haptics use an <i>orthogonal frame of constraints</i> to produce feedback. The method is incapable of handling <i>non-orthogonal</i> constraints correctly, as is shown in Lundin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005b" title="Lundin K, Cooper M, Ynnerman A (2005b) The orthogonal constraints problem with the constraint approach to proxy-based volume haptics and a solution. In: Proceedings of SIGRAD conference, Lund, Sweden, November 2005. SIGRAD, pp 45–49" href="/article/10.1007/s10055-006-0033-7#ref-CR14" id="ref-link-section-d44671e773">2005b</a>). The orthogonal requirement is far too restrictive and for a more general applicability and larger set of possible haptic schemes we have to address this problem.</p><h3 class="c-article__sub-heading" id="Sec5">Haptic primitives</h3><p>Haptic primitives for proxy-based volume haptics were introduced in Lundin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005c" title="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557–560" href="/article/10.1007/s10055-006-0033-7#ref-CR15" id="ref-link-section-d44671e785">2005c</a>). We describe them briefly here again since they constitute a foundation for the rest of this paper. For more details please refer to the earlier publication.</p><p>Haptic primitives both form a comprehensive abstraction layer for the implementation of haptic interaction schemes and provide an effective means of calculating the feedback. Constraints are represented using primitives of one, two and three degrees of freedom: <i>plane</i>, <i>line</i> and <i>point</i>, respectively. Active forces and other force functions are included through a fourth primitive: <i>directed force</i>. The effects of the primitives are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig2">2</a>. Superpositions of these four primitives are sufficient to represent any force feedback scheme (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005c" title="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557–560" href="/article/10.1007/s10055-006-0033-7#ref-CR15" id="ref-link-section-d44671e807">2005c</a>). In addition this method avoids the requirement of orthogonal constraints that is a persistent problem with the proxy-based approach (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104–109" href="/article/10.1007/s10055-006-0033-7#ref-CR12" id="ref-link-section-d44671e810">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005a" title="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31–41" href="/article/10.1007/s10055-006-0033-7#ref-CR13" id="ref-link-section-d44671e813">2005a</a>; Ikits et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e816">2003</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The four haptic primitives and their effects. Haptic schemes are implemented by selecting haptic primitives and controlling their parameters, such as the orientation, as functions of the local data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Each haptic primitive is characterized by a simple force equation, with parameters strength, <i>s</i>, position, <b>x</b>, and direction, <b>q</b>:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Directed force, a position-independent force:
</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{F}}_i\left(\hat{{\mathbf{x}}}_{\rm p} \right) = s_i {\mathbf{q}}_i.$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                                 
                    </li>
                    <li>
                      <p>Point, an attractor to a point in space:
</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{F}}_i\left(\hat{{\mathbf{x}}}_{\rm p} \right) = \left\{\begin{array}{*{20}l} {\mathbf{0}}, &amp; \hbox{if}\; \left| {\mathbf{x}}_i - \hat{{\mathbf{x}}}_{\rm p} \right| = 0\\ s_i \frac{{\mathbf{x}}_i - \hat{{\mathbf{x}}}_{\rm p}}{\left| {\mathbf{x}}_i - \hat{{\mathbf{x}}}_{\rm p} \right|}, &amp; \hbox{if}\; \left| {\mathbf{x}}_i - \hat{{\mathbf{x}}}_{\rm p} \right| \neq 0 \\
\end{array} \right.$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                                 
                    </li>
                    <li>
                      <p>Line, an attractor towards the closest point on a line:
</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{F}}_i \left(\hat{{\mathbf{x}}}_{\rm p} \right) = \left\{\begin{array}{*{20}l} {\mathbf{0}}, &amp; \hbox{if}\; \left|{\mathbf{m}}\right| = 0 \\
s_i \frac{{\mathbf{m}}}{\left|{\mathbf{m}}\right|}, &amp; \hbox{if}\; \left|{\mathbf{m}}\right| \neq 0\\
\end{array} \right., $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p> where <b>m</b> is the vector to the closest point on the line defined by <b>x</b> and <b>q</b>,
<span class="mathjax-tex">\({\mathbf{m}} = {\mathbf{q}}_i\left[ {\mathbf{q}}_i \cdot \left(\hat{{\mathbf{x}}}_{\rm p} - {\mathbf{x}}_i \right) \right] - \left(\hat{{\mathbf{x}}}_{\rm p} - {\mathbf{x}}_i \right).\)</span>
                                 </p>
                    </li>
                    <li>
                      <p>Plane, a directed force which exists only on one side of the plane:
</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{F}}_i\left(\hat{{\mathbf{x}}}_{\rm p} \right) = \left\{\begin{array}{*{20}l} 0, &amp; \hbox{if}\; \left(\hat{{\mathbf{x}}}_{\rm p} - {\mathbf{x}}_i \right) \cdot {\mathbf{q}}_i \geq 0 \\
s_i{\mathbf{q}}_i,&amp; \hbox{if}\; \left(\hat{{\mathbf{x}}}_{\rm p} - {\mathbf{x}}_i \right)\cdot{\mathbf{q}}_i &lt;0 \\
\end{array} \right..$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                                 
                    </li>
                  </ul><p> The proxy position for each time frame is then found by balancing the force feedback from the coupling equation against the force from the primitives, by numerically minimizing the residual
<span class="mathjax-tex">\(\varvec{\varepsilon}\)</span> in
</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \varvec{\varepsilon} = -{\mathbf{f}}_{\rm fb}\left(\hat{{\mathbf{x}}}_{\rm p} \right) + \sum_i {\mathbf{F}}_i\left(\hat{{\mathbf{x}}}_{\rm p} \right)$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p> with respect to
<span class="mathjax-tex">\(\hat{{\mathbf{x}}}_{\rm p}.\)</span> All other primitive parameters, such as the primitive positions, are held constant.</p><p>To simplify the expressions of
<span class="mathjax-tex">\(\varvec{\varepsilon}\)</span> in the following section we let <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Figa"> </a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Figb"> </a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Figc"> </a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Figd"> </a> represent the force functions for the point, line, plane and directed force primitives, respectively.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figa_HTML.gif" alt="figurea" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div>
                           <div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figb_HTML.gif?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figb_HTML.gif" alt="figureb" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div>
                           <div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figc_HTML.gif?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figc_HTML.gif" alt="figurec" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div>
                           <div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-d"><figure><div class="c-article-section__figure-content" id="Figd"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figd_HTML.gif?as=webp"></source><img aria-describedby="figure-d-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figd_HTML.gif" alt="figured" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-d-desc"></div></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Haptic modes</h2><div class="c-article-section__content" id="Sec6-content"><p>Using the primitives, schemes for haptic interaction can be handled as entities, <i>haptic modes</i>, simplifying the design and implementation of visio-haptic interfaces. In this section we describe the relationship between the haptic primitives and the implementation of haptic modes.</p><p>The haptic modes are implemented by selecting haptic primitives and controlling their parameters, such as the orientation, as functions of the local data. In that respect the haptic mode acts as a link between data and its haptic representation. Since the approach allows haptic primitives to be freely combined, so can the haptic modes. Thus, a large set of fairly simple haptic modes can be used, both individually or combined into more advanced haptic schemes. Below a set of haptic modes is described. Some of these modes also provide typical examples of each haptic primitive; its effect and use.</p><p>The primitives, selected to generate the feedback for a haptic mode, are placed at the location of the proxy point from the previous time-frame. Thus, the primitives generate local haptic shapes at any probed position in the volume, which produces a smooth continuous representation of the data. By locating a primitive at other positions, alternative effects can be provided, for example a snap-drag effect (Ikits et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e1060">2003</a>).</p>
                <h3 class="c-article__sub-heading">
                  <b>Viscosity mode</b>
                </h3>
                <p>Viscosity can be simulated by adding an attraction towards the position where the proxy point was located in the previous time-frame,
<span class="mathjax-tex">\(\hat{{\mathbf{x}}}_{\rm p}.\)</span> This will introduce a braking force on the proxy. For this we use a point primitive placed at the old proxy position. To produce velocity-based viscosity, the strength of the primitive may be defined as a function of the proxy velocity. Using the velocity, however, makes the feedback dependent on the speed of exploration as well as the scalar value of the data. To avoid that we choose to control the strength using a transfer function, τ<sub>visc</sub>, of the scalar value at the proxy position, as discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec4">2.2</a>. The residual to minimize is then expressed as</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-e"><figure><div class="c-article-section__figure-content" id="Fige"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fige_HTML.gif?as=webp"></source><img aria-describedby="figure-e-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fige_HTML.gif" alt="figuree" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-e-desc"></div></div></figure></div>
                        
              
                <h3 class="c-article__sub-heading">
                  <b>Gradient force mode</b>
                </h3>
                <p>The gradient term in function-based haptic interaction, discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec2">2</a>, is useful when interacting with pressure or fluid data. A force from the gradient of a scalar dataset can easily be produced using the directed force primitive. We let the strength of the force be specified through a transfer function, τ<sub>grad</sub>, from the magnitude of the gradient vector and use the normalized gradient vector,
<span class="mathjax-tex">\({\mathbf{n}} = \frac{\varvec{\nabla} V (\hat{{\mathbf{x}}}_{\rm p})} {\left| \varvec{\nabla}V(\hat{{\mathbf{x}}}_{\rm p})\right|},\)</span> as the direction of the force,</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-f"><figure><div class="c-article-section__figure-content" id="Figf"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figf_HTML.gif?as=webp"></source><img aria-describedby="figure-f-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figf_HTML.gif" alt="figuref" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-f-desc"></div></div></figure></div>
                        
                <p>If the gradient magnitude is zero we use an arbitrary vector to maintain consistency. By setting the transfer function to zero for zero magnitude gradients this arbitrary vector does not contribute to the force feedback. This way of avoiding division by zero is also applied in the following modes.</p>
              
                <h3 class="c-article__sub-heading">
                  <b>Force mode</b>
                </h3>
                <p>The simple mapping between vector field and force, mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec2">2</a>, is used in our user study, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec15">6</a>. The mapping is implemented using a force primitive, so the residual is expressed as</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-g"><figure><div class="c-article-section__figure-content" id="Figg"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figg_HTML.gif?as=webp"></source><img aria-describedby="figure-g-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figg_HTML.gif" alt="figureg" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-g-desc"></div></div></figure></div><p>where τ<sub>force</sub> is a transfer function and<span class="mathjax-tex">\(\varvec{\nu}= \frac{{\mathbf{V}} (\hat{{\mathbf{x}}}_{\rm p})} {\left|{\mathbf{V}}(\hat{{\mathbf{x}}}_{\rm p})\right|}.\)</span>
                        </p>
              
                <h3 class="c-article__sub-heading">
                  <b>Vector follow mode</b>
                </h3>
                <p>In previous work, we have encountered haptics that guides the user to follow the vectors of vector fields (Ikits et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e1188">2003</a>; Donald and Henle <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Donald BR, Henle F (2000) Using haptics vector fields for animation motion control. In: Proceedings of IEEE international conference on robotics and automation" href="/article/10.1007/s10055-006-0033-7#ref-CR4" id="ref-link-section-d44671e1191">2000</a>; Pao and Lawrence <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Pao L, Lawrence D (1998) Synergistic visual/haptic computer interfaces. In: Proceedings of Japan/USA/Vietnam workshop on research and education in systems, computation, and control engineering" href="/article/10.1007/s10055-006-0033-7#ref-CR18" id="ref-link-section-d44671e1194">1998</a>). This is useful when exploring flow data, such as heart MRI and data from Computational Fluid Dynamics (CFD). It guides the haptic instrument in the orientation of the vector field by presenting a resistance in directions orthogonal to the vector field. This mode is easily implemented using a line primitive. To orient the primitive the normalized vector value at the proxy position is used and the strength is defined by the vector length through a transfer function, τ<sub>vec</sub>,</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-h"><figure><div class="c-article-section__figure-content" id="Figh"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figh_HTML.gif?as=webp"></source><img aria-describedby="figure-h-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figh_HTML.gif" alt="figureh" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-h-desc"></div></div></figure></div>
                        
              
                <h3 class="c-article__sub-heading">
                  <b>Vortex tube mode</b>
                </h3>
                <p>In a recent application (Lundin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005a" title="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31–41" href="/article/10.1007/s10055-006-0033-7#ref-CR13" id="ref-link-section-d44671e1219">2005a</a>) for exploring data from CFD we implemented a version of the vortex core mode described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec2">2</a>. This mode is implemented using a plane primitive with the vortex core direction of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0033-7#Equ2">2</a>,
<span class="mathjax-tex">\(\varvec{\varphi} = \left(\varvec{\nabla} \times {\mathbf{V}} (\hat{{\mathbf{x}}}_{\rm p})\right) \times {\mathbf{V}} (\hat{{\mathbf{x}}}_{\rm p}),\)</span> defining the orientation of the primitive. Thus haptic descriptions of the vortex shape and extension are generated rather than just of the vortex core. The strength of the primitive, and thus of the rendered tube, is determined through a transfer function, τ<sub>tube</sub>, from the magnitude of the vector. This corresponds to the vorticity of the vector field at the exploration point. The residual is then expressed as</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-i"><figure><div class="c-article-section__figure-content" id="Figi"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figi_HTML.gif?as=webp"></source><img aria-describedby="figure-i-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figi_HTML.gif" alt="figurei" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-i-desc"></div></div></figure></div>
                        
              
                <h3 class="c-article__sub-heading">
                  <b>Surface and friction</b>
                </h3>
                <p>The surface-and-friction feedback described in Lundin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104–109" href="/article/10.1007/s10055-006-0033-7#ref-CR12" id="ref-link-section-d44671e1262">2002</a>) can also be implemented using haptic primitives. A plane primitive oriented by the normalized gradient vector simulates surfaces. Since friction feedback is limited to two dimensions, that is the plane exerting the friction feedback, the friction is effectively simulated using a line primitive. The extension of the line primitive makes the friction effect consistent even if the surface is penetrated. This setup is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig3">3</a>. The strength of the surface is defined by the scalar data using a transfer function, τ<sub>surf</sub>. A friction force, however, is generally calculated from the normal force, which is not known until the residual has been minimized. We, therefore, use the strength of the normal force from the previous time-frame,
<span class="mathjax-tex">\(k{\mathbf{n}}\cdot\left(\hat{{\mathbf{x}}}_{\rm p} - {\mathbf{x}}_{\rm p} \right),\)</span> where <b>n</b> is the surface normal, as an estimate of the current normal force. This force must, however, not exceed the surface strength. The strength of the line primitive is then calculated by multiplying the normal force strength with a friction value obtained from a transfer function, τ<sub>μ</sub>, so </p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-j"><figure><div class="c-article-section__figure-content" id="Figj"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figj_HTML.gif?as=webp"></source><img aria-describedby="figure-j-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figj_HTML.gif" alt="figurej" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-j-desc"></div></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Surface and friction simulation by using plane and line primitives at the position of the proxy point of the previous time-frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        
              
                <h3 class="c-article__sub-heading">
                  <b>Combined modes</b>
                </h3>
                <p>When two or more modes are used simultaneously their individual force function contributions are combined linearly, as expressed by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0033-7#Equ8">8</a>. As an example consider the combination of the surface and friction mode and the viscosity mode. The combined residual to minimize becomes</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-k"><figure><div class="c-article-section__figure-content" id="Figk"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figk_HTML.gif?as=webp"></source><img aria-describedby="figure-k-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Figk_HTML.gif" alt="figurek" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-k-desc"></div></div></figure></div>
                        
              <p>This less restricted handling of haptic modes has the potential to widen the possible applications of volume haptics and increase its availability.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Volume haptics toolkit</h2><div class="c-article-section__content" id="Sec7-content"><p>There are a number of systems currently available for general haptic rendering. To the authors knowledge, however, none exists that natively supports direct haptic interaction with volumetric data. Any example of direct volume haptics follows one of the two main approaches described above, deployed on a system primarily designed for surface haptics. Our volume haptics toolkit (VHTK) aims to meet this need for a framework for building applications for multi-modal volume data exploration. This section describes the toolkit and the measures taken to facilitate the design of haptic interaction.</p><p>The toolkit is implemented using H3D from SenseGraphics AB, extending it into the domain of volume visualization and haptics. H3D is a cross platform, open-source system based on the X3D standard. It has a common scenegraph for both haptics and graphics and provides, in addition to the standard graphical nodes, nodes for specifying haptic properties for geometries and to implement custom force models. The system is programmed at three different levels: X3D files are used to build scenes and set up simple dependencies between nodes with routes. For more complex behaviour and changes to the scenegraph the Python scripting language is used. Finally, C++ is used to create new nodes for haptics as well as for graphics and other tasks that require low-level programming.</p><p>Volume haptics toolkit enhances H3D by adding the nodes needed for producing haptic interaction with volumetric data including visual feedback, such as visualization nodes, data container nodes and data processing nodes. The nodes provided by the toolkit are implemented in C++ but can also be used and controlled from X3D and Python, allowing a programmer to either build the application in pure X3D and/or Python, or extend the toolkit further using C++. The toolkit has already been used in related projects, for example the fluid flow application shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig4">4</a>. It has also been released to public use under the GNU General Public License and can be downloaded from the H3D website (<a href="http://www.h3d.org">http://www.h3d.org</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Computational Fluid Dynamics (<i>CFD</i>) dataset of an experimental UAV. In this visualization the air-flow field and the flow magnitude are rendered using stream–ribbons and volume rendering, respectively. The follow mode produces a haptic representation of the flow field while the vortex mode facilitates the exploration of the vorticity in the data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec8">Processing pipeline</h3><p>To provide both flexibility and real-time configurability we have designed the toolkit to support a highly configurable data flow model similar to that of rapid software implementation APIs, such as the visualization toolkit (VTK), see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig5">5</a>. The haptic modes, constructed from the primitives, are the building blocks that are treated as configurable filters in the data flow model. A typical multi-modal pipeline for generating haptic feedback from volumetric data is as follows: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Volumetric data is read and trilinearly interpolated at the local position currently being explored by the haptic probe.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Simple data processing is used to extract local features from the volumetric data, for example the gradient vector or vector curl.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Scalar properties of the data, such as gradient magnitude, are fed through a transfer function to generate material properties, such as friction or surface strength.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>The material properties and data features are used to control the parameters of haptic primitives chosen to haptically represent the volumetric features.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>All haptic primitives are together mapped to a single force that represents the combined feedback, through the algorithm described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec5">2.3</a>.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">6.</span>
                      
                        <p>Finally, typically at a rate of 1 kHz, the force derived from the primitives is exerted through the haptic instrument.</p>
                      
                    </li>
                  </ol><p> In steps 2–4 the choices of processing, transfer functions and types of the primitives affect the haptic behaviour, that is, they define the haptic mode.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The conceptual data flow model of VHTK. The abbreviation “TF” denotes a transfer function</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The X3D standard defines an event handling and processing system to support dynamic behaviour and changes in the scenegraph. In this system a node can be made aware of the changes that are made in the data of a child node. This event propagation system is the basis for our conceptual data visualization pipeline. In the scenegraph a source is set as a child of the node using it. An example of this is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig6">6</a>. If a parameter in a reader or a filter is updated this will trigger an event that propagates updates up the scenegraph and so through the data pipeline. This way the event propagation is also optimized to only perform time-consuming data processing when needed. It allows, in addition to fast and easy construction, real-time fine-tuning of new haptic interaction schemes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>An example of how parts of the scenegraph are treated as a processing pipeline</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec9">Haptic nodes and rendering</h3><p>The toolkit encapsulates the steps forming the haptic behaviour into scenegraph nodes, thereby hiding the low-level processing and haptic primitives, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig5">5</a>. The haptic nodes thus form a palette of modes that can be freely selected and combined to generate a wide array of different haptic schemes, allowing a developer to tailor the task specific haptic scheme of an application. Each node also provides an X3D interface to mode specific data and parameters. Analogous to visual models in visual scenegraphs, the transforms above the node affect the position and orientation of the haptic representation of the node’s data source. Letting haptic nodes and visualization nodes share parent transform and data source, as in the example shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig6">6</a>, thus provides co-located haptics and graphics.</p><p>Currently eight pre-implemented haptic modes are provided for representing features in both scalar and vector data. Some of the modes are described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec6">3</a> and some are re-implementations of schemes presented in Donald and Henle (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Donald BR, Henle F (2000) Using haptics vector fields for animation motion control. In: Proceedings of IEEE international conference on robotics and automation" href="/article/10.1007/s10055-006-0033-7#ref-CR4" id="ref-link-section-d44671e1525">2000</a>), Lawrence et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic interface. In: Proceedings of IEEE conference on visualization and computer graphics" href="/article/10.1007/s10055-006-0033-7#ref-CR11" id="ref-link-section-d44671e1528">2000</a>), Ikits et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269" href="/article/10.1007/s10055-006-0033-7#ref-CR7" id="ref-link-section-d44671e1531">2003</a>), Iwata and Noma (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23" href="/article/10.1007/s10055-006-0033-7#ref-CR9" id="ref-link-section-d44671e1534">1993</a>) and Lundin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005a" title="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31–41" href="/article/10.1007/s10055-006-0033-7#ref-CR13" id="ref-link-section-d44671e1538">2005a</a>). For some application areas and selected tasks the available predefined haptic modes, or combinations thereof, may not suffice to represent the most interesting features. If so, a fundamentally new haptic mode is needed. The low-level abstraction layer constituted by the haptic primitives is made available for the implementation of new haptic modes. New modes can easily be integrated into the toolkit framework by extending the abstract haptic node type and implementing the new node to provide haptic primitives describing the desired effect, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec6">3</a>.</p><h3 class="c-article__sub-heading" id="Sec10">Data processing scenegraph nodes</h3><p>Data sources and data filters are also implemented as scenegraph nodes. They provide a general data extraction interface for subsequent nodes to use and the filters differ from the other sources, such as readers, only in that their X3D interface allows the assignment of a source to read data from. The data handling structure allows for both analytical and sampled volume data and defines interfaces for extracting the basic features from scalar and vector data: scalar value, scalar gradient vector, vector value, vector curl vector and vector divergence. Among the filters provided by the toolkit are support for conversion between data types and extraction of the magnitude of vector features, such as vector curl, as scalar data for visual volume rendering or haptic feedback. By changing the filtering of the data used by a haptic mode its possible uses can be widely expanded. For example, in a related project a classification algorithm is used as filtering to enhance the haptic feedback.</p><p>To provide flexible and intuitive control of the scenegraph nodes, the toolkit makes extensive use of transfer functions. Filters for rescaling data use transfer functions to control the input/output conversion and both visual and haptic nodes use transfer functions to control material, colour and size properties. There are, therefore, several different types of transfer function nodes available, providing different control interfaces. Examples are specification of piecewise linear segment and using the window function common in radiology.</p><h3 class="c-article__sub-heading" id="Sec11">Visual scenegraph nodes</h3><p>The main purpose of the toolkit is to provide an interface to advanced volume haptics, so only a few visualization nodes have, so far, been provided, such as volume rendering and stream–ribbons. However, more specialized packages for graphical visualization can also be used together with H3D and VHTK to provide more elaborate visual representations of the data.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Interactive environment</h2><div class="c-article-section__content" id="Sec12-content"><p>To facilitate the process of building the haptic interaction scheme for a particular problem and demonstrate how dynamic applications can be implemented using VHTK, we have used the toolkit to build an interactive tool for deployment of haptic exploration. With this environment it is possible to load volumetric data, perform simple visualizations and interactively select and setup haptic modes from the palette provided by the toolkit.</p><h3 class="c-article__sub-heading" id="Sec13">Implementation</h3><p>The graphical user interface (GUI) for the interactive environment is implemented in Tcl/Tk, through the Python module <i>Tkinter</i>. The interface runs from a Python script in the H3D scenegraph and the individual event handlers of Tk and H3D are connected through the Python interface.</p><p>In the design of the environment we group together each volume data reader with all the haptic modes that can be used on that type of data and the visual renderers for that type of data. All involved modes are automatically added to the scenegraph when a reader is selected, but, through our GUI, the user may turn on or off individual modes so that either one or several may be active at a time. Transfer functions, used to control the parameters of the modes, can also be adjusted through the interface. The transfer functions are drawn freehand, as can be seen in the example shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig7">7</a>. As the user updates the parameters the node configurations are updated in real-time, giving immediate feedback through the visualization and haptic interaction. The user can thus try out different haptic modes and different combinations and can also fine-tune the behaviour of the modes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The graphical user interface: visualization window with stylus visible, list of readers, window for configuring a scalar volume reader, its haptic modes and the volume renderer that visualize the volume, and dialogs for real-time transfer functions. In this setup the viscosity mode is used to enhance the visual impression. The dataset is a simulation of the spatial probability distribution of the electrons in a high potential protein molecule, courtesy of VolVis distribution of SUNY Stony Brook (see <a href="http://www.volvis.org">http://www.volvis.org</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Vector data is visualized using interactive stream–ribbons provided by the <span class="u-sans-serif">StreamRibbons</span> node. When this function is activated through the GUI, the user can interactively place ribbon seed points throughout the volume and move them with the resulting stream–ribbons being updated in real-time. Several parameters for the ribbons, such as colour transfer functions and ribbon length, can be controlled from the GUI.</p><p>The volume renderer provided by the toolkit is used to visualize both scalar and vector data. Through a property extractor, also provided by the toolkit, the property to be rendered can be selected. For scalar data either the scalar data or the gradient magnitude is extracted and for vector data one of the magnitude of the vector, curl or divergence is extracted.</p><h3 class="c-article__sub-heading" id="Sec14">Interactive example</h3><p>As an example of how data can be explored using this environment, consider the simulation of the spatial probability distribution of the electrons in a high potential protein molecule. This data was obtained from the VolVis distribution of SUNY Stony Brook (see <a href="http://www.volvis.org">http://www.volvis.org</a>). It is unsigned 8 bit integer data, 64<sup>3</sup> voxels in size. Using the volume visualization provided in the interactive environment for the scalar electron probability data we get the result shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig7">7</a>. The figure also shows the controls for selecting haptic modes and visualization, and fine-tuning material and colour properties. Currently activated are the viscosity mode and the surface-and-friction-mode, rendering a continuous set of surfaces from the probability density distribution. The haptic instrument can be freely moved into a high density region but when moving outwards a surface is perceived, conveying the shape of the local density distribution.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">A pilot study on volume haptics</h2><div class="c-article-section__content" id="Sec15-content"><p>In this section we present a pilot study designed to review the effect of haptics in volume exploration. Many previous studies, for example (Wall and Harwin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Wall S, Harwin W (2000) Quantification of the effects of haptic feedback during a motor skills task in a simulated environment. In: Proceedings at phantom user research symposium’00, 2000" href="/article/10.1007/s10055-006-0033-7#ref-CR20" id="ref-link-section-d44671e1648">2000</a>; Kirkpatrick and Douglas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kirkpatrick AE, Douglas SA (2002) Application-based evaluation of haptic interfaces. In: Proceedings of the 10th symposium on haptic interfaces for virtual environments and teleoperator systems" href="/article/10.1007/s10055-006-0033-7#ref-CR10" id="ref-link-section-d44671e1651">2002</a>; Passmore et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Passmore PJ, Nielsen CF, Cosh WJ, Darzi A (2001) Effects of viewing and orientation on path following in a medical teleoperation environment. In: Proceedings of IEEE virtual reality 2001" href="/article/10.1007/s10055-006-0033-7#ref-CR19" id="ref-link-section-d44671e1654">2001</a>; Iwata and Noma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23" href="/article/10.1007/s10055-006-0033-7#ref-CR9" id="ref-link-section-d44671e1657">1993</a>; Adams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Adams RJ, Klowden D, Hannaford B (2001) Virtual training for manual assembly task. Haptics-e. Electron J Haptics Res (&#xA;                    http://www.haptics-e.org&#xA;                    &#xA;                  ), 2(2), October 2001" href="/article/10.1007/s10055-006-0033-7#ref-CR1" id="ref-link-section-d44671e1660">2001</a>; Wall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Wall SA, Paynter K, Shillito AM, Wright M, Scali S (2002) The effect of haptic feedback and stereo graphics in a 3D target acquisition task. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom" href="/article/10.1007/s10055-006-0033-7#ref-CR21" id="ref-link-section-d44671e1664">2002</a>), have successfully shown that haptic feedback can produce positive effects, by showing the efficiency of adding haptic feedback to isolated sub-tasks. In contrast, we try to create an application scenario and so find how the potential user reacts to the new experiences introduced by haptic interaction and so deepen the understanding of <i>how</i>, and not <i>if</i>, volume haptics can assist in everyday tasks.</p><p>The primary aim of the study is to show how the formalization of haptic interaction facilitates the haptic design and thus justify the deployment of the new toolkit and validate the design choices. We do this by identifying important aspects and issues that need to be addressed by an application for visio-haptic volume visualization and interaction, in the context of haptic modes as a description of the haptic interaction. A secondary aim is to be able to provide suggestions for the implementation of future applications with visio-haptic interfaces for volume data exploration.</p><p>The study is a formative evaluation and follows the cooperative approach, an empirical technique common in HCI studies. The main basis for the study is conversations between subjects and an experimenter during a controlled case scenario. This is complemented by interviews and a short questionnaire. The study is designed to register subjective reactions and is, therefore, setup without specific questions. It is not expected to provide specific answers or be amenable to statistical significance, but rather give a deeper understanding of haptic interaction with volumetric data and indications and suggestions for future implementations and further studies.</p><h3 class="c-article__sub-heading" id="Sec16">Application and case setup</h3><p>The case chosen for the pilot study is taken from medical visualization. With modern magnetic resonance imaging (MRI) modalities, not only morphologic data but also fully 3D flow data can be acquired (Wigström et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Wigström L, Sjöqvist L, Wranne B (1996) Temporally resolved 3D phase-contrast imaging. Magn Reson Med 36(5):800–803 " href="/article/10.1007/s10055-006-0033-7#ref-CR22" id="ref-link-section-d44671e1684">1996</a>). This provides a basis for research on, for example, the blood flow patterns in the human heart (Fyrenius et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Fyrenius A, Wigström L, Ebbers T, Karlsson M, Engvall J, Bolger AF (2001) Three dimensional flow in the human left atrium. Heart 86:448–455" href="/article/10.1007/s10055-006-0033-7#ref-CR5" id="ref-link-section-d44671e1687">2001</a>; Wigström et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Wigström L, Ebbers T, Fyrenius A, Karlsson M, Engvall J, Wranne B, Bolger AF (1999) Particle trace visualization of intracardiac flow using time-resolved 3D phase contrast mri. Magn Reson Med 41:793–799" href="/article/10.1007/s10055-006-0033-7#ref-CR23" id="ref-link-section-d44671e1690">1999</a>). The task in the current case is to explore the flow data and identify paths.</p><p>The MR scanner used to acquire the data produces two datasets, one scalar and one vector. The haptic interaction is derived from a 3×32 bit floating point vector dataset of 120×90×30 voxels. The scalar dataset is of the same resolution and used for the visual volume rendering. The method used to acquire fully 3D flow information, however, produces poor tissue contrast, so the visual quality of the dataset is limited even though pre-processing has been applied to enhance the contrast, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig8">8</a>. This makes understanding the flow information crucial for the clinicians, since they can not rely on the morphological information.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Identifying blood flow in haptic exploration. The haptic feedback makes it easier to find and follow the blood flow in the dataset. The bad tissue contrast caused by the pulse sequence used when acquiring the vector data makes the sense of touch even more important in the exploration process</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Since this study is neither a comparison nor competition between visual and haptic rendering, we feel no need for more advanced visualization than standard techniques. We use classical volume rendering techniques to visualize the heart morphology and the blood flow data is visualized using interactive stream–ribbons. Each ribbon can be placed at any position and moved, in real-time, through the volume using the haptic interaction device.</p><p>In this application we make use of a combination of three different haptic modes: the follow mode, the force mode and the gradient mode. We use the follow mode to convey the blood flow orientation. It guides the user to follow the field orientation and also gives a feeling of both the orientation of the local flow and of the strength of the flow, through the strength of the feedback. The force mode is applied to the flow data. Although this feedback is not identical to real flow drag, this pushes the haptic instrument in the direction of the flow and thus conveys the flow direction to the user. In a pre-processing step we also generate a scalar field from the vector magnitude and on this the gradient mode is applied. Since the gradient points towards stronger flow, this mode will produce a gentle pull towards regions with fast blood flow. Together these haptic modes are used to convey both the orientation and direction of the blood flow combined with a sense of where the blood flow is stronger. This is anticipated to help the users to understand the data and guide them to the main flow patterns, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig8">8</a>. The underlying techniques, i.e. haptic modes and primitives, also allows individual modes to be activated and deactivated during run-time.</p><h3 class="c-article__sub-heading" id="Sec17">User study</h3><p>Seven experienced radiologists participated as test subjects, six with prior experience of 3D medical visualization and one without. Each radiologist was invited to a private supervised session following four steps: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Introduction to haptics.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Exploration of synthetic training data.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Exploration of blood flow data, and</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Interview, questions and discussion.</p>
                      
                    </li>
                  </ol>
                        <p>The training data was used to familiarize the subjects with the interface and make them understand the nature of the haptic feedback. All three haptic modes described above were demonstrated. The exploration of the authentic blood flow data was the main part of the session. Each subject was given the task of finding and marking blood flow paths in the heart dataset using stream ribbons—a task easy to describe but difficult in practice. To focus on the haptic interaction, the user interface was simplified by using preset visual and haptic settings.</p><p>The subjects’ experiences were discussed during four phases of the exploration: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Using stream–ribbons without haptics.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Using haptics without stream–ribbons.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Using haptics together with stream–ribbons, and</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Using stream–ribbons without haptics again.</p>
                      
                    </li>
                  </ol><p> During exploration with haptic feedback, the follow mode was set as the default haptic scheme and the other two modes were turned on or off at the subjects’ request.</p><p>The subjects were asked to continuously describe and discuss their experiences during the session. When needed the supervisor asked general open questions to trigger the subject to describe the experience. At the end of the session a questionnaire was filled in together with the supervisor, involving a number of fixed choice questions. These, together with the answers, are listed in Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0033-7#Sec21">1</a>.</p><h3 class="c-article__sub-heading" id="Sec18">Results from the user study</h3><p>As a formative evaluation, this study is focused on qualitative aspects, for example how the subjects experience the haptic feedback and how the feedback helps or hinders the process of exploring the data. Both interview answers and feedback through the questionnaire have contributed to the results presented here, but the interviews are the primarily source. This section describes only the opinions and answers from the subjects, which are case specific and associated with the discipline, the subject’s previous experience and the interface itself. In the following section a wider discussion is presented where we aim at extracting the more general aspects and conclusions from the results.</p><p>During the first phase, using stream–ribbons without haptic feedback, the subjects found that, even though some understanding of the flow could be obtained, it was hard to distinguish between noise and detailed information about the flow. It should be noted that stream–ribbons typically do not distinguish between different vector magnitudes and thus does not allow the user to discriminate between noise and flow data. In particular the direction of the flow was hard to perceive, and so some important information about the heart anatomy was lost. It was also verified that the visual morphological information was insufficient for navigating the anatomy.</p><p>In the second phase, using haptics only, some subjects expressed that the local information provided by the haptic feedback made it possible to produce some mental image of the flow distribution. Also, even though the pull from the gradient mode made it easier to find and follow flow and required less precision in the user’s movements, some of the subjects expressed that it had a negative impact on the presentation of fine details and that it was therefore not suitable for close examination of the identified flow. The follow mode, both stand-alone and in combination with direct force mapping, was found to be more accurate and conveyed more detail. The direct force mapping was considered to add vital information about flow direction although one subject noted that there was a possibility that it could lead to misinterpretation of the path of the flow. One subject expressed that the haptic feedback was only partly helpful—stating that when the probe is “correctly positioned” the feedback works well, but outside distinct flow it becomes confusing.</p><p>With combined haptic and graphical exploration, the third phase, the subjects found the interaction to be considerably improved both with respect to only haptic feedback and only visual feedback. They found the feedback helpful when searching for areas with flow and when trying to distribute stream–ribbons throughout the dataset. While the haptic feedback is the primary means to find and follow flow, the graphical stream–ribbons provide global information and confirmation of the first impression. A majority of the subjects also believed that the presence of haptic feedback helped them understand the distributed flow. The overall opinion was that using the combination gave both faster and more reliable interaction than using either haptics or graphics alone.</p><p>This was confirmed in the last phase, when the haptic feedback was again removed. The subjects found that the interaction slowed down and required more concentration. One subject also stated that he found his exploration less structured without the guidance from the haptic feedback. Another subject had expressed that the haptic feedback was hard to understand and believed it would require much more training, but still found that some assistance was lost with the deactivation of the feedback. This subject had bad stereo-vision and expressed that the haptic feedback provided useful guidance in judging depth.</p><p>The general opinion on the semi-immersive haptics exploration environment was that it was easy to work with. The haptic feedback was considered helpful and the combination of haptics/graphics produced a better result than using visual feedback alone. We observed that all subjects found controlling the haptic interaction easy to learn and that no-one found the main haptic scheme to have a direct negative impact on the process of exploring and understanding the data.</p><h3 class="c-article__sub-heading" id="Sec19">Conclusions from the user study</h3><p>Potential users have expressed the opinion that deploying combined haptic and visual feedback gives considerable advantages over using a purely graphical interface in the exploration process. This concurs with the results from earlier evaluations of haptic interaction, however the nature of this study leaves the verification of this claim to future, more specialized, studies. A number of more general conclusions, however, can be drawn from the behaviour of our subjects and their comments, regarding both the haptic interaction itself and the design of haptic applications.</p><p>Four important aspects of how the haptic feedback may assist the exploration task have been noted:</p>
                  <h3 class="c-article__sub-heading">
                    <b>Physical guidance</b>
                  </h3>
                  <p>First of all the feedback can physically guide the user in the exploration process, as anticipated; it can help the user find features by providing physical guides through the volume. For example, the pull from the gradient mode guides the user towards high flow and the follow mode helps the user to find the continuation of an already located path.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>Mental guidance</b>
                  </h3>
                  <p>The feedback may also, to some degree, help the user to perform a more structured search. Only one of the subjects mentioned this effect. Even so, since this is an abstract notion, we believe that it should be taken into consideration when designing haptic interaction schemes.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>Supplementary information</b>
                  </h3>
                  <p>The haptic feedback has the potential to convey information that is not represented visually—in this case the direction of the flow.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>Complementary information</b>
                  </h3>
                  <p>Even if the haptic mode chosen for interaction represents the same features as are visually shown, the feedback can reinforce the visual impression and enhance the understanding. In our example the flow orientation and path is represented by haptic feedback through the follow mode and visually through the interactive stream ribbons. Even so the subjects expressed that the haptic dimension deepened the understanding of the flow data.</p>
                <p>The emphasis on these four different aspects varies between modes, and the experience of our test subjects during the exploration depended heavily on the choice of haptic modes and their design. This implies that, for some data, haptics can be of great assistance if the mode is well designed but, if the mode is poorly designed, the haptics can be of little or no help at all or even have a negative impact on the exploration. This has been indicated before (Pao and Lawrence <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Pao L, Lawrence D (1998) Synergistic visual/haptic computer interfaces. In: Proceedings of Japan/USA/Vietnam workshop on research and education in systems, computation, and control engineering" href="/article/10.1007/s10055-006-0033-7#ref-CR18" id="ref-link-section-d44671e1906">1998</a>; Maciejewski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Maciejewski R, Choi S, Ebert D, Tan H (2005) Multi-modal perceptualization of volumetric data and its application to molecular docking. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 511–514" href="/article/10.1007/s10055-006-0033-7#ref-CR16" id="ref-link-section-d44671e1909">2005</a>) and shows the vital importance of tailoring and testing specific haptic schemes for each given problem area and task at hand.</p><p>Finding the most appropriate haptic mode or set of haptic modes, with corresponding parameters and transfer functions, to most effectively represent a dataset and facilitate the exploration process is a non-trivial task. The optimal choice of haptic feedback may also differ between different applications, users and tasks. While one user may prefer physical guidance from the haptic instrument, another may feel better assisted by extra information about non-visual properties of the data. This emphasizes the vital importance of tailoring haptic modes and performing iterative development of the application for each given problem area and purpose. There is thus a need to speed up the development process and even allow interactive design of haptic interaction in order to enable users to rapidly:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Choose, try out and replace the haptic modes.</p>
                    </li>
                    <li>
                      <p>Evaluate the impact of the chosen modes.</p>
                    </li>
                    <li>
                      <p>Compare different modes with respect to both the different desired effects of the feedback and the key factors of the current application.</p>
                    </li>
                  </ul>
                        </div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusions and future work</h2><div class="c-article-section__content" id="Sec20-content"><p>We have shown how the recently introduced haptic primitives can be used to build “haptic modes”, entities describing the relation between a data type and a haptic representation. With this formalization of volume haptics, the haptic representations of volumetric data can be selected, designed and used in a manner similar to how visual components are setup for a certain problem.</p><p>We have presented a toolkit using haptic modes as base components, that aims to remedy the lack of viable solutions for volume haptics. To allow for fast and easy construction, testing and fine-tuning, we have designed our toolkit to form a data processing pipeline, supporting real-time manipulation and configuration. It allows easy construction of a wide variety of haptic schemes and, by careful selection between them, during runtime, various features of the data can be emphasized.</p><p>Using the current technology for volume haptics we have performed a formative evaluation identifying four important aspects to consider when designing haptic interfaces for volume data exploration. The study also showed the impact of the form and design of the haptic scheme on its impression and effect. This indicates the importance of interactive design and fine-tuning of haptic interaction schemes to find the optimal balance between the mentioned aspects in a certain task.</p><p>These results also indicate that there is great potential for the use of haptics in volumetric data exploration, however there is a need for further studies. For example, there is a need for research on <i>how</i> to optimize the design of haptic interaction for an application area, task and user. Furthermore, the connection between data and material properties, here manifested as haptic transfer functions, is not fully understood. Future research include studies on the impact of specific transfer functions on user performance and understanding of the haptic feedback.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adams RJ, Klowden D, Hannaford B (2001) Virtual training for manual assembly task. Haptics-e. Electron J Hapti" /><p class="c-article-references__text" id="ref-CR1">Adams RJ, Klowden D, Hannaford B (2001) Virtual training for manual assembly task. Haptics-e. Electron J Haptics Res (<a href="http://www.haptics-e.org">http://www.haptics-e.org</a>), 2(2), October 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: Proceedings at IEEE " /><p class="c-article-references__text" id="ref-CR2">Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: Proceedings at IEEE visualization, October 1996, pp 197–204</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aviles W, Ranta J (1999) Haptic interaction with geoscientific data. In: Proceedings at phantom user group wor" /><p class="c-article-references__text" id="ref-CR3">Aviles W, Ranta J (1999) Haptic interaction with geoscientific data. In: Proceedings at phantom user group workshop’99, 1999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Donald BR, Henle F (2000) Using haptics vector fields for animation motion control. In: Proceedings of IEEE in" /><p class="c-article-references__text" id="ref-CR4">Donald BR, Henle F (2000) Using haptics vector fields for animation motion control. In: Proceedings of IEEE international conference on robotics and automation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Fyrenius, L. Wigström, T. Ebbers, M. Karlsson, J. Engvall, AF. Bolger, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Fyrenius A, Wigström L, Ebbers T, Karlsson M, Engvall J, Bolger AF (2001) Three dimensional flow in the human " /><p class="c-article-references__text" id="ref-CR5">Fyrenius A, Wigström L, Ebbers T, Karlsson M, Engvall J, Bolger AF (2001) Three dimensional flow in the human left atrium. Heart 86:448–455</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1136%2Fheart.86.4.448" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Three%20dimensional%20flow%20in%20the%20human%20left%20atrium&amp;journal=Heart&amp;volume=86&amp;pages=448-455&amp;publication_year=2001&amp;author=Fyrenius%2CA&amp;author=Wigstr%C3%B6m%2CL&amp;author=Ebbers%2CT&amp;author=Karlsson%2CM&amp;author=Engvall%2CJ&amp;author=Bolger%2CAF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hashimoto W, Iwata H (1997) A versatile software platform for visual/haptic environment. In: Proceedings of IC" /><p class="c-article-references__text" id="ref-CR6">Hashimoto W, Iwata H (1997) A versatile software platform for visual/haptic environment. In: Proceedings of ICAT'97, pp 106–114</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration" /><p class="c-article-references__text" id="ref-CR7">Ikits M, Brederson JD, Hansen CD, Johnson CR (2003) A constraint-based technique for haptic volume exploration. In: Proceedings of IEEE visualization ’03, pp 263–269</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Infed F, Brown SV, Lee CD, Lawrence DA, Dougherty AM, Pao LY (1999) Combined visual/haptic rendering modes for" /><p class="c-article-references__text" id="ref-CR8">Infed F, Brown SV, Lee CD, Lawrence DA, Dougherty AM, Pao LY (1999) Combined visual/haptic rendering modes for scientific visualization. In: Proceedings of 8th annual symposium on haptic interfaces for virtual environment and teleoperator systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in vir" /><p class="c-article-references__text" id="ref-CR9">Iwata H, Noma H (1993) Volume haptization. In: Proceedings of IEEE 1993 symposium on research frontiers in virtual reality, pp 16–23</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kirkpatrick AE, Douglas SA (2002) Application-based evaluation of haptic interfaces. In: Proceedings of the 10" /><p class="c-article-references__text" id="ref-CR10">Kirkpatrick AE, Douglas SA (2002) Application-based evaluation of haptic interfaces. In: Proceedings of the 10th symposium on haptic interfaces for virtual environments and teleoperator systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic" /><p class="c-article-references__text" id="ref-CR11">Lawrence DA, Lee CD, Pao LY, Novoselov RY (2000) Shock and vortex visualization using a combined visual/haptic interface. In: Proceedings of IEEE conference on visualization and computer graphics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proce" /><p class="c-article-references__text" id="ref-CR12">Lundin K, Ynnerman A, Gudmundsson B (2002) Proxy-based haptic feedback from volumetric density data. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom, pp 104–109</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data usi" /><p class="c-article-references__text" id="ref-CR13">Lundin K, Sillen M, Cooper M, Ynnerman A (2005a) Haptic visualization of computational fluid dynamics data using reactive forces. In: Proceedings of conference on visualization and data analysis, part of IS&amp;T/SPIE symposium on Electronic imaging 2005, San Jose, January 2005, pp 31–41</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lundin K, Cooper M, Ynnerman A (2005b) The orthogonal constraints problem with the constraint approach to prox" /><p class="c-article-references__text" id="ref-CR14">Lundin K, Cooper M, Ynnerman A (2005b) The orthogonal constraints problem with the constraint approach to proxy-based volume haptics and a solution. In: Proceedings of SIGRAD conference, Lund, Sweden, November 2005. SIGRAD, pp 45–49</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedi" /><p class="c-article-references__text" id="ref-CR15">Lundin K, Gudmundsson B, Ynnerman A (2005c) General proxy-based haptics for volume visualization. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 557–560</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maciejewski R, Choi S, Ebert D, Tan H (2005) Multi-modal perceptualization of volumetric data and its applicat" /><p class="c-article-references__text" id="ref-CR16">Maciejewski R, Choi S, Ebert D, Tan H (2005) Multi-modal perceptualization of volumetric data and its application to molecular docking. In: Proceedings of the world haptics conference, Pisa, March 2005. IEEE, pp 511–514</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mor A, Gibson S, Samosky J (1996) Interacting with 3-dimensional medical data: haptic feedback for surgical si" /><p class="c-article-references__text" id="ref-CR17">Mor A, Gibson S, Samosky J (1996) Interacting with 3-dimensional medical data: haptic feedback for surgical simulation. In: Proceedings of phantom user group workshop’96, 1996</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pao L, Lawrence D (1998) Synergistic visual/haptic computer interfaces. In: Proceedings of Japan/USA/Vietnam w" /><p class="c-article-references__text" id="ref-CR18">Pao L, Lawrence D (1998) Synergistic visual/haptic computer interfaces. In: Proceedings of Japan/USA/Vietnam workshop on research and education in systems, computation, and control engineering</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Passmore PJ, Nielsen CF, Cosh WJ, Darzi A (2001) Effects of viewing and orientation on path following in a med" /><p class="c-article-references__text" id="ref-CR19">Passmore PJ, Nielsen CF, Cosh WJ, Darzi A (2001) Effects of viewing and orientation on path following in a medical teleoperation environment. In: Proceedings of IEEE virtual reality 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wall S, Harwin W (2000) Quantification of the effects of haptic feedback during a motor skills task in a simul" /><p class="c-article-references__text" id="ref-CR20">Wall S, Harwin W (2000) Quantification of the effects of haptic feedback during a motor skills task in a simulated environment. In: Proceedings at phantom user research symposium’00, 2000</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wall SA, Paynter K, Shillito AM, Wright M, Scali S (2002) The effect of haptic feedback and stereo graphics in" /><p class="c-article-references__text" id="ref-CR21">Wall SA, Paynter K, Shillito AM, Wright M, Scali S (2002) The effect of haptic feedback and stereo graphics in a 3D target acquisition task. In: Proceedings of eurohaptics. University of Edinburgh, United Kingdom</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wigström, L. Sjöqvist, B. Wranne, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Wigström L, Sjöqvist L, Wranne B (1996) Temporally resolved 3D phase-contrast imaging. Magn Reson Med 36(5):80" /><p class="c-article-references__text" id="ref-CR22">Wigström L, Sjöqvist L, Wranne B (1996) Temporally resolved 3D phase-contrast imaging. Magn Reson Med 36(5):800–803 </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fmrm.1910360521" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Temporally%20resolved%203D%20phase-contrast%20imaging&amp;journal=Magn%20Reson%20Med&amp;volume=35&amp;issue=5&amp;pages=800-803&amp;publication_year=1996&amp;author=Wigstr%C3%B6m%2CL&amp;author=Sj%C3%B6qvist%2CL&amp;author=Wranne%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wigström, T. Ebbers, A. Fyrenius, M. Karlsson, J. Engvall, B. Wranne, AF. Bolger, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Wigström L, Ebbers T, Fyrenius A, Karlsson M, Engvall J, Wranne B, Bolger AF (1999) Particle trace visualizati" /><p class="c-article-references__text" id="ref-CR23">Wigström L, Ebbers T, Fyrenius A, Karlsson M, Engvall J, Wranne B, Bolger AF (1999) Particle trace visualization of intracardiac flow using time-resolved 3D phase contrast mri. Magn Reson Med 41:793–799</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2F%28SICI%291522-2594%28199904%2941%3A4%3C793%3A%3AAID-MRM19%3E3.0.CO%3B2-2" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Particle%20trace%20visualization%20of%20intracardiac%20flow%20using%20time-resolved%203D%20phase%20contrast%20mri&amp;journal=Magn%20Reson%20Med&amp;volume=41&amp;pages=793-799&amp;publication_year=1999&amp;author=Wigstr%C3%B6m%2CL&amp;author=Ebbers%2CT&amp;author=Fyrenius%2CA&amp;author=Karlsson%2CM&amp;author=Engvall%2CJ&amp;author=Wranne%2CB&amp;author=Bolger%2CAF">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0033-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>Lars Wigström at the Center for Medical Image Science and Visualization (CMIV) at Linköping University and Mattias Sillén at Saab AB are gratefully acknowledged for providing high quality datasets. The staff at CMIV are also gratefully acknowledged for participation in the pilot study and Lena Tibell at the department of biomedicine and surgery for help with the study.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Norrköping Visualization and Interaction Studio, Linköping University, Linköping, Sweden</p><p class="c-article-author-affiliation__authors-list">Karljohan Lundin, Matthew Cooper &amp; Anders Ynnerman</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Center for Medical Image Science and Visualization, Linköping University, Linköping, Sweden</p><p class="c-article-author-affiliation__authors-list">Anders Persson</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">SenseGraphics AB, Stockholm, Sweden</p><p class="c-article-author-affiliation__authors-list">Daniel Evestedt</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Karljohan-Lundin"><span class="c-article-authors-search__title u-h3 js-search-name">Karljohan Lundin</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Karljohan+Lundin&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Karljohan+Lundin" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Karljohan+Lundin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Matthew-Cooper"><span class="c-article-authors-search__title u-h3 js-search-name">Matthew Cooper</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Matthew+Cooper&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Matthew+Cooper" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Matthew+Cooper%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Anders-Persson"><span class="c-article-authors-search__title u-h3 js-search-name">Anders Persson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Anders+Persson&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Anders+Persson" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Anders+Persson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Daniel-Evestedt"><span class="c-article-authors-search__title u-h3 js-search-name">Daniel Evestedt</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daniel+Evestedt&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniel+Evestedt" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniel+Evestedt%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Anders-Ynnerman"><span class="c-article-authors-search__title u-h3 js-search-name">Anders Ynnerman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Anders+Ynnerman&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Anders+Ynnerman" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Anders+Ynnerman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0033-7/email/correspondent/c1/new">Karljohan Lundin</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix 1 The Questionnaire</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix 1 The Questionnaire</h3><p>The questions in the questionnaire are answered with a value between one and five corresponding to “do not agree” and “fully agree”, respectively (Likert scaling). Each question listed below is marked with a letter, corresponding to a line in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0033-7#Fig9">9</a>. The questions have been translated from the original Swedish.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0033-7/MediaObjects/10055_2006_33_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The answers from the questionnaire. Each question is answered using a five level Likert scale. The order and colour of the blocks corresponds to the answer and their size and number corresponds to the number of subjects giving that answer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0033-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The visual representation...<dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>A:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...has a resolution high enough</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>B:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...is well classified</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>C:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...has good update rate</p>
                      </dd></dl>
                        </p><p>The stream–ribbons...
<dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>D:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...give good visual representation of the flow</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>E:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...are easy to use and handle</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>F:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...behave in a predictable manner</p>
                      </dd></dl>
                        </p><p>The haptic interaction...
<dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>G:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...is easy to learn to use</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>H:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...describes the flow in a comprehensive manner</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>I:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...makes it easier to understand the flow</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>J:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...makes it easier to find flow</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>K:</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>...helps when distributing stream–ribbons</p>
                      </dd></dl>
                        </p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Enabling%20design%20and%20interactive%20selection%20of%20haptic%20modes&amp;author=Karljohan%20Lundin%20et%20al&amp;contentID=10.1007%2Fs10055-006-0033-7&amp;publication=1359-4338&amp;publicationDate=2006-07-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lundin, K., Cooper, M., Persson, A. <i>et al.</i> Enabling design and interactive selection of haptic modes.
                    <i>Virtual Reality</i> <b>11, </b>1–13 (2007). https://doi.org/10.1007/s10055-006-0033-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0033-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-13">13 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05-02">02 May 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-07-11">11 July 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2007-03">March 2007</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0033-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0033-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Volume haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic modes</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Toolkit</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User study</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0033-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=33;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

