<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Virtual grasps recognition using fusion of Leap Motion and force myogr"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, wearable devices could be an effective..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Virtual grasps recognition using fusion of Leap Motion and force myography"/>

    <meta name="dc.source" content="Virtual Reality 2018 22:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-03-01"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, wearable devices could be an effective supplement. This study presents a hand grasps recognition method in virtual reality settings, by fusing signals acquired using force myography (FMG), a muscular activity-based hand gesture recognition method, and Leap Motion. We conducted an experiment where participants performed grasping of virtual objects with VR goggles on their head, an FMG band on their wrist, and a Leap Motion positioned either on the desk or on the goggles (two experimental settings). The FMG, Leap Motion, and fusion of both signals were used for training and testing a simple, but effective linear discriminant analysis classifier, as well as three other mainstream classification algorithms. The results showed that the fusion of both signals achieved a significant improvement in classification accuracy, compared to using Leap Motion alone in both experimental settings."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-03-01"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="297"/>

    <meta name="prism.endingPage" content="308"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0339-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0339-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0339-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0339-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Virtual grasps recognition using fusion of Leap Motion and force myography"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2018/11"/>

    <meta name="citation_online_date" content="2018/03/01"/>

    <meta name="citation_firstpage" content="297"/>

    <meta name="citation_lastpage" content="308"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0339-2"/>

    <meta name="DOI" content="10.1007/s10055-018-0339-2"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0339-2"/>

    <meta name="description" content="Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, we"/>

    <meta name="dc.creator" content="Xianta Jiang"/>

    <meta name="dc.creator" content="Zhen Gang Xiao"/>

    <meta name="dc.creator" content="Carlo Menon"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Syst Rehabil Eng; citation_title=Improving the performance against force variation of EMG controlled multifunctional upper-limb prostheses for transradial amputees; citation_author=AH Al-Timemy, RN Khushaba, G Bugmann, J Escudero; citation_publication_date=2015; citation_doi=10.1109/tnsre.2015.2445634; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Biomed Eng; citation_title=Self-correcting pattern recognition system of surface EMG signals for upper limb prosthesis control; citation_author=S Amsuss, PM Goebel, N Jiang, B Graimann, L Paredes, D Farina; citation_volume=61; citation_publication_date=2014; citation_pages=1167-1176; citation_doi=10.1109/TBME.2013.2296274; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_title=Virtual reality technology; citation_publication_date=2003; citation_id=CR3; citation_author=GC Burdea; citation_author=P Coiffet; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=Biomed Eng Online; citation_title=Selection of suitable hand gestures for reliable myoelectric human computer interface; citation_author=MCF Castro, SP Arjunan, DK Kumar; citation_volume=14; citation_publication_date=2015; citation_pages=1-11; citation_doi=10.1186/s12938-015-0025-5; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Intell Syst Technol; citation_title=LIBSVM: a library for support vector machines; citation_author=C Chang, C Lin; citation_volume=2; citation_publication_date=2011; citation_pages=1-27; citation_doi=10.1145/1961189.1961199; citation_id=CR5"/>

    <meta name="citation_reference" content="Chuan CH, Regina E, Guardino C (2014) American sign language recognition using leap motion sensor. In: 2014 13th international conference on machine learning applications, 2014, pp 541&#8211;544. 
                    https://doi.org/10.1109/icmla.2014.110
                    
                  
                        "/>

    <meta name="citation_reference" content="Colgan A (2018) How does the leap motion controller work. Leap Motion Blog. 
                    http://blog.leapmotion.com/hardware-to-software-how-does-the-leap-motion-controller-work/
                    
                  . Accessed February 16, 2018"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Autom IEEE Trans; citation_title=On grasp choice, grasp models, and the design of hands for manufacturing tasks; citation_author=MR Cutkosky; citation_volume=5; citation_publication_date=1989; citation_pages=269-279; citation_doi=10.1109/70.34763; citation_id=CR8"/>

    <meta name="citation_reference" content="Dementyev A, Paradiso JA (2014) WristFlex: low-power gesture input with wrist-worn pressure sensors. In: Proceedings of the 27th annual ACM symposium user interface software technology, UIST&#8217;14, ACM Press, New York, New York, USA, 2014, pp 161&#8211;166. 
                    https://doi.org/10.1145/2642918.2647396
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization; citation_author=TG Dietterich; citation_volume=40; citation_publication_date=2000; citation_pages=139-157; citation_doi=10.1023/A:1007607513941; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Biomed Eng IEEE Trans; citation_title=A robust, real-time control scheme for multifunction myoelectric control; citation_author=K Englehart, B Hudgins; citation_volume=50; citation_publication_date=2003; citation_pages=848-854; citation_doi=10.1109/TBME.2003.813539; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Neural Syst Rehabil Eng; citation_title=The optimal controller delay for myoelectric prostheses; citation_author=TR Farrell, RF Weir; citation_publication_date=2007; citation_doi=10.1109/tnsre.2007.891391; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Simul. Gaming.; citation_title=Developments in business gaming: a review of the past 40 years; citation_author=AJ Faria, D Hutchinson, WJ Wellington, S Gold; citation_volume=40; citation_publication_date=2009; citation_pages=464-487; citation_doi=10.1177/1046878108327585; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Haptics; citation_title=Analysis of human grasping behavior: object characteristics and grasp type; citation_author=T Feix, IM Bullock, AM Dollar; citation_volume=7; citation_publication_date=2014; citation_pages=311-323; citation_doi=10.1109/TOH.2014.2326871; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Eugen; citation_title=The use of multiple measurements in taxonomic problems; citation_author=RA Fisher; citation_volume=7; citation_publication_date=1936; citation_pages=179-188; citation_doi=10.1111/j.1469-1809.1936.tb02137.x; citation_id=CR15"/>

    <meta name="citation_reference" content="Fove Inc. (2017) FOVE 0 eye tracking virtual reality devkit user manual. archive.getfove.com/setup/FOVE0_User_Manual.pdf. Accessed April 5, 2017"/>

    <meta name="citation_reference" content="citation_journal_title=Front Neurosci; citation_title=Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation; citation_author=F Grimm, G Naros, A Gharabaghi; citation_volume=10; citation_publication_date=2016; citation_pages=518; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Sens (Switz); citation_title=An analysis of the precision and reliability of the leap motion sensor and its suitability for static and dynamic tracking; citation_author=J Guna, G Jakus, M Poga&#269;nik, S Toma&#382;i&#269;, J Sodnik; citation_volume=14; citation_publication_date=2014; citation_pages=3702-3720; citation_doi=10.3390/s140203702; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Virtual environments for motor rehabilitation: review; citation_author=MK Holden; citation_volume=8; citation_publication_date=2005; citation_pages=187-211; citation_doi=10.1089/cpb.2005.8.187; citation_id=CR19"/>

    <meta name="citation_reference" content="InterlinkElectronics (2010) FSR&#174; integration guide &amp; evaluation parts catalog with suggested electrical interfaces"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Human-Mach Syst; citation_title=Force exertion affects grasp classification using force myography; citation_author=X Jiang, L-K Merhi, C Menon; citation_publication_date=2017; citation_doi=10.1109/thms.2017.2693245; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Med Eng Phys; citation_title=Exploration of force myography and surface electromyography in hand gesture classification; citation_author=X Jiang, L-K Merhi, ZG Xiao, C Menon; citation_volume=41; citation_publication_date=2017; citation_pages=63-73; citation_doi=10.1016/j.medengphy.2017.01.015; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=CAAI Trans Intell Technol; citation_title=Multi-LeapMotion sensor based demonstration for robotic refine tabletop object manipulation task; citation_author=H Jin, Q Chen, Z Chen, Y Hu, J Zhang; citation_volume=1; citation_publication_date=2016; citation_pages=104-113; citation_doi=10.1016/j.trit.2016.03.010; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_title=Vision based hand gesture interfaces for wearable computing and virtual environments; citation_publication_date=2004; citation_id=CR24; citation_author=M Kolsch; citation_publisher=University of California"/>

    <meta name="citation_reference" content="citation_journal_title=J Bionic Eng; citation_title=Combined use of FSR sensor array and SVM classifier for finger motion recognition based on pressure distribution map; citation_author=N Li, D Yang, L Jiang, H Liu, H Cai; citation_volume=9; citation_publication_date=2012; citation_pages=39-47; citation_doi=10.1016/S1672-6529(11)60095-4; citation_id=CR25"/>

    <meta name="citation_reference" content="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565&#8211;1569"/>

    <meta name="citation_reference" content="Mine M et al (1995) Virtual environment interaction techniques. UNC Chapel Hill Comput. Sci. Tech. Rep. TR95-018, pp 507242&#8211;507248"/>

    <meta name="citation_reference" content="Motion L (2017) Leap motion SDK, 
                    https://developer.leapmotion.com/get-started
                    
                  . Accessed November 17, 2017"/>

    <meta name="citation_reference" content="citation_title=Virtual and augmented reality applications in manufacturing; citation_publication_date=2013; citation_id=CR29; citation_author=SK Ong; citation_author=AYC Nee; citation_publisher=Springer Science &amp; Business Media"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors.; citation_title=Human-computer interaction based on hand gestures using RGB-D sensors; citation_author=JM Palacios, C Sag&#252;&#233;s, E Montijano, S Llorente; citation_volume=13; citation_publication_date=2013; citation_pages=11842-11860; citation_doi=10.3390/s130911842; citation_id=CR30"/>

    <meta name="citation_reference" content="Potter LE, Araullo J, Carter L (2013) The leap motion controller: a view on sign language. In: Proceedings of the 25th Australian computer-human interaction conference: augmentation, application, innovation, collaboration, ACM, New York, NY, USA, 2013, pp 175&#8211;178. 
                    https://doi.org/10.1145/2541016.2541072
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Biomed Signal Process Control; citation_title=Optimization of EMG-based hand gesture recognition: supervised vs. unsupervised data preprocessing on healthy subjects and transradial amputees; citation_author=F Riillo, LR Quitadamo, F Cavrini, E Gruppioni, CA Pinto, NC Past&#242;, L Sbernini, L Albero, G Saggio; citation_volume=14; citation_publication_date=2014; citation_pages=117-125; citation_doi=10.1016/j.bspc.2014.07.007; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Front Bioeng Biotechnol; citation_title=Force myography for monitoring grasping in individuals with stroke with mild to moderate upper-extremity impairments: a preliminary investigation in a controlled environment; citation_author=GP Sadarangani, X Jiang, LA Simpson, JJ Eng, C Menon; citation_volume=5; citation_publication_date=2017; citation_pages=42; citation_doi=10.3389/fbioe.2017.00042; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Hand posture and gesture recognition techniques for virtual reality applications: a survey; citation_author=KM Sagayam, DJ Hemanth; citation_volume=21; citation_publication_date=2017; citation_pages=91-107; citation_doi=10.1007/s10055-016-0301-0; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Acad Med Singapore; citation_title=Virtual reality and telepresence for military medicine; citation_author=RM Satava; citation_volume=26; citation_publication_date=1997; citation_pages=118-120; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=J Rehabil Res Dev; citation_title=Electromyogram pattern recognition for control of powered upper-limb prostheses: state of the art and challenges for clinical use; citation_author=E Scheme, K Englehart; citation_volume=48; citation_publication_date=2011; citation_pages=643; citation_doi=10.1682/JRRD.2010.09.0177; citation_id=CR36"/>

    <meta name="citation_reference" content="Silva ECP, Clua EWG, Montenegro AA (2015) Sensor data fusion for full arm tracking using Myo Armband and leap motion. In: 2015 14th Brazilian symposium on computer games digital entertainment (SBGames), pp 128&#8211;134"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Surg; citation_title=Surgical simulation: a systematic review; citation_author=LM Sutherland, PF Middleton, A Anthony, J Hamdorf, P Cregan, D Scott, GJ Maddern; citation_volume=243; citation_publication_date=2006; citation_pages=291-300; citation_doi=10.1097/01.sla.0000200839.93965.26; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_title=Statistical learning theory; citation_publication_date=1998; citation_id=CR39; citation_author=V Vapnik; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="Vargas HF, Vivas OA (2014) Gesture recognition system for surgical robot&#8217;s manipulation. In: 2014 XIX symposium on image, signal process and artificial vision (STSIVA), 2014, pp 1&#8211;5"/>

    <meta name="citation_reference" content="citation_journal_title=Sens (Switz); citation_title=Analysis of the accuracy and robustness of the leap motion controller; citation_author=F Weichert, D Bachmann, B Rudak, D Fisseler; citation_volume=13; citation_publication_date=2013; citation_pages=6380-6393; citation_doi=10.3390/s130506380; citation_id=CR41"/>

    <meta name="citation_reference" content="citation_journal_title=Sens (Switz); citation_title=Analysis of the accuracy and robustness of the leap motion controller; citation_author=F Weichert, D Bachmann, B Rudak, D Fisseler; citation_volume=13; citation_publication_date=2013; citation_pages=6380-6393; citation_doi=10.3390/s130506380; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_journal_title=J Rehabil Res Dev; citation_title=Pressure signature of forearm as predictor of grip force; citation_author=M Wininger, N-H Kim, W Craelius; citation_volume=45; citation_publication_date=2008; citation_pages=883-892; citation_doi=10.1682/JRRD.2007.11.0187; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=Electro Int; citation_title=Force sensing resistors: a review of the technology; citation_author=SI Yaniger; citation_publication_date=1991; citation_doi=10.1109/electr.1991.718294; citation_id=CR44"/>

    <meta name="citation_reference" content="Zhang H, Zhao Y, Yao F, Xu L, Shang P, Li G (2013) An adaptation strategy of using LDA classifier for EMG pattern recognition. In: 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC). 
                    https://doi.org/10.1109/embc.2013.6610488
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_title=Introduction to artificial neural systems; citation_publication_date=1992; citation_id=CR46; citation_author=JM Zurada; citation_publisher=West"/>

    <meta name="citation_author" content="Xianta Jiang"/>

    <meta name="citation_author_institution" content="Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, Canada"/>

    <meta name="citation_author" content="Zhen Gang Xiao"/>

    <meta name="citation_author_institution" content="Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, Canada"/>

    <meta name="citation_author" content="Carlo Menon"/>

    <meta name="citation_author_email" content="cmenon@sfu.ca"/>

    <meta name="citation_author_institution" content="Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, Canada"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0339-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0339-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Virtual grasps recognition using fusion of Leap Motion and force myography"/>
        <meta property="og:description" content="Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, wearable devices could be an effective supplement. This study presents a hand grasps recognition method in virtual reality settings, by fusing signals acquired using force myography (FMG), a muscular activity-based hand gesture recognition method, and Leap Motion. We conducted an experiment where participants performed grasping of virtual objects with VR goggles on their head, an FMG band on their wrist, and a Leap Motion positioned either on the desk or on the goggles (two experimental settings). The FMG, Leap Motion, and fusion of both signals were used for training and testing a simple, but effective linear discriminant analysis classifier, as well as three other mainstream classification algorithms. The results showed that the fusion of both signals achieved a significant improvement in classification accuracy, compared to using Leap Motion alone in both experimental settings."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Virtual grasps recognition using fusion of Leap Motion and force myography | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0339-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"VR, Hand gesture recognition, Grasp classification, Leap Motion, Force myography","kwrd":["VR","Hand_gesture_recognition","Grasp_classification","Leap_Motion","Force_myography"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0339-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0339-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=339;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0339-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Virtual grasps recognition using fusion of Leap Motion and force myography
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0339-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0339-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-03-01" itemprop="datePublished">01 March 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Virtual grasps recognition using fusion of Leap Motion and force myography</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xianta-Jiang" data-author-popup="auth-Xianta-Jiang">Xianta Jiang</a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-3219-1871"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-3219-1871</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Simon Fraser University" /><meta itemprop="address" content="0000 0004 1936 7494, grid.61971.38, Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, BC, V3T 0A3, Canada" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Zhen_Gang-Xiao" data-author-popup="auth-Zhen_Gang-Xiao">Zhen Gang Xiao</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Simon Fraser University" /><meta itemprop="address" content="0000 0004 1936 7494, grid.61971.38, Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, BC, V3T 0A3, Canada" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Carlo-Menon" data-author-popup="auth-Carlo-Menon" data-corresp-id="c1">Carlo Menon<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Simon Fraser University" /><meta itemprop="address" content="0000 0004 1936 7494, grid.61971.38, Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, BC, V3T 0A3, Canada" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">297</span>–<span itemprop="pageEnd">308</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">592 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0339-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, wearable devices could be an effective supplement. This study presents a hand grasps recognition method in virtual reality settings, by fusing signals acquired using force myography (FMG), a muscular activity-based hand gesture recognition method, and Leap Motion. We conducted an experiment where participants performed grasping of virtual objects with VR goggles on their head, an FMG band on their wrist, and a Leap Motion positioned either on the desk or on the goggles (two experimental settings). The FMG, Leap Motion, and fusion of both signals were used for training and testing a simple, but effective linear discriminant analysis classifier, as well as three other mainstream classification algorithms. The results showed that the fusion of both signals achieved a significant improvement in classification accuracy, compared to using Leap Motion alone in both experimental settings.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>VR technologies (Burdea and Coiffet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Burdea GC, Coiffet P (2003) Virtual reality technology. Wiley, New York" href="/article/10.1007/s10055-018-0339-2#ref-CR3" id="ref-link-section-d28835e359">2003</a>) have been emerging in recent years in a variety of application domains, including gaming (Faria et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Faria AJ, Hutchinson D, Wellington WJ, Gold S (2009) Developments in business gaming: a review of the past 40 years. Simul. Gaming. 40:464–487" href="/article/10.1007/s10055-018-0339-2#ref-CR13" id="ref-link-section-d28835e362">2009</a>), manufacturing (Ong and Nee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ong SK, Nee AYC (2013) Virtual and augmented reality applications in manufacturing. Springer Science &amp; Business Media, Berlin" href="/article/10.1007/s10055-018-0339-2#ref-CR29" id="ref-link-section-d28835e365">2013</a>), military (Satava <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Satava RM (1997) Virtual reality and telepresence for military medicine. Ann Acad Med Singapore 26:118–120" href="/article/10.1007/s10055-018-0339-2#ref-CR35" id="ref-link-section-d28835e368">1997</a>), surgical simulations and teleoperation (Sutherland et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sutherland LM, Middleton PF, Anthony A, Hamdorf J, Cregan P, Scott D, Maddern GJ (2006) Surgical simulation: a systematic review. Ann Surg 243:291–300" href="/article/10.1007/s10055-018-0339-2#ref-CR38" id="ref-link-section-d28835e371">2006</a>; Vargas and Vivas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Vargas HF, Vivas OA (2014) Gesture recognition system for surgical robot’s manipulation. In: 2014 XIX symposium on image, signal process and artificial vision (STSIVA), 2014, pp 1–5" href="/article/10.1007/s10055-018-0339-2#ref-CR40" id="ref-link-section-d28835e375">2014</a>), and rehabilitation (Holden <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol Behav 8:187–211" href="/article/10.1007/s10055-018-0339-2#ref-CR19" id="ref-link-section-d28835e378">2005</a>). The booming needs of VR applications opened a wide range of research opportunities for interaction between humans and virtual environments (VE), including hand position tracking and gesture recognition (Burdea and Coiffet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Burdea GC, Coiffet P (2003) Virtual reality technology. Wiley, New York" href="/article/10.1007/s10055-018-0339-2#ref-CR3" id="ref-link-section-d28835e381">2003</a>; Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mine M et al (1995) Virtual environment interaction techniques. UNC Chapel Hill Comput. Sci. Tech. Rep. TR95-018, pp 507242–507248" href="/article/10.1007/s10055-018-0339-2#ref-CR27" id="ref-link-section-d28835e384">1995</a>; Sagayam and Hemanth <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Sagayam KM, Hemanth DJ (2017) Hand posture and gesture recognition techniques for virtual reality applications: a survey. Virtual Real 21:91–107. &#xA;                    https://doi.org/10.1007/s10055-016-0301-0&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR34" id="ref-link-section-d28835e387">2017</a>). Similar to real-world scenarios, our hands are one of the most essential tools for the interaction. For instance, we can use our hands to manipulate a virtual object during a game or a virtual surgical training session (Vargas and Vivas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Vargas HF, Vivas OA (2014) Gesture recognition system for surgical robot’s manipulation. In: 2014 XIX symposium on image, signal process and artificial vision (STSIVA), 2014, pp 1–5" href="/article/10.1007/s10055-018-0339-2#ref-CR40" id="ref-link-section-d28835e390">2014</a>). In order to manipulate the object, we need to grasp it first. Grasping is also paramount for activities of daily living (ADL) (Cutkosky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. Robot Autom IEEE Trans 5:269–279. &#xA;                    https://doi.org/10.1109/70.34763&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR8" id="ref-link-section-d28835e394">1989</a>; Feix et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Feix T, Bullock IM, Dollar AM (2014) Analysis of human grasping behavior: object characteristics and grasp type. IEEE Trans Haptics 7:311–323. &#xA;                    https://doi.org/10.1109/TOH.2014.2326871&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR14" id="ref-link-section-d28835e397">2014</a>); hence, various grasping exercises have been adopted in physical rehabilitation routines (Grimm et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Grimm F, Naros G, Gharabaghi A (2016) Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation. Front Neurosci 10:518" href="/article/10.1007/s10055-018-0339-2#ref-CR17" id="ref-link-section-d28835e400">2016</a>; Sadarangani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Sadarangani GP, Jiang X, Simpson LA, Eng JJ, Menon C (2017) Force myography for monitoring grasping in individuals with stroke with mild to moderate upper-extremity impairments: a preliminary investigation in a controlled environment. Front Bioeng Biotechnol 5:42. &#xA;                    https://doi.org/10.3389/fbioe.2017.00042&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR33" id="ref-link-section-d28835e403">2017</a>). Therefore, being able to accurately detect grasping is of importance for the above-mentioned applications.</p><p>One of the most common ways for tracking hand movements and detecting gestures is using vision-based technique, which can be implemented simply via a web camera (Kolsch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kolsch M (2004) Vision based hand gesture interfaces for wearable computing and virtual environments. University of California, Santa Barbara" href="/article/10.1007/s10055-018-0339-2#ref-CR24" id="ref-link-section-d28835e409">2004</a>). More recently, the introduction of low-cost commercially available depth cameras, such as Microsoft Kinect and Leap Motion, offered the opportunity to improve gesture recognition performance by using depth information. For example, Palacios et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Palacios JM, Sagüés C, Montijano E, Llorente S (2013) Human-computer interaction based on hand gestures using RGB-D sensors. Sensors. 13:11842–11860" href="/article/10.1007/s10055-018-0339-2#ref-CR30" id="ref-link-section-d28835e412">2013</a>) developed an algorithm for hand gesture recognition based on RGB-D sensors, which take advantage of depth information to compensate for cluttered backgrounds and occlusions during hand image segmentation. Leap Motion is designed for hand and fingertips position tracking and is reported to be able to achieve a sub-millimeter tracking accuracy (&lt; 0.5 mm), provided that hand movements are relatively static and within the limited sensory space of the Leap Motion sensor (Weichert et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Weichert F, Bachmann D, Rudak B, Fisseler D (2013a) Analysis of the accuracy and robustness of the leap motion controller. Sens (Switz) 13:6380–6393. &#xA;                    https://doi.org/10.3390/s130506380&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR41" id="ref-link-section-d28835e415">2013</a>; Guna et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Guna J, Jakus G, Pogačnik M, Tomažič S, Sodnik J (2014) An analysis of the precision and reliability of the leap motion sensor and its suitability for static and dynamic tracking. Sens (Switz) 14:3702–3720. &#xA;                    https://doi.org/10.3390/s140203702&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR18" id="ref-link-section-d28835e418">2014</a>). Based on the tracked hand and fingertips position, the Leap Motion system itself is able to recognize four types of hand gestures, including Circle, Swipe, Key Tap, and Screen Tap (Motion <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Motion L (2017) Leap motion SDK, &#xA;                    https://developer.leapmotion.com/get-started&#xA;                    &#xA;                  . Accessed November 17, 2017" href="/article/10.1007/s10055-018-0339-2#ref-CR28" id="ref-link-section-d28835e421">2017</a>). More hand gesture recognition algorithms have been developed by researchers based on Leap Motion, mostly for sign language recognition (Potter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Potter LE, Araullo J, Carter L (2013) The leap motion controller: a view on sign language. In: Proceedings of the 25th Australian computer-human interaction conference: augmentation, application, innovation, collaboration, ACM, New York, NY, USA, 2013, pp 175–178. &#xA;                    https://doi.org/10.1145/2541016.2541072&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR31" id="ref-link-section-d28835e425">2013</a>; Chuan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Chuan CH, Regina E, Guardino C (2014) American sign language recognition using leap motion sensor. In: 2014 13th international conference on machine learning applications, 2014, pp 541–544. &#xA;                    https://doi.org/10.1109/icmla.2014.110&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR6" id="ref-link-section-d28835e428">2014</a>; Jin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jin H, Chen Q, Chen Z, Hu Y, Zhang J (2016) Multi-LeapMotion sensor based demonstration for robotic refine tabletop object manipulation task. CAAI Trans Intell Technol 1:104–113" href="/article/10.1007/s10055-018-0339-2#ref-CR23" id="ref-link-section-d28835e431">2016</a>). However, occlusions caused by the palm and fingers themselves still exist during complex hand gestures such as grasps, where the fingertips will be lost during a fist action. To address this problem, multiple Leap Motions (Jin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jin H, Chen Q, Chen Z, Hu Y, Zhang J (2016) Multi-LeapMotion sensor based demonstration for robotic refine tabletop object manipulation task. CAAI Trans Intell Technol 1:104–113" href="/article/10.1007/s10055-018-0339-2#ref-CR23" id="ref-link-section-d28835e434">2016</a>) or fusion of Leap Motion and Kinect (Marin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e437">2014</a>) was employed. However, these approaches would largely increase the complexity of the system and are not currently supported by mainstream 3D development platforms such as Unity (Unity Technologies, San Francisco, CA, USA).</p><p>Another method for tracking hand movements and detecting gestures is to classify muscle activity signals extracted from the forearm, including the wrist. These signals can be captured using various myographic techniques, such as surface electromyography (sEMG) and force myography (FMG). sEMG captures the voltage potential due to the firing of motor neurons (Al-Timemy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Al-Timemy AH, Khushaba RN, Bugmann G, Escudero J (2015) Improving the performance against force variation of EMG controlled multifunctional upper-limb prostheses for transradial amputees. IEEE Trans Neural Syst Rehabil Eng. &#xA;                    https://doi.org/10.1109/tnsre.2015.2445634&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR1" id="ref-link-section-d28835e443">2015</a>; Riillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Riillo F, Quitadamo LR, Cavrini F, Gruppioni E, Pinto CA, Pastò NC, Sbernini L, Albero L, Saggio G (2014) Optimization of EMG-based hand gesture recognition: supervised vs. unsupervised data preprocessing on healthy subjects and transradial amputees. Biomed Signal Process Control 14:117–125. &#xA;                    https://doi.org/10.1016/j.bspc.2014.07.007&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR32" id="ref-link-section-d28835e446">2014</a>; Castro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Castro MCF, Arjunan SP, Kumar DK (2015) Selection of suitable hand gestures for reliable myoelectric human computer interface. Biomed Eng Online 14:1–11. &#xA;                    https://doi.org/10.1186/s12938-015-0025-5&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR4" id="ref-link-section-d28835e449">2015</a>), while FMG captures the gross movements of muscle groups using pressure sensors surrounding the targeted location of a limb (Dementyev and Paradiso <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Dementyev A, Paradiso JA (2014) WristFlex: low-power gesture input with wrist-worn pressure sensors. In: Proceedings of the 27th annual ACM symposium user interface software technology, UIST’14, ACM Press, New York, New York, USA, 2014, pp 161–166. &#xA;                    https://doi.org/10.1145/2642918.2647396&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR9" id="ref-link-section-d28835e452">2014</a>; Wininger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wininger M, Kim N-H, Craelius W (2008) Pressure signature of forearm as predictor of grip force. J Rehabil Res Dev 45:883–892. &#xA;                    https://doi.org/10.1682/JRRD.2007.11.0187&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR43" id="ref-link-section-d28835e455">2008</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Li N, Yang D, Jiang L, Liu H, Cai H (2012) Combined use of FSR sensor array and SVM classifier for finger motion recognition based on pressure distribution map. J Bionic Eng 9:39–47. &#xA;                    https://doi.org/10.1016/S1672-6529(11)60095-4&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR25" id="ref-link-section-d28835e459">2012</a>; Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017a" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e462">2017a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. &#xA;                    https://doi.org/10.1109/thms.2017.2693245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR21" id="ref-link-section-d28835e465">b</a>). While sEMG is a more conventional technique to detect muscle activity, FMG is a newer technique, offering many advantages, including ease-of-use, robustness against external electrical interference, and lower cost (Yaniger <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Yaniger SI (1991) Force sensing resistors: a review of the technology. Electro Int. &#xA;                    https://doi.org/10.1109/electr.1991.718294&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR44" id="ref-link-section-d28835e468">1991</a>). We have previously demonstrated the feasibility of using FMG in distinguishing 16 grasps with one fixed arm position. Our results showed a comparable performance between using FMG and sEMG approaches (Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e471">2017</a>) (85 versus 77%); therefore, FMG can potentially be an alternative method to sEMG for hand activity detection. However, using the FMG approach alone might not be sufficient to accurately predict hand gestures in different positions within the 3D space, as FMG signal patterns vary for different arm orientations. Thus, a fusion of devices that can provide position tracking in VR should be considered (Silva et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Silva ECP, Clua EWG, Montenegro AA (2015) Sensor data fusion for full arm tracking using Myo Armband and leap motion. In: 2015 14th Brazilian symposium on computer games digital entertainment (SBGames), pp 128–134" href="/article/10.1007/s10055-018-0339-2#ref-CR37" id="ref-link-section-d28835e474">2015</a>).</p><p>After considering both the advantages and limitations of the vision-based and FMG approaches, we conducted a study to investigate the feasibility of using the fusion of FMG and hand position data of a vision-based motion capture system, i.e., Leap Motion, to improve hand grasps recognition under VR environments. In this study, we examined the classification accuracy of 6 commonly used grasps, using the finger joint position data from Leap Motion and the signals from a wrist-based FMG device. We collected both Leap Motion and FMG data simultaneously, while participants performed 6 selected grasps in a VR setting. Since the Leap Motion system can be used as a stationary as well as a wearable hand position tracking device, we collected the data for each setting separately. The collected data were processed offline to evaluate the accuracy of using the fusion of FMG and Leap Motion, and this accuracy was compared to those of using FMG or Leap Motion data alone, in both settings.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Materials and experiment setup</h2><div class="c-article-section__content" id="Sec2-content"><p>We designed an experiment to use both Leap Motion and the FMG band for hand grasping recognition under VR environment. The experimental setup is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>, which includes three components, the FOVE VR goggles (FOVE Inc. San Mateo, CA, USA), the customized FSR wrist band, and the Leap Motion mounted on the goggles or on the desk. All the three parts were connected to a working laptop. The illustration of the distribution of FSR sensors on the wrist is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>a, and the wearing position of the FSR band on the wrist is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>b.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Experimental setup. <b>a</b> Illustration of the FSR sensors positioning on the wrist sites. <b>b</b> Participant performing hand gestures with the VR goggles and Leap Motion on the head, and the FMG band on the wrist. <b>c</b> Schematic illustration of the data acquisition setting. There were a total of 10 FSRs embedded in the band. The resister changes of the FSRs cause corresponding voltage changes output by the voltage divider, which are digitized by the AD converter in the control unit. For a detailed description of the control unit, see (Jiang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017a" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e516">2017a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. &#xA;                    https://doi.org/10.1109/thms.2017.2693245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR21" id="ref-link-section-d28835e519">b</a>). FCU: flexor carpi ulnaris muscle, FDS: flexor digitorum superficialis muscle, PL: palmaris longus muscle, FCR: flexor carpi radialis muscle, APL: abductor pollicis longus muscle, EPB: extensor pollicis brevis muscle, ECRL: extensor carpi radialis longus muscle, ECRB: extensor carpi radialis brevis muscle, EPL: extensor pollicis longus muscle, EDC: extensor digitorum communis muscle, EDM: extensor digitorum muscle, ECU: extensor carpi ulnaris muscle</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec3">FSR band</h3><p>The FSR band consists of 10 FSR sensors and an FMG signal acquisition unit, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>c. The FSR sensors are custom-printed and embedded in a silicone band, each 1.3 cm × 1.5 cm in size and 0.6 cm apart from each other. The characteristics of the printed FSR sensors are the same as the FSR402 from Interlink (InterlinkElectronics et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="InterlinkElectronics (2010) FSR® integration guide &amp; evaluation parts catalog with suggested electrical interfaces" href="/article/10.1007/s10055-018-0339-2#ref-CR20" id="ref-link-section-d28835e541">2010</a>). The total length of the FSR band is 26.5 cm, and the width is 2.2 cm. The 10 FSR sensors are aligned from one end of the band, where a male Velcro tape is adhered on the back side, to ~ 19 cm length, which is longer than the average wrist size of the participants in this study (16.5 cm ± 1.0 cm). The remaining part of the band (about 7 cm) is covered with a female Velcro tape, where no FSR sensors are embedded. A built-in, low-energy Bluetooth (BLE) model connects the band to a dongle, which is then connected to a USB port on the working laptop. The control unit has similar inside components as those reported in Jiang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017a" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e544">2017a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. &#xA;                    https://doi.org/10.1109/thms.2017.2693245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR21" id="ref-link-section-d28835e547">b</a>). The sampling frequency of the FMG band is 50 Hz.</p><h3 class="c-article__sub-heading" id="Sec4">Leap Motion</h3><p>The Leap Motion is a small rectangular box with dimensions of 6 mm high, 30 mm wide, and 80 mm long, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>b. It contains 2 cameras, 3 infrared LEDs, and wide-angle lenses. The Leap Motion can accurately track hand and fingers’ positions within a sphere of radius 80 cm, with view angles of 150° on the long side and 120° on the short side (Colgan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Colgan A (2018) How does the leap motion controller work. Leap Motion Blog. &#xA;                    http://blog.leapmotion.com/hardware-to-software-how-does-the-leap-motion-controller-work/&#xA;                    &#xA;                  . Accessed February 16, 2018" href="/article/10.1007/s10055-018-0339-2#ref-CR7" id="ref-link-section-d28835e561">2018</a>), and with a tracking accuracy of ~ 200 μm (Weichert et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Weichert F, Bachmann D, Rudak B, Fisseler D (2013b) Analysis of the accuracy and robustness of the leap motion controller. Sens (Switz) 13:6380–6393. &#xA;                    https://doi.org/10.3390/s130506380&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR42" id="ref-link-section-d28835e564">2013</a>). However, despite the high accuracy and large view angle, finger occlusion is still a problem, as Leap Motion views things from only the plane of where the screen is placed. Therefore, not all fingers can be in view at the same time.</p><p>Two commonly used positions of the Leap Motion were explored: one on the VR goggles (experimental setting 1) and the other on the desk, right under the working hand (experimental setting 2). Setting 1 is designed to be used for mobile hand tracking, while setting 2 is designed to be used only in a fixed working space.</p><h3 class="c-article__sub-heading" id="Sec5">FOVE VR system</h3><p>The FOVE VR system contains a VR headset and an infrared camera. The camera is used for head position tracking by detecting LED arrays embedded in the headset, but invisible to the eye. FOVE displays images on a 2560 × 1440 pixel display with a frame rate of 70 Hz and a field of view of 90°–100° (For more information, see Fove Inc <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Fove Inc. (2017) FOVE 0 eye tracking virtual reality devkit user manual. archive.getfove.com/setup/FOVE0_User_Manual.pdf. Accessed April 5, 2017" href="/article/10.1007/s10055-018-0339-2#ref-CR16" id="ref-link-section-d28835e578">2017</a>).</p><p>FOVE also provides plug in software for the Unity system. The software mainly specifies an origin for the camera, imitating where the eyes are, and a field of view which moves with respect to the origin depending on the rotation and displacement of the actual headset. The display is also slightly shifted between the left and right eye to give the environment a 3D look.</p><h3 class="c-article__sub-heading" id="Sec6">VR experiment environment</h3><p>We designed software interface in Unity to facilitate data collection. The interface contains a VR scene showing a virtual object in the 3D VR space and a hand model to reach and grasp the object during an execution of grasping task. A LabView visual interface was also customized to read-in and show the FMG signals. The hand model was provided by the plug in software of Leap Motion and driven by the user’s hand position data detected by the Leap Motion system.</p><p>Raw data recorded from the Leap Motion included the positions of the fingertips and palm center in 3D space. The position of the hands detected by the Leap Motion was linked to the environment by specifying the location of the device itself with respect to the origin of the environment (specified by Unity as described in sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0339-2#Sec5">2.3</a>). The hand is then displayed in relation to that origin, mirroring its displacement from the device in real life.</p><p>The local machine time was recorded from both LabView and Unity and saved to the FSR signals, the Leap Motion, and the scene data files, respectively, for the purpose of synchronization. The hand and fingertips positions data captured by the Leap Motion and FSR signal data from the FSR band together with timestamps for both Leap Motion and FSR data samples were saved to a text document for offline data analysis.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Experimental protocol</h2><div class="c-article-section__content" id="Sec7-content"><p>Six grasps involving interactions with small virtual objects were explored, including pinching a ball using different number of fingers (G1–G3), lateral key-pinching thin disk (G4), and hand wrapping a slender cylinder (G5 and G6, as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0339-2#Tab1">1</a>). These grasps were variations from the taxonomies of Cutkosky (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. Robot Autom IEEE Trans 5:269–279. &#xA;                    https://doi.org/10.1109/70.34763&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR8" id="ref-link-section-d28835e614">1989</a>) and Feix et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Feix T, Bullock IM, Dollar AM (2014) Analysis of human grasping behavior: object characteristics and grasp type. IEEE Trans Haptics 7:311–323. &#xA;                    https://doi.org/10.1109/TOH.2014.2326871&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR14" id="ref-link-section-d28835e617">2014</a>), which are commonly used in daily life. Also, these 6 grasps were chosen as they are relatively likely to be confused by both FMG and Leap Motion. For example, the three pinch grasps (G1–G3) use quite similar configurations of the digits and palm for FMG and Leap Motion sensors. There were 4 virtual objects (shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0339-2#Tab1">1</a>) created, corresponding to the 6 grasps. Each time, one object was shown in the VR space within the viewing scope of the Leap Motion.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Illustration of hand gestures and virtual objects for grasping</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0339-2/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In a sitting position and wearing FOVE VR goggles on the head, participants performed grasps of the virtual object shown in the VR goggles, while FMG and Leap Motion signals were recorded. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig1">1</a>b illustrates the participant wearing the VR goggles and the FMG band in experiment setting 1 (with Leap Motion on the VR goggles). The execution order of the 6 grasps was randomized; the position of each object in the VR space was also randomized, but within the viewing scope of the Leap Motion. To facilitate grasping, a picture of the corresponding hand gesture was shown on the bottom-left of the virtual space. During a specific grasp’s execution, each participant reached to grasp the virtual object and kept grasping for at least 3 s. Then, the gesture was changed to the next one. A trial ended when all six grasps were tested. An animation icon of a timer was also shown on the right side of the virtual object, indicating the grasping time left.</p><p>Each participant performed 10 trials of the above-described 6 grasps in each of the two experimental settings (with the Leap Motion on the desk or on the goggles). Participants had a short rest between each trial, while the operator was saving the data, and could request extra rest time if needed. Before the experiment started, the experimenter briefly explained the procedures to the participant and the participant was asked to perform at least 5 practice trials. Five participants started trials with the Leap Motion in head-mounted setting, then the on-desk setting, while the remaining participants performed the trials in the opposite order.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Data analysis</h2><div class="c-article-section__content" id="Sec8-content"><h3 class="c-article__sub-heading" id="Sec9">Signal synchronization and preprocessing</h3><p>The performance of the grasp postures recognition was evaluated based on the Leap Motion signal, FMG signal, and the fusion of both, respectively. The illustration of signal processing and classification is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig2">2</a>. The Leap Motion signals were recorded in the frame update event in Unity (sampling rate of 60 Hz). However, due to variations of computing workload of the working laptop, we observed some variation during data collection (typically ranging from 30 to 60 Hz). The sample rate of FMG signals recorded by LabView was relatively stable at 50 Hz. In order to be fused, the Leap Motion and FMG signals were both linearly interpolated to 100 Hz and were synchronized by the local clock time (from the working laptop) recorded by both Unity and LabView together with the signals.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Schematic illustration of the method for signal processing and classification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Both Leap Motion and FMG signals were median filtered using a 11-sample window (110 ms), which is usually chosen in practice for real-time applications without significant perceptible delay created by the window (Farrell and Weir <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Farrell TR, Weir RF (2007) The optimal controller delay for myoelectric prostheses. IEEE Neural Syst Rehabil Eng. &#xA;                    https://doi.org/10.1109/tnsre.2007.891391&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR12" id="ref-link-section-d28835e685">2007</a>). The last 3 s of data for each grasping in a trial were used when the gesture was in a steady state (see Section III).</p><h3 class="c-article__sub-heading" id="Sec10">Leap Motion features extraction</h3><p>Three types of features, including the angle, distance, and elevation of fingertips, were derived from each of the fingers from the data output of Leap Motion (Marin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e696">2014</a>). The fingertips angle was defined as the angle between the direction on the palm plane projected from the finger direction and the palm direction. Fingertips distance feature was defined as the distance between the fingertips and the palm center. The fingertips elevation was defined as the distance between the fingertip and its projected tip on the palm plane. The palm plane was determined by the palm norm vector and the palm center position. The detailed algorithms for deriving these features are described in Marin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e699">2014</a>). Since the fingertips tracking algorithm was improved in the latest version of the Leap Motion system, and all five fingers can be identified one-to-one matching by the Leap Motion model, there was no need for extra calibration as required in Marin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e702">2014</a>) for this purpose. There were a total of 15 feature data points derived from each of the raw Leap Motion data points.</p><h3 class="c-article__sub-heading" id="Sec11">Classification algorithm</h3><p>Linear discriminant analysis (LDA) was the major algorithm employed to evaluate the classification performance of the collected data. Therefore, our major findings, including the performance of the fusion method, as well as discussion and comparison with other papers, are based on the results of LDA. LDA was chosen for this study because of its ease to be applied in real time and ability to achieve similar or better classification results than other, more complex methods (Englehart and Hudgins <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Englehart K, Hudgins B (2003) A robust, real-time control scheme for multifunction myoelectric control. Biomed Eng IEEE Trans 50:848–854. &#xA;                    https://doi.org/10.1109/TBME.2003.813539&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR11" id="ref-link-section-d28835e713">2003</a>; Scheme and Englehart <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Scheme E, Englehart K (2011) Electromyogram pattern recognition for control of powered upper-limb prostheses: state of the art and challenges for clinical use. J Rehabil Res Dev 48:643. &#xA;                    https://doi.org/10.1682/JRRD.2010.09.0177&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR36" id="ref-link-section-d28835e716">2011</a>; Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zhang H, Zhao Y, Yao F, Xu L, Shang P, Li G (2013) An adaptation strategy of using LDA classifier for EMG pattern recognition. In: 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC). &#xA;                    https://doi.org/10.1109/embc.2013.6610488&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR45" id="ref-link-section-d28835e719">2013</a>; Amsuss et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Amsuss S, Goebel PM, Jiang N, Graimann B, Paredes L, Farina D (2014) Self-correcting pattern recognition system of surface EMG signals for upper limb prosthesis control. IEEE Trans Biomed Eng 61:1167–1176. &#xA;                    https://doi.org/10.1109/TBME.2013.2296274&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR2" id="ref-link-section-d28835e722">2014</a>). The LDA classifier used here is from MATLAB Statistics and Machine Learning Toolbox, which is an extension of Fisher linear discriminant analysis (Fisher <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1936" title="Fisher RA (1936) The use of multiple measurements in taxonomic problems. Ann Eugen 7:179–188. &#xA;                    https://doi.org/10.1111/j.1469-1809.1936.tb02137.x&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR15" id="ref-link-section-d28835e725">1936</a>). The discriminant type of LDA was pseudolinear, and the linear coefficient threshold was set to a default of 0.</p><p>To further explore the validity of the fusion method using general machine learning algorithms, 3 additional mainstream supervised learning algorithms (support vector machine (SVM), bagging of decision trees (TreeBagger), and neural network (NN)) were employed to evaluate the performance of grasp classification in a way similar as LDA. For a detailed description of these learning algorithms, see (Dietterich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Dietterich TG (2000) An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization. Mach Learn 40:139–157. &#xA;                    https://doi.org/10.1023/A:1007607513941&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR10" id="ref-link-section-d28835e731">2000</a>; Vapnik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Vapnik V (1998) Statistical learning theory. Wiley, New York" href="/article/10.1007/s10055-018-0339-2#ref-CR39" id="ref-link-section-d28835e734">1998</a>; Zurada <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Zurada JM (1992) Introduction to artificial neural systems. West, St Paul" href="/article/10.1007/s10055-018-0339-2#ref-CR46" id="ref-link-section-d28835e737">1992</a>). The TreeBagger and NN were implemented using the corresponding functions from MATLAB Statistics and Machine Learning Toolbox. The number of trees used in the TreeBagger was empirically set to 50. The number of hidden layers of NN was set to 1, and the number of neurons in each hidden layer was set to 16, 21, 31 for FMG, Leap Motion, and Fusion models, respectively. The SVM was from LibSVM, using the kernel of Radial Based Function (RBF) and the parameters (gamma and alpha) set to default (gamma = 1/feature number, and alpha = 1), as suggested in Chang and Lin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 2:1–27. &#xA;                    https://doi.org/10.1145/1961189.1961199&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR5" id="ref-link-section-d28835e740">2011</a>).</p><h3 class="c-article__sub-heading" id="Sec12">Cross-trial evaluation</h3><p>A ten-fold cross-trial validation method was employed to evaluate the performance of the grasp classification using FMG, Leap Motion, and the fusion of both, respectively. Data from each participant in each experimental setting (on-desk and head-mounted) were divided into testing and training data to evaluate the classifiers. The testing data used one trial out of the 10 trials with the other 9 trials being used as training data. To explore whether the training data size affected the accuracy of the grasp recognition, the system was tested using 4 different sizes of the training data, i.e., using the first 3, 5, 7, and 9 trials in the training data to train the model, respectively.</p><p>Before feeding the data into a classifier, both the training and the testing data sets were normalized to values ranging from 0 to 1, using the minimum and maximum values of the training data. The normalization equation is as in (1).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$y = \frac{{x - { \hbox{min} }}}{{{ \hbox{max} } - { \hbox{min} }}}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>x</i> is the data sample to be normalized, max and min are the minimum and maximum values from the training data set as described before, and <i>y</i> is the normalized sample.</p><h3 class="c-article__sub-heading" id="Sec13">ANOVA</h3><p>A three-way ANOVA was conducted to examine the effect of the independent variables signal type (FMG, Leap Motion, and Fusion), classification algorithm (LDA, TreeBagger, SVM, and NN), and experimental setting (on-desk and head-mounted) to the dependent variable classification accuracy. Post Hoc pair comparison (Tukey HSD) was further conducted if there was any significant effect of the variables on the accuracy. The significance level was set to <i>p</i> value = .05.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Results</h2><div class="c-article-section__content" id="Sec14-content"><p>A total of 11 participants (3 females and 8 males) were recruited in the study. All participants reported to have 100% functionality with their hands. The average wrist size was 16.5 ± 1.0 cm. All participants signed a consent form, which had been approved by Simon Fraser University, before entering the study.</p><p>There were a total of 220 trials collected from the participants (each performed 10 trials in each of the two settings). Each gesture lasted for 3 s, with 300 data samples for each gesture of each data type (FMG and Leap Motion signals). Therefore, there were a total of 36,000 data samples for each of FMG and Leap signals for a given participant (300 samples per gesture × 6 gestures × 10 trials × 2 settings).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig3">3</a> shows an example trial (subject 1, trial 1, setting 1) of FMG raw signals (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig3">3</a>a) and features of Leap Motion data (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig3">3</a>b), respectively. The FMG signals shown in the figure are the raw FSR signals output from 10 FSR sensors on the band, and the Leap Motion signals are the 15 features of angle, distance, and elevation from the 5 digits to the palm plane. The 6 grasps were performed in a random order, which resulted in the specific sequence shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig3">3</a>b. We can see that both FMG and Leap Motion signals show distinct patterns between different types of grasps.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Example trial (subject 1, trial 1, setting 1) of the signals from FMG (A) and Leap Motion (B), respectively. The signals for each of the gestures are arranged in the sequence executed in this particular trial. For gesture names of the IDs in <i>x</i>-axis, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0339-2#Tab1">1</a>. The signals are normalized to [0, 1] using minimum and maximum of the training data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig4">4</a> shows classification accuracies of using LDA for FMG, Leap Motion features, and fusion of both signals, respectively, averaged across 11 participants, spanned by different training sample sizes of 3–9 trials. Using all 9 trials for training in on-desk setting, the FMG band alone achieved an accuracy of 87.7%, which is slightly higher than that of Leap Motion (85.4%). The fusion of both FMG and Leap Motion signals improved the accuracy to about 93.4%. In the head-mounted setting, both FMG and Leap Motion achieved lower accuracies (76.4 and 73.6%) compared to those in the on-desk setting; however, the fusion of FMG and Leap Motion signals greatly increased the classification accuracy to 85.3%.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Classification accuracies of using FMG, Leap Motion features, and fusion of both, respectively, averaged across 10 trials from 11 participants, spanned by different training sample sizes. The classification algorithm is LDA. The error bars are in 1 standard deviation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Training sample size has effects on the classification accuracy, but differently in the two experimental settings. In the on-desk setting, the accuracy increases gradually from 83.3% when using 3 trials to 93.4% when using 9 trials in signal fusion condition. However, in the head-mounted setting using fusion of the signals, the classification accuracies are relatively low (51.6 and 59.1% while using 3 trials and 5 trials, respectively), but quickly increase to 82.6 and 85.3% when using 7 trials and 9 trials training data. This means not only lower accuracies achieved in the head-mounted setting, but also more data are needed for training a model in order to achieve a relatively good accuracy.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig5">5</a> shows the classification accuracies of using FMG, Leap Motion features, and fusion of both signals, respectively, averaged across 11 participants, spanned by different classification algorithms in two settings. Fusion accuracies are consistently higher than those using FMG and Leap Motion alone for all four classification algorithms, although FMG achieves similar or slightly lower accuracies in other three classification algorithms (TreeBagger, SVM, and NN). The summary of the classification of different signals, classification algorithms, and experimental settings is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0339-2#Tab2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Classification accuracies of using FMG, Leap Motion features, and fusion of both, respectively, averaged across 10 trials from 11 participants, spanned by different classification algorithms. The error bars are in 1 standard deviation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Classification accuracies in terms of experimental signal type, classification algorithm, and setting</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0339-2/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Three-way ANOVA shows that there are both significant effects of the signal type (<i>F</i><sub>2, 251</sub> = 54.19, <i>p</i> &lt; .00001) and experimental setting (<i>F</i><sub>1, 251</sub> = 111.14, <i>p</i> &lt; .00001) on the classification accuracy of the gestures, but there is no significant effect of classification algorithm. Also, there was no significant interaction effect between any of the combinations of the three independent factors signal type, classification algorithm, and experimental setting. The Post Hoc test (Tukey HSD) on the effect of signal type showed that the classification accuracy of using fusion of FMG and Leap signals is significantly higher than that of using Leap Motion only (<i>p</i> &lt; .00001) and that of FMG (<i>p</i> &lt; .00001); there is no significant difference between the Leap Motion and FMG.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig6">6</a> shows an example of comparisons between predicted grasps label to the true label of using FMG, Leap Motion features, and fusion of both, respectively (subject 1, trial 1, setting 1 using all 9 trials for training). The accuracies for FMG, Leap Motion, and the fusion of both in this particular case are 65.6, 87.7, and 100%, respectively, predicted by LDA. An overlay between the true and predicted label represents a correct prediction and a displacement for a misclassification. Most of the misclassifications in the gestures predictions using either FMG or Leap Motion signals alone in this particular case have been improved by the fusion of FMG and Leap Motion features.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Example comparisons between predicted grasps label to the true label of using FMG, Leap Motion features, and fusion of both, respectively (subject 1, trial 1, setting 1). An overlay between the true and predicted label represents a correct prediction and a displacement for a misclassification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0339-2#Fig7">7</a> shows the confusion matrixes of the cross-trial evaluation of 6 gestures across 10 trials from 11 participants performed in 2 settings, for FMG, Leap Motion features, and fusion of both, using LDA. The number of each cell in the matrixes is the percentage of true gesture class (in <i>y</i>-axis) that had been predicted as in <i>x</i>-axis; the diagonals show the gestures class has been correctly predicted. FMG has difficulty distinguishing G2 and G3, the tripod and five-digits grips, in both on-desk and head-mounted settings. However, Leap Motion has more confusion under head-mounted setting for G2 and G3, since both these gestures are not easily distinguished from one another from a top-to-bottom view.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0339-2/MediaObjects/10055_2018_339_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Confusion matrixes of average results from 11 participants spanned by two settings: (1) on-desk setting and (2) head-mounted setting. Each number in the cell is the percentage of the samples belonging to the category indicated by the <i>y</i>-axis label and was predicted to be the class indicated by it corresponding <i>x</i>-axis label. The labels on <i>x-</i> and <i>y</i>-axis are six gestures G1–G6 representing G1: pinch, G2: tripod grasp, G3: five-digits grasp, G4: lateral pinch, G5: small diameter grasp, and G6: light tool grasps</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0339-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Discussion</h2><div class="c-article-section__content" id="Sec15-content"><p>Grasp posture detection is challenging for vision-based technologies due to self-occlusions between digits and palm. By using fusion of FSR and Leap Motion signals, grasp classification accuracies were significantly improved in the present study, from 85.4% using Leap Motion only to 93.4% using fusion signals in the on-desk setting, and more significantly improved in the head-mounted setting, from 73.6 to 85.3%.</p><p>In the present study, the Leap Motion achieved a comparable accuracy (85.4%) to that seen in the study of Marin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e1276">2014</a>), which was 81% in classifying 10 sign language hand postures using the same Leap Motion features. Considering the number of classification classes, the improvement from random chance classification accuracy in classifying 6 gestures (17% = 100% ÷  6) in the present study by only using Leap Motion is 68%, which is also close to that of the study of Marin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e1279">2014</a>), where 71% classification accuracy was improved compared to random chance in classifying 10 gestures (10% = 100% ÷ 10). One possible reason affecting the performance of Leap Motion in this study is the severe self-occlusion existing between digits and palm. In addition, some of the gestures were very similar, such as the tripod and five-digits grips, as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0339-2#Tab1">1</a>. Therefore, using Leap Motion alone in classifying complex hand postures such as grasps might not be sufficient.</p><p>The fusion of Leap Motion and FMG signals achieved a similar level (93.4%) of accuracy compared to the fusion of Leap Motion and Kinect (Marin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569" href="/article/10.1007/s10055-018-0339-2#ref-CR26" id="ref-link-section-d28835e1288">2014</a>), e.g., 91%. However, fusion of Leap Motion and FMG band offers much better portability than the fusion of Leap Motion and Kinect. Also, the viewing scope of the Kinect sensor is still limited.</p><p>A fairly high accuracy of 87.7% was obtained by using FMG signals only in the on-desk setting of this study. In our earlier study (Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017a" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e1294">2017a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. &#xA;                    https://doi.org/10.1109/thms.2017.2693245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR21" id="ref-link-section-d28835e1297">b</a>), a classification accuracy of 71% was achieved for 16 grasps when simply contacting physical objects without exerting force. Considering the number of grasps types, there is 71% improvement from random chance classification accuracy in classifying 6 grasps (17% = 100% ÷ 6) in the present study by only using FMG, which is comparable to that of the non-force condition in Jiang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017a" title="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. &#xA;                    https://doi.org/10.1016/j.medengphy.2017.01.015&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR22" id="ref-link-section-d28835e1300">2017a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. &#xA;                    https://doi.org/10.1109/thms.2017.2693245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0339-2#ref-CR21" id="ref-link-section-d28835e1303">b</a>), where 65% classification accuracy was improved compared to random chance in classifying 16 grasps (6% = 100% ÷ 16). This reasonable accuracy using the FMG band alone implies that the wearable FMG wrist band has the potential to enhance gesture recognition and widen hand movement space, even when the hand moves out of the viewing scope of the Leap Motion.</p><p>Leap Motion achieved relatively lower accuracies (73.2%) in the head-mounted setting compared to the on-desk setting in the present study. One possible reason is that in the head-mounted setting, the finger tips are occluded more severely by the palm, since the Leap Motion looks at the hand from the backside of the palm. Another reason is that slight movement of the Leap Motion itself on the VR goggles causes a decrease of the classification accuracy due to the decreased signal stability.</p><p>Not surprisingly, the accuracy of using FMG also correspondingly decreased (only 76.4%) in the head-mounted setting compared to the on-desk setting (87.7%), since the decreased stability of the hand model generated by the Leap Motion would cause more variation of the FMG signal. For example, when the quality of the hand model captured by the Leap Motion decreases, the participants sometimes moved or adjusted their hand grasp configuration unconsciously, trying to get a better hand model shown in the VR space, even though they were instructed to keep performing the gestures in a stable manner for 3 s.</p><p>Interestingly, although the accuracies of using Leap Motion and FMG alone in head-mounted setting were low, the fusion of Leap Motion and FMG signals greatly increased the accuracy to a similar level to that in the on-desk setting. The head-mounted setting is important in real-life scenarios, since it offers much more portability, as the Leap motion can be conveniently attached to the VR goggles.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Limitations and future work</h2><div class="c-article-section__content" id="Sec16-content"><p>As the main purpose of this preliminary study was to demonstrate the viability of the employing FMG technology to improve gesture recognition under VR environments by fusing with FMG and Leap Motion signals, 6 grasps were chosen for the fact that they were easily confused by both FMG and Leap Motion. In the future, a full taxonomy of hand grasps will be evaluated.</p><p>Future research should also improve the usability of the FSR band, including improving the machine learning model to tolerate the FMG band displacement, inter-wearing (taking off and re-attaching) and inter-subject variations without re-training the band. Implementation of the present algorithm into a real-time system is also considered.</p><p>One main concern is that the comparison between the accuracies using Leap Motion and FMG is not a fair one, since we used the hand model generated by the Leap Motion as feedback to guide the reaching and grasping in the experiment and then derive the accuracies. However, the primary goal of this study was to explore how FMG technology can improve the grasps recognition based on Leap Motion, instead of a throughout comparison between the performances of FMG and Leap Motion in terms of hand gesture classification.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusion</h2><div class="c-article-section__content" id="Sec17-content"><p>A significant improvement of the accuracy was achieved in the present study by using fusion of a relatively new wearable sensing technology, i.e., force myography, and the traditional portable Leap Motion sensor, for a set of 6 grasps classification under VR environments, compared to using Leap Motion alone. This finding sheds light on complex grasps recognition under portable VR environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AH. Al-Timemy, RN. Khushaba, G. Bugmann, J. Escudero, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Al-Timemy AH, Khushaba RN, Bugmann G, Escudero J (2015) Improving the performance against force variation of E" /><p class="c-article-references__text" id="ref-CR1">Al-Timemy AH, Khushaba RN, Bugmann G, Escudero J (2015) Improving the performance against force variation of EMG controlled multifunctional upper-limb prostheses for transradial amputees. IEEE Trans Neural Syst Rehabil Eng. <a href="https://doi.org/10.1109/tnsre.2015.2445634">https://doi.org/10.1109/tnsre.2015.2445634</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2Ftnsre.2015.2445634" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Improving%20the%20performance%20against%20force%20variation%20of%20EMG%20controlled%20multifunctional%20upper-limb%20prostheses%20for%20transradial%20amputees&amp;journal=IEEE%20Trans%20Neural%20Syst%20Rehabil%20Eng&amp;doi=10.1109%2Ftnsre.2015.2445634&amp;publication_year=2015&amp;author=Al-Timemy%2CAH&amp;author=Khushaba%2CRN&amp;author=Bugmann%2CG&amp;author=Escudero%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Amsuss, PM. Goebel, N. Jiang, B. Graimann, L. Paredes, D. Farina, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Amsuss S, Goebel PM, Jiang N, Graimann B, Paredes L, Farina D (2014) Self-correcting pattern recognition syste" /><p class="c-article-references__text" id="ref-CR2">Amsuss S, Goebel PM, Jiang N, Graimann B, Paredes L, Farina D (2014) Self-correcting pattern recognition system of surface EMG signals for upper limb prosthesis control. IEEE Trans Biomed Eng 61:1167–1176. <a href="https://doi.org/10.1109/TBME.2013.2296274">https://doi.org/10.1109/TBME.2013.2296274</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTBME.2013.2296274" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-correcting%20pattern%20recognition%20system%20of%20surface%20EMG%20signals%20for%20upper%20limb%20prosthesis%20control&amp;journal=IEEE%20Trans%20Biomed%20Eng&amp;doi=10.1109%2FTBME.2013.2296274&amp;volume=61&amp;pages=1167-1176&amp;publication_year=2014&amp;author=Amsuss%2CS&amp;author=Goebel%2CPM&amp;author=Jiang%2CN&amp;author=Graimann%2CB&amp;author=Paredes%2CL&amp;author=Farina%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="GC. Burdea, P. Coiffet, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Burdea GC, Coiffet P (2003) Virtual reality technology. Wiley, New York" /><p class="c-article-references__text" id="ref-CR3">Burdea GC, Coiffet P (2003) Virtual reality technology. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20technology&amp;publication_year=2003&amp;author=Burdea%2CGC&amp;author=Coiffet%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MCF. Castro, SP. Arjunan, DK. Kumar, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Castro MCF, Arjunan SP, Kumar DK (2015) Selection of suitable hand gestures for reliable myoelectric human com" /><p class="c-article-references__text" id="ref-CR4">Castro MCF, Arjunan SP, Kumar DK (2015) Selection of suitable hand gestures for reliable myoelectric human computer interface. Biomed Eng Online 14:1–11. <a href="https://doi.org/10.1186/s12938-015-0025-5">https://doi.org/10.1186/s12938-015-0025-5</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1186%2Fs12938-015-0025-5" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Selection%20of%20suitable%20hand%20gestures%20for%20reliable%20myoelectric%20human%20computer%20interface&amp;journal=Biomed%20Eng%20Online&amp;doi=10.1186%2Fs12938-015-0025-5&amp;volume=14&amp;pages=1-11&amp;publication_year=2015&amp;author=Castro%2CMCF&amp;author=Arjunan%2CSP&amp;author=Kumar%2CDK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Chang, C. Lin, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 2:1–27. htt" /><p class="c-article-references__text" id="ref-CR5">Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 2:1–27. <a href="https://doi.org/10.1145/1961189.1961199">https://doi.org/10.1145/1961189.1961199</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1961189.1961199" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=LIBSVM%3A%20a%20library%20for%20support%20vector%20machines&amp;journal=ACM%20Trans%20Intell%20Syst%20Technol&amp;doi=10.1145%2F1961189.1961199&amp;volume=2&amp;pages=1-27&amp;publication_year=2011&amp;author=Chang%2CC&amp;author=Lin%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chuan CH, Regina E, Guardino C (2014) American sign language recognition using leap motion sensor. In: 2014 13" /><p class="c-article-references__text" id="ref-CR6">Chuan CH, Regina E, Guardino C (2014) American sign language recognition using leap motion sensor. In: 2014 13th international conference on machine learning applications, 2014, pp 541–544. <a href="https://doi.org/10.1109/icmla.2014.110">https://doi.org/10.1109/icmla.2014.110</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Colgan A (2018) How does the leap motion controller work. Leap Motion Blog. http://blog.leapmotion.com/hardwar" /><p class="c-article-references__text" id="ref-CR7">Colgan A (2018) How does the leap motion controller work. Leap Motion Blog. <a href="http://blog.leapmotion.com/hardware-to-software-how-does-the-leap-motion-controller-work/">http://blog.leapmotion.com/hardware-to-software-how-does-the-leap-motion-controller-work/</a>. Accessed February 16, 2018</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MR. Cutkosky, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. Robot Autom" /><p class="c-article-references__text" id="ref-CR8">Cutkosky MR (1989) On grasp choice, grasp models, and the design of hands for manufacturing tasks. Robot Autom IEEE Trans 5:269–279. <a href="https://doi.org/10.1109/70.34763">https://doi.org/10.1109/70.34763</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.34763" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20grasp%20choice%2C%20grasp%20models%2C%20and%20the%20design%20of%20hands%20for%20manufacturing%20tasks&amp;journal=Robot%20Autom%20IEEE%20Trans&amp;doi=10.1109%2F70.34763&amp;volume=5&amp;pages=269-279&amp;publication_year=1989&amp;author=Cutkosky%2CMR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dementyev A, Paradiso JA (2014) WristFlex: low-power gesture input with wrist-worn pressure sensors. In: Proce" /><p class="c-article-references__text" id="ref-CR9">Dementyev A, Paradiso JA (2014) WristFlex: low-power gesture input with wrist-worn pressure sensors. In: Proceedings of the 27th annual ACM symposium user interface software technology, UIST’14, ACM Press, New York, New York, USA, 2014, pp 161–166. <a href="https://doi.org/10.1145/2642918.2647396">https://doi.org/10.1145/2642918.2647396</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TG. Dietterich, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Dietterich TG (2000) An experimental comparison of three methods for constructing ensembles of decision trees:" /><p class="c-article-references__text" id="ref-CR10">Dietterich TG (2000) An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization. Mach Learn 40:139–157. <a href="https://doi.org/10.1023/A:1007607513941">https://doi.org/10.1023/A:1007607513941</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1007607513941" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20comparison%20of%20three%20methods%20for%20constructing%20ensembles%20of%20decision%20trees%3A%20bagging%2C%20boosting%2C%20and%20randomization&amp;journal=Mach%20Learn&amp;doi=10.1023%2FA%3A1007607513941&amp;volume=40&amp;pages=139-157&amp;publication_year=2000&amp;author=Dietterich%2CTG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Englehart, B. Hudgins, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Englehart K, Hudgins B (2003) A robust, real-time control scheme for multifunction myoelectric control. Biomed" /><p class="c-article-references__text" id="ref-CR11">Englehart K, Hudgins B (2003) A robust, real-time control scheme for multifunction myoelectric control. Biomed Eng IEEE Trans 50:848–854. <a href="https://doi.org/10.1109/TBME.2003.813539">https://doi.org/10.1109/TBME.2003.813539</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTBME.2003.813539" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robust%2C%20real-time%20control%20scheme%20for%20multifunction%20myoelectric%20control&amp;journal=Biomed%20Eng%20IEEE%20Trans&amp;doi=10.1109%2FTBME.2003.813539&amp;volume=50&amp;pages=848-854&amp;publication_year=2003&amp;author=Englehart%2CK&amp;author=Hudgins%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TR. Farrell, RF. Weir, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Farrell TR, Weir RF (2007) The optimal controller delay for myoelectric prostheses. IEEE Neural Syst Rehabil E" /><p class="c-article-references__text" id="ref-CR12">Farrell TR, Weir RF (2007) The optimal controller delay for myoelectric prostheses. IEEE Neural Syst Rehabil Eng. <a href="https://doi.org/10.1109/tnsre.2007.891391">https://doi.org/10.1109/tnsre.2007.891391</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2Ftnsre.2007.891391" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20optimal%20controller%20delay%20for%20myoelectric%20prostheses&amp;journal=IEEE%20Neural%20Syst%20Rehabil%20Eng&amp;doi=10.1109%2Ftnsre.2007.891391&amp;publication_year=2007&amp;author=Farrell%2CTR&amp;author=Weir%2CRF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Faria, D. Hutchinson, WJ. Wellington, S. Gold, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Faria AJ, Hutchinson D, Wellington WJ, Gold S (2009) Developments in business gaming: a review of the past 40 " /><p class="c-article-references__text" id="ref-CR13">Faria AJ, Hutchinson D, Wellington WJ, Gold S (2009) Developments in business gaming: a review of the past 40 years. Simul. Gaming. 40:464–487</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F1046878108327585" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developments%20in%20business%20gaming%3A%20a%20review%20of%20the%20past%2040%20years&amp;journal=Simul.%20Gaming.&amp;volume=40&amp;pages=464-487&amp;publication_year=2009&amp;author=Faria%2CAJ&amp;author=Hutchinson%2CD&amp;author=Wellington%2CWJ&amp;author=Gold%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Feix, IM. Bullock, AM. Dollar, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Feix T, Bullock IM, Dollar AM (2014) Analysis of human grasping behavior: object characteristics and grasp typ" /><p class="c-article-references__text" id="ref-CR14">Feix T, Bullock IM, Dollar AM (2014) Analysis of human grasping behavior: object characteristics and grasp type. IEEE Trans Haptics 7:311–323. <a href="https://doi.org/10.1109/TOH.2014.2326871">https://doi.org/10.1109/TOH.2014.2326871</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTOH.2014.2326871" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20human%20grasping%20behavior%3A%20object%20characteristics%20and%20grasp%20type&amp;journal=IEEE%20Trans%20Haptics&amp;doi=10.1109%2FTOH.2014.2326871&amp;volume=7&amp;pages=311-323&amp;publication_year=2014&amp;author=Feix%2CT&amp;author=Bullock%2CIM&amp;author=Dollar%2CAM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Fisher, " /><meta itemprop="datePublished" content="1936" /><meta itemprop="headline" content="Fisher RA (1936) The use of multiple measurements in taxonomic problems. Ann Eugen 7:179–188. https://doi.org/" /><p class="c-article-references__text" id="ref-CR15">Fisher RA (1936) The use of multiple measurements in taxonomic problems. Ann Eugen 7:179–188. <a href="https://doi.org/10.1111/j.1469-1809.1936.tb02137.x">https://doi.org/10.1111/j.1469-1809.1936.tb02137.x</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1469-1809.1936.tb02137.x" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20use%20of%20multiple%20measurements%20in%20taxonomic%20problems&amp;journal=Ann%20Eugen&amp;doi=10.1111%2Fj.1469-1809.1936.tb02137.x&amp;volume=7&amp;pages=179-188&amp;publication_year=1936&amp;author=Fisher%2CRA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fove Inc. (2017) FOVE 0 eye tracking virtual reality devkit user manual. archive.getfove.com/setup/FOVE0_User_" /><p class="c-article-references__text" id="ref-CR16">Fove Inc. (2017) FOVE 0 eye tracking virtual reality devkit user manual. archive.getfove.com/setup/FOVE0_User_Manual.pdf. Accessed April 5, 2017</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Grimm, G. Naros, A. Gharabaghi, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Grimm F, Naros G, Gharabaghi A (2016) Closed-loop task difficulty adaptation during virtual reality reach-to-g" /><p class="c-article-references__text" id="ref-CR17">Grimm F, Naros G, Gharabaghi A (2016) Closed-loop task difficulty adaptation during virtual reality reach-to-grasp training assisted with an exoskeleton for stroke rehabilitation. Front Neurosci 10:518</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Closed-loop%20task%20difficulty%20adaptation%20during%20virtual%20reality%20reach-to-grasp%20training%20assisted%20with%20an%20exoskeleton%20for%20stroke%20rehabilitation&amp;journal=Front%20Neurosci&amp;volume=10&amp;publication_year=2016&amp;author=Grimm%2CF&amp;author=Naros%2CG&amp;author=Gharabaghi%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Guna, G. Jakus, M. Pogačnik, S. Tomažič, J. Sodnik, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Guna J, Jakus G, Pogačnik M, Tomažič S, Sodnik J (2014) An analysis of the precision and reliability of the le" /><p class="c-article-references__text" id="ref-CR18">Guna J, Jakus G, Pogačnik M, Tomažič S, Sodnik J (2014) An analysis of the precision and reliability of the leap motion sensor and its suitability for static and dynamic tracking. Sens (Switz) 14:3702–3720. <a href="https://doi.org/10.3390/s140203702">https://doi.org/10.3390/s140203702</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs140203702" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20analysis%20of%20the%20precision%20and%20reliability%20of%20the%20leap%20motion%20sensor%20and%20its%20suitability%20for%20static%20and%20dynamic%20tracking&amp;journal=Sens%20%28Switz%29&amp;doi=10.3390%2Fs140203702&amp;volume=14&amp;pages=3702-3720&amp;publication_year=2014&amp;author=Guna%2CJ&amp;author=Jakus%2CG&amp;author=Poga%C4%8Dnik%2CM&amp;author=Toma%C5%BEi%C4%8D%2CS&amp;author=Sodnik%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MK. Holden, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol Behav 8:187–211" /><p class="c-article-references__text" id="ref-CR19">Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol Behav 8:187–211</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fcpb.2005.8.187" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environments%20for%20motor%20rehabilitation%3A%20review&amp;journal=Cyberpsychol%20Behav&amp;volume=8&amp;pages=187-211&amp;publication_year=2005&amp;author=Holden%2CMK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="InterlinkElectronics (2010) FSR® integration guide &amp; evaluation parts catalog with suggested electrical interf" /><p class="c-article-references__text" id="ref-CR20">InterlinkElectronics (2010) FSR<sup>®</sup> integration guide &amp; evaluation parts catalog with suggested electrical interfaces</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Jiang, L-K. Merhi, C. Menon, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Tr" /><p class="c-article-references__text" id="ref-CR21">Jiang X, Merhi L-K, Menon C (2017a) Force exertion affects grasp classification using force myography. IEEE Trans Human-Mach Syst. <a href="https://doi.org/10.1109/thms.2017.2693245">https://doi.org/10.1109/thms.2017.2693245</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2Fthms.2017.2693245" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Force%20exertion%20affects%20grasp%20classification%20using%20force%20myography&amp;journal=IEEE%20Trans%20Human-Mach%20Syst&amp;doi=10.1109%2Fthms.2017.2693245&amp;publication_year=2017&amp;author=Jiang%2CX&amp;author=Merhi%2CL-K&amp;author=Menon%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Jiang, L-K. Merhi, ZG. Xiao, C. Menon, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in ha" /><p class="c-article-references__text" id="ref-CR22">Jiang X, Merhi L-K, Xiao ZG, Menon C (2017b) Exploration of force myography and surface electromyography in hand gesture classification. Med Eng Phys 41:63–73. <a href="https://doi.org/10.1016/j.medengphy.2017.01.015">https://doi.org/10.1016/j.medengphy.2017.01.015</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.medengphy.2017.01.015" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploration%20of%20force%20myography%20and%20surface%20electromyography%20in%20hand%20gesture%20classification&amp;journal=Med%20Eng%20Phys&amp;doi=10.1016%2Fj.medengphy.2017.01.015&amp;volume=41&amp;pages=63-73&amp;publication_year=2017&amp;author=Jiang%2CX&amp;author=Merhi%2CL-K&amp;author=Xiao%2CZG&amp;author=Menon%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Jin, Q. Chen, Z. Chen, Y. Hu, J. Zhang, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Jin H, Chen Q, Chen Z, Hu Y, Zhang J (2016) Multi-LeapMotion sensor based demonstration for robotic refine tab" /><p class="c-article-references__text" id="ref-CR23">Jin H, Chen Q, Chen Z, Hu Y, Zhang J (2016) Multi-LeapMotion sensor based demonstration for robotic refine tabletop object manipulation task. CAAI Trans Intell Technol 1:104–113</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.trit.2016.03.010" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-LeapMotion%20sensor%20based%20demonstration%20for%20robotic%20refine%20tabletop%20object%20manipulation%20task&amp;journal=CAAI%20Trans%20Intell%20Technol&amp;volume=1&amp;pages=104-113&amp;publication_year=2016&amp;author=Jin%2CH&amp;author=Chen%2CQ&amp;author=Chen%2CZ&amp;author=Hu%2CY&amp;author=Zhang%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kolsch, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kolsch M (2004) Vision based hand gesture interfaces for wearable computing and virtual environments. Universi" /><p class="c-article-references__text" id="ref-CR24">Kolsch M (2004) Vision based hand gesture interfaces for wearable computing and virtual environments. University of California, Santa Barbara</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision%20based%20hand%20gesture%20interfaces%20for%20wearable%20computing%20and%20virtual%20environments&amp;publication_year=2004&amp;author=Kolsch%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Li, D. Yang, L. Jiang, H. Liu, H. Cai, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Li N, Yang D, Jiang L, Liu H, Cai H (2012) Combined use of FSR sensor array and SVM classifier for finger moti" /><p class="c-article-references__text" id="ref-CR25">Li N, Yang D, Jiang L, Liu H, Cai H (2012) Combined use of FSR sensor array and SVM classifier for finger motion recognition based on pressure distribution map. J Bionic Eng 9:39–47. <a href="https://doi.org/10.1016/S1672-6529(11)60095-4">https://doi.org/10.1016/S1672-6529(11)60095-4</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1672-6529%2811%2960095-4" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Combined%20use%20of%20FSR%20sensor%20array%20and%20SVM%20classifier%20for%20finger%20motion%20recognition%20based%20on%20pressure%20distribution%20map&amp;journal=J%20Bionic%20Eng&amp;doi=10.1016%2FS1672-6529%2811%2960095-4&amp;volume=9&amp;pages=39-47&amp;publication_year=2012&amp;author=Li%2CN&amp;author=Yang%2CD&amp;author=Jiang%2CL&amp;author=Liu%2CH&amp;author=Cai%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 " /><p class="c-article-references__text" id="ref-CR26">Marin G, Dominio F, Zanuttigh P (2014) Hand gesture recognition with leap motion and kinect devices. In: 2014 IEEE international conference on image process (ICIP), 2014, pp 1565–1569</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine M et al (1995) Virtual environment interaction techniques. UNC Chapel Hill Comput. Sci. Tech. Rep. TR95-0" /><p class="c-article-references__text" id="ref-CR27">Mine M et al (1995) Virtual environment interaction techniques. UNC Chapel Hill Comput. Sci. Tech. Rep. TR95-018, pp 507242–507248</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Motion L (2017) Leap motion SDK, https://developer.leapmotion.com/get-started. Accessed November 17, 2017" /><p class="c-article-references__text" id="ref-CR28">Motion L (2017) Leap motion SDK, <a href="https://developer.leapmotion.com/get-started">https://developer.leapmotion.com/get-started</a>. Accessed November 17, 2017</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SK. Ong, AYC. Nee, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Ong SK, Nee AYC (2013) Virtual and augmented reality applications in manufacturing. Springer Science &amp; Busines" /><p class="c-article-references__text" id="ref-CR29">Ong SK, Nee AYC (2013) Virtual and augmented reality applications in manufacturing. Springer Science &amp; Business Media, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20and%20augmented%20reality%20applications%20in%20manufacturing&amp;publication_year=2013&amp;author=Ong%2CSK&amp;author=Nee%2CAYC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Palacios, C. Sagüés, E. Montijano, S. Llorente, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Palacios JM, Sagüés C, Montijano E, Llorente S (2013) Human-computer interaction based on hand gestures using " /><p class="c-article-references__text" id="ref-CR30">Palacios JM, Sagüés C, Montijano E, Llorente S (2013) Human-computer interaction based on hand gestures using RGB-D sensors. Sensors. 13:11842–11860</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs130911842" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human-computer%20interaction%20based%20on%20hand%20gestures%20using%20RGB-D%20sensors&amp;journal=Sensors.&amp;volume=13&amp;pages=11842-11860&amp;publication_year=2013&amp;author=Palacios%2CJM&amp;author=Sag%C3%BC%C3%A9s%2CC&amp;author=Montijano%2CE&amp;author=Llorente%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Potter LE, Araullo J, Carter L (2013) The leap motion controller: a view on sign language. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR31">Potter LE, Araullo J, Carter L (2013) The leap motion controller: a view on sign language. In: Proceedings of the 25th Australian computer-human interaction conference: augmentation, application, innovation, collaboration, ACM, New York, NY, USA, 2013, pp 175–178. <a href="https://doi.org/10.1145/2541016.2541072">https://doi.org/10.1145/2541016.2541072</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Riillo, LR. Quitadamo, F. Cavrini, E. Gruppioni, CA. Pinto, NC. Pastò, L. Sbernini, L. Albero, G. Saggio, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Riillo F, Quitadamo LR, Cavrini F, Gruppioni E, Pinto CA, Pastò NC, Sbernini L, Albero L, Saggio G (2014) Opti" /><p class="c-article-references__text" id="ref-CR32">Riillo F, Quitadamo LR, Cavrini F, Gruppioni E, Pinto CA, Pastò NC, Sbernini L, Albero L, Saggio G (2014) Optimization of EMG-based hand gesture recognition: supervised vs. unsupervised data preprocessing on healthy subjects and transradial amputees. Biomed Signal Process Control 14:117–125. <a href="https://doi.org/10.1016/j.bspc.2014.07.007">https://doi.org/10.1016/j.bspc.2014.07.007</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.bspc.2014.07.007" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimization%20of%20EMG-based%20hand%20gesture%20recognition%3A%20supervised%20vs.%20unsupervised%20data%20preprocessing%20on%20healthy%20subjects%20and%20transradial%20amputees&amp;journal=Biomed%20Signal%20Process%20Control&amp;doi=10.1016%2Fj.bspc.2014.07.007&amp;volume=14&amp;pages=117-125&amp;publication_year=2014&amp;author=Riillo%2CF&amp;author=Quitadamo%2CLR&amp;author=Cavrini%2CF&amp;author=Gruppioni%2CE&amp;author=Pinto%2CCA&amp;author=Past%C3%B2%2CNC&amp;author=Sbernini%2CL&amp;author=Albero%2CL&amp;author=Saggio%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GP. Sadarangani, X. Jiang, LA. Simpson, JJ. Eng, C. Menon, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Sadarangani GP, Jiang X, Simpson LA, Eng JJ, Menon C (2017) Force myography for monitoring grasping in individ" /><p class="c-article-references__text" id="ref-CR33">Sadarangani GP, Jiang X, Simpson LA, Eng JJ, Menon C (2017) Force myography for monitoring grasping in individuals with stroke with mild to moderate upper-extremity impairments: a preliminary investigation in a controlled environment. Front Bioeng Biotechnol 5:42. <a href="https://doi.org/10.3389/fbioe.2017.00042">https://doi.org/10.3389/fbioe.2017.00042</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3389%2Ffbioe.2017.00042" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Force%20myography%20for%20monitoring%20grasping%20in%20individuals%20with%20stroke%20with%20mild%20to%20moderate%20upper-extremity%20impairments%3A%20a%20preliminary%20investigation%20in%20a%20controlled%20environment&amp;journal=Front%20Bioeng%20Biotechnol&amp;doi=10.3389%2Ffbioe.2017.00042&amp;volume=5&amp;publication_year=2017&amp;author=Sadarangani%2CGP&amp;author=Jiang%2CX&amp;author=Simpson%2CLA&amp;author=Eng%2CJJ&amp;author=Menon%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Sagayam, DJ. Hemanth, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Sagayam KM, Hemanth DJ (2017) Hand posture and gesture recognition techniques for virtual reality applications" /><p class="c-article-references__text" id="ref-CR34">Sagayam KM, Hemanth DJ (2017) Hand posture and gesture recognition techniques for virtual reality applications: a survey. Virtual Real 21:91–107. <a href="https://doi.org/10.1007/s10055-016-0301-0">https://doi.org/10.1007/s10055-016-0301-0</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-016-0301-0" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20posture%20and%20gesture%20recognition%20techniques%20for%20virtual%20reality%20applications%3A%20a%20survey&amp;journal=Virtual%20Real&amp;doi=10.1007%2Fs10055-016-0301-0&amp;volume=21&amp;pages=91-107&amp;publication_year=2017&amp;author=Sagayam%2CKM&amp;author=Hemanth%2CDJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RM. Satava, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Satava RM (1997) Virtual reality and telepresence for military medicine. Ann Acad Med Singapore 26:118–120" /><p class="c-article-references__text" id="ref-CR35">Satava RM (1997) Virtual reality and telepresence for military medicine. Ann Acad Med Singapore 26:118–120</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20and%20telepresence%20for%20military%20medicine&amp;journal=Ann%20Acad%20Med%20Singapore&amp;volume=26&amp;pages=118-120&amp;publication_year=1997&amp;author=Satava%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Scheme, K. Englehart, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Scheme E, Englehart K (2011) Electromyogram pattern recognition for control of powered upper-limb prostheses: " /><p class="c-article-references__text" id="ref-CR36">Scheme E, Englehart K (2011) Electromyogram pattern recognition for control of powered upper-limb prostheses: state of the art and challenges for clinical use. J Rehabil Res Dev 48:643. <a href="https://doi.org/10.1682/JRRD.2010.09.0177">https://doi.org/10.1682/JRRD.2010.09.0177</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1682%2FJRRD.2010.09.0177" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Electromyogram%20pattern%20recognition%20for%20control%20of%20powered%20upper-limb%20prostheses%3A%20state%20of%20the%20art%20and%20challenges%20for%20clinical%20use&amp;journal=J%20Rehabil%20Res%20Dev&amp;doi=10.1682%2FJRRD.2010.09.0177&amp;volume=48&amp;publication_year=2011&amp;author=Scheme%2CE&amp;author=Englehart%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Silva ECP, Clua EWG, Montenegro AA (2015) Sensor data fusion for full arm tracking using Myo Armband and leap " /><p class="c-article-references__text" id="ref-CR37">Silva ECP, Clua EWG, Montenegro AA (2015) Sensor data fusion for full arm tracking using Myo Armband and leap motion. In: 2015 14th Brazilian symposium on computer games digital entertainment (SBGames), pp 128–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LM. Sutherland, PF. Middleton, A. Anthony, J. Hamdorf, P. Cregan, D. Scott, GJ. Maddern, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Sutherland LM, Middleton PF, Anthony A, Hamdorf J, Cregan P, Scott D, Maddern GJ (2006) Surgical simulation: a" /><p class="c-article-references__text" id="ref-CR38">Sutherland LM, Middleton PF, Anthony A, Hamdorf J, Cregan P, Scott D, Maddern GJ (2006) Surgical simulation: a systematic review. Ann Surg 243:291–300</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F01.sla.0000200839.93965.26" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Surgical%20simulation%3A%20a%20systematic%20review&amp;journal=Ann%20Surg&amp;volume=243&amp;pages=291-300&amp;publication_year=2006&amp;author=Sutherland%2CLM&amp;author=Middleton%2CPF&amp;author=Anthony%2CA&amp;author=Hamdorf%2CJ&amp;author=Cregan%2CP&amp;author=Scott%2CD&amp;author=Maddern%2CGJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="V. Vapnik, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Vapnik V (1998) Statistical learning theory. Wiley, New York" /><p class="c-article-references__text" id="ref-CR39">Vapnik V (1998) Statistical learning theory. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20learning%20theory&amp;publication_year=1998&amp;author=Vapnik%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vargas HF, Vivas OA (2014) Gesture recognition system for surgical robot’s manipulation. In: 2014 XIX symposiu" /><p class="c-article-references__text" id="ref-CR40">Vargas HF, Vivas OA (2014) Gesture recognition system for surgical robot’s manipulation. In: 2014 XIX symposium on image, signal process and artificial vision (STSIVA), 2014, pp 1–5</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Weichert, D. Bachmann, B. Rudak, D. Fisseler, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Weichert F, Bachmann D, Rudak B, Fisseler D (2013a) Analysis of the accuracy and robustness of the leap motion" /><p class="c-article-references__text" id="ref-CR41">Weichert F, Bachmann D, Rudak B, Fisseler D (2013a) Analysis of the accuracy and robustness of the leap motion controller. Sens (Switz) 13:6380–6393. <a href="https://doi.org/10.3390/s130506380">https://doi.org/10.3390/s130506380</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs130506380" aria-label="View reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20the%20accuracy%20and%20robustness%20of%20the%20leap%20motion%20controller&amp;journal=Sens%20%28Switz%29&amp;doi=10.3390%2Fs130506380&amp;volume=13&amp;pages=6380-6393&amp;publication_year=2013&amp;author=Weichert%2CF&amp;author=Bachmann%2CD&amp;author=Rudak%2CB&amp;author=Fisseler%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Weichert, D. Bachmann, B. Rudak, D. Fisseler, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Weichert F, Bachmann D, Rudak B, Fisseler D (2013b) Analysis of the accuracy and robustness of the leap motion" /><p class="c-article-references__text" id="ref-CR42">Weichert F, Bachmann D, Rudak B, Fisseler D (2013b) Analysis of the accuracy and robustness of the leap motion controller. Sens (Switz) 13:6380–6393. <a href="https://doi.org/10.3390/s130506380">https://doi.org/10.3390/s130506380</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs130506380" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20the%20accuracy%20and%20robustness%20of%20the%20leap%20motion%20controller&amp;journal=Sens%20%28Switz%29&amp;doi=10.3390%2Fs130506380&amp;volume=13&amp;pages=6380-6393&amp;publication_year=2013&amp;author=Weichert%2CF&amp;author=Bachmann%2CD&amp;author=Rudak%2CB&amp;author=Fisseler%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Wininger, N-H. Kim, W. Craelius, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Wininger M, Kim N-H, Craelius W (2008) Pressure signature of forearm as predictor of grip force. J Rehabil Res" /><p class="c-article-references__text" id="ref-CR43">Wininger M, Kim N-H, Craelius W (2008) Pressure signature of forearm as predictor of grip force. J Rehabil Res Dev 45:883–892. <a href="https://doi.org/10.1682/JRRD.2007.11.0187">https://doi.org/10.1682/JRRD.2007.11.0187</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1682%2FJRRD.2007.11.0187" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pressure%20signature%20of%20forearm%20as%20predictor%20of%20grip%20force&amp;journal=J%20Rehabil%20Res%20Dev&amp;doi=10.1682%2FJRRD.2007.11.0187&amp;volume=45&amp;pages=883-892&amp;publication_year=2008&amp;author=Wininger%2CM&amp;author=Kim%2CN-H&amp;author=Craelius%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SI. Yaniger, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Yaniger SI (1991) Force sensing resistors: a review of the technology. Electro Int. https://doi.org/10.1109/el" /><p class="c-article-references__text" id="ref-CR44">Yaniger SI (1991) Force sensing resistors: a review of the technology. Electro Int. <a href="https://doi.org/10.1109/electr.1991.718294">https://doi.org/10.1109/electr.1991.718294</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2Felectr.1991.718294" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Force%20sensing%20resistors%3A%20a%20review%20of%20the%20technology&amp;journal=Electro%20Int&amp;doi=10.1109%2Felectr.1991.718294&amp;publication_year=1991&amp;author=Yaniger%2CSI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang H, Zhao Y, Yao F, Xu L, Shang P, Li G (2013) An adaptation strategy of using LDA classifier for EMG patt" /><p class="c-article-references__text" id="ref-CR45">Zhang H, Zhao Y, Yao F, Xu L, Shang P, Li G (2013) An adaptation strategy of using LDA classifier for EMG pattern recognition. In: 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC). <a href="https://doi.org/10.1109/embc.2013.6610488">https://doi.org/10.1109/embc.2013.6610488</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JM. Zurada, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Zurada JM (1992) Introduction to artificial neural systems. West, St Paul" /><p class="c-article-references__text" id="ref-CR46">Zurada JM (1992) Introduction to artificial neural systems. West, St Paul</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Introduction%20to%20artificial%20neural%20systems&amp;publication_year=1992&amp;author=Zurada%2CJM">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0339-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgement</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canadian Institutes of Health Research (CIHR), and the Canada Research Chair (CRC) program. The authors thank Mary Yu, Tingyu Hu, and Wenxuan Song for helping with the VR content development and data collection.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Menrva Research Group, Schools of Mechatronic Systems and Engineering Science, Simon Fraser University, Surrey, BC, V3T 0A3, Canada</p><p class="c-article-author-affiliation__authors-list">Xianta Jiang, Zhen Gang Xiao &amp; Carlo Menon</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Xianta-Jiang"><span class="c-article-authors-search__title u-h3 js-search-name">Xianta Jiang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Xianta+Jiang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xianta+Jiang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xianta+Jiang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Zhen_Gang-Xiao"><span class="c-article-authors-search__title u-h3 js-search-name">Zhen Gang Xiao</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Zhen Gang+Xiao&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zhen Gang+Xiao" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zhen Gang+Xiao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Carlo-Menon"><span class="c-article-authors-search__title u-h3 js-search-name">Carlo Menon</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Carlo+Menon&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Carlo+Menon" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Carlo+Menon%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0339-2/email/correspondent/c1/new">Carlo Menon</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Virtual%20grasps%20recognition%20using%20fusion%20of%20Leap%20Motion%20and%20force%20myography&amp;author=Xianta%20Jiang%20et%20al&amp;contentID=10.1007%2Fs10055-018-0339-2&amp;publication=1359-4338&amp;publicationDate=2018-03-01&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0339-2" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0339-2" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Jiang, X., Xiao, Z.G. &amp; Menon, C. Virtual grasps recognition using fusion of Leap Motion and force myography.
                    <i>Virtual Reality</i> <b>22, </b>297–308 (2018). https://doi.org/10.1007/s10055-018-0339-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0339-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-12-07">07 December 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-23">23 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03-01">01 March 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11">November 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0339-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0339-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">VR</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand gesture recognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Grasp classification</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Leap Motion</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Force myography</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0339-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=339;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

