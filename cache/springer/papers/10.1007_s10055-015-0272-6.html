<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Localization of self-generated synthetic footstep sounds on different "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/20/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones"/>

    <meta name="dc.source" content="Virtual Reality 2015 20:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-08-21"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving a pair of sandals enhanced with pressure sensors and a footstep synthesizer capable of simulating two typologies of surface materials: solid (e.g., wood) and aggregate (e.g., gravel). Different sound delivery methods (mono, stereo, binaural) as well as several surface materials, in the presence or absence of concurrent contextual auditory information provided as soundscapes, were evaluated in a vertical localization task. Results showed that solid surfaces were localized significantly farther from the walker&#8217;s feet than the aggregate ones. This effect was independent of the used rendering technique, of the presence of soundscapes, and of merely temporal or spectral attributes of sound.
 The effect is hypothesized to be due to a semantic conflict between auditory and haptic information such that the higher the semantic incongruence the greater the distance of the perceived sound source from the feet. The presented results contribute to the development of further knowledge toward a basis for the design of continuous multimodal feedback in virtual reality applications
."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-08-21"/>

    <meta name="prism.volume" content="20"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="16"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0272-6"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0272-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0272-6.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0272-6"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones"/>

    <meta name="citation_volume" content="20"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2016/03"/>

    <meta name="citation_online_date" content="2015/08/21"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="16"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0272-6"/>

    <meta name="DOI" content="10.1007/s10055-015-0272-6"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0272-6"/>

    <meta name="description" content="This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiment"/>

    <meta name="dc.creator" content="Luca Turchet"/>

    <meta name="dc.creator" content="Simone Spagnol"/>

    <meta name="dc.creator" content="Michele Geronazzo"/>

    <meta name="dc.creator" content="Federico Avanzini"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Approximating the head-related transfer function using simple geometric models of the head and torso; citation_author=VR Algazi, RO Duda, R Duraiswami, NA Gumerov, Z Tang; citation_volume=112; citation_issue=5; citation_publication_date=2002; citation_pages=2053-2064; citation_doi=10.1121/1.1508780; citation_id=CR1"/>

    <meta name="citation_reference" content="Algazi VR, Duda RO, Thompson DM (2002) The use of head-and-torso models for improved spatial sound synthesis. In: Proceedings of 113th convention audio engineering society, Los Angeles, pp 1&#8211;18"/>

    <meta name="citation_reference" content="Algazi VR, Duda RO, Thompson DM, Avendano C (2001) The CIPIC HRTF database. In: Proceedings of IEEE workshop on applications signal processing, audio and acoustic. New Paltz, New York, pp 1&#8211;4"/>

    <meta name="citation_reference" content="Avanzini F, Rocchesso D (2001) Modeling collision sounds: non-linear contact force. In: Proceedings of digital audio effects conference, pp 61&#8211;66"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Direct comparison of the impact of head tracking, reverberation, and individualized head-related transfer functions on the spatial perception of a virtual speech source; citation_author=DR Begault, EM Wenzel, MR Anderson; citation_volume=49; citation_issue=10; citation_publication_date=2001; citation_pages=904-916; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_title=Spatial hearing: the psychophysics of human sound localization; citation_publication_date=1983; citation_id=CR6; citation_author=J Blauert; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="Boren BB, Geronazzo M, Majdak P, Choueiri E (2014) PHOnA: a public dataset of measured headphone transfer functions. In: Proceedings of 137th audio engineering society convention"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Anthropometric manikin for acoustic research; citation_author=MD Burkhard, RM Sachs; citation_volume=58; citation_issue=1; citation_publication_date=1975; citation_pages=214-222; citation_doi=10.1121/1.380648; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Tactile&#8220;capture&#8221; of audition; citation_author=A Caclin, S Soto-Faraco, A Kingstone, C Spence; citation_volume=64; citation_issue=4; citation_publication_date=2002; citation_pages=616-630; citation_doi=10.3758/BF03194730; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Introduction to head-related transfer functions (HRTFs): representations of HRTFs in time, frequency, and space; citation_author=CI Cheng, GH Wakefield; citation_volume=49; citation_issue=4; citation_publication_date=2001; citation_pages=231-249; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Music J; citation_title=Physically informed sonic modeling (phism): synthesis of percussive sounds; citation_author=P Cook; citation_volume=21; citation_issue=3; citation_publication_date=1997; citation_pages=38-49; citation_doi=10.2307/3681012; citation_id=CR11"/>

    <meta name="citation_reference" content="Geronazzo M, Spagnol S, Avanzini F (2013) Mixed structural modeling of head-related transfer functions for customized binaural audio delivery. In Proceedings of 18th international conference on digital signal processing (DSP 2013). Santorini, Greece"/>

    <meta name="citation_reference" content="Geronazzo M, Spagnol S, Avanzini F (2013) A modular framework for the analysis and synthesis of head-related transfer functions. In: Proceedings of 134th audio engineering society convention, Rome, Italy"/>

    <meta name="citation_reference" content="Geronazzo M, Spagnol S, Bedin A, Avanzini F (2014) Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions. In: Proceedings of IEEE international conference on acoustics, speech, and signal processing (ICASSP 2014), Firenze, Italy, pp 4496&#8211;4500"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Identification of walked-upon materials in auditory, kinesthetic, haptic and audio-haptic conditions; citation_author=B Giordano, Y Visell, HY Yao, V Hayward, J Cooperstock, S McAdams; citation_volume=131; citation_publication_date=2012; citation_pages=4002-4012; citation_doi=10.1121/1.3699205; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Spectral cues used in the localization of sound sources on the median plane; citation_author=J Hebrank, D Wright; citation_volume=56; citation_issue=6; citation_publication_date=1974; citation_pages=1829-1834; citation_doi=10.1121/1.1903520; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_title=Human spatial orientation; citation_publication_date=1966; citation_id=CR17; citation_author=IP Howard; citation_author=WB Templeton; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=ASME J Appl Mech; citation_title=Coefficient of restitution interpreted as damping in vibroimpact; citation_author=KH Hunt, FRE Crossley; citation_volume=42; citation_issue=2; citation_publication_date=1975; citation_pages=440-445; citation_doi=10.1115/1.3423596; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Acust United Acust; citation_title=Modeling and customization of head-related impulse responses based on general basis functions in time domain; citation_author=S Hwang, Y Park, Y Park; citation_volume=94; citation_issue=6; citation_publication_date=2008; citation_pages=965-980; citation_doi=10.3813/AAA.918113; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Curr Biol; citation_title=Parchment-skin illusion: sound-biased touch; citation_author=V Jousm&#228;ki, R Hari; citation_volume=8; citation_issue=6; citation_publication_date=1998; citation_pages=R190-R191; citation_doi=10.1016/S0960-9822(98)70120-4; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Jpn Psychol Res; citation_title=Audiotactile multisensory interactions in human information processing; citation_author=N Kitagawa, C Spence; citation_volume=48; citation_issue=3; citation_publication_date=2006; citation_pages=158-173; citation_doi=10.1111/j.1468-5884.2006.00317.x; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Rehab Syst Eng; citation_title=How accurately people can discriminate the differences of floor materials with various elasticities; citation_author=Y Kobayashi, R Osaka, T Hara, H Fujimoto; citation_volume=16; citation_issue=1; citation_publication_date=2008; citation_pages=99-105; citation_doi=10.1109/TNSRE.2007.910283; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res; citation_title=Semantic congruence is a critical factor in multisensory behavioral performance; citation_author=P Laurienti, R Kraft, J Maldjian, J Burdette, M Wallace; citation_volume=158; citation_issue=4; citation_publication_date=2004; citation_pages=405-414; citation_doi=10.1007/s00221-004-1913-2; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Individual differences in external-ear transfer functions reduced by scaling in frequency; citation_author=JC Middlebrooks; citation_volume=106; citation_issue=3; citation_publication_date=1999; citation_pages=1480-1492; citation_doi=10.1121/1.427176; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Binaural technique: Do we need individual recordings?; citation_author=H M&#248;ller, MF S&#248;rensen, CB Jensen, D Hammersh&#248;i; citation_volume=44; citation_issue=6; citation_publication_date=1996; citation_pages=451-469; citation_id=CR25"/>

    <meta name="citation_reference" content="Nordahl R, Berrezag A, Dimitrov S, Turchet L, Hayward V, Serafin S (2010) Preliminary experiment combining virtual reality haptic shoes and audio synthesis. In: Haptics: generating and perceiving tangible sensations, lecture notes in computer science, Springer, Berlin, vol 6192, pp 123&#8211;129"/>

    <meta name="citation_reference" content="Nordahl R, Serafin S, Turchet L (2010) Sound synthesis and evaluation of interactive footsteps for virtual reality applications. In: Proceedings of the IEEE virtual reality conference. IEEE Press, pp 147&#8211;153"/>

    <meta name="citation_reference" content="Papetti S, Civolani M, Fontana F (2011) Rhythm&#8217;n&#8217;shoes: a wearable foot tapping interface with audio-tactile feedback. In: Proceedings of the international conference on new interfaces for musical expression, pp 473&#8211;476"/>

    <meta name="citation_reference" content="Papetti S, Fontana F, Civolani M, Berrezag A, Hayward V (2010) Audio-tactile display of ground properties using interactive shoes. In: Haptic and audio interaction design, Lecture notes in computer science, Springer, Berlin, vol 6306, pp 117&#8211;128"/>

    <meta name="citation_reference" content="Serafin S, Turchet L, Nordahl R, Dimitrov S, Berrezag A, Hayward V (2010) Identification of virtual grounds using virtual reality haptic shoes and sound synthesis. In: Proceedings of eurohaptics symposium on haptic and audio-visual stimuli: enhancing experiences and interaction, pp 61&#8211;70"/>

    <meta name="citation_reference" content="citation_journal_title=Anuario de Psicologia; citation_title=How we experience immersive virtual environments: the concept of presence and its measurement; citation_author=M Slater, B Lotto, MM Arnold, MV Sanchez-Vives; citation_volume=40; citation_issue=2; citation_publication_date=2009; citation_pages=193-210; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Process; citation_title=On the relation between pinna reflection patterns and head-related transfer function features; citation_author=S Spagnol, M Geronazzo, F Avanzini; citation_volume=21; citation_issue=3; citation_publication_date=2013; citation_pages=508-519; citation_doi=10.1109/TASL.2012.2227730; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Pervasive Comput Commun; citation_title=Synthetic individual binaural audio delivery by pinna image processing; citation_author=S Spagnol, M Geronazzo, D Rocchesso, F Avanzini; citation_volume=10; citation_issue=3; citation_publication_date=2014; citation_pages=239-254; citation_doi=10.1108/IJPCC-06-2014-0035; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_title=The merging of the senses; citation_publication_date=1993; citation_id=CR34; citation_author=B Stein; citation_author=M Meredith; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_title=Human walking in virtual environments: perception, technology, and applications; citation_publication_date=2013; citation_id=CR35; citation_author=F Steinicke; citation_author=Y Visell; citation_author=J Campos; citation_author=A L&#233;cuyer; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Philos Trans R Soc Lond; citation_title=On the acoustic shadow of a sphere; citation_author=JW Strutt; citation_volume=203; citation_publication_date=1904; citation_pages=87-110; citation_doi=10.1098/rsta.1904.0016; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis; citation_title=I can see you better if i can hear you coming: action-consistent sounds facilitate the visual detection of human gait; citation_author=JP Thomas, M Shiffrar; citation_volume=10; citation_issue=12; citation_publication_date=2010; citation_pages=14; citation_doi=10.1167/10.12.14; citation_id=CR37"/>

    <meta name="citation_reference" content="Turchet L (2015) Designing presence for real locomotion in immersive virtual environments: an affordance-based experiential approach. Virtual Real (accepted)"/>

    <meta name="citation_reference" content="Turchet L (2015) Footstep sounds synthesis: design, implementation, and evaluation of foot-floor interactions, surface materials, shoe types, and walkers&#8217; features. Appl Acoust (in press)"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res; citation_title=Interactive footstep sounds modulate the perceptual-motor aftereffect of treadmill walking; citation_author=L Turchet, I Camponogara, P Cesari; citation_volume=233; citation_publication_date=2015; citation_pages=205-214; citation_doi=10.1007/s00221-014-4104-9; citation_id=CR40"/>

    <meta name="citation_reference" content="Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simulation of walking on different grounds. In: Proceedings of IEEE international workshop on multimedia signal processing, IEEE Press, pp 269&#8211;273"/>

    <meta name="citation_reference" content="Turchet L, Serafin S (2011) A preliminary study on sound delivery methods for footstep sounds. In: Proceedings of digital audio effects conference, pp 53&#8211;58"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Acoust; citation_title=Investigating the amplitude of interactive footstep sounds and soundscape reproduction; citation_author=L Turchet, S Serafin; citation_volume=74; citation_issue=4; citation_publication_date=2013; citation_pages=566-574; citation_doi=10.1016/j.apacoust.2012.10.010; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Acoust; citation_title=Semantic congruence in audio-haptic simulation of footsteps; citation_author=L Turchet, S Serafin; citation_volume=75; citation_issue=1; citation_publication_date=2014; citation_pages=59-66; citation_doi=10.1016/j.apacoust.2013.06.016; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=Walking pace affected by interactive sounds simulating stepping on different terrains; citation_author=L Turchet, S Serafin, P Cesari; citation_volume=10; citation_issue=4; citation_publication_date=2013; citation_pages=23:1-23:14; citation_doi=10.1145/2536764.2536770; citation_id=CR45"/>

    <meta name="citation_reference" content="Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In: Proceedings of sound and music computing conference"/>

    <meta name="citation_reference" content="citation_journal_title=Lect Notes Comput Sci; citation_title=A vibrotactile device for display of virtual ground materials in walking; citation_author=Y Visell, J Cooperstock, B Giordano, K Franinovic, A Law, S McAdams, K Jathal, F Fontana; citation_volume=5024; citation_publication_date=2008; citation_pages=420-426; citation_doi=10.1007/978-3-540-69057-3_55; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Sound design and perception in walking interactions; citation_author=Y Visell, F Fontana, B Giordano, R Nordahl, S Serafin, R Bresin; citation_volume=67; citation_issue=11; citation_publication_date=2009; citation_pages=947-959; citation_doi=10.1016/j.ijhcs.2009.07.007; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=The influence of duration and level on human sound localization; citation_author=J Vliegen, AJ Opstal; citation_volume=115; citation_issue=4; citation_publication_date=2004; citation_pages=1705-1713; citation_doi=10.1121/1.1687423; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Localization using nonindividualized head-related transfer functions; citation_author=EM Wenzel, M Arruda, DJ Kistler, FL Wightman; citation_volume=94; citation_issue=1; citation_publication_date=1993; citation_pages=111-123; citation_doi=10.1121/1.407089; citation_id=CR50"/>

    <meta name="citation_reference" content="Zanotto D, Turchet L, Boggs E, Agrawal S (2014) Solesound: Towards a novel portable system for audio-tactile underfoot feedback. In: Proceedings of the 5th IEEE international conference on biomedical robotics and biomechatronics, pp 193&#8211;198"/>

    <meta name="citation_author" content="Luca Turchet"/>

    <meta name="citation_author_email" content="tur@create.aau.dk"/>

    <meta name="citation_author_institution" content="Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, Copenhagen, Denmark"/>

    <meta name="citation_author" content="Simone Spagnol"/>

    <meta name="citation_author_email" content="spagnols@hi.is"/>

    <meta name="citation_author_institution" content="Faculty of Industrial Engineering, Mechanical Engineering and Computer Science, School of Engineering and Natural Sciences, University of Iceland, Reykjav&#237;k, Iceland"/>

    <meta name="citation_author" content="Michele Geronazzo"/>

    <meta name="citation_author_email" content="geronazzo@dei.unipd.it"/>

    <meta name="citation_author_institution" content="Department of Information Engineering, University of Padova, Padua, Italy"/>

    <meta name="citation_author" content="Federico Avanzini"/>

    <meta name="citation_author_email" content="avanzini@dei.unipd.it"/>

    <meta name="citation_author_institution" content="Department of Information Engineering, University of Padova, Padua, Italy"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0272-6&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2016/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0272-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones"/>
        <meta property="og:description" content="This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving a pair of sandals enhanced with pressure sensors and a footstep synthesizer capable of simulating two typologies of surface materials: solid (e.g., wood) and aggregate (e.g., gravel). Different sound delivery methods (mono, stereo, binaural) as well as several surface materials, in the presence or absence of concurrent contextual auditory information provided as soundscapes, were evaluated in a vertical localization task. Results showed that solid surfaces were localized significantly farther from the walker’s feet than the aggregate ones. This effect was independent of the used rendering technique, of the presence of soundscapes, and of merely temporal or spectral attributes of sound. The effect is hypothesized to be due to a semantic conflict between auditory and haptic information such that the higher the semantic incongruence the greater the distance of the perceived sound source from the feet. The presented results contribute to the development of further knowledge toward a basis for the design of continuous multimodal feedback in virtual reality applications ."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0272-6","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Walking, Interactive auditory feedback, Localization","kwrd":["Walking","Interactive_auditory_feedback","Localization"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0272-6","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0272-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=272;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0272-6">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0272-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0272-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-08-21" itemprop="datePublished">21 August 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Luca-Turchet" data-author-popup="auth-Luca-Turchet" data-corresp-id="c1">Luca Turchet<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalborg University Copenhagen" /><meta itemprop="address" content="grid.5117.2, 000000010742471X, Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, A.C. Meyers Vænge 15, 2450, Copenhagen, Denmark" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Simone-Spagnol" data-author-popup="auth-Simone-Spagnol">Simone Spagnol</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Iceland" /><meta itemprop="address" content="grid.14013.37, 0000000406400021, Faculty of Industrial Engineering, Mechanical Engineering and Computer Science, School of Engineering and Natural Sciences, University of Iceland, Tæknigarður Dunhagi 5, 107, Reykjavík, Iceland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michele-Geronazzo" data-author-popup="auth-Michele-Geronazzo">Michele Geronazzo</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Padova" /><meta itemprop="address" content="grid.5608.b, 0000000417573470, Department of Information Engineering, University of Padova, Via Gradenigo 6/A, 35131, Padua, Italy" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Federico-Avanzini" data-author-popup="auth-Federico-Avanzini">Federico Avanzini</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Padova" /><meta itemprop="address" content="grid.5608.b, 0000000417573470, Department of Information Engineering, University of Padova, Via Gradenigo 6/A, 35131, Padua, Italy" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">1</span>–<span itemprop="pageEnd">16</span>(<span data-test="article-publication-year">2016</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">546 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0272-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving a pair of sandals enhanced with pressure sensors and a footstep synthesizer capable of simulating two typologies of surface materials: solid (e.g., wood) and aggregate (e.g., gravel). Different sound delivery methods (mono, stereo, binaural) as well as several surface materials, in the presence or absence of concurrent contextual auditory information provided as soundscapes, were evaluated in a vertical localization task. Results showed that solid surfaces were localized significantly farther from the walker’s feet than the aggregate ones. This effect was independent of the used rendering technique, of the presence of soundscapes, and of merely temporal or spectral attributes of sound.
 The effect is hypothesized to be due to a semantic conflict between auditory and haptic information such that the higher the semantic incongruence the greater the distance of the perceived sound source from the feet. The presented results contribute to the development of further knowledge toward a basis for the design of continuous multimodal feedback in virtual reality applications
.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Recent research in the field of multimodal virtual environments has focused on the simulation of foot–floor interactions (Steinicke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Steinicke F, Visell Y, Campos J, Lécuyer A (2013) Human walking in virtual environments: perception, technology, and applications. Springer, Berlin" href="/article/10.1007/s10055-015-0272-6#ref-CR35" id="ref-link-section-d50499e391">2013</a>; Visell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Visell Y, Fontana F, Giordano B, Nordahl R, Serafin S, Bresin R (2009) Sound design and perception in walking interactions. Int J Hum Comput Stud 67(11):947–959" href="/article/10.1007/s10055-015-0272-6#ref-CR48" id="ref-link-section-d50499e394">2009</a>) by addressing the problem of enhancing their realism at auditory and haptic levels in order to achieve higher level of presence (Slater et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Slater M, Lotto B, Arnold MM, Sanchez-Vives MV (2009) How we experience immersive virtual environments: the concept of presence and its measurement. Anuario de Psicologia 40(2):193–210" href="/article/10.1007/s10055-015-0272-6#ref-CR31" id="ref-link-section-d50499e397">2009</a>; Turchet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Turchet L (2015) Designing presence for real locomotion in immersive virtual environments: an affordance-based experiential approach. Virtual Real (accepted)" href="/article/10.1007/s10055-015-0272-6#ref-CR38" id="ref-link-section-d50499e400">2015</a>). As a matter of fact, the human brain relies on inputs from different senses to form a coherent percept of the environment. These pieces of information usually complement and confirm each other, thereby enhancing reliability of percepts (Stein and Meredith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Stein B, Meredith M (1993) The merging of the senses. MIT Press, Cambridge" href="/article/10.1007/s10055-015-0272-6#ref-CR34" id="ref-link-section-d50499e403">1993</a>).</p><p>In particular, several results have indicated that the typology of the surface onto which we walk is processed very consistently in both the auditory and haptic modalities. The excellent somatosensory capacities of the human feet have been demonstrated to be capable of discriminating with high accuracy different types of surfaces (Kobayashi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kobayashi Y, Osaka R, Hara T, Fujimoto H (2008) How accurately people can discriminate the differences of floor materials with various elasticities. IEEE Trans Neural Rehab Syst Eng 16(1):99–105" href="/article/10.1007/s10055-015-0272-6#ref-CR22" id="ref-link-section-d50499e409">2008</a>; Giordano et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Giordano B, Visell Y, Yao HY, Hayward V, Cooperstock J, McAdams S (2012) Identification of walked-upon materials in auditory, kinesthetic, haptic and audio-haptic conditions. J Acoust Soc Am 131:4002–4012" href="/article/10.1007/s10055-015-0272-6#ref-CR15" id="ref-link-section-d50499e412">2012</a>). Similarly, studies on the ability to identify ground materials simulated either with auditory or with haptic information (Serafin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Serafin S, Turchet L, Nordahl R, Dimitrov S, Berrezag A, Hayward V (2010) Identification of virtual grounds using virtual reality haptic shoes and sound synthesis. In: Proceedings of eurohaptics symposium on haptic and audio-visual stimuli: enhancing experiences and interaction, pp 61–70" href="/article/10.1007/s10055-015-0272-6#ref-CR30" id="ref-link-section-d50499e415">2010</a>; Nordahl et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Nordahl R, Berrezag A, Dimitrov S, Turchet L, Hayward V, Serafin S (2010) Preliminary experiment combining virtual reality haptic shoes and audio synthesis. In: Haptics: generating and perceiving tangible sensations, lecture notes in computer science, Springer, Berlin, vol 6192, pp 123–129" href="/article/10.1007/s10055-015-0272-6#ref-CR26" id="ref-link-section-d50499e418">2010</a>) revealed that material typology is consistently recognized by using both modalities.</p><p>Turchet proposed a footstep sound synthesis engine, based on physical models, which allows the simulation of two typologies of ground materials: solid (i.e., homogeneous floors like wood or metal) and aggregate (i.e., grounds possessing a granular structure like gravel or snow) (Turchet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Turchet L (2015) Footstep sounds synthesis: design, implementation, and evaluation of foot-floor interactions, surface materials, shoe types, and walkers’ features. Appl Acoust (in press)" href="/article/10.1007/s10055-015-0272-6#ref-CR39" id="ref-link-section-d50499e424">2015</a>). The ecological validity of such simulations was assessed with experiments in which subjects were asked to recognize the synthesized materials (Nordahl et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Nordahl R, Serafin S, Turchet L (2010) Sound synthesis and evaluation of interactive footsteps for virtual reality applications. In: Proceedings of the IEEE virtual reality conference. IEEE Press, pp 147–153" href="/article/10.1007/s10055-015-0272-6#ref-CR27" id="ref-link-section-d50499e427">2010</a>). Results showed that subjects were able to recognize the synthesized surfaces with an accuracy comparable to that of real recorded footstep sounds, which was an indication of the success of the proposed algorithms and their control.</p><p>A complicating factor is that various sound reproduction methods can be used to deliver the synthesized sounds to the walker: loudspeakers directly placed on top of the shoes (Papetti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Papetti S, Fontana F, Civolani M, Berrezag A, Hayward V (2010) Audio-tactile display of ground properties using interactive shoes. In: Haptic and audio interaction design, Lecture notes in computer science, Springer, Berlin, vol 6306, pp 117–128" href="/article/10.1007/s10055-015-0272-6#ref-CR29" id="ref-link-section-d50499e433">2010</a>; Zanotto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zanotto D, Turchet L, Boggs E, Agrawal S (2014) Solesound: Towards a novel portable system for audio-tactile underfoot feedback. In: Proceedings of the 5th IEEE international conference on biomedical robotics and biomechatronics, pp 193–198" href="/article/10.1007/s10055-015-0272-6#ref-CR51" id="ref-link-section-d50499e436">2014</a>), on their soles (Papetti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Papetti S, Civolani M, Fontana F (2011) Rhythm’n’shoes: a wearable foot tapping interface with audio-tactile feedback. In: Proceedings of the international conference on new interfaces for musical expression, pp 473–476" href="/article/10.1007/s10055-015-0272-6#ref-CR28" id="ref-link-section-d50499e439">2011</a>), or embedded in the walking surface (Visell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Visell Y, Cooperstock J, Giordano B, Franinovic K, Law A, McAdams S, Jathal K, Fontana F (2008) A vibrotactile device for display of virtual ground materials in walking. Lect Notes Comput Sci 5024:420–426" href="/article/10.1007/s10055-015-0272-6#ref-CR47" id="ref-link-section-d50499e442">2008</a>). Also, the interactive delivery of footstep sounds can be achieved by means of a surround sound systems composed of loudspeakers (Turchet and Serafin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Turchet L, Serafin S (2011) A preliminary study on sound delivery methods for footstep sounds. In: Proceedings of digital audio effects conference, pp 53–58" href="/article/10.1007/s10055-015-0272-6#ref-CR42" id="ref-link-section-d50499e445">2011</a>), while no extensive research has been conducted into headphone-based reproduction of interactive locomotion sounds.</p><p>Even more importantly, to our knowledge no previous research has systematically addressed the issue of footstep sound localization in VR contexts. The main goal of this work is thus to investigate the role of auditory information in modulating the localization of self-generated footstep sounds and to test whether differences in perceived localization of footstep sounds affect the realism and naturalness of the walking experience as well as the sense of disorientation associated with different layers of auditory information. To this end, we consider different techniques for footstep sounds rendering by means of headphones, which despite presenting possible disadvantages (e.g., invasiveness), possess a number of desirable features. In particular they eliminate reverberation and other acoustic effects of the real listening space, reduce background noise, and provide adaptable audio displays. More importantly, they allow the delivery of stimuli with different degrees of spatiality, e.g., mono (=0 dimensions), stereo (=1 dimension), and binaural (=2/3 dimensions) reproduction by means of head-related transfer functions (HRTFs) (Cheng and Wakefield <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Cheng CI, Wakefield GH (2001) Introduction to head-related transfer functions (HRTFs): representations of HRTFs in time, frequency, and space. J Audio Eng Soc 49(4):231–249" href="/article/10.1007/s10055-015-0272-6#ref-CR10" id="ref-link-section-d50499e452">2001</a>). Furthermore, we assess the relative importance of auditory spatial cues with respect to semantic information such as walking surface and context as well as to signal-level features.</p><p>The remainder of the paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec2">2</a> reports the design and results of experiment 1, whose main goal is to investigate whether different sound rendering techniques have an influence on the localization of solid and aggregate footstep sounds. The role of contextual information (soundscapes) is instead explored in experiment 2, described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec8">3</a>. In the final experiment, reported in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec12">4</a>, we consider a larger sonic palette to test whether signal-level features affect the results found in the previous two experiments. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec16">5</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec17">6</a> conclude the paper with a general discussion on the global results of the three experiments and the implications they provide to the design of walking VR experiences.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Experiment 1</h2><div class="c-article-section__content" id="Sec2-content"><p>This first experiment was designed so as to explore whether different audio-rendering techniques over headphones (mono, stereo, binaural) affect localization judgments of synthetic self-generated footstep sounds on four different surface materials simulating two different surface typologies, i.e., aggregate and solid. Such a distinction is motivated by a previous preliminary study (Turchet and Serafin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Turchet L, Serafin S (2011) A preliminary study on sound delivery methods for footstep sounds. In: Proceedings of digital audio effects conference, pp 53–58" href="/article/10.1007/s10055-015-0272-6#ref-CR42" id="ref-link-section-d50499e481">2011</a>) that highlighted significant differences (in terms of localization, realism, naturalness of the interaction, and sense of disorientation) between the perception of dynamically generated footstep sounds on aggregate and solid surfaces provided via loudspeakers.</p><p>The basic idea of the binaural technique is that by recording real-life sounds inside a person’s ears, the appropriately post-processed sound file played back through headphones will be perceived by that person almost as realistic as the original one. In order to find the correct sound pressure that an arbitrary source produces at the eardrum, we need the impulse response from the source to the eardrum, called head-related impulse response (HRIR), whose Fourier transform is known as head-related transfer function (HRTF) (Cheng and Wakefield <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Cheng CI, Wakefield GH (2001) Introduction to head-related transfer functions (HRTFs): representations of HRTFs in time, frequency, and space. J Audio Eng Soc 49(4):231–249" href="/article/10.1007/s10055-015-0272-6#ref-CR10" id="ref-link-section-d50499e487">2001</a>). The HRTF captures all the acoustic cues used for source localization; once the HRTFs for the left and the right ear are known, accurate binaural signals can be generated starting from a monaural sound source.</p><p>Our starting hypothesis is that if the footstep sound has sufficient duration and high-frequency content (Vliegen and Opstal <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Vliegen J, Van Opstal AJ (2004) The influence of duration and level on human sound localization. J Acoust Soc Am 115(4):1705–1713" href="/article/10.1007/s10055-015-0272-6#ref-CR49" id="ref-link-section-d50499e493">2004</a>; Hebrank and Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Hebrank J, Wright D (1974) Spectral cues used in the localization of sound sources on the median plane. J Acoust Soc Am 56(6):1829–1834" href="/article/10.1007/s10055-015-0272-6#ref-CR16" id="ref-link-section-d50499e496">1974</a>) in order to enable vertical localization mechanisms, which is the case for aggregate surface sounds as opposed to solid surface sounds, then different rendering techniques should result in different localization ratings. In particular, binaural techniques should allow the walker to perceive synthesized aggregate footstep sounds as coming from below, despite the known difficulty in localizing virtual sources near the median plane, with an accuracy that shall depend on the degree of customization of the used HRTFs (Møller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Møller H, Sørensen MF, Jensen CB, Hammershøi D (1996) Binaural technique: Do we need individual recordings? J Audio Eng Soc 44(6):451–469" href="/article/10.1007/s10055-015-0272-6#ref-CR25" id="ref-link-section-d50499e499">1996</a>; Wenzel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Wenzel EM, Arruda M, Kistler DJ, Wightman FL (1993) Localization using nonindividualized head-related transfer functions. J Acoust Soc Am 94(1):111–123" href="/article/10.1007/s10055-015-0272-6#ref-CR50" id="ref-link-section-d50499e502">1993</a>). Different localization ratings should in turn modulate the perception of the realism, naturalness, and sense of disorientation of the walking experience.</p><h3 class="c-article__sub-heading" id="Sec3">Participants</h3><p>Twelve participants, seven males and five females, aged between 19 and <span class="mathjax-tex">\(31\,({M} = 22.41, {\text {SD}}= 4.23)\)</span>, took part in the experiment. All participants reported normal hearing and no impairment in locomotion.</p><h3 class="c-article__sub-heading" id="Sec4">Apparatus</h3><p>The experiment was carried out in a quiet room where the setup was installed, and the walking area was <span class="mathjax-tex">\(3.2 \times 2.9\,{\text {m}}\)</span> wide (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig1">1</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig2">2</a>). It consisted of a MacBook Pro laptop, running a sound synthesis engine (Turchet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Turchet L (2015) Footstep sounds synthesis: design, implementation, and evaluation of foot-floor interactions, surface materials, shoe types, and walkers’ features. Appl Acoust (in press)" href="/article/10.1007/s10055-015-0272-6#ref-CR39" id="ref-link-section-d50499e602">2015</a>); a pair of soft sole sandals enhanced with pressure sensors (placed in correspondence with the heel); an Arduino UNO board, managing the sensors’ data acquisition; a Fireface 800 soundcard; a pair of Sennheiser HDA 200 headphones. These headphones were mainly chosen because of their closed form facilitating isolation from external noise and the flatness of their frequency response (Boren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Boren BB, Geronazzo M, Majdak P, Choueiri E (2014) PHOnA: a public dataset of measured headphone transfer functions. In: Proceedings of 137th audio engineering society convention" href="/article/10.1007/s10055-015-0272-6#ref-CR7" id="ref-link-section-d50499e605">2014</a>).</p><p>Footstep sound synthesis was interactively driven during locomotion of the subject wearing the shoes. The description of the control algorithms based on the analysis of the values of the pressure sensors, implemented in Max/MSP, can be found in (Turchet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simulation of walking on different grounds. In: Proceedings of IEEE international workshop on multimedia signal processing, IEEE Press, pp 269–273" href="/article/10.1007/s10055-015-0272-6#ref-CR41" id="ref-link-section-d50499e611">2010</a>). The generated audio stream was then sent in real time to a Pure Data patch responsible for the different audio-rendering techniques.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A subject performing the experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Block diagram of the interactive system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec5">Stimuli</h3><p>The used hardware allowed real-time control of the sound synthesis engine, which was set so as to synthesize footstep sounds on four surface materials: two solid (wood and metal) and two aggregate (snow and gravel).<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>
                        </p><p>Solid materials were simulated using an impact model (Avanzini and Rocchesso <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Avanzini F, Rocchesso D (2001) Modeling collision sounds: non-linear contact force. In: Proceedings of digital audio effects conference, pp 61–66" href="/article/10.1007/s10055-015-0272-6#ref-CR4" id="ref-link-section-d50499e681">2001</a>). In the simulation of impact with solids, the contact was modeled by a Hunt–Crossley-type interaction where the force <i>f</i> between two bodies combines hardening elasticity and a dissipation term (Hunt and Crossley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Hunt KH, Crossley FRE (1975) Coefficient of restitution interpreted as damping in vibroimpact. ASME J Appl Mech 42(2):440–445" href="/article/10.1007/s10055-015-0272-6#ref-CR18" id="ref-link-section-d50499e687">1975</a>):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$f(x, \dot{x}) = - k x^\alpha - \lambda x^\alpha \dot{x}\quad {\text {if}}\,\,\, x &gt;0,\quad 0\,\,\, {\text {otherwise}}.$$</span></div></div><p>where <i>x</i> represents contact interpenetration (when <span class="mathjax-tex">\(x &gt; 0\)</span> the two objects are in contact), <span class="mathjax-tex">\(\dot{x}\)</span> is compression velocity, <i>k</i> accounts for material stiffness, <span class="mathjax-tex">\(\lambda \)</span> represents the force dissipation due to internal friction during the impact, and <span class="mathjax-tex">\(\alpha \)</span> is a coefficient which depends on the local geometry around the contact surface. The described model was discretized as proposed by Avanzini and Rocchesso (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Avanzini F, Rocchesso D (2001) Modeling collision sounds: non-linear contact force. In: Proceedings of digital audio effects conference, pp 61–66" href="/article/10.1007/s10055-015-0272-6#ref-CR4" id="ref-link-section-d50499e899">2001</a>).</p><p>To simulate aggregate surfaces, the physically informed sonic models algorithm was adopted (Cook <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cook P (1997) Physically informed sonic modeling (phism): synthesis of percussive sounds. Comput Music J 21(3):38–49" href="/article/10.1007/s10055-015-0272-6#ref-CR11" id="ref-link-section-d50499e905">1997</a>). This algorithm simulates particle interactions by using a stochastic parameterization, thereby avoiding modeling each of the many particles explicitly. Instead, particles are assigned a probability to create an acoustic waveform. In the case of many particles, the interaction can be represented using a simple Poisson distribution, where the sound probability is constant at each time step. This gives rise to an exponential probability weighing time between events. The four signals had different features in terms of duration, amplitude, temporal evolution, and spectrum (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Typical waveforms and spectrograms of the four simulated materials: <b>a</b> metal, <b>b</b> gravel, <b>c</b> wood, <b>d</b> snow</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Since both males and females were involved in the experiment, footstep sounds were synthesized in order to avoid any specific cue about the gender of the walker, i.e., trying to simulate a sound which could generally be accepted as genderless. This was achieved by modeling the contribution of a type of shoe which fitted for both males and females, as ascertained in a previous gender recognition experiment (Turchet and Serafin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduction. Appl Acoust 74(4):566–574" href="/article/10.1007/s10055-015-0272-6#ref-CR43" id="ref-link-section-d50499e945">2013</a>).</p><p>Three different sound reproduction techniques were considered: monophonic (mono, M), stereophonic (stereo panning, S), and binaural reproduction (B). In the diotically presented mono condition, the peak level of the sounds was set to 55.4, 57.8, 54.2, and 61.5 dB(A) for snow, gravel, wood, and metal, respectively;<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> these sound levels were taken as reference for the other reproduction conditions (S and B).</p><p>The stereo signals were obtained by adding half the mean interaural level difference (ILD) of a KEMAR mannequin (Burkhard and Sachs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Burkhard MD, Sachs RM (1975) Anthropometric manikin for acoustic research. J Acoust Soc Am 58(1):214–222" href="/article/10.1007/s10055-015-0272-6#ref-CR8" id="ref-link-section-d50499e964">1975</a>) at ±5° azimuth to the ipsilateral channel and subtracting the same half-ILD from the contralateral channel.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> The 5° value qualitatively corresponds to the displacement of each foot from the median vertical plane, allowing differentiation of left foot from right foot.</p><p>Regarding binaural reproduction, a mixed structural modeling (MSM) approach (Geronazzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Geronazzo M, Spagnol S, Avanzini F (2013) Mixed structural modeling of head-related transfer functions for customized binaural audio delivery. In Proceedings of 18th international conference on digital signal processing (DSP 2013). Santorini, Greece" href="/article/10.1007/s10055-015-0272-6#ref-CR12" id="ref-link-section-d50499e979">2013</a>) to the construction of HRTFs was used. This approach was preferred over individual HRTF measurement because it simulates a typical application scenario where it is not feasible to individually collect HRTFs (a procedure which strictly requires specific hardware, anechoic spaces, and long collection times) and because of the inherent difficulty in measuring and interpreting HRTF data for low-elevation sources such as our own footsteps. By the MSM approach, we approximate the influence of the listener’s body on the incoming sounds through a pair of non-individual HRTFs [either generic or selected from the CIPIC database (Geronazzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Geronazzo M, Spagnol S, Bedin A, Avanzini F (2014) Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions. In: Proceedings of IEEE international conference on acoustics, speech, and signal processing (ICASSP 2014), Firenze, Italy, pp 4496–4500" href="/article/10.1007/s10055-015-0272-6#ref-CR14" id="ref-link-section-d50499e982">2014</a>)] and the optional addition of a spherical torso approximation accounting for shadowing effects on sources coming from below (Algazi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Algazi VR, Duda RO, Thompson DM (2002) The use of head-and-torso models for improved spatial sound synthesis. In: Proceedings of 113th convention audio engineering society, Los Angeles, pp 1–18" href="/article/10.1007/s10055-015-0272-6#ref-CR2" id="ref-link-section-d50499e985">2002</a>). The combination of such choices gave rise to four more reproduction conditions:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>nonparametric binaural reproduction (B–NP): HRTFs of a KEMAR mannequin;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>parametric binaural reproduction (B–P): HRTF selection of the best CIPIC subject according to an anthropometry-based distance metric (details follow);</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>nonparametric binaural reproduction with torso (B-NPT): B-NP plus a spherical torso approximation;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>parametric binaural reproduction with torso (B–PT): B–P plus a spherical torso approximation.</p>
                      
                    </li>
                  </ol><p>The drawback with non-individual HRTFs such as the KEMAR’s is that such peculiar transfer functions will probably never match with the listener’s unique anthropometry, and especially his/her outer ear (Spagnol et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Spagnol S, Geronazzo M, Avanzini F (2013) On the relation between pinna reflection patterns and head-related transfer function features. IEEE Trans Audio Speech Lang Process 21(3):508–519" href="/article/10.1007/s10055-015-0272-6#ref-CR32" id="ref-link-section-d50499e1035">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Spagnol S, Geronazzo M, Rocchesso D, Avanzini F (2014) Synthetic individual binaural audio delivery by pinna image processing. Int J Pervasive Comput Commun 10(3):239–254" href="/article/10.1007/s10055-015-0272-6#ref-CR33" id="ref-link-section-d50499e1039">2014</a>), resulting in frequent localization errors such as front/back reversals, elevation angle misperception, and inside-the-head localization (Wenzel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Wenzel EM, Arruda M, Kistler DJ, Wightman FL (1993) Localization using nonindividualized head-related transfer functions. J Acoust Soc Am 94(1):111–123" href="/article/10.1007/s10055-015-0272-6#ref-CR50" id="ref-link-section-d50499e1042">1993</a>; Møller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Møller H, Sørensen MF, Jensen CB, Hammershøi D (1996) Binaural technique: Do we need individual recordings? J Audio Eng Soc 44(6):451–469" href="/article/10.1007/s10055-015-0272-6#ref-CR25" id="ref-link-section-d50499e1045">1996</a>). Still, a previous study (Middlebrooks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Middlebrooks JC (1999) Individual differences in external-ear transfer functions reduced by scaling in frequency. J Acoust Soc Am 106(3):1480–1492" href="/article/10.1007/s10055-015-0272-6#ref-CR24" id="ref-link-section-d50499e1048">1999</a>) highlighted the high correlation between the pinna cavity height, i.e., the distance from the superior internal helix border to the intertragic incisure, and an optimal frequency scaling factor aligning spectral HRTF features between subjects and thus minimizing intersubject spectral differences. We used such insight knowledge to guide the selection of the optimal HRTF set in the CIPIC database for a specific subject. Following the CIPIC database anthropometric parameters, the pinna cavity height <span class="mathjax-tex">\(p_h\)</span> is given by the sum of <span class="mathjax-tex">\(d_1\)</span> (cavum concha height), <span class="mathjax-tex">\(d_3\)</span> (cymba concha height), and <span class="mathjax-tex">\(d_4\)</span> (fossa height). A simple “best match” of the mean measured <span class="mathjax-tex">\(p_h\)</span> between the left and right pinnae detected the best subject for condition B–P (Geronazzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Geronazzo M, Spagnol S, Avanzini F (2013) A modular framework for the analysis and synthesis of head-related transfer functions. In: Proceedings of 134th audio engineering society convention, Rome, Italy" href="/article/10.1007/s10055-015-0272-6#ref-CR13" id="ref-link-section-d50499e1172">2013</a>).</p><p>Considering the impulsive nature of the footstep sound, one single spatial position for the left and right HRTFs is sufficient. Since no HRTF data for very low elevations are generally available in any public HRTF database because of the difficulty in measuring and interpreting it (Algazi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Algazi VR, Duda RO, Duraiswami R, Gumerov NA, Tang Z (2002) Approximating the head-related transfer function using simple geometric models of the head and torso. J Acoust Soc Am 112(5):2053–2064" href="/article/10.1007/s10055-015-0272-6#ref-CR1" id="ref-link-section-d50499e1178">2002</a>), the lowest-elevation HRTFs were considered in all conditions. These correspond in the CIPIC database to the interaural polar coordinates <span class="mathjax-tex">\((\theta _l,\phi _l)=(-5^\circ ,-45^\circ )\)</span> and <span class="mathjax-tex">\((\theta _r,\phi _r)=(5^\circ ,-45^\circ )\)</span> for the left and right foot, respectively, where <span class="mathjax-tex">\(\theta \)</span> denotes azimuth and <span class="mathjax-tex">\(\phi \)</span> denotes elevation.</p><p>It has to be recognized that since the used HRTFs were measured at knee height, the elevation impression given to the listener might not be accurate. However, following the simplified geometry of the spherical torso approximation (Algazi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Algazi VR, Duda RO, Thompson DM (2002) The use of head-and-torso models for improved spatial sound synthesis. In: Proceedings of 113th convention audio engineering society, Los Angeles, pp 1–18" href="/article/10.1007/s10055-015-0272-6#ref-CR2" id="ref-link-section-d50499e1380">2002</a>), we assumed that the sound wave coming from below travels around the sphere spanning an angle <span class="mathjax-tex">\(\theta _{{\text {inc}}} = 135^\circ \)</span> before reaching the ear(s) at approximately <span class="mathjax-tex">\(-45^\circ \)</span> elevation. This approximation was considered in the B-NPT and B-PT conditions, where the theoretical solution for diffraction around a rigid sphere (Strutt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1904" title="Strutt JW (1904) On the acoustic shadow of a sphere. Philos Trans R Soc Lond 203:87–110" href="/article/10.1007/s10055-015-0272-6#ref-CR36" id="ref-link-section-d50499e1449">1904</a>) with <span class="mathjax-tex">\(\theta _{{\text {inc}}} = 135^\circ \)</span> was used to design a FIR filter reproducing its magnitude behavior. The only independent variable of the spherical model, i.e., the sphere radius, was adapted to the maximum circumference <span class="mathjax-tex">\(t_{c}\)</span> of the subject’s torso.</p><p>In order to guarantee the best localization accuracy possible, even to the detriment of perceived realism, no reverberation was applied to the sound stimuli. The combination of the six rendering techniques and the four surface materials gave rise to 24 stimuli, each repeated twice for a total of 48 trials. Trials were randomized across participants.</p><h3 class="c-article__sub-heading" id="Sec6">Procedure</h3><p>Participants were first subjected to a short anthropometric measurement session where parameters <span class="mathjax-tex">\(p_h\)</span> and <span class="mathjax-tex">\(t_c\)</span> were acquired. Then, each subject wore the pair of shoes and a belt which allowed the wires from shoes and headphones to be fixed to the user’s back and to then be directed to the Arduino board. In addition, wires were attached to the subject’s trousers with Velcro tape and secured to the waist. The wires were long enough (5 m) to allow free motion in the experimental space. The experiment was conducted in a laboratory whose floor was covered with a thin carpet in order to mask the footstep sounds resulting from the interaction of sandals with the floor. Such a masking was further enhanced by the use of the closed headphone set, in addition to the softness of the sandals’ sole.</p><p>Participants, who were never informed about which material was simulated at each trial, were instructed to walk freely inside the walking area and listen to the headphone-provided footstep sounds as long as they wanted before concluding the trial. At the end of each trial, participants were provided with a printed questionnaire and required to fill the following items:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">Q1</span>
                      
                        <p>Indicate in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig4">4</a> the circlet corresponding to the direction where the sound came from;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">Q2</span>
                      
                        <p>Evaluate the degree of realism of the sounds you have produced;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">Q3</span>
                      
                        <p>Evaluate to what extent your way of walking seems natural to you;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">Q4</span>
                      
                        <p>Evaluate to what extent you feel confused or disoriented while walking.</p>
                      
                    </li>
                  </ol><p>The choice of a graphical self-report instead of a verbal report is due to avoiding cognitive factors when having to represent the elevation of a sound source. Similar reporting methods for source elevation are commonly found in the literature of 3D auditory localization (Begault et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Begault DR, Wenzel EM, Anderson MR (2001) Direct comparison of the impact of head tracking, reverberation, and individualized head-related transfer functions on the spatial perception of a virtual speech source. J Audio Eng Soc 49(10):904–916" href="/article/10.1007/s10055-015-0272-6#ref-CR5" id="ref-link-section-d50499e1626">2001</a>; Hwang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Hwang S, Park Y, Park Y (2008) Modeling and customization of head-related impulse responses based on general basis functions in time domain. Acta Acust United Acust 94(6):965–980" href="/article/10.1007/s10055-015-0272-6#ref-CR19" id="ref-link-section-d50499e1629">2008</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Figure for questionnaire item Q1</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The circlets in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig4">4</a> indicate sound location relative to the listener and are 10° equally spaced because of the high localization uncertainty in the median vertical plane (Blauert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1983" title="Blauert J (1983) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge" href="/article/10.1007/s10055-015-0272-6#ref-CR6" id="ref-link-section-d50499e1656">1983</a>). Notice that, although the subject moves, the use of headphones guarantees that the virtual location of the footstep sound never changes with respect to the subject himself. Questions Q2, Q3, and Q4 were instead evaluated on a visual analog scale (VAS) [0 = not at all,  10 = very much]. Such questions were motivated by the necessity of having additional information concerning the subjective experience of interacting with the provided virtual world. Specifically, they were chosen because the realism of the provided sounds, the naturalness of the walking experience, and the sense of confusion or disorientation while walking are factors related to the sense of presence (Slater et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Slater M, Lotto B, Arnold MM, Sanchez-Vives MV (2009) How we experience immersive virtual environments: the concept of presence and its measurement. Anuario de Psicologia 40(2):193–210" href="/article/10.1007/s10055-015-0272-6#ref-CR31" id="ref-link-section-d50499e1659">2009</a>; Turchet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Turchet L (2015) Designing presence for real locomotion in immersive virtual environments: an affordance-based experiential approach. Virtual Real (accepted)" href="/article/10.1007/s10055-015-0272-6#ref-CR38" id="ref-link-section-d50499e1662">2015</a>).</p><p>Before performing the task, subjects were presented with six practice trials, one for each rendering technique, in order to become familiar with the system. To this purpose, the forest underbrush material was chosen [delivered at 53.5 dB(A)]. This material was not among those involved in the experiment.</p><h3 class="c-article__sub-heading" id="Sec7">Results and discussion</h3><p>
Data corresponding to questionnaire item Q1 were first analyzed with respect to scores corresponding to the circlets placed in the front and back half-circumferences (FHC and BHC) in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig4">4</a> (i.e., the points in which the sound was perceived as coming from the front and from the back, respectively). Such an analysis was performed in order to verify the presence of a preference for localization of the sound at the front or at the back. The number of scores in FHC and BHC was counted for each technique and each material separately and subsequently analyzed by means of an exact binomial test. This statistical analysis revealed that in all cases the difference between the counts in FHC and BHC was not significant. Localization scores in the two half-circumferences (negative scores <span class="mathjax-tex">\([-18,0]\)</span> anticlockwise in the BHC and positive scores [0, 18] anticlockwise in the FHC, where 0 is the lowest point in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig4">4</a>) were then subjected to a Friedman test for each of the six levels of rendering technique. No significant main effect was found. As a consequence, the localization scores corresponding to BHC were normalized in absolute value and added to those in FHC for further analyses. The resulting data were subjected to three Friedman tests, for rendering technique, for material, and for rendering technique for each material. Only the main effect of material was significant, <span class="mathjax-tex">\(\chi ^2(3) = 27.7,\,p &lt; 0.001\)</span>.</p><p>As illustrated in the top-left panel of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig5">5</a>, the post hoc analysis, performed by using the Wilcoxon–Nemenyi–McDonald–Thompson test, revealed that localization scores for the four materials were all significantly different except between the gravel and snow conditions. In particular, localization scores for the snow and gravel conditions were both significantly lower (i.e., toward the feet of the human silhouette in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig4">4</a>) than the metal and wood conditions. For the sake of brevity, in the remainder of the paper results of the post hoc tests (all conducted by means of the Wilcoxon–Nemenyi–McDonald–Thompson procedure) are reported in the figures.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Results of experiment 1: graphical representation of the mean and standard deviation for questionnaire items Q1 (<i>top-left</i>), Q2 (<i>top-right</i>), Q3 (<i>bottom-left</i>), and Q4 (<i>bottom-right</i>). *<span class="mathjax-tex">\(p \le 0.05\)</span>; ***<span class="mathjax-tex">\(p \le 0.001\)</span>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig5">5</a> also shows the evaluations expressed as VAS scores for questions Q2 (realism), Q3 (naturalness), and Q4 (disorientation) considering the data grouped by material. The three questionnaire items were subjected to a Friedman test for rendering technique and material. Concerning Q2, the main effect of rendering technique was nonsignificant, while the main effect of material was <span class="mathjax-tex">\(\chi ^2(3) = 23.3,\,p &lt; 0.001\)</span>. The post hoc test paralleled that of localization scores, indicating that realism scores were significantly different among all conditions and in ascending order for the metal, wood, gravel, and snow conditions. As regards Q3 and Q4, a significant main effect was again found only for material (Q3: <span class="mathjax-tex">\(\chi ^2(3) = 15.4,\,p &lt; 0.01\)</span>, Q4: <span class="mathjax-tex">\(\chi ^2(3) = 11.1,\,p &lt; 0.05\)</span>). The results of the post hoc test are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig5">5</a>.</p><p>In addition, linear mixed-effects model analyses were performed in order to search for correlations between each localization score (in absolute value) and each VAS evaluation expressed for Q2, Q3, and Q4. Such analyses revealed that the localization scores were linearly related to perceived realism (<span class="mathjax-tex">\(\beta = -7.23,\,{t}(563) = -13.39,\,p &lt; 0.001\)</span>), naturalness (<span class="mathjax-tex">\(\beta = -5.1,\,{t}(563) = -5.93,\, p &lt; 0.001\)</span>), and disorientation (<span class="mathjax-tex">\(\beta = 5.58,\,{t}(563) = 6.72,\,p &lt; 0.001\)</span>).</p><p>The four questionnaire items were then subjected to a Wilcoxon signed-rank test having two levels of surface typology (solid and aggregate). In all cases, a significant main effect was found, showing that localization and disorientation scores were higher for the solid typology compared to the aggregate one (<span class="mathjax-tex">\({Z} = 10.178,\,p &lt; 0.001\)</span> and <span class="mathjax-tex">\(Z = 7.691\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> respectively), and realism and naturalness scores were lower for the solid typology compared to the aggregate one (<span class="mathjax-tex">\({Z} = -15.519,\,p &lt; 0.001\)</span> and <span class="mathjax-tex">\(Z = -6.163\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> respectively).</p><p>No significant differences among the six rendering techniques were found. This is in accordance with our initial hypothesis for solid surfaces, whose associated sounds do not have enough energy at high frequencies to enable vertical localization mechanisms (Hebrank and Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Hebrank J, Wright D (1974) Spectral cues used in the localization of sound sources on the median plane. J Acoust Soc Am 56(6):1829–1834" href="/article/10.1007/s10055-015-0272-6#ref-CR16" id="ref-link-section-d50499e2409">1974</a>). As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig3">3</a> shows, the frequency content of solid footstep sounds (wood and metal) only overshoots the 4–5 kHz threshold that enables vertical localization by the pinna in very short temporal windows. For footstep sounds in particular, the presence of high-frequency energy is needed to trigger not only pinna-related elevation cues (i.e., frequency notches), but also torso-related ones (i.e., shadowing effects).</p><p>However, binaural techniques were all unexpectedly found to be ineffective also for aggregate surfaces, independently of the degree of customization. Instead, results showed that materials belonging to the aggregate surface typology were always localized significantly lower than the solid ones. Therefore, taken together these results suggest that surface typology has an influence on the localization judgments and that such an influence is strong enough to mask differences between the involved rendering techniques.</p><p>Coherently, significant differences were also found between evaluations of aggregate and solid surfaces as far as the perceived realism of the simulations is concerned, as well as the naturalness of the walk and the degree of confusion or disorientation. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig5">5</a>, those judgments scaled monotonically with the localization scores, and regression analyses proved the presence of linear correlations in all cases.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Experiment 2</h2><div class="c-article-section__content" id="Sec8-content"><p>In order to test the strength of the surface typology effect in localization perception and to confirm the results of the first experiment concerning the absence of differences in localization judgments between the rendering techniques, a second experiment was designed. Specifically, the directionality of footstep sounds was studied in the presence of sonically simulated virtual environments, i.e., adding a soundscape.</p><p>The role of contextual information, sonically provided as soundscape, on the perception of footstep sounds was studied by Turchet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In: Proceedings of sound and music computing conference" href="/article/10.1007/s10055-015-0272-6#ref-CR46" id="ref-link-section-d50499e2436">2010</a>). Soundscapes sonically simulated either the environment typically associated with the surface material synthesized (i.e., coherently) or with a totally different one (i.e., incoherently). Results showed that adding a coherent soundscape significantly improved both recognition of surface materials and realism evaluations when compared to both footstep sounds alone and with footstep sounds with an accompanying incoherent soundscape.</p><p>In our experiment, adding auditory information concurrent to the footstep sounds might decrease the accuracy of their localization, and such a decrement could be greater when incoherent soundscapes are provided compared to the case in which coherent ones are involved. However, if the effect is still present in such conditions this would mean that the effect is strong and that its causes might not only be due to the auditory channel per se but should be searched in the multimodal perceptual mechanisms involved in locomotion.</p><h3 class="c-article__sub-heading" id="Sec9">Participants</h3><p>Twelve participants, six males and six females, aged between 19 and 26 (<span class="mathjax-tex">\({M} = 22.66,\,{\text {SD}} = 2.49\)</span>), not one of whom was involved in the previous experiment, took part in the experiment. All participants reported normal hearing and no impairment in locomotion.</p><h3 class="c-article__sub-heading" id="Sec10">Stimuli and procedure</h3><p>The same apparatus was used as in the previous experiment. In addition to footstep sounds, the soundscapes of the following four environments were used: a courtyard of a farm during summer; a ski slope; a house interior; and a submarine. Such ad hoc built soundscapes were the same adopted by Turchet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In: Proceedings of sound and music computing conference" href="/article/10.1007/s10055-015-0272-6#ref-CR46" id="ref-link-section-d50499e2495">2010</a>) and were chosen in order to coherently fit with the synthesized footstep sounds (gravel, snow, wood, and metal, respectively). When incoherently provided, they were coupled with metal, wood, snow, and gravel, respectively. The used soundscapes were designed so as to provide a clear indication of the designed environments after the first few seconds.</p><p>The RMS amplitudes of the soundscapes were set to 54.1, 67.2, 62.7, and 63 dB(A) for the house, the submarine, the courtyard, and the ski slope, respectively. Such values were again chosen according to the results of Turchet and Serafin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduction. Appl Acoust 74(4):566–574" href="/article/10.1007/s10055-015-0272-6#ref-CR43" id="ref-link-section-d50499e2501">2013</a>), whose goal was to find the appropriate sound level for those soundscapes in the presence of synthesized footstep sounds set to the amplitudes indicated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0272-6#Sec5">2.3</a>.</p><p>The experimental protocol was analogous to that of the first experiment. The training phase consisted of presenting the footstep sounds of forest underbrush alone, with a coherent soundscape corresponding to a forest, and with an incoherent soundscape corresponding to a beach seaside in summer. Both the material and the two soundscapes were not among those involved in the experiment.</p><p>Footstep sounds were rendered using the M and B-PT techniques only. This choice was made in order to check whether the delivery method affects the quality of the results as far as the aggregate surfaces are concerned in the presence of an accompanying soundscape. Results were expected to confirm those of the first experiment, i.e., no significant differences between M and B-PT. The combination of the two rendering techniques, the four surface materials, and the three soundscape conditions (coherent, incoherent, and no soundscape) gave rise to 24 stimuli, each repeated twice for a total of 48 trials. Trials were randomized across subjects.
</p><h3 class="c-article__sub-heading" id="Sec11">Results and discussion</h3><p>Results of the second experiment are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig6">6</a>. Localization scores were analyzed by means of a Friedman test for stimulus type (footstep sounds alone, with coherent soundscape, with incoherent soundscape), rendering technique, material, and for rendering technique for each material. A significant main effect was found for material (<span class="mathjax-tex">\(\chi ^2(3) = 23.319,\,p &lt; 0.001\)</span>). The post hoc analysis revealed that localization scores were significantly lower for both the snow and gravel conditions when compared to both the metal and wood conditions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig6a_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig6a_HTML.gif" alt="figure6" loading="lazy" /></picture><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig6b_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig6b_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Results of experiment 2: graphical representation of the mean and standard deviation for questionnaire items Q1, Q2, Q3, and Q4 analyzed by material (<i>left</i>) and by type of stimulus (<i>right</i>). **<span class="mathjax-tex">\(p \le 0.01\)</span>; ***<span class="mathjax-tex">\(p \le 0.001\)</span>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The evaluations of Q2, Q3, and Q4 were subjected to the same analyses. As regards Q2, a significant main effect was found for material (<span class="mathjax-tex">\(\chi ^2(3) = 23.5,\,p &lt; 0.001\)</span>) and stimulus type (<span class="mathjax-tex">\(\chi ^2(2) = 11.166, \,p &lt; 0.01\)</span>). The first post hoc test indicated that realism scores were all significantly different except between the gravel and snow conditions; in particular, scores for the snow and gravel conditions were both significantly higher than the metal and wood conditions. The second post hoc test indicated that realism scores were significantly higher for coherent soundscapes when compared to footstep sounds alone and incoherent soundscapes and lower for incoherent soundscapes compared to footstep sounds alone.</p><p>Concerning Q3, a significant main effect was found for material (<span class="mathjax-tex">\(\chi ^2(3) = 23.5,\,p &lt; 0.001\)</span>) and stimulus type (<span class="mathjax-tex">\(\chi ^2(2) = 11.166,\,p &lt; 0.01\)</span>). The first post hoc test indicated that naturalness scores were significantly lower for the metal condition when compared to all of the other conditions. The second post hoc test indicated that naturalness scores were significantly higher for coherent soundscapes when compared to footstep sounds alone and incoherent soundscapes and lower for incoherent soundscapes compared to footstep sounds alone. Regarding Q4, a significant main effect was found for material (<span class="mathjax-tex">\(\chi ^2(3) = 11.533,\,p &lt; 0.01\)</span>) and stimulus type (<span class="mathjax-tex">\(\chi ^2(2) = 11.555,\,p &lt; 0.01\)</span>). The results of the two post hoc test were analogous to those of Q3.</p><p>The analyses performed with linear mixed-effects models revealed that localization scores (in absolute value) were linearly related to perceived realism (<span class="mathjax-tex">\(\beta = -5.97,\,{t}(563) = -9.73, \,p &lt; 0.001\)</span>), naturalness (<span class="mathjax-tex">\(\beta = -3.08,\,{t}(563) = -3.9,\,p &lt; 0.001\)</span>), and disorientation (<span class="mathjax-tex">\(\beta = 3.02,\,{t}(563) = 4.1,\,p &lt; 0.001\)</span>).</p><p>The four questionnaire items were then subjected to a Wilcoxon signed-rank test having two levels of surface typology (solid and aggregate). In all cases, a significant main effect was found, showing that localization and disorientation scores were higher for the solid typology compared to the aggregate one (<span class="mathjax-tex">\(Z = 8.974\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> and <span class="mathjax-tex">\(Z = 5.421\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> respectively), and realism and naturalness scores were lower for the solid typology compared to the aggregate one (<span class="mathjax-tex">\(Z = -11.41\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> and <span class="mathjax-tex">\(Z = -6.5479\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span> respectively).</p><p>The results of this second experiment confirm, as expected, the prevalence of the information related to surface typology over the spatial rendering technique as far as perceived localization is concerned. Independently of the surface typology, localization scores were only slightly affected by the presence of a soundscape (precisely by the coherent soundscapes provided compared to the case of footstep sounds alone). Analogously to the findings of the previous experiment, they were linearly related to judgments of realism, naturalness, and disorientation. These results, therefore, indicate that localization of footstep sounds is affected by the simulated surface typology and that this effect is roughly independent of the presence of a soundscape. Concerning the perceived realism of footstep sounds, an influence of the presence of contextual information was noticed: Footstep sounds accompanied by a coherent soundscape were judged significantly more realistic than when provided alone or with an incoherent soundscape. These findings confirm the results reported by Turchet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In: Proceedings of sound and music computing conference" href="/article/10.1007/s10055-015-0272-6#ref-CR46" id="ref-link-section-d50499e3394">2010</a>). The results of both the first and second experiments thus suggest that the influence of surface typology on localization judgments is a robust effect, since it is independent of the used rendering technique and of the presence of contextual information.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Experiment 3</h2><div class="c-article-section__content" id="Sec12-content"><p>The set of surface materials involved in the previous two experiments was relatively small. Only four synthesized materials were used, and no comparison against recordings of real footstep sounds was conducted. Another critical point arising from the first two experiments is that at signal-level aggregate sounds are significantly longer in time and significantly richer in high-frequency content than solid sounds; hence, the found effect could be merely dependent on temporal or spectral factors.</p><p>From all these considerations, a third experiment was designed with the goal of (1) replicating the results of the first two experiments using a larger palette of surface materials; (2) testing the effectiveness of synthesized footsteps sounds compared to recorded samples; and more importantly (3) assessing whether the found effect could be due to signal-level features of the involved sound stimulus.</p><h3 class="c-article__sub-heading" id="Sec13">Participants</h3><p>Twelve participants, three males and nine females, aged between 19 and 39 (<span class="mathjax-tex">\({M} = 25.75,\,{\text {SD}} = 6.09\)</span>), all of whom were not involved in the previous experiments, took part in this experiment. All participants reported normal hearing and no impairment in locomotion.</p><h3 class="c-article__sub-heading" id="Sec14">Stimuli and procedure</h3><p>The same apparatus was used as in the first two experiments. Both recordings of real and synthesized footstep sounds were used, for a total of 21 surface materials (9 solid, 10 aggregate, and 2 control conditions). In particular, the solid materials were wood, concrete, and metal all provided as real and synthesized [54.2, 56.3 and 61.5 dB(A), respectively] sounds. Moreover, three sounds were created by coupling the synthesized materials with a reverberation tail corresponding to a room of size <span class="mathjax-tex">\(9 \times 9 \times 2.5 {\text {m}}\,(T_{60} = 0.505\,{\text {s}})\)</span>. Concerning the aggregate materials, the following surfaces were used (all provided as real and synthesized): snow, gravel, dry leaves, dirt pebbles, and forest underbrush [55.4, 57.8, 54.4, 53.5, 53.5 dB(A), respectively]. The same amplitude for the corresponding real and synthesized materials was adopted and set according to the amplitude indicated in previous research (Turchet and Serafin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduction. Appl Acoust 74(4):566–574" href="/article/10.1007/s10055-015-0272-6#ref-CR43" id="ref-link-section-d50499e3518">2013</a>). The recordings of real surfaces were the same as those used in a previous recognition experiment (Nordahl et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Nordahl R, Serafin S, Turchet L (2010) Sound synthesis and evaluation of interactive footsteps for virtual reality applications. In: Proceedings of the IEEE virtual reality conference. IEEE Press, pp 147–153" href="/article/10.1007/s10055-015-0272-6#ref-CR27" id="ref-link-section-d50499e3521">2010</a>).</p><p>The recordings of real footstep sounds were used to increase the sonic palette and to search for possible differences with the synthesized sounds in the four questionnaire items. Analogously, the addition of reverberation to synthesized solid surfaces was used in order to verify possible differences in participants’ evaluations compared to synthesized solid surfaces without reverberation: Indeed, the duration of the reverberated stimuli lasted for a time long enough to cover the average duration of real footsteps, i.e., the whole temporal duration of the haptic stimulus, as opposed to the drier unreverbed sounds.</p><p>Moreover, two control conditions were considered. They consisted of white noise bursts, lasting 80 and 420 ms, respectively, both provided at 56 dB(A). The two durations were set to the minimum and maximum duration of the involved solid and aggregate surface sounds, respectively, while amplitudes were set to the average amplitude of all sounds. These control conditions were chosen to verify possible localization biases due to the stimulus’ duration or frequency content. As a matter of fact, one of the salient differences between footstep sounds on aggregate and solid surfaces is the duration, which is longer for the first compared to the second. Furthermore, noise bursts have more high-frequency content than aggregate surface sounds; hence, if frequency content were responsible for the localization bias then the noise bursts would be localized even lower.</p><p>Since the previous experiments revealed no significant differences between the techniques used for sound delivery, only one technique, M, was used. Each of the 21 stimuli was repeated twice for a total of 42 trials. Trials were randomized across subjects. The procedure was identical to that of the first two experiments, anthropometric measurements excluded. The training phase consisted of presenting recordings of both real and synthesized footstep sounds on sand delivered at 51.9 dB(A). These stimuli were not among those involved in the experiment.</p><h3 class="c-article__sub-heading" id="Sec15">Results and discussion</h3><p>
Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0272-6#Fig7">7</a> shows the results of the third experiment. Localization scores were analyzed by means of a Friedman test for the three levels of surface typology (control, solid, aggregate), yielding a significant main effect (<span class="mathjax-tex">\(\chi ^2(2) = 15.5,\, p &lt; 0.001\)</span>). The post hoc comparisons indicated that the localization scores were significantly higher for the control condition when compared to solid and aggregate conditions and significantly higher for the solid condition when compared to the aggregate condition. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0272-6/MediaObjects/10055_2015_272_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Results of experiment 3: graphical representation of the mean and standard deviation for questionnaire items Q1, Q2, Q3, and Q4 analyzed by surface typology. **<span class="mathjax-tex">\(p \le 0.01\)</span>; <span class="mathjax-tex">\(p \le 0.001\)</span>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0272-6/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>A Wilcoxon signed-rank test showed no significant differences between localization scores of the synthesized surfaces and the real ones. Similarly, the same test showed no significant differences between localization scores of the synthesized solid surfaces with and without reverberation. Also, no significant differences between localization scores of the two noise bursts were found.</p><p>The evaluations of Q2, Q3, and Q4 were subjected to the same analyses. The main effect of surface typology was <span class="mathjax-tex">\(\chi ^2(2) = 18.666,\,p &lt; 0.001\)</span> for Q2, <span class="mathjax-tex">\(\chi ^2(2) = 15.166,\,p &lt; 0.001\)</span> for Q3, and <span class="mathjax-tex">\(\chi ^2(2) = 10.34,\,p &lt; 0.01\)</span> for Q4. The post hoc test indicated that realism and naturalness (disorientation) scores were significantly lower (higher) for the control condition when compared to solid and aggregate conditions, while no significant differences were found either between the synthesized surfaces and the real ones or the synthesized solid surfaces with and without reverberation. A further Wilcoxon signed-rank test was conducted on the four questionnaire items to compare the two control conditions. In none of the analyses, statistical significance was noticed.</p><p>The analyses performed with linear mixed-effects models revealed that localization scores (in absolute value) were linearly related to perceived realism (<span class="mathjax-tex">\(\beta = -6.34, {t}(563) = -8.55,\,p &lt; 0.001\)</span>), naturalness (<span class="mathjax-tex">\(\beta = -4.77, {t}(563) = -5.4,\,p &lt; 0.001\)</span>), and disorientation (<span class="mathjax-tex">\(\beta = 4.17, {t}(563) = 4.94,\,p &lt; 0.001\)</span>).</p><p>Taken together, results of the third experiment confirm that footstep sounds on aggregate surfaces are localized nearer to the feet than those on solid surfaces. Furthermore, both the noise bursts were localized in positions higher than those corresponding to the real and synthesized solid surfaces, and their localization scores did not differ significantly. Last but not least, no significant localization difference was found between solid surfaces with and without reverberation. Therefore, these findings exclude any explanation of the cause of the found effect due to the duration or frequency content of the sound stimulus.</p><p>Contrary to the previous two experiments, realism, naturalness, and disorientation scores were not significantly different for the solid and aggregate surface typologies, while as expected control conditions were judged as the least realistic. Furthermore, similar ratings were given for the real and synthesized sounds for all the questionnaire items; this suggests the success of the synthesis algorithms in mimicking real footsteps sounds. Analogously, in each of the four questionnaire items no significant difference was found for the synthesized solid surfaces with and without reverberation. This finding parallels the corresponding localization results.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">General discussion</h2><div class="c-article-section__content" id="Sec16-content"><p>The main result common to the three experiments is that solid surfaces are localized significantly farther from the walker’s feet than aggregate ones independently of rendering technique, presence or absence of contextual information, duration and frequency content of the sound stimulus. Such an effect could be explained by the presence of a semantic conflict between the haptic and auditory sensory channels, coupled with the hypothesis that the auditory system uses the information coming from the haptic channel to enhance sensitivity in the localization of sounds apparently coming from the walker’s feet.</p><p>Such a hypothesis is inspired to the findings reported by Thomas and Shiffrar (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Thomas JP, Shiffrar M (2010) I can see you better if i can hear you coming: action-consistent sounds facilitate the visual detection of human gait. J Vis 10(12):14. doi:&#xA;                    10.1167/10.12.14&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0272-6#ref-CR37" id="ref-link-section-d50499e4033">2010</a>) who argued that the visual system could make use of auditory cues during visual analysis of human action (in their case, footsteps) when there is a meaningful match between the auditory and visual cues. In our study, the source of the auditory and haptic (i.e., the foot-shoe contact while walking) stimuli was not unique, and therefore, the two sensory channels received conflicting information. Still, our interpretation is supported by the evidence that audiotactile interactions can happen independently of spatial coincidence in the region close to the head (see Kitagawa and Spence <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kitagawa N, Spence C (2006) Audiotactile multisensory interactions in human information processing. Jpn Psychol Res 48(3):158–173" href="/article/10.1007/s10055-015-0272-6#ref-CR21" id="ref-link-section-d50499e4036">2006</a> for a review) and parallels the findings on how information presented on one sensory modality can influence information processing in another sensory modality [e.g., the ventriloquism illusion (Howard and Templeton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1966" title="Howard IP, Templeton WB (1966) Human spatial orientation. Wiley, New York" href="/article/10.1007/s10055-015-0272-6#ref-CR17" id="ref-link-section-d50499e4039">1966</a>) and the “parchment-skin” illusion (Jousmäki and Hari <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Jousmäki V, Hari R (1998) Parchment-skin illusion: sound-biased touch. Curr Biol 8(6):R190–R191" href="/article/10.1007/s10055-015-0272-6#ref-CR20" id="ref-link-section-d50499e4042">1998</a>)]. However, it is interesting to notice that since the apparent location of the presented footstep sounds is not particularly biased toward the source of the synchronous tactile stimulation (i.e., the feet), the phenomenon of tactile capture of audition (Caclin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile“capture” of audition. Percept Psychophys 64(4):616–630" href="/article/10.1007/s10055-015-0272-6#ref-CR9" id="ref-link-section-d50499e4045">2002</a>) does not happen.</p><p>Our hypothesis is further fostered by the findings reported by Laurienti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Laurienti P, Kraft R, Maldjian J, Burdette J, Wallace M (2004) Semantic congruence is a critical factor in multisensory behavioral performance. Exp Brain Res 158(4):405–414" href="/article/10.1007/s10055-015-0272-6#ref-CR23" id="ref-link-section-d50499e4051">2004</a>) that highlighted how the semantic content of a multisensory stimulus plays a critical role in determining how it is processed by the nervous system, and by the results recently reported by Turchet and Serafin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Turchet L, Serafin S (2014) Semantic congruence in audio-haptic simulation of footsteps. Appl Acoust 75(1):59–66" href="/article/10.1007/s10055-015-0272-6#ref-CR44" id="ref-link-section-d50499e4054">2014</a>). That study presented a set of experiments whose goal was to investigate subjects ability to match pairs of synthetic auditory stimuli (created with the same engine used in the present work) and haptic stimuli [delivered through haptic shoes (Turchet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simulation of walking on different grounds. In: Proceedings of IEEE international workshop on multimedia signal processing, IEEE Press, pp 269–273" href="/article/10.1007/s10055-015-0272-6#ref-CR41" id="ref-link-section-d50499e4057">2010</a>)]. The involved stimuli were both semantically congruent (e.g., wood delivered at both auditory and haptic level) and incongruent (e.g., snow delivered at haptic level and metal simultaneously delivered at auditory level) and presented in both active (i.e., while walking) and passive (i.e., while sitting on a chair) sensorymotor activity. Results showed that in the active condition pairs of stimuli consisting of an auditory aggregate material and a haptic solid material were not judged less semantically congruent than pairs of solid materials, as well as of aggregate materials. Conversely, aggregate–solid pairs were judged, with statistical significance, to be less semantically congruent than solid–solid pairs in the passive condition. The cause for this result was attributed to technological limitations. Indeed, although the impact sound produced by hard sole shoes with a solid surface was realistically rendered, haptic stimuli induced by the actuators were not effective in masking the haptic sensation due to the softness of the sandals’ sole and the presence of a carpeted floor. This is also the case of the current study, in which haptic shoes simulating solid surfaces were not even used.</p><p>Therefore, we hypothesize that the haptic sensation arising when walking with sandals over a floor covered with carpet is more semantically incongruent with the simultaneous presentation of an impact sound between a hard sole and a solid surface than with the simultaneous presentation of a footstep sound on an aggregate surface. From this, it follows that the different localization ratings reported in the present study could be attributable to the different levels of semantic congruence between auditory and haptic information: The lower the semantic congruence, the greater the distance of the sound source from the feet.</p><p>Besides the described incongruence between auditory and tactile information, there are two more sources of conflicting multisensory information that could have contributed to the found effect. The first concerns the role of vision. This hypothesis is supported by different studies on the ventriloquism effect (Howard and Templeton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1966" title="Howard IP, Templeton WB (1966) Human spatial orientation. Wiley, New York" href="/article/10.1007/s10055-015-0272-6#ref-CR17" id="ref-link-section-d50499e4067">1966</a>) that showed an influence of visual cues on auditory localization. During all trials subjects could see the carpeted surface which they were walking upon as well as the whole laboratory space. These visuals could have created an expectation of sound that corresponds to walking on a carpet in an indoor environment, violated in the presence of the delivered auditory feedback. According to this hypothesis, the greater the discrepancy between the heard sound and the expected sound the higher the perceived localization. Although this hypothesis was not systematically investigated in the reported experiments, our current results do not support it. First, incongruence is always present for all stimuli, as none of them simulates a carpeted surface. Second, aggregate surfaces should produce the highest auditory–visual conflict (because they are associated with outdoor environments), but according to our results these sounds produce the lowest localization scores and the highest degrees of realism.</p><p>The second source of conflicting multisensory information regards the role of proprioception. In fact, previous research highlighted cross-modal effects between audition and proprioception while walking on a solid surface with sandals and listening to the sound of an aggregate material, such as an alteration of the perceived softness of the walked-upon surface and the induction of a sense of sinking (Turchet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Turchet L, Serafin S, Cesari P (2013) Walking pace affected by interactive sounds simulating stepping on different terrains. ACM Trans Appl Percept 10(4):23:1–23:14" href="/article/10.1007/s10055-015-0272-6#ref-CR45" id="ref-link-section-d50499e4073">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Turchet L, Camponogara I, Cesari P (2015) Interactive footstep sounds modulate the perceptual-motor aftereffect of treadmill walking. Exp Brain Res 233:205–214" href="/article/10.1007/s10055-015-0272-6#ref-CR40" id="ref-link-section-d50499e4076">2015</a>). Again, the higher is the conflict between auditory and proprioceptive information, the higher the source should be perceived. However, also in this case our current results do not support this hypothesis, as auditory information of solid surfaces is more congruent with the proprioceptive information given by the solid surface of the laboratory compared to that of the aggregate ones. More precisely, metal, wood, gravel, and snow can be ordered by increasing compliance, whereas in our results perceived elevation increases with decreasing compliance.</p><p>It is undoubted that different levels of congruence between the involved sensory information (including the contextual information provided as soundscape) produce different levels of presence, as the realism, naturalness, and disorientation scores demonstrate. Consequently, despite our results not supporting visual or proprioceptive effects, the possibility that localization of different surface typologies depends on a combination of the listed incongruences cannot be completely ruled out. In particular, in order to confirm the dominance of auditory–haptic semantic congruence over the above-mentioned conflicts, an experiment could be conducted where subjects wear shoes with a solid sole while walking on an uncarpeted surface. Our hypothesis would predict lower localization scores for solid surfaces compared to the less semantically congruent aggregate surfaces.</p><p>In addition, it is worthwhile to notice that the present study involved auditory stimuli both valid and not valid from the ecological point of view. In the presence of non-ecological stimuli (i.e., noise), the location of the sound source was rated higher than the corresponding congruent and incongruent ecologically valid stimuli. This is a further indication that when the association between the information arriving to ears and feet is not meaningful, interaction between the two sensory channels produces percepts which are not reliable. On a separate note, realism, naturalness, and disorientation scores were found to be unaffected by semantic congruence but were linearly correlated with the localization scores in all experiments. This suggests the importance of using interactive footstep sounds that are perceivable as realistic and capable to induce a high sense of naturalness during walking, as well as not to create a sense of disorientation. In short, the ecological validity of the auditory feedback is a relevant aspect in the design of locomotion-based interfaces.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusions</h2><div class="c-article-section__content" id="Sec17-content"><p>These findings have interesting applicative as well as theoretical implications. In terms of designing audio-haptic locomotion interfaces for virtual reality contexts, care should be taken to provide users with feedback fully valid from the ecological point of view, and capable of producing a meaningful association between the two sensory modalities. Our results coupled with the interpretation of previous works suggest that the type of shoe plays a relevant role in the meaningfulness of the association between simulations of auditory and haptic stimuli. This aspect has received scarce attention from designers of synthetic footstep sounds and vibrotactile feedback. Furthermore, our findings suggest that the use of spatial sound reproduction techniques (through headphones) is less relevant than the meaningfulness of bimodal associations.</p><p>In practical terms, two are the main implications to the design of locomotion interfaces for virtual reality. The first is that the technology for sound reproduction can be simplified by omitting the simulation of spatial effects. The second is that semantic congruence between auditory and tactile stimuli should be ensured in order to avoid bias in the localization of self-generated footstep sounds when provided through headphones. For this purpose, the tactile shoes presented in Turchet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simulation of walking on different grounds. In: Proceedings of IEEE international workshop on multimedia signal processing, IEEE Press, pp 269–273" href="/article/10.1007/s10055-015-0272-6#ref-CR41" id="ref-link-section-d50499e4097">2010</a>) could be used. Nevertheless, results of Turchet and Serafin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Turchet L, Serafin S (2014) Semantic congruence in audio-haptic simulation of footsteps. Appl Acoust 75(1):59–66" href="/article/10.1007/s10055-015-0272-6#ref-CR44" id="ref-link-section-d50499e4100">2014</a>) suggest that by means of that technology, which involves soft sole shoes, the haptic rendering of solid surfaces is not as effective as that of aggregates. To cope with this limitation, our results would suggest to wear shoes with hard sole and a non-carpeted solid surface when solid surfaces are delivered at auditory level.</p><p>On the other hand, understanding how different perceptual and cognitive factors influence localization of sounds produced by self-generated actions fosters our theoretical understanding of human multimodal perception and cue integration, a field that receives growing research interest. In particular, our results contribute to the development of a theoretical framework of the perceptual mechanisms involved in sonically simulated foot–floor interactions mediated by locomotion interfaces. Ultimately, future research will allow investigation of how audio-haptic interactions in walking contribute to the internal multisensory representation of the body.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Audio examples of the involved stimuli can be found at <a href="http://www.ahws-project.net/audio.html">http://www.ahws-project.net/audio.html</a>. A video of an apparatus similar to that involved in the experiment can be found at <a href="http://www.youtube.com/watch?v=kRKcKgYCPCY">http://www.youtube.com/watch?v=kRKcKgYCPCY</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>Such values were chosen according to the results of a previous experiment whose goal was to find the appropriate level of amplitude for those synthesized sounds (Turchet and Serafin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduction. Appl Acoust 74(4):566–574" href="/article/10.1007/s10055-015-0272-6#ref-CR43" id="ref-link-section-d50499e956">2013</a>). Measurements were conducted by placing the microphone of an SPL meter inside one of the two headphones: Such microphone was inserted in a hole, having its same diameter, created in a piece of hardwood which was subsequently sealed against one of the two headphones. The amplitude peak value of the footstep sound was considered.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>The mean ILDs were extracted from the CIPIC HRTF database (Algazi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Algazi VR, Duda RO, Thompson DM, Avendano C (2001) The CIPIC HRTF database. In: Proceedings of IEEE workshop on applications signal processing, audio and acoustic. New Paltz, New York, pp 1–4" href="/article/10.1007/s10055-015-0272-6#ref-CR3" id="ref-link-section-d50499e971">2001</a>).</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VR. Algazi, RO. Duda, R. Duraiswami, NA. Gumerov, Z. Tang, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Algazi VR, Duda RO, Duraiswami R, Gumerov NA, Tang Z (2002) Approximating the head-related transfer function u" /><p class="c-article-references__text" id="ref-CR1">Algazi VR, Duda RO, Duraiswami R, Gumerov NA, Tang Z (2002) Approximating the head-related transfer function using simple geometric models of the head and torso. J Acoust Soc Am 112(5):2053–2064</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1508780" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Approximating%20the%20head-related%20transfer%20function%20using%20simple%20geometric%20models%20of%20the%20head%20and%20torso&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=112&amp;issue=5&amp;pages=2053-2064&amp;publication_year=2002&amp;author=Algazi%2CVR&amp;author=Duda%2CRO&amp;author=Duraiswami%2CR&amp;author=Gumerov%2CNA&amp;author=Tang%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Algazi VR, Duda RO, Thompson DM (2002) The use of head-and-torso models for improved spatial sound synthesis. " /><p class="c-article-references__text" id="ref-CR2">Algazi VR, Duda RO, Thompson DM (2002) The use of head-and-torso models for improved spatial sound synthesis. In: Proceedings of 113th convention audio engineering society, Los Angeles, pp 1–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Algazi VR, Duda RO, Thompson DM, Avendano C (2001) The CIPIC HRTF database. In: Proceedings of IEEE workshop o" /><p class="c-article-references__text" id="ref-CR3">Algazi VR, Duda RO, Thompson DM, Avendano C (2001) The CIPIC HRTF database. In: Proceedings of IEEE workshop on applications signal processing, audio and acoustic. New Paltz, New York, pp 1–4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avanzini F, Rocchesso D (2001) Modeling collision sounds: non-linear contact force. In: Proceedings of digital" /><p class="c-article-references__text" id="ref-CR4">Avanzini F, Rocchesso D (2001) Modeling collision sounds: non-linear contact force. In: Proceedings of digital audio effects conference, pp 61–66</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DR. Begault, EM. Wenzel, MR. Anderson, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Begault DR, Wenzel EM, Anderson MR (2001) Direct comparison of the impact of head tracking, reverberation, and" /><p class="c-article-references__text" id="ref-CR5">Begault DR, Wenzel EM, Anderson MR (2001) Direct comparison of the impact of head tracking, reverberation, and individualized head-related transfer functions on the spatial perception of a virtual speech source. J Audio Eng Soc 49(10):904–916</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Direct%20comparison%20of%20the%20impact%20of%20head%20tracking%2C%20reverberation%2C%20and%20individualized%20head-related%20transfer%20functions%20on%20the%20spatial%20perception%20of%20a%20virtual%20speech%20source&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=49&amp;issue=10&amp;pages=904-916&amp;publication_year=2001&amp;author=Begault%2CDR&amp;author=Wenzel%2CEM&amp;author=Anderson%2CMR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Blauert, " /><meta itemprop="datePublished" content="1983" /><meta itemprop="headline" content="Blauert J (1983) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR6">Blauert J (1983) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20hearing%3A%20the%20psychophysics%20of%20human%20sound%20localization&amp;publication_year=1983&amp;author=Blauert%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Boren BB, Geronazzo M, Majdak P, Choueiri E (2014) PHOnA: a public dataset of measured headphone transfer func" /><p class="c-article-references__text" id="ref-CR7">Boren BB, Geronazzo M, Majdak P, Choueiri E (2014) PHOnA: a public dataset of measured headphone transfer functions. In: Proceedings of 137th audio engineering society convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MD. Burkhard, RM. Sachs, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Burkhard MD, Sachs RM (1975) Anthropometric manikin for acoustic research. J Acoust Soc Am 58(1):214–222" /><p class="c-article-references__text" id="ref-CR8">Burkhard MD, Sachs RM (1975) Anthropometric manikin for acoustic research. J Acoust Soc Am 58(1):214–222</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.380648" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Anthropometric%20manikin%20for%20acoustic%20research&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=58&amp;issue=1&amp;pages=214-222&amp;publication_year=1975&amp;author=Burkhard%2CMD&amp;author=Sachs%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Caclin, S. Soto-Faraco, A. Kingstone, C. Spence, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile“capture” of audition. Percept Psychophys 64(4):6" /><p class="c-article-references__text" id="ref-CR9">Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile“capture” of audition. Percept Psychophys 64(4):616–630</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03194730" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tactile%E2%80%9Ccapture%E2%80%9D%20of%20audition&amp;journal=Percept%20Psychophys&amp;volume=64&amp;issue=4&amp;pages=616-630&amp;publication_year=2002&amp;author=Caclin%2CA&amp;author=Soto-Faraco%2CS&amp;author=Kingstone%2CA&amp;author=Spence%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CI. Cheng, GH. Wakefield, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Cheng CI, Wakefield GH (2001) Introduction to head-related transfer functions (HRTFs): representations of HRTF" /><p class="c-article-references__text" id="ref-CR10">Cheng CI, Wakefield GH (2001) Introduction to head-related transfer functions (HRTFs): representations of HRTFs in time, frequency, and space. J Audio Eng Soc 49(4):231–249</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Introduction%20to%20head-related%20transfer%20functions%20%28HRTFs%29%3A%20representations%20of%20HRTFs%20in%20time%2C%20frequency%2C%20and%20space&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=49&amp;issue=4&amp;pages=231-249&amp;publication_year=2001&amp;author=Cheng%2CCI&amp;author=Wakefield%2CGH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Cook, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Cook P (1997) Physically informed sonic modeling (phism): synthesis of percussive sounds. Comput Music J 21(3)" /><p class="c-article-references__text" id="ref-CR11">Cook P (1997) Physically informed sonic modeling (phism): synthesis of percussive sounds. Comput Music J 21(3):38–49</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F3681012" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physically%20informed%20sonic%20modeling%20%28phism%29%3A%20synthesis%20of%20percussive%20sounds&amp;journal=Comput%20Music%20J&amp;volume=21&amp;issue=3&amp;pages=38-49&amp;publication_year=1997&amp;author=Cook%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geronazzo M, Spagnol S, Avanzini F (2013) Mixed structural modeling of head-related transfer functions for cus" /><p class="c-article-references__text" id="ref-CR12">Geronazzo M, Spagnol S, Avanzini F (2013) Mixed structural modeling of head-related transfer functions for customized binaural audio delivery. In Proceedings of 18th international conference on digital signal processing (DSP 2013). Santorini, Greece</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geronazzo M, Spagnol S, Avanzini F (2013) A modular framework for the analysis and synthesis of head-related t" /><p class="c-article-references__text" id="ref-CR13">Geronazzo M, Spagnol S, Avanzini F (2013) A modular framework for the analysis and synthesis of head-related transfer functions. In: Proceedings of 134th audio engineering society convention, Rome, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geronazzo M, Spagnol S, Bedin A, Avanzini F (2014) Enhancing vertical localization with image-guided selection" /><p class="c-article-references__text" id="ref-CR14">Geronazzo M, Spagnol S, Bedin A, Avanzini F (2014) Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions. In: Proceedings of IEEE international conference on acoustics, speech, and signal processing (ICASSP 2014), Firenze, Italy, pp 4496–4500</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Giordano, Y. Visell, HY. Yao, V. Hayward, J. Cooperstock, S. McAdams, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Giordano B, Visell Y, Yao HY, Hayward V, Cooperstock J, McAdams S (2012) Identification of walked-upon materia" /><p class="c-article-references__text" id="ref-CR15">Giordano B, Visell Y, Yao HY, Hayward V, Cooperstock J, McAdams S (2012) Identification of walked-upon materials in auditory, kinesthetic, haptic and audio-haptic conditions. J Acoust Soc Am 131:4002–4012</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.3699205" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Identification%20of%20walked-upon%20materials%20in%20auditory%2C%20kinesthetic%2C%20haptic%20and%20audio-haptic%20conditions&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=131&amp;pages=4002-4012&amp;publication_year=2012&amp;author=Giordano%2CB&amp;author=Visell%2CY&amp;author=Yao%2CHY&amp;author=Hayward%2CV&amp;author=Cooperstock%2CJ&amp;author=McAdams%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hebrank, D. Wright, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Hebrank J, Wright D (1974) Spectral cues used in the localization of sound sources on the median plane. J Acou" /><p class="c-article-references__text" id="ref-CR16">Hebrank J, Wright D (1974) Spectral cues used in the localization of sound sources on the median plane. J Acoust Soc Am 56(6):1829–1834</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1903520" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spectral%20cues%20used%20in%20the%20localization%20of%20sound%20sources%20on%20the%20median%20plane&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=56&amp;issue=6&amp;pages=1829-1834&amp;publication_year=1974&amp;author=Hebrank%2CJ&amp;author=Wright%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="IP. Howard, WB. Templeton, " /><meta itemprop="datePublished" content="1966" /><meta itemprop="headline" content="Howard IP, Templeton WB (1966) Human spatial orientation. Wiley, New York" /><p class="c-article-references__text" id="ref-CR17">Howard IP, Templeton WB (1966) Human spatial orientation. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20spatial%20orientation&amp;publication_year=1966&amp;author=Howard%2CIP&amp;author=Templeton%2CWB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KH. Hunt, FRE. Crossley, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Hunt KH, Crossley FRE (1975) Coefficient of restitution interpreted as damping in vibroimpact. ASME J Appl Mec" /><p class="c-article-references__text" id="ref-CR18">Hunt KH, Crossley FRE (1975) Coefficient of restitution interpreted as damping in vibroimpact. ASME J Appl Mech 42(2):440–445</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1115%2F1.3423596" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Coefficient%20of%20restitution%20interpreted%20as%20damping%20in%20vibroimpact&amp;journal=ASME%20J%20Appl%20Mech&amp;volume=42&amp;issue=2&amp;pages=440-445&amp;publication_year=1975&amp;author=Hunt%2CKH&amp;author=Crossley%2CFRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Hwang, Y. Park, Y. Park, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Hwang S, Park Y, Park Y (2008) Modeling and customization of head-related impulse responses based on general b" /><p class="c-article-references__text" id="ref-CR19">Hwang S, Park Y, Park Y (2008) Modeling and customization of head-related impulse responses based on general basis functions in time domain. Acta Acust United Acust 94(6):965–980</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3813%2FAAA.918113" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20and%20customization%20of%20head-related%20impulse%20responses%20based%20on%20general%20basis%20functions%20in%20time%20domain&amp;journal=Acta%20Acust%20United%20Acust&amp;volume=94&amp;issue=6&amp;pages=965-980&amp;publication_year=2008&amp;author=Hwang%2CS&amp;author=Park%2CY&amp;author=Park%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Jousmäki, R. Hari, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Jousmäki V, Hari R (1998) Parchment-skin illusion: sound-biased touch. Curr Biol 8(6):R190–R191" /><p class="c-article-references__text" id="ref-CR20">Jousmäki V, Hari R (1998) Parchment-skin illusion: sound-biased touch. Curr Biol 8(6):R190–R191</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0960-9822%2898%2970120-4" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parchment-skin%20illusion%3A%20sound-biased%20touch&amp;journal=Curr%20Biol&amp;volume=8&amp;issue=6&amp;pages=R190-R191&amp;publication_year=1998&amp;author=Jousm%C3%A4ki%2CV&amp;author=Hari%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Kitagawa, C. Spence, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Kitagawa N, Spence C (2006) Audiotactile multisensory interactions in human information processing. Jpn Psycho" /><p class="c-article-references__text" id="ref-CR21">Kitagawa N, Spence C (2006) Audiotactile multisensory interactions in human information processing. Jpn Psychol Res 48(3):158–173</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1468-5884.2006.00317.x" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Audiotactile%20multisensory%20interactions%20in%20human%20information%20processing&amp;journal=Jpn%20Psychol%20Res&amp;volume=48&amp;issue=3&amp;pages=158-173&amp;publication_year=2006&amp;author=Kitagawa%2CN&amp;author=Spence%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Kobayashi, R. Osaka, T. Hara, H. Fujimoto, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Kobayashi Y, Osaka R, Hara T, Fujimoto H (2008) How accurately people can discriminate the differences of floo" /><p class="c-article-references__text" id="ref-CR22">Kobayashi Y, Osaka R, Hara T, Fujimoto H (2008) How accurately people can discriminate the differences of floor materials with various elasticities. IEEE Trans Neural Rehab Syst Eng 16(1):99–105</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTNSRE.2007.910283" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20accurately%20people%20can%20discriminate%20the%20differences%20of%20floor%20materials%20with%20various%20elasticities&amp;journal=IEEE%20Trans%20Neural%20Rehab%20Syst%20Eng&amp;volume=16&amp;issue=1&amp;pages=99-105&amp;publication_year=2008&amp;author=Kobayashi%2CY&amp;author=Osaka%2CR&amp;author=Hara%2CT&amp;author=Fujimoto%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Laurienti, R. Kraft, J. Maldjian, J. Burdette, M. Wallace, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Laurienti P, Kraft R, Maldjian J, Burdette J, Wallace M (2004) Semantic congruence is a critical factor in mul" /><p class="c-article-references__text" id="ref-CR23">Laurienti P, Kraft R, Maldjian J, Burdette J, Wallace M (2004) Semantic congruence is a critical factor in multisensory behavioral performance. Exp Brain Res 158(4):405–414</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00221-004-1913-2" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Semantic%20congruence%20is%20a%20critical%20factor%20in%20multisensory%20behavioral%20performance&amp;journal=Exp%20Brain%20Res&amp;volume=158&amp;issue=4&amp;pages=405-414&amp;publication_year=2004&amp;author=Laurienti%2CP&amp;author=Kraft%2CR&amp;author=Maldjian%2CJ&amp;author=Burdette%2CJ&amp;author=Wallace%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Middlebrooks, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Middlebrooks JC (1999) Individual differences in external-ear transfer functions reduced by scaling in frequen" /><p class="c-article-references__text" id="ref-CR24">Middlebrooks JC (1999) Individual differences in external-ear transfer functions reduced by scaling in frequency. J Acoust Soc Am 106(3):1480–1492</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.427176" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20differences%20in%20external-ear%20transfer%20functions%20reduced%20by%20scaling%20in%20frequency&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=106&amp;issue=3&amp;pages=1480-1492&amp;publication_year=1999&amp;author=Middlebrooks%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Møller, MF. Sørensen, CB. Jensen, D. Hammershøi, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Møller H, Sørensen MF, Jensen CB, Hammershøi D (1996) Binaural technique: Do we need individual recordings? J " /><p class="c-article-references__text" id="ref-CR25">Møller H, Sørensen MF, Jensen CB, Hammershøi D (1996) Binaural technique: Do we need individual recordings? J Audio Eng Soc 44(6):451–469</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Binaural%20technique%3A%20Do%20we%20need%20individual%20recordings%3F&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=44&amp;issue=6&amp;pages=451-469&amp;publication_year=1996&amp;author=M%C3%B8ller%2CH&amp;author=S%C3%B8rensen%2CMF&amp;author=Jensen%2CCB&amp;author=Hammersh%C3%B8i%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nordahl R, Berrezag A, Dimitrov S, Turchet L, Hayward V, Serafin S (2010) Preliminary experiment combining vir" /><p class="c-article-references__text" id="ref-CR26">Nordahl R, Berrezag A, Dimitrov S, Turchet L, Hayward V, Serafin S (2010) Preliminary experiment combining virtual reality haptic shoes and audio synthesis. In: Haptics: generating and perceiving tangible sensations, lecture notes in computer science, Springer, Berlin, vol 6192, pp 123–129</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nordahl R, Serafin S, Turchet L (2010) Sound synthesis and evaluation of interactive footsteps for virtual rea" /><p class="c-article-references__text" id="ref-CR27">Nordahl R, Serafin S, Turchet L (2010) Sound synthesis and evaluation of interactive footsteps for virtual reality applications. In: Proceedings of the IEEE virtual reality conference. IEEE Press, pp 147–153</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Papetti S, Civolani M, Fontana F (2011) Rhythm’n’shoes: a wearable foot tapping interface with audio-tactile f" /><p class="c-article-references__text" id="ref-CR28">Papetti S, Civolani M, Fontana F (2011) Rhythm’n’shoes: a wearable foot tapping interface with audio-tactile feedback. In: Proceedings of the international conference on new interfaces for musical expression, pp 473–476</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Papetti S, Fontana F, Civolani M, Berrezag A, Hayward V (2010) Audio-tactile display of ground properties usin" /><p class="c-article-references__text" id="ref-CR29">Papetti S, Fontana F, Civolani M, Berrezag A, Hayward V (2010) Audio-tactile display of ground properties using interactive shoes. In: Haptic and audio interaction design, Lecture notes in computer science, Springer, Berlin, vol 6306, pp 117–128</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Serafin S, Turchet L, Nordahl R, Dimitrov S, Berrezag A, Hayward V (2010) Identification of virtual grounds us" /><p class="c-article-references__text" id="ref-CR30">Serafin S, Turchet L, Nordahl R, Dimitrov S, Berrezag A, Hayward V (2010) Identification of virtual grounds using virtual reality haptic shoes and sound synthesis. In: Proceedings of eurohaptics symposium on haptic and audio-visual stimuli: enhancing experiences and interaction, pp 61–70</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Slater, B. Lotto, MM. Arnold, MV. Sanchez-Vives, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Slater M, Lotto B, Arnold MM, Sanchez-Vives MV (2009) How we experience immersive virtual environments: the co" /><p class="c-article-references__text" id="ref-CR31">Slater M, Lotto B, Arnold MM, Sanchez-Vives MV (2009) How we experience immersive virtual environments: the concept of presence and its measurement. Anuario de Psicologia 40(2):193–210</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20we%20experience%20immersive%20virtual%20environments%3A%20the%20concept%20of%20presence%20and%20its%20measurement&amp;journal=Anuario%20de%20Psicologia&amp;volume=40&amp;issue=2&amp;pages=193-210&amp;publication_year=2009&amp;author=Slater%2CM&amp;author=Lotto%2CB&amp;author=Arnold%2CMM&amp;author=Sanchez-Vives%2CMV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Spagnol, M. Geronazzo, F. Avanzini, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Spagnol S, Geronazzo M, Avanzini F (2013) On the relation between pinna reflection patterns and head-related t" /><p class="c-article-references__text" id="ref-CR32">Spagnol S, Geronazzo M, Avanzini F (2013) On the relation between pinna reflection patterns and head-related transfer function features. IEEE Trans Audio Speech Lang Process 21(3):508–519</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2012.2227730" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20relation%20between%20pinna%20reflection%20patterns%20and%20head-related%20transfer%20function%20features&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Process&amp;volume=21&amp;issue=3&amp;pages=508-519&amp;publication_year=2013&amp;author=Spagnol%2CS&amp;author=Geronazzo%2CM&amp;author=Avanzini%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Spagnol, M. Geronazzo, D. Rocchesso, F. Avanzini, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Spagnol S, Geronazzo M, Rocchesso D, Avanzini F (2014) Synthetic individual binaural audio delivery by pinna i" /><p class="c-article-references__text" id="ref-CR33">Spagnol S, Geronazzo M, Rocchesso D, Avanzini F (2014) Synthetic individual binaural audio delivery by pinna image processing. Int J Pervasive Comput Commun 10(3):239–254</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2FIJPCC-06-2014-0035" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthetic%20individual%20binaural%20audio%20delivery%20by%20pinna%20image%20processing&amp;journal=Int%20J%20Pervasive%20Comput%20Commun&amp;volume=10&amp;issue=3&amp;pages=239-254&amp;publication_year=2014&amp;author=Spagnol%2CS&amp;author=Geronazzo%2CM&amp;author=Rocchesso%2CD&amp;author=Avanzini%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Stein, M. Meredith, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Stein B, Meredith M (1993) The merging of the senses. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR34">Stein B, Meredith M (1993) The merging of the senses. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20merging%20of%20the%20senses&amp;publication_year=1993&amp;author=Stein%2CB&amp;author=Meredith%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="F. Steinicke, Y. Visell, J. Campos, A. Lécuyer, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Steinicke F, Visell Y, Campos J, Lécuyer A (2013) Human walking in virtual environments: perception, technolog" /><p class="c-article-references__text" id="ref-CR35">Steinicke F, Visell Y, Campos J, Lécuyer A (2013) Human walking in virtual environments: perception, technology, and applications. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20walking%20in%20virtual%20environments%3A%20perception%2C%20technology%2C%20and%20applications&amp;publication_year=2013&amp;author=Steinicke%2CF&amp;author=Visell%2CY&amp;author=Campos%2CJ&amp;author=L%C3%A9cuyer%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Strutt, " /><meta itemprop="datePublished" content="1904" /><meta itemprop="headline" content="Strutt JW (1904) On the acoustic shadow of a sphere. Philos Trans R Soc Lond 203:87–110" /><p class="c-article-references__text" id="ref-CR36">Strutt JW (1904) On the acoustic shadow of a sphere. Philos Trans R Soc Lond 203:87–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1098%2Frsta.1904.0016" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20acoustic%20shadow%20of%20a%20sphere&amp;journal=Philos%20Trans%20R%20Soc%20Lond&amp;volume=203&amp;pages=87-110&amp;publication_year=1904&amp;author=Strutt%2CJW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JP. Thomas, M. Shiffrar, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Thomas JP, Shiffrar M (2010) I can see you better if i can hear you coming: action-consistent sounds facilitat" /><p class="c-article-references__text" id="ref-CR37">Thomas JP, Shiffrar M (2010) I can see you better if i can hear you coming: action-consistent sounds facilitate the visual detection of human gait. J Vis 10(12):14. doi:<a href="https://doi.org/10.1167/10.12.14">10.1167/10.12.14</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1167%2F10.12.14" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=I%20can%20see%20you%20better%20if%20i%20can%20hear%20you%20coming%3A%20action-consistent%20sounds%20facilitate%20the%20visual%20detection%20of%20human%20gait&amp;journal=J%20Vis&amp;doi=10.1167%2F10.12.14&amp;volume=10&amp;issue=12&amp;publication_year=2010&amp;author=Thomas%2CJP&amp;author=Shiffrar%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turchet L (2015) Designing presence for real locomotion in immersive virtual environments: an affordance-based" /><p class="c-article-references__text" id="ref-CR38">Turchet L (2015) Designing presence for real locomotion in immersive virtual environments: an affordance-based experiential approach. Virtual Real (accepted)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turchet L (2015) Footstep sounds synthesis: design, implementation, and evaluation of foot-floor interactions," /><p class="c-article-references__text" id="ref-CR39">Turchet L (2015) Footstep sounds synthesis: design, implementation, and evaluation of foot-floor interactions, surface materials, shoe types, and walkers’ features. Appl Acoust (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Turchet, I. Camponogara, P. Cesari, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Turchet L, Camponogara I, Cesari P (2015) Interactive footstep sounds modulate the perceptual-motor aftereffec" /><p class="c-article-references__text" id="ref-CR40">Turchet L, Camponogara I, Cesari P (2015) Interactive footstep sounds modulate the perceptual-motor aftereffect of treadmill walking. Exp Brain Res 233:205–214</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00221-014-4104-9" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20footstep%20sounds%20modulate%20the%20perceptual-motor%20aftereffect%20of%20treadmill%20walking&amp;journal=Exp%20Brain%20Res&amp;volume=233&amp;pages=205-214&amp;publication_year=2015&amp;author=Turchet%2CL&amp;author=Camponogara%2CI&amp;author=Cesari%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simula" /><p class="c-article-references__text" id="ref-CR41">Turchet L, Nordahl R, Berrezag A, Dimitrov S, Hayward V, Serafin S (2010) Audio-haptic physically based simulation of walking on different grounds. In: Proceedings of IEEE international workshop on multimedia signal processing, IEEE Press, pp 269–273</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turchet L, Serafin S (2011) A preliminary study on sound delivery methods for footstep sounds. In: Proceedings" /><p class="c-article-references__text" id="ref-CR42">Turchet L, Serafin S (2011) A preliminary study on sound delivery methods for footstep sounds. In: Proceedings of digital audio effects conference, pp 53–58</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Turchet, S. Serafin, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduc" /><p class="c-article-references__text" id="ref-CR43">Turchet L, Serafin S (2013) Investigating the amplitude of interactive footstep sounds and soundscape reproduction. Appl Acoust 74(4):566–574</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.apacoust.2012.10.010" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Investigating%20the%20amplitude%20of%20interactive%20footstep%20sounds%20and%20soundscape%20reproduction&amp;journal=Appl%20Acoust&amp;volume=74&amp;issue=4&amp;pages=566-574&amp;publication_year=2013&amp;author=Turchet%2CL&amp;author=Serafin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Turchet, S. Serafin, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Turchet L, Serafin S (2014) Semantic congruence in audio-haptic simulation of footsteps. Appl Acoust 75(1):59–" /><p class="c-article-references__text" id="ref-CR44">Turchet L, Serafin S (2014) Semantic congruence in audio-haptic simulation of footsteps. Appl Acoust 75(1):59–66</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.apacoust.2013.06.016" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Semantic%20congruence%20in%20audio-haptic%20simulation%20of%20footsteps&amp;journal=Appl%20Acoust&amp;volume=75&amp;issue=1&amp;pages=59-66&amp;publication_year=2014&amp;author=Turchet%2CL&amp;author=Serafin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Turchet, S. Serafin, P. Cesari, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Turchet L, Serafin S, Cesari P (2013) Walking pace affected by interactive sounds simulating stepping on diffe" /><p class="c-article-references__text" id="ref-CR45">Turchet L, Serafin S, Cesari P (2013) Walking pace affected by interactive sounds simulating stepping on different terrains. ACM Trans Appl Percept 10(4):23:1–23:14</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2536764.2536770" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Walking%20pace%20affected%20by%20interactive%20sounds%20simulating%20stepping%20on%20different%20terrains&amp;journal=ACM%20Trans%20Appl%20Percept&amp;volume=10&amp;issue=4&amp;pages=23%3A1-23%3A14&amp;publication_year=2013&amp;author=Turchet%2CL&amp;author=Serafin%2CS&amp;author=Cesari%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In:" /><p class="c-article-references__text" id="ref-CR46">Turchet L, Serafin S, Nordahl R (2010) Examining the role of context in the recognition of walking sounds. In: Proceedings of sound and music computing conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Visell, J. Cooperstock, B. Giordano, K. Franinovic, A. Law, S. McAdams, K. Jathal, F. Fontana, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Visell Y, Cooperstock J, Giordano B, Franinovic K, Law A, McAdams S, Jathal K, Fontana F (2008) A vibrotactile" /><p class="c-article-references__text" id="ref-CR47">Visell Y, Cooperstock J, Giordano B, Franinovic K, Law A, McAdams S, Jathal K, Fontana F (2008) A vibrotactile device for display of virtual ground materials in walking. Lect Notes Comput Sci 5024:420–426</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F978-3-540-69057-3_55" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20vibrotactile%20device%20for%20display%20of%20virtual%20ground%20materials%20in%20walking&amp;journal=Lect%20Notes%20Comput%20Sci&amp;volume=5024&amp;pages=420-426&amp;publication_year=2008&amp;author=Visell%2CY&amp;author=Cooperstock%2CJ&amp;author=Giordano%2CB&amp;author=Franinovic%2CK&amp;author=Law%2CA&amp;author=McAdams%2CS&amp;author=Jathal%2CK&amp;author=Fontana%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Visell, F. Fontana, B. Giordano, R. Nordahl, S. Serafin, R. Bresin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Visell Y, Fontana F, Giordano B, Nordahl R, Serafin S, Bresin R (2009) Sound design and perception in walking " /><p class="c-article-references__text" id="ref-CR48">Visell Y, Fontana F, Giordano B, Nordahl R, Serafin S, Bresin R (2009) Sound design and perception in walking interactions. Int J Hum Comput Stud 67(11):947–959</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2009.07.007" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sound%20design%20and%20perception%20in%20walking%20interactions&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=67&amp;issue=11&amp;pages=947-959&amp;publication_year=2009&amp;author=Visell%2CY&amp;author=Fontana%2CF&amp;author=Giordano%2CB&amp;author=Nordahl%2CR&amp;author=Serafin%2CS&amp;author=Bresin%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Vliegen, AJ. Opstal, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Vliegen J, Van Opstal AJ (2004) The influence of duration and level on human sound localization. J Acoust Soc " /><p class="c-article-references__text" id="ref-CR49">Vliegen J, Van Opstal AJ (2004) The influence of duration and level on human sound localization. J Acoust Soc Am 115(4):1705–1713</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1687423" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20influence%20of%20duration%20and%20level%20on%20human%20sound%20localization&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=115&amp;issue=4&amp;pages=1705-1713&amp;publication_year=2004&amp;author=Vliegen%2CJ&amp;author=Opstal%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EM. Wenzel, M. Arruda, DJ. Kistler, FL. Wightman, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Wenzel EM, Arruda M, Kistler DJ, Wightman FL (1993) Localization using nonindividualized head-related transfer" /><p class="c-article-references__text" id="ref-CR50">Wenzel EM, Arruda M, Kistler DJ, Wightman FL (1993) Localization using nonindividualized head-related transfer functions. J Acoust Soc Am 94(1):111–123</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.407089" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Localization%20using%20nonindividualized%20head-related%20transfer%20functions&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=94&amp;issue=1&amp;pages=111-123&amp;publication_year=1993&amp;author=Wenzel%2CEM&amp;author=Arruda%2CM&amp;author=Kistler%2CDJ&amp;author=Wightman%2CFL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zanotto D, Turchet L, Boggs E, Agrawal S (2014) Solesound: Towards a novel portable system for audio-tactile u" /><p class="c-article-references__text" id="ref-CR51">Zanotto D, Turchet L, Boggs E, Agrawal S (2014) Solesound: Towards a novel portable system for audio-tactile underfoot feedback. In: Proceedings of the 5th IEEE international conference on biomedical robotics and biomechatronics, pp 193–198</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0272-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The work of the first author was supported by the Danish Council for Independent Research, Grant No. 12-131985.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, A.C. Meyers Vænge 15, 2450, Copenhagen, Denmark</p><p class="c-article-author-affiliation__authors-list">Luca Turchet</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Faculty of Industrial Engineering, Mechanical Engineering and Computer Science, School of Engineering and Natural Sciences, University of Iceland, Tæknigarður Dunhagi 5, 107, Reykjavík, Iceland</p><p class="c-article-author-affiliation__authors-list">Simone Spagnol</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Information Engineering, University of Padova, Via Gradenigo 6/A, 35131, Padua, Italy</p><p class="c-article-author-affiliation__authors-list">Michele Geronazzo &amp; Federico Avanzini</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Luca-Turchet"><span class="c-article-authors-search__title u-h3 js-search-name">Luca Turchet</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Luca+Turchet&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Luca+Turchet" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Luca+Turchet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Simone-Spagnol"><span class="c-article-authors-search__title u-h3 js-search-name">Simone Spagnol</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Simone+Spagnol&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Simone+Spagnol" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Simone+Spagnol%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Michele-Geronazzo"><span class="c-article-authors-search__title u-h3 js-search-name">Michele Geronazzo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Michele+Geronazzo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michele+Geronazzo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michele+Geronazzo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Federico-Avanzini"><span class="c-article-authors-search__title u-h3 js-search-name">Federico Avanzini</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Federico+Avanzini&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Federico+Avanzini" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Federico+Avanzini%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0272-6/email/correspondent/c1/new">Luca Turchet</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Localization%20of%20self-generated%20synthetic%20footstep%20sounds%20on%20different%20walked-upon%20materials%20through%20headphones&amp;author=Luca%20Turchet%20et%20al&amp;contentID=10.1007%2Fs10055-015-0272-6&amp;publication=1359-4338&amp;publicationDate=2015-08-21&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0272-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0272-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Turchet, L., Spagnol, S., Geronazzo, M. <i>et al.</i> Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones.
                    <i>Virtual Reality</i> <b>20, </b>1–16 (2016). https://doi.org/10.1007/s10055-015-0272-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0272-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-09-10">10 September 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-05">05 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-21">21 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-03">March 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0272-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0272-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Walking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Interactive auditory feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Localization</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0272-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=272;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

