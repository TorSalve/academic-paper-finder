<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Untethered gesture acquisition and recognition for virtual world manip"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a system that incorporates user body-pose..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Untethered gesture acquisition and recognition for virtual world manipulation"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-07-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a system that incorporates user body-pose estimation, gesture recognition and speech recognition for interaction in virtual reality environments. We describe a vision-based method for tracking the pose of a user in real time and introduce a technique that provides parameterized gesture recognition. More precisely, we train a support vector classifier to model the boundary of the space of possible gestures, and train Hidden Markov Models (HMM) on specific gestures. Given a sequence, we can find the start and end of various gestures using a support vector classifier, and find gesture likelihoods and parameters with a HMM. A multimodal recognition process is performed using rank-order fusion to merge speech and vision hypotheses. Finally we describe the use of our multimodal framework in a virtual world application that allows users to interact using gestures and speech."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-07-12"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="222"/>

    <meta name="prism.endingPage" content="230"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0155-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0155-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0155-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0155-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Untethered gesture acquisition and recognition for virtual world manipulation"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2005/09"/>

    <meta name="citation_online_date" content="2005/07/12"/>

    <meta name="citation_firstpage" content="222"/>

    <meta name="citation_lastpage" content="230"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0155-3"/>

    <meta name="DOI" content="10.1007/s10055-005-0155-3"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0155-3"/>

    <meta name="description" content="Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a "/>

    <meta name="dc.creator" content="David Demirdjian"/>

    <meta name="dc.creator" content="Teresa Ko"/>

    <meta name="dc.creator" content="Trevor Darrell"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_title=Nonlinear programming: theory and algorithms; citation_publication_date=1993; citation_id=CR1; citation_author=M Bazaraa; citation_author=H Sherali; citation_author=C Shetty; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Analysis Mach Intell; citation_title=A method for registration of 3-d shapes; citation_author=P Besl, N MacKay; citation_volume=14; citation_publication_date=1992; citation_pages=239-256; citation_doi=10.1109/34.121791; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Auton Syst; citation_title=Towards sociable robots; citation_author=C Breazeal; citation_volume=42; citation_issue=3&#8211;4; citation_publication_date=2003; citation_pages=167-175; citation_doi=10.1016/S0921-8890(02)00373-1; citation_id=CR3"/>

    <meta name="citation_reference" content="Bregler C, Malik J (1998) Tracking people with twists and exponential maps. In: Proceedings of computer vision and pattern recognition (CVPR&#8217;98)"/>

    <meta name="citation_reference" content="citation_journal_title=Data Min Knowl Disc; citation_title=A tutorial on support vector machines for pattern recognition; citation_author=C Burges; citation_volume=2; citation_issue=2; citation_publication_date=1998; citation_pages=121-167; citation_doi=10.1023/A:1009715923555; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_title=Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents; citation_inbook_title=Embodied conversational agents; citation_publication_date=2000; citation_id=CR6; citation_author=J Cassell; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="Collobert R, Bengio S, MariTthoz J (2002) Torch: a modular machine learning software library. Technical Report IDIAP-RR 02-46,IDIAP(2002)"/>

    <meta name="citation_reference" content="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI&#8217;02). Pittsburgh, PA, pp 191&#8211;196"/>

    <meta name="citation_reference" content="Darrell T, Demirdjian D, Checka N, Felzenszwalb P (2001) Plan-view trajectory estimation with dense stereo background models. In: Proceedings of international conference on computer vision (ICCV&#8217;01). Vancouver, Canada"/>

    <meta name="citation_reference" content="Darrell T, Maes P, Blumberg B, Pentland A (1994) A novel environment for situated vision and behavior. In: IEEE workshop on visual behaviors"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Patt Anal Mach intell; citation_title=The recognition of human movement using temporal templates; citation_author=JW Davis, AF Bobick; citation_volume=23; citation_issue=3; citation_publication_date=2001; citation_pages=257-267; citation_doi=10.1109/34.910878; citation_id=CR11"/>

    <meta name="citation_reference" content="Delamarre Q, Faugeras OD (1999) 3D articulated models and multi-view tracking with silhouettes. In:Proceedings of international conference on computer vision (ICCV&#8217;99), pp 716&#8211;721"/>

    <meta name="citation_reference" content="Demirdjian D.(2003) Enforcing constraints for human body tracking. In: Proceedings of workshop on multi-object tracking, Madison, Wisconsin"/>

    <meta name="citation_reference" content="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI&#8217;02), Pittsburgh, PA"/>

    <meta name="citation_reference" content="Fua P, Brechbuhler C (1996) Imposing hard constraints on soft snakes. In: Proceedings of european conference on computer vision (ECCV&#8217;96), pp 495&#8211;506"/>

    <meta name="citation_reference" content="Gavrila D, Davis L (1996) 3D model-based tracking of humans in action: A multi-view approach. In:Proceedings of computer vision and pattern recognition (CVPR&#8217;96)"/>

    <meta name="citation_reference" content="Hall D, Le Gal C, Martin J, Chomat O, Crowley JL (2001) Magicboard: a contribution to an intelligent office environment. In: Intelligent robotic systems"/>

    <meta name="citation_reference" content="Isard M, Blake A (1998) Icondensation: unifying low-level and high-level tracking in a stochastic framework. In: Proceedings of european conference on computer vision (ECCV&#8217;98)"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence; citation_title=Recognition of visual activities and interactions by stochastic parsing; citation_author=YA Ivanov, AF Bobick; citation_volume=22; citation_issue=8; citation_publication_date=2000; citation_pages=852-872; citation_doi=10.1109/34.868686; citation_id=CR19"/>

    <meta name="citation_reference" content="Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of international conference on computational linguisitics, pp 369&#8211;375"/>

    <meta name="citation_reference" content="Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International conference on computer vision, pp 123&#8211;130"/>

    <meta name="citation_reference" content="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Feiner S, Cohen P (2003) Mutual disambiguation of 3d multimodal interaction in augmented and virtual reality. In: Proceedings of international conference on multimodal interfaces (ICMI&#8217;03). Vancouver, BC, pp 12&#8211;19"/>

    <meta name="citation_reference" content="citation_journal_title=Int Jf Comput Vis; citation_title=3D human body model acquisition from multiple views; citation_author=I Kakadiaris, D Metaxas; citation_volume=30; citation_issue=3; citation_publication_date=1998; citation_pages=191-218; citation_doi=10.1023/A:1008071332753; citation_id=CR23"/>

    <meta name="citation_reference" content="Koons D, Sparrell C, Thrisson K (1993) Integrating simultaneous input from speech, gaze and hand gestures. Intell Multimedia Interfaces, pp 257&#8211;276"/>

    <meta name="citation_reference" content="Krahnstoever N, Kettebekov S, Yeasin M, Sharma R (2002) A real-time framework for natural multimodal interaction with large screen displays. In: Proceedings of international conference on multimodal interfaces (ICMI&#8217;02). Pittsburgh, PA"/>

    <meta name="citation_reference" content="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: IEEE international conference on automatic face and gesture recognition"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE ASSP Mag; citation_title=An introduction to hidden markov models; citation_author=L Rabiner, B Juang; citation_volume=3; citation_issue=1; citation_publication_date=1986; citation_pages=4-16; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_title=Advances in kernel methods; citation_publication_date=1998; citation_id=CR28; citation_author=B Scholkopf; citation_author=C Burges; citation_author=A Smola; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversational system development. In: ICSLP, vol 3. Sydney, Australia, pp 931&#8211;934"/>

    <meta name="citation_reference" content="Sidenbladh H, Black MJ, Fleet DJ (2000) Stochastic tracking of 3D human figures using 2d image motion. In:Proceedings of European conference on computer vision (ECCV&#8217;00), pp 702&#8211;718"/>

    <meta name="citation_reference" content="Sminchisescu C, Triggs B (2001) Covariance scaled sampling for monocular 3D body tracking. In:Proceedings of the conference on computer vision and pattern recognition (CVPR&#8217;01), Kauai, Hawaii"/>

    <meta name="citation_reference" content="Vogler C, Metaxas D (1999) Parallel hidden markov models for american sign language recognition. In:International conference on computer vision, Kerkyra, Greece"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Parametric hidden markov models for gesture recognition; citation_author=A Wilson, A Bobick; citation_volume=21; citation_issue=9; citation_publication_date=1999; citation_pages=884-900; citation_doi=10.1109/34.790429; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal and Mach Intell; citation_title=Pfinder: Real-time tracking of the human body; citation_author=C Wren, A Azarbayejani, T Darrell, A Pentland; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=780-785; citation_doi=10.1109/34.598236; citation_id=CR34"/>

    <meta name="citation_reference" content="Yamamoto M, Yagishita K (2000) Scene constraints-aided tracking of human body. In:Proceedings of computer vision and pattern recognition (CVPR&#8217;00)"/>

    <meta name="citation_author" content="David Demirdjian"/>

    <meta name="citation_author_institution" content="Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, USA"/>

    <meta name="citation_author" content="Teresa Ko"/>

    <meta name="citation_author_institution" content="Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, USA"/>

    <meta name="citation_author" content="Trevor Darrell"/>

    <meta name="citation_author_institution" content="Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0155-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0155-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Untethered gesture acquisition and recognition for virtual world manipulation"/>
        <meta property="og:description" content="Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a system that incorporates user body-pose estimation, gesture recognition and speech recognition for interaction in virtual reality environments. We describe a vision-based method for tracking the pose of a user in real time and introduce a technique that provides parameterized gesture recognition. More precisely, we train a support vector classifier to model the boundary of the space of possible gestures, and train Hidden Markov Models (HMM) on specific gestures. Given a sequence, we can find the start and end of various gestures using a support vector classifier, and find gesture likelihoods and parameters with a HMM. A multimodal recognition process is performed using rank-order fusion to merge speech and vision hypotheses. Finally we describe the use of our multimodal framework in a virtual world application that allows users to interact using gestures and speech."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Untethered gesture acquisition and recognition for virtual world manipulation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0155-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Support Vector Machine, Hide Markov Model, Virtual World, Support Vector Machine Classifier, Gesture Recognition","kwrd":["Support_Vector_Machine","Hide_Markov_Model","Virtual_World","Support_Vector_Machine_Classifier","Gesture_Recognition"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0155-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0155-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=155;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0155-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Untethered gesture acquisition and recognition for virtual world manipulation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0155-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0155-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-07-12" itemprop="datePublished">12 July 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Untethered gesture acquisition and recognition for virtual world manipulation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-David-Demirdjian" data-author-popup="auth-David-Demirdjian" data-corresp-id="c1">David Demirdjian<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Massachusetts Institute of Technology" /><meta itemprop="address" content="grid.116068.8, 0000000123412786, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Teresa-Ko" data-author-popup="auth-Teresa-Ko">Teresa Ko</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Massachusetts Institute of Technology" /><meta itemprop="address" content="grid.116068.8, 0000000123412786, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Trevor-Darrell" data-author-popup="auth-Trevor-Darrell">Trevor Darrell</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Massachusetts Institute of Technology" /><meta itemprop="address" content="grid.116068.8, 0000000123412786, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">222</span>–<span itemprop="pageEnd">230</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">154 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">17 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0155-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Humans use a combination of gesture and speech to interact with objects and usually do so more naturally without holding a device or pointer. We present a system that incorporates user body-pose estimation, gesture recognition and speech recognition for interaction in virtual reality environments. We describe a vision-based method for tracking the pose of a user in real time and introduce a technique that provides parameterized gesture recognition. More precisely, we train a support vector classifier to model the boundary of the space of possible gestures, and train Hidden Markov Models (HMM) on specific gestures. Given a sequence, we can find the start and end of various gestures using a support vector classifier, and find gesture likelihoods and parameters with a HMM. A multimodal recognition process is performed using rank-order fusion to merge speech and vision hypotheses. Finally we describe the use of our multimodal framework in a virtual world application that allows users to interact using gestures and speech.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>By providing different input channels, multimodal interfaces allow a more natural and efficient interaction between user and virtual world. Recent years have seen the emergence of many systems using speech and gestures, where users interact with an application by talking to it, pointing (or looking) at icons and/or performing gestures. Research in multimodal interfaces aim at building the tools to implement these abilities in as natural and unobtrusive a manner as possible.</p><p>Several successful multimodal gesture systems have been developed which integrate speech input with pen and other haptic gestures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA, pp 191–196" href="/article/10.1007/s10055-005-0155-3#ref-CR8" id="ref-link-section-d6879e269">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of international conference on computational linguisitics, pp 369–375" href="/article/10.1007/s10055-005-0155-3#ref-CR20" id="ref-link-section-d6879e272">20</a>]; these generally use a physical stylus, or just a user’s fingertip. For interaction with a kiosk [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Krahnstoever N, Kettebekov S, Yeasin M, Sharma R (2002) A real-time framework for natural multimodal interaction with large screen displays. In: Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA" href="/article/10.1007/s10055-005-0155-3#ref-CR25" id="ref-link-section-d6879e275">25</a>], video wall [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Darrell T, Maes P, Blumberg B, Pentland A (1994) A novel environment for situated vision and behavior. In: IEEE workshop on visual behaviors" href="/article/10.1007/s10055-005-0155-3#ref-CR10" id="ref-link-section-d6879e278">10</a>], or conversational robot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Breazeal C (2003) Towards sociable robots. Robot Auton Syst 42(3–4):167–175" href="/article/10.1007/s10055-005-0155-3#ref-CR3" id="ref-link-section-d6879e281">3</a>], it is desirable to have untethered tracking of full-body gestures. Full-body gesture processing consists of two components: <i>acquisition</i> to estimate the pose of the user (e.g. arm, body position and orientation) and <i>recognition</i> to recognize the gestures corresponding to sequences of poses.</p><p>To date, full-body gesture acquisition has been mainly developed around tethered interfaces because of their robustness and accuracy. Devices such as data gloves and magnetic position systems (e.g. Flock of Birds) have been successfully used for virtual reality [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Feiner S, Cohen P (2003) Mutual disambiguation of 3d multimodal interaction in augmented and virtual reality. In: Proceedings of international conference on multimodal interfaces (ICMI’03). Vancouver, BC, pp 12–19" href="/article/10.1007/s10055-005-0155-3#ref-CR22" id="ref-link-section-d6879e294">22</a>] and map exploration [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Koons D, Sparrell C, Thrisson K (1993) Integrating simultaneous input from speech, gaze and hand gestures. Intell Multimedia Interfaces, pp 257–276" href="/article/10.1007/s10055-005-0155-3#ref-CR24" id="ref-link-section-d6879e297">24</a>]. Schemes with explicit markers attached to hands or fingers have also been proposed, as in systems for optical motion capture in computer animation. Unfortunately, these systems’ difficulty of use (e.g. attached wires, magnetic isolation of the room) prevents them from being generally usable by casual users. There have been many attempts to build untethered interfaces based on vision systems, but, to our knowledge, none of them has proven to be robust and fast enough to extract full articulated models for virtual reality purposes. In this paper, we present an untethered interface based on the tracking of the user body using stereo cameras.</p><p>Many body-pose gesture-recognition systems use techniques adapted from speech recognition research such as Hidden Markov Models (HHM) or Finite-State Transducers (FST). Such techniques consider consecutive poses of the user given by the acquisition system and estimate the most probable gesture. There are many challenges in gesture recognition: the inputs are highly dimensional (the dimension of body poses are usually greater than 20); and the beginning and end of gestures are difficult to detect (contrary to speech where sentences are isolated by detecting surrounding silence).</p><p>In our multimodal system, speech is processed using the GALAXY system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversational system development. In: ICSLP, vol 3. Sydney, Australia, pp 931–934" href="/article/10.1007/s10055-005-0155-3#ref-CR29" id="ref-link-section-d6879e306">29</a>]. A stereo camera captures images of the user and transfers them to the articulated body tracker that estimates the corresponding body pose (described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0155-3#Sec4">3.1</a>). Sequences of body poses are used in the gesture-recognition system to identify gestures (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0155-3#Sec7">3.2</a>). A rank-order fusion algorithm is used to merge command recognition; parameters are estimated for each visual gesture (e.g., size, location).</p><p>In the following sections, we present the architecture of our multimodal system for virtual reality. We then introduce our vision-based body-pose acquisition technique. A framework for gesture recognition is then described. Finally we demonstrate its use for recognizing typical gestures in a virtual world environment and show some results.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous work</h2><div class="c-article-section__content" id="Sec2-content"><p>Systems based on tethered interfaces (e.g., datagloves) have been used for virtual reality. However untethered interfaces are more usable and provide more natural interactions. In this review of the literature, we focus exclusively on untethered body tracking techniques.</p><p>Many techniques for tracking people in image sequences have been developed in the computer-vision community. Techniques using cues such as contour and skin color detection have been popular for finger and hand tracking [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Darrell T, Maes P, Blumberg B, Pentland A (1994) A novel environment for situated vision and behavior. In: IEEE workshop on visual behaviors" href="/article/10.1007/s10055-005-0155-3#ref-CR10" id="ref-link-section-d6879e327">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Hall D, Le Gal C, Martin J, Chomat O, Crowley JL (2001) Magicboard: a contribution to an intelligent office environment. In: Intelligent robotic systems" href="/article/10.1007/s10055-005-0155-3#ref-CR17" id="ref-link-section-d6879e330">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Isard M, Blake A (1998) Icondensation: unifying low-level and high-level tracking in a stochastic framework. In: Proceedings of european conference on computer vision (ECCV’98)" href="/article/10.1007/s10055-005-0155-3#ref-CR18" id="ref-link-section-d6879e333">18</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: IEEE international conference on automatic face and gesture recognition" href="/article/10.1007/s10055-005-0155-3#ref-CR26" id="ref-link-section-d6879e336">26</a>] but are limited to planar interactions.</p><p>Articulated model-based techniques have been proposed to track people in monocular image sequences [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bregler C, Malik J (1998) Tracking people with twists and exponential maps. In: Proceedings of computer vision and pattern recognition (CVPR’98)" href="/article/10.1007/s10055-005-0155-3#ref-CR4" id="ref-link-section-d6879e342">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Gavrila D, Davis L (1996) 3D model-based tracking of humans in action: A multi-view approach. In:Proceedings of computer vision and pattern recognition (CVPR’96)" href="/article/10.1007/s10055-005-0155-3#ref-CR16" id="ref-link-section-d6879e345">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Kakadiaris I, Metaxas D (1998) 3D human body model acquisition from multiple views. Int Jf Comput Vis 30(3):191-218" href="/article/10.1007/s10055-005-0155-3#ref-CR23" id="ref-link-section-d6879e348">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Wren C, Azarbayejani A, Darrell T, Pentland A (1997) Pfinder: Real-time tracking of the human body. IEEE Trans Pattern Anal and Mach Intell 19(7):780–785" href="/article/10.1007/s10055-005-0155-3#ref-CR34" id="ref-link-section-d6879e351">34</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Yamamoto M, Yagishita K (2000) Scene constraints-aided tracking of human body. In:Proceedings of computer vision and pattern recognition (CVPR’00)" href="/article/10.1007/s10055-005-0155-3#ref-CR35" id="ref-link-section-d6879e354">35</a>]. Due to the numerous ambiguities (usually caused by cluttered background or occlusions) that may arise while tracking people in monocular image sequences, multiple-hypothesis frameworks may be more suitable. Many researchers investigated such stochastic optimization techniques as particle filtering [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Sidenbladh H, Black MJ, Fleet DJ (2000) Stochastic tracking of 3D human figures using 2d image motion. In:Proceedings of European conference on computer vision (ECCV’00), pp 702–718" href="/article/10.1007/s10055-005-0155-3#ref-CR30" id="ref-link-section-d6879e358">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Sminchisescu C, Triggs B (2001) Covariance scaled sampling for monocular 3D body tracking. In:Proceedings of the conference on computer vision and pattern recognition (CVPR’01), Kauai, Hawaii" href="/article/10.1007/s10055-005-0155-3#ref-CR31" id="ref-link-section-d6879e361">31</a>]. Though promising, these approaches are not computationally efficient and real-time implementations are not yet available.</p><p>Stereo image-based techniques [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International conference on computer vision, pp 123–130" href="/article/10.1007/s10055-005-0155-3#ref-CR21" id="ref-link-section-d6879e367">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Wren C, Azarbayejani A, Darrell T, Pentland A (1997) Pfinder: Real-time tracking of the human body. IEEE Trans Pattern Anal and Mach Intell 19(7):780–785" href="/article/10.1007/s10055-005-0155-3#ref-CR34" id="ref-link-section-d6879e370">34</a>] have given better pose estimates. Jojic et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International conference on computer vision, pp 123–130" href="/article/10.1007/s10055-005-0155-3#ref-CR21" id="ref-link-section-d6879e373">21</a>] use a generative mixture model to track body gestures with real-time stereo. However, the model used in this system was approximate and the system could only accurately detect arm configurations where the arm was fully extended.</p><p>The system described in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Wren C, Azarbayejani A, Darrell T, Pentland A (1997) Pfinder: Real-time tracking of the human body. IEEE Trans Pattern Anal and Mach Intell 19(7):780–785" href="/article/10.1007/s10055-005-0155-3#ref-CR34" id="ref-link-section-d6879e380">34</a>] was successfully applied to detecting and classifying hand gestures in a conversational system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Cassell J (2000) Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents. In: Cassell J, Prevost S, Sullivan J, Churchill E (eds) Embodied conversational agents. MIT Press, cambridge" href="/article/10.1007/s10055-005-0155-3#ref-CR6" id="ref-link-section-d6879e383">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Wilson A, Bobick A (1999) Parametric hidden markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900" href="/article/10.1007/s10055-005-0155-3#ref-CR33" id="ref-link-section-d6879e386">33</a>]. While the system worked in real-time, it could sense only coarse “blob” features. Hence it was of limited use in tracking natural pointing gestures, although it was able to recognize parametric gestures defined by the relative position of both hands [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Wilson A, Bobick A (1999) Parametric hidden markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900" href="/article/10.1007/s10055-005-0155-3#ref-CR33" id="ref-link-section-d6879e389">33</a>] using a variation of HMM. Other HMM approaches to gesture recognition include [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(8):852–872" href="/article/10.1007/s10055-005-0155-3#ref-CR19" id="ref-link-section-d6879e392">19</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Vogler C, Metaxas D (1999) Parallel hidden markov models for american sign language recognition. In:International conference on computer vision, Kerkyra, Greece" href="/article/10.1007/s10055-005-0155-3#ref-CR32" id="ref-link-section-d6879e396">32</a>].</p><p>Bobick and Davis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Davis JW, Bobick AF (2001) The recognition of human movement using temporal templates. IEEE Trans Patt Anal Mach intell 23(3):257–267" href="/article/10.1007/s10055-005-0155-3#ref-CR11" id="ref-link-section-d6879e402">11</a>] created a view-dependent approach to gesture recognition using temporal templates. It captures motion and time in one 2D image by varying the intensity of pixels where motion had occurred. Motion is captured by the change in the foreground image (the person) versus the background image. As time passes, the intensity of the pixels will decrease. This is then compared to the temporal templates to find the gesture that is most similar.</p><p>Previous systems using speech and gesture inputs [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA, pp 191–196" href="/article/10.1007/s10055-005-0155-3#ref-CR8" id="ref-link-section-d6879e408">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of international conference on computational linguisitics, pp 369–375" href="/article/10.1007/s10055-005-0155-3#ref-CR20" id="ref-link-section-d6879e411">20</a>] have taken advantage of the progress of the research in the corresponding fields and the advent of new devices and sensors. Although some approaches such as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of international conference on computational linguisitics, pp 369–375" href="/article/10.1007/s10055-005-0155-3#ref-CR20" id="ref-link-section-d6879e414">20</a>] integrate speech and gesture at an early stage, most systems perform the recognition of speech and gesture separately and use a unification mechanism [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA, pp 191–196" href="/article/10.1007/s10055-005-0155-3#ref-CR8" id="ref-link-section-d6879e417">8</a>] to fuse the different modalities.</p><p>Speech processing usually consists of two components: <i>word recognition</i> to recognize individual words and <i>language understanding</i> to recognize full sentences (given a predefined grammar). Research in speech processing has known excellent progress in the last 10 years. Although there are still problems with crowds (the “cocktail-party” problem) and noisy sound input (especially when the microphone is far from the speaker), natural speech recognition systems offer very high recognition rates, particularly in conversational systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversational system development. In: ICSLP, vol 3. Sydney, Australia, pp 931–934" href="/article/10.1007/s10055-005-0155-3#ref-CR29" id="ref-link-section-d6879e429">29</a>]. Inspired by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA, pp 191–196" href="/article/10.1007/s10055-005-0155-3#ref-CR8" id="ref-link-section-d6879e432">8</a>], we fuse speech and gesture data after recognition.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Articulated tracking and gesture recognition</h2><div class="c-article-section__content" id="Sec3-content"><p>We propose a cascaded approach to efficiently and reliably recognize gestures. The system can be separated into two components, an articulated tracker and a gesture recognizer. We describe each in turn. We track articulated body motion and recognize full-body gestures of a user using a stereo-based technique. The body model used in this paper consists of a set of <i>N</i> rigid limbs linked with each other in a hierarchical system.</p><h3 class="c-article__sub-heading" id="Sec4">Tracking</h3><p>A pose Π of a body is defined as the position and orientation of each of its <i>N</i> constituent limbs in a world coordinate system <span class="mathjax-tex">\((\Pi \in {\mathcal{R}}^{6N}).\)</span> We parameterize rigid motions using twists [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bregler C, Malik J (1998) Tracking people with twists and exponential maps. In: Proceedings of computer vision and pattern recognition (CVPR’98)" href="/article/10.1007/s10055-005-0155-3#ref-CR4" id="ref-link-section-d6879e469">4</a>]. A twist, <span class="mathjax-tex">\(\varvec{\xi},\)</span> is defined as a 6-vector such that </p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\varvec{\xi }} = {\left( {\begin{array}{*{20}c} {{\user2{t}}} \\ {{\varvec{\omega }}} \\ \end{array} } \right)}$$</span></div></div><p> where <i>t</i> is a 3-vector representing the location of the rotation axis and translation along this axis, and ω is a 3-vector pointing in the direction of the rotation axis.</p><p>Let Δ define a set of rigid transformations applied to a set of rigid objects. Δ is represented as a 6<i>N</i>-vector such that </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\Delta = ({\varvec{\xi}}_1,\ldots,{\varvec{\xi}}_N)^{\rm T}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <i>N</i> is the number of limbs in the body model.</p><p>Many algorithms have been proposed for articulated tracking using stereo data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Delamarre Q, Faugeras OD (1999) 3D articulated models and multi-view tracking with silhouettes. In:Proceedings of international conference on computer vision (ICCV’99), pp 716–721" href="/article/10.1007/s10055-005-0155-3#ref-CR12" id="ref-link-section-d6879e529">12</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI’02), Pittsburgh, PA" href="/article/10.1007/s10055-005-0155-3#ref-CR14" id="ref-link-section-d6879e532">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International conference on computer vision, pp 123–130" href="/article/10.1007/s10055-005-0155-3#ref-CR21" id="ref-link-section-d6879e535">21</a>]. Such algorithms give an estimate of the body motion by minimizing an error function based on the distance between the 3D articulated model and the reconstructed 3D points.</p><p>In the case of articulated models, motions <span class="mathjax-tex">\({\varvec{\xi}}_i\)</span> are constrained by spherical joints. As a result, Δ only spans a manifold <span class="mathjax-tex">\({\mathcal{A}} \subset {\mathcal{R}}^{6N},\)</span> which we will call the <i>articulated motion space</i>. Efficient tracking is possible by enforcing the spherical joint constraints using a projection-based approach such as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI’02), Pittsburgh, PA" href="/article/10.1007/s10055-005-0155-3#ref-CR14" id="ref-link-section-d6879e570">14</a>]. This technique consists in finding the motion, Δ<sup>*</sup>, which minimizes the Mahalanobis distance: </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} E^{2} (\Delta^{*}) =&amp; \left\|{\Delta^{*} - \Delta}\right\|^{2}_{\Sigma} \\ =&amp; (\Delta^{*} - \Delta)^{\rm T} \Sigma^{-1} (\Delta^{*} - \Delta) \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p> with </p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \Delta = ({\varvec{\xi}}_1,\ldots,{\varvec{\xi}}_{N})^{\rm T} \quad \Sigma = {\text{diag}}(\Sigma_1, \Sigma_2,\ldots)$$</span></div></div><p> where <span class="mathjax-tex">\({\varvec{\xi}}_i\)</span> is the rigid motion estimated by a 3D rigid object tracker and Σ<sub>
                    <i>i</i>
                  </sub> the corresponding uncertainty. In our implementation, we used a tracker based on the Iterative Closest Point (ICP) algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Besl P, MacKay N (1992) A method for registration of 3-d shapes. IEEE Trans Pattern Analysis Mach Intell 14:239–256" href="/article/10.1007/s10055-005-0155-3#ref-CR2" id="ref-link-section-d6879e623">2</a>]. The ICP algorithm was originally designed to register 3D surfaces. We use it to estimate attraction forces between 3D shapes (such as limbs) and 3D point clouds observed by the stereo camera.</p><p>The approach presented in this paper can be considered as an extension of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI’02), Pittsburgh, PA" href="/article/10.1007/s10055-005-0155-3#ref-CR14" id="ref-link-section-d6879e630">14</a>] accounting for nonlinear constraints related to human body pose. Indeed, the human body is highly constrained due to various factors which are not possible to capture in a linear manifold (e.g., joint angles between limbs are bounded, some poses are unreachable due to body mechanics or behavior). To enforce these constraints, we use a learning-based approach, and build a human-body pose classifier using examples extracted from motion capture (mocap) data. We represent the space of valid poses defined by mocap data using a support vector machine (SVM) classifier.</p><p>Support vector machine classifiers have been very popular in the computer vision community for their ability to learn complex boundaries between classes and also for their speed and efficiency. See [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Burges C (1998) A tutorial on support vector machines for pattern recognition. Data Min Knowl Disc 2(2):121–167" href="/article/10.1007/s10055-005-0155-3#ref-CR5" id="ref-link-section-d6879e636">5</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Scholkopf B, Burges C, Smola A (1998) Advances in kernel methods. MIT Press, Cambridge" href="/article/10.1007/s10055-005-0155-3#ref-CR28" id="ref-link-section-d6879e639">28</a>] for a detailed description of SVM (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig1">1</a>) .</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Body poses corresponding to nine support vectors (out of 382) estimated by the SVM</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Given a data set, {<i>x</i><sub>
                    <i>i</i>
                  </sub>, <i>y</i><sub>
                    <i>i</i>
                  </sub>}, of examples <i>x</i><sub>
                    <i>i</i>
                  </sub> with labels <i>y</i><sub>
                    <i>i</i>
                  </sub> ∈{+1, −1}, an SVM estimates a decision function <i>f</i>(<i>x</i>) such that</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$f(x) = \sum\limits_{i}{y_{i} \alpha_{i} k(x, x_{i}) + b}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> where <i>b</i> is a scalar and α<sub>
                    <i>i</i>
                  </sub> some (nonnegative) weights estimated by the SVM. A subset only of the weights, α<sub>
                    <i>i</i>
                  </sub>,are nonnull. Examples <i>x</i><sub>
                    <i>i</i>
                  </sub> corresponding to nonzero α<sub>
                    <i>i</i>
                  </sub> are the <i>support vectors</i>. The support vectors are the training examples that lie closest to the decision boundary. Each α<sub>
                    <i>i</i>
                  </sub> defines the contribution of the corresponding support vector to the shape of the boundary. <i>k</i> (<i>x</i>, <i>x</i><sub>
                    <i>i</i>
                  </sub>) is the kernel function corresponding to the dot product of the nonlinear mapping of <i>x</i> and <i>x</i><sub>
                    <i>i</i>
                  </sub> in a (high-dimensional) feature space. Linear, polynomial and Gaussian kernels are usually used. In this paper, we used a Gaussian kernel, <span class="mathjax-tex">\(k(x, x_{i}) ={\text{e}}^{-||x - x_{i}||^{2}/{(2\sigma^2})}.\)</span></p><p>In practice, an error cost, <i>C</i>, is introduced to account for outliers during the SVM training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Scholkopf B, Burges C, Smola A (1998) Advances in kernel methods. MIT Press, Cambridge" href="/article/10.1007/s10055-005-0155-3#ref-CR28" id="ref-link-section-d6879e803">28</a>]. This allows for noise in data that would cause classes to overlap. Once the SVM has been trained, new test vectors, <i>x</i>, are classified based on the sign of the function <i>f</i>(<i>x</i>). In this work, we used the SVM implementation from the machine learning software library <i>Torch</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Collobert R, Bengio S, MariTthoz J (2002) Torch: a modular machine learning software library. Technical Report IDIAP-RR 02-46,IDIAP(2002)" href="/article/10.1007/s10055-005-0155-3#ref-CR7" id="ref-link-section-d6879e819">7</a>].</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Training</h4><p>We trained a SVM classifier to model valid poses of human bodies. The features, <i>x</i>, used in the SVM are the relative orientation of the body with respect to the world coordinate system and the relative orientations of connected limbs.</p><p>Training data consisted of a collection of more than 200 mocap sequences of people walking, running, doing sports, etc, which amounts to about 150,000 (positive) body pose examples. The models used in these sequences describe the full body, including hands, fingers, and eyes. However, only the parameters used in our model (torso, arms, forearms and head) were retained for the SVM training. Negative examples were randomly generated. Because the space of valid poses is small compared to the space of all possible poses, a pose with randomly generated angles for each joint will most likely be invalid. From this and the fact that SVM can account for outliers, negative examples could safely be generated with this approach.</p><p>We experimented with different types of kernels (linear, polynomial, Gaussian) and varying error costs. The corresponding SVMs were evaluated using standard cross-validation techniques: the classifiers were trained using all-but-one sequences and the average misclassification error, ε, of the remaining sequence was estimated.</p><p>Results clearly show that linear and polynomial kernels model human body poses very poorly (ε &gt; 0.5). Gaussian kernels, which are more local, give very good classification error rates. Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab1">1</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab2">2</a> report the classification error rates, ε, as well as the number of support vectors, <i>N</i><sub>sv</sub>, for Gaussian kernels with varying kernel size σ and error cost <i>C</i>. The SVM used in the rest of the paper uses a Gaussian kernel with σ=10 and <i>C</i>=100, which provides a good trade off between error rate and number of support vectors.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Classification error rates, ε, and number, <i>N</i><sub>sv</sub>, of support vectors for SVMs trained with Gaussian kernels (σ=10) versus error cost <i>C</i></b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Classification error rates, ε, and number, <i>N</i><sub>sv</sub>, of support vectors for SVMs trained with Gaussian kernels (<i>C</i>=100) versus kernel size σ</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Tracking with SVMs</h4><p>The tracking problem then consists of finding the motion transformation, Δ<sup>*</sup>, that maps the previous body pose, Π<sub><i>t</i>-1</sub>, to a body pose, Π<sup>*</sup>, while minimizing </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} E^{2} (\Delta^{*}) =&amp; \left\|{\Delta^{*} - \Delta}\right\|^2_{\Sigma} \\ =&amp; (\Delta^{*} - \Delta)^{\rm T} \Sigma^{-1} (\Delta^{*} - \Delta).\\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p> Articulated constraints are guaranteed by using the minimal parameterization Δ<sup>*</sup>=<b>V</b> δ<sup>*</sup> introduced in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI’02), Pittsburgh, PA" href="/article/10.1007/s10055-005-0155-3#ref-CR14" id="ref-link-section-d6879e1179">14</a>]. Let <span class="mathjax-tex">\(\bar{\Delta} = {\mathbf{V}} \bar{\delta}\)</span> be the (unconstrained) articulated transformation. The constrained minimization of criteria <i>E</i><sup>2</sup>(Δ<sup>*</sup>) is replaced with </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \bar{E}^{2} (\delta^{*}) =&amp; \left\|{\Delta^{*} - \bar{\Delta}}\right\|^{2}_{\Sigma} \\ = &amp; (\Delta^{*} - \bar{\Delta})^{\rm T} \Sigma^{-1} (\Delta^{*} - \bar{\Delta}) \\ = &amp; (\delta^{*} - \bar{\delta})^{\rm T} {\mathbf{V}}^{\rm T} \Sigma^{-1}{\mathbf{V}} (\delta^{*} - \bar{\delta}).\\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p> with the constraint <span class="mathjax-tex">\(g(\delta^{*}) = f(\Pi^{*}) = f(T_{{\Delta^{*}}}(\Pi_{t-1})) &gt; 0\)</span> where <i>f</i>(.) is the decision function estimated by the SVM, as in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0155-3#Equb">3</a>).</p><p>This is a standard constrained optimization problem that can be solved using Lagrangian methods or gradient projection methods [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Bazaraa M, Sherali H, Shetty C (1993) Nonlinear programming: theory and algorithms. Wiley, London" href="/article/10.1007/s10055-005-0155-3#ref-CR1" id="ref-link-section-d6879e1239">1</a>]. Because of its simplicity, we implemented a variant of Rosen’s gradient projection method described in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Fua P, Brechbuhler C (1996) Imposing hard constraints on soft snakes. In: Proceedings of european conference on computer vision (ECCV’96), pp 495–506" href="/article/10.1007/s10055-005-0155-3#ref-CR15" id="ref-link-section-d6879e1242">15</a>]. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig2">2</a> shows two images extracted from a video sequence and their corresponding articulated models estimated by our tracking algorithm.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Images (<i>top</i>) and corresponding articulated model (<i>bottom</i>) extracted from a tracking sequence</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec7">Gesture recognition</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Detection</h4><p>In trying to recognize a gesture, we start with the simpler problem of detecting its occurrence. We partition the space of possible poses into poses corresponding to gestures we wish to classify and those that do not. On examination of these spaces, it is clear that some individual gestures have overlapping poses, while others are clearly separated. Gestures that overlap are grouped in the same group.</p><p>We used SVM classifiers to learn the space of static poses corresponding to gestures. For each gesture group, a SVM is trained, using the static poses corresponding to all gestures in that gesture group as positive examples, and all static poses corresponding to nongestures and other gesture groups as negative examples. The feature <i>x</i> used in the SVM classifier is the 3D pose generated by the articulated tracker. As the articulated tracker tracks the body, the pose is tested against all SVM classifiers. When one is triggered, meaning a SVM has detected that the current pose is one of the static poses in the gesture group it is responsible for, we note the detection. With a perfect detector, we would immediately begin passing 3D hand positions to our recognizer, and stop when the SVM no longer triggers. Because we do not have a perfect detector, we ignore rapid oscillations in the signal, effectively running a low-pass temporal filter on the SVM decision function. For example, if a SVM has been triggering for a while and it stops for a frame and then starts detecting again, we assume the SVM has made an error, and continue sending 3D hand positions to our recognizer. The reverse is also true. If a SVM has not been triggering for a while, and it triggers for a frame, and stops triggering again, we assume the SVM has made an error, and do not send any information to our recognizer.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Recognition</h4><p>Our recognizer is made up of continuous HMM which model specific gestures. HMM deal well with the time element of gestures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Rabiner L, Juang B (1986) An introduction to hidden markov models. IEEE ASSP Mag 3(1):4–16" href="/article/10.1007/s10055-005-0155-3#ref-CR27" id="ref-link-section-d6879e1298">27</a>]. One of the main advantages of using HMMs is the ability to know the probability of an incomplete observation sequence being produced by a particular model. This allows us to predict at any point in time what gesture might be occurring, and respond with the appropriate actions. Also, we can identify various parts of a gesture, and easily extract various parameters.</p><p>Our recognizer runs only when a detection has occurred. When a SVM which models a specific gesture group triggers and starts passing 3D hand positions to the recognizer, each HMM corresponding to gestures within the gesture group starts computing the probability of that sequence being generated by that model. When the SVM stops triggering, our recognizer uses the probability of the sequence to classify the gesture. It orders the gestures within the gesture group according to the most probable, and calculates their corresponding parameters. This, along with their associated probabilities,is the input of the command recognizer.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Interactions in a virtual environment</h2><div class="c-article-section__content" id="Sec10-content"><p>Not only can a vision interface supplement a speech interface, it can also provide an alternative way to convey the same information. Using gestures to do various actions like selecting an application, playing a video stream, or flipping through a photo album, leaves the voice free for such tasks as carrying on a conversation or giving a talk.</p><p>A user can then select any application by pointing to the window or icon, and specify an action through speech, like saying “Open” while pointing at an icon. If a user wants to resize or shrink a window, they can simply frame the window with their hands and resize to its desired size, with or without the aid of speech. This same gesture can be used in various contexts to accomplish related tasks. For example, this gesture can be used to zoom in or out of a image, or map. Rather than having to learn a new interface with different icons or ways of using the mouse, a user can do the most appropriate gesture or speech to get their point across.</p><p>For drawing applications, using vision and speech to interface with a computer can often be much easier than using a keyboard and mouse. A vision and speech interface allows us to get rid of all those icons that take up the screen, limiting workspace, because we can just say “edit”, “drawing mode”, “add square”, draw a square with our finger, or resize or rotate an object. In general, we can access our 3D canvas using intuitive 3D motions, rather than learning to express 3D space with icons.</p><h3 class="c-article__sub-heading" id="Sec11">Virtual studio</h3><p>We used our multimodal infrastructure to create a virtual studio application that allows a user to edit a virtual world and navigate around it using speech and gesture inputs. The physical setup consists of a stereo camera that observes the user, a projector that displays the 3D virtual world on a 2-m wide screen and a microphone array to capture audio signals from the user while removing a considerable amount of background noise.</p><p>The stereo camera provides dense disparity maps. The disparity maps are grayscale images whose pixel intensities (disparities) are inversely proportional to the depth of the corresponding point. In our framework, the disparity maps are further processed to recover the approximate position of the user using background subtraction techniques [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Darrell T, Demirdjian D, Checka N, Felzenszwalb P (2001) Plan-view trajectory estimation with dense stereo background models. In: Proceedings of international conference on computer vision (ICCV’01). Vancouver, Canada" href="/article/10.1007/s10055-005-0155-3#ref-CR9" id="ref-link-section-d6879e1325">9</a>]. Given the parameters of the stereo camera (focal lengths, baseline) a full 3D reconstruction of the user is computed and used as input for our articulated tracking algorithm.</p><p>A set of commands has been defined to manipulate (create, delete, edit, move,...) different geometric shapes and objects (cubes, spheres,...). Moreover, we take deictic references and pronouns into account by using anaphora resolution rules. Therefore our system can handle requests such as: </p><ul class="u-list-style-dash">
                    <li>
                      <p>“Make a blue sphere here [user pointing at the screen]”</p>
                    </li>
                    <li>
                      <p>“Make a red cube in the middle of the screen”</p>
                    </li>
                    <li>
                      <p>“Move it [user pointing at the sphere] there [user pointing at the screen]”</p>
                    </li>
                    <li>
                      <p>“Resize the cube” [user making a resizing gesture]</p>
                    </li>
                    <li>
                      <p>etc.</p>
                    </li>
                  </ul><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig3">3</a> shows a user interacting with the <i>Virtual studio</i> application. Full videos showing the system can be seen at: <a href="http://www.ai.mit.edu/$\sim$demirdji/movie/multimodal/">http://www.ai.mit.edu/∼demirdji/movie/multimodal/</a> </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>User interacting with the <i>Virtual Studio</i> application, which allows manipulation of objects with natural gestures and no wires or special markers. A stereo camera (<i>above the projection screen</i>) gives visual information to the tracking system, which estimates the position of the arms of the user (<i>in red on the projection screen</i>) and detects pointed objects (e.g. <i>yellow pyramid</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec12">Implementation</h3><p>In our multimodal system, speech is processed using GALAXY [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversational system development. In: ICSLP, vol 3. Sydney, Australia, pp 931–934" href="/article/10.1007/s10055-005-0155-3#ref-CR29" id="ref-link-section-d6879e1413">29</a>]. GALAXY is an architecture for integrating speech technologies to create conversational spoken language systems. The current version handles specialized servers for word recognition, language understanding, database access and speech synthesis.</p><p>The vision system consists of a stereo camera connected to a standard 2-GHz Pentium 4 PC. The stereo camera captures images of the user and transfers them to the articulated body tracker that estimates the corresponding body pose (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0155-3#Sec4">3.1</a>). Sequences of body poses are used in the gesture-recognition system to identify gestures (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0155-3#Sec7">3.2</a>). We constrained the set of actions a user can perform when interfacing with the computer to an experimental set of actions that is smaller in size while maintaining most of the complexity of the problem. In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab3">3</a>, the set of actions we experimented on are listed.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Multimodal actions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Finally, a multimodal fusion algorithm integrates speech and gesture recognition (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0155-3#Sec15">4.3</a>) and generates the corresponding commands/requests to the virtual world (rendered using Java/Java3D).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Training our detector</h4><p>We partitioned the gestures into two groups: one-handed gestures and two-handed gestures. The first two gestures, “point” and “draw a path”, are grouped into the one-handed gestures group. The rest are in the two-handed gestures group.</p><p>We collected an average of 25 sequences of each gesture across a sample space of 8 people. Each sequence is a collection of images of size 320×240 provided by a stereo camera. The 3D model used in the experiments consists only of the upper body parts (torso, arms, forearms and head). The torso has been purposely made long to compensate for the lack of hips and legs in the model.</p><p>We trained two SVM classifiers to model our two groups of gestures. The features, <i>x</i>, used in the SVM are the relative orientation of the body with respect to the world coordinate system and the relative orientations of connected limbs. Each image in our data collection is labeled as either a pose corresponding to a one-handed gesture, a two-handed gesture or a neutral position.</p><p>The SVMs have been evaluated using standard cross-validation techniques: The classifiers have been trained using 90% of the training data, and the average misclassification error, ε, on the remaining training data is calculated. Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab4">4</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab5">5</a> report the classification error rates, ε, for Gaussian kernels with varying kernel size, σ, and error cost, <i>C</i>. The SVM used in the rest of the paper uses a Gaussian kernel with σ=5 and <i>C</i>=200.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Classification error rates, ε, for a one-handed SVM trained with Gaussian kernels with varying error cost, <i>C</i>, and kernel size, σ</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Classification error rates, ε, for two-handed SVM trained with Gaussian kernels with varying error cost, <i>C</i>, and kernel size, σ</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Training our recognizer</h4><p>Each gesture in the two-handed gesture group has its own model. The model for “resize” allows for different ways of performing this gesture, different starting positions, and has large spatial variations. This action is modeled with a 4-state HMM, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig4">4</a>. The first state consists of both hands moving to frame the window. This can either be done by selecting the upper left corner and the lower right corner, or selecting the upper right corner and the lower left corner. The next state can be resizing along either diagonal, allowing the choice of two states. Both states return to the same final state to return to the neutral position .</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb4.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb4.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Resize HMM</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The action “next” and the action “previous” have a more complicated structure, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig5">5</a>. The first state consists of both hands moving out in front of the user, with the freedom of placing the hands together or apart. The next states follow this path, starting with two hands together: (1) the hands move apart. (2) the hands move together. This can be repeated as many times as a user wants, resulting in high temporal variation, specifying the number of times the computer should respond, for example by flipping through a sequence of images or web pages.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb5.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb5.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Next and Previous HMM</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>While the two-handed gesture group demonstrates how successfully we can classify different gestures, we defined only one HMM for both gestures to demonstrate the flexibility of our system. Rather than focusing on the probability of a sequence of frames being generated by a particular HMM, we focus on its ability to parse a sequence into discrete states. The action <i>select</i> and <i>select region</i> are modeled with the same 3-state left-to-right HMM. The three states are moving from a neutral position to the object to be selected (state 0), pointing at the selected object (state 1), and moving back to a neutral position (state 2). The corresponding state diagram is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0155-3#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb6.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0155-3/MediaObjects/s10055-005-0155-3flb6.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Select and Select Region HMM</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0155-3/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The feature vector for two-handed gestures and one-handed gestures are different. Because we know that one of the hands in a one-handed gesture contains no useful information towards classification, we use a feature vector that uses only information from one hand. Because we do not know which hand the gesture is performed on, we test two sequence of feature vectors corresponding to either hand with our HMM, and use the results pertaining to the model that generates the highest probability.</p><p>The feature vectors for both two-handed gestures and one-handed gestures are directly computed from the 3D hand positions given by the articulated tracker. Given a 3D hand position, we estimate velocity,<span class="mathjax-tex">\(\vec{v},\)</span> by finding the difference between two frames. The velocity is decomposed into its unit vector, <span class="mathjax-tex">\(\hat{v},\)</span> and its magnitude, |<i>v</i>|. The relative position, <span class="mathjax-tex">\(\overrightarrow{p_{r}},\)</span> is computed by subtracting the right-hand position from the left-hand position. The relative velocity, <span class="mathjax-tex">\(\overrightarrow{v_{r}},\)</span> uses this newly calculated position, to estimate the local relative velocity between the left and right hands. Their unit vectors, <span class="mathjax-tex">\(\widehat{p_{r}}\)</span> and <span class="mathjax-tex">\(\widehat{v_{r}},\)</span> and magnitudes, |<i>p</i><sub>
                      <i>r</i>
                    </sub>| and |<i>v</i><sub>
                      <i>r</i>
                    </sub>|, respectively, are computed as well.</p><p>To test which feature vectors were actually effective in distinguishing two-handed gestures, we ran four cross-validation tests and computed their classification rates: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>F1 used all the features described above,</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>F2 used <span class="mathjax-tex">\(\vec{v},\overrightarrow{p_{r}}\)</span> and <span class="mathjax-tex">\(\overrightarrow{v_{r}},\)</span></p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>F3 used <span class="mathjax-tex">\(\hat{v}, \widehat{p_{r}}\)</span> and <span class="mathjax-tex">\(\widehat{v_{r}},\)</span></p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>and F4 uses both unit vectors, <span class="mathjax-tex">\(\hat{v},\widehat{p_{r}}\)</span> and <span class="mathjax-tex">\(\widehat{v_{r}}\)</span> and magnitudes, |<i>v</i>|, |<i>p</i><sub>
                              <i>r</i>
                            </sub>| and |<i>v</i><sub>
                              <i>r</i>
                            </sub>|.</p>
                        
                      </li>
                    </ol><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab6">6</a> shows the cross validation error when trained with the four different feature vectors described above. Each model was trained ten times, leaving out a few sequences of each type each time. The error is the ratio of how many times a sequence was classified as a different gesture and how many sequences of each gesture were tested. This happens when the probability for that sequence is higher in a model that does not correspond to its actual gesture. We can see the best overall classifier is F1, which used the feature vector with all calculated features. Another good classifier was F4, actually having a better classification rate for the action <i>previous</i>, than F1 did. The results shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab6">6</a> used a Gaussian as a model for their observation probability. Mixtures of Gaussians were tested for each feature case, but resulted in poor classification rates.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Cross validation error of the gesture recognizer with the feature vectors F1, F2, F3 and F4. The values reported correspond to the average error rates obtained by running a standard leave-one-out test ten times</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec15">Command recognizer results</h3><p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0155-3#Tab7">7</a>, we show some results on a small data set of how using different modes effect the classification error of our system. A speech utterance was captured corresponding to each gesture in our data set. The n-best list of both the speech recognizer and the gesture recognizer are compared against each other, and the best overall is used to calculate the third column. When using both vision and speech, the results are promising, although we expect that the error may increase as we generalize to larger task vocabularies.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Classification error with different modes. The values reported correspond to the average error rates obtained by running a standard leave-one-out test ten times</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0155-3/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusion and future work</h2><div class="c-article-section__content" id="Sec16-content"><p>Multimodal interfaces allow users to interact with a virtual world in a natural and intuitive manner. In order to explore this emerging area of research, we developed an untethered body tracker interface that incorporates recognition of both gestures and speech. We believe this system is flexible and robust enough to cover a range of possible interaction styles for interacting with virtual environments.</p><p>The tracking system works well in contexts such as the one corresponding to the use of our virtual studio application (person standing, facing the large display, moderate motion speed). However the performance of the tracking system may decrease when used in a more general context (e.g., any movement allowed) or if the user performs very fast motions. The tracking system may then provide inaccurate body pose estimates, start losing track and provide incorrect gesture recognition. The visual feedback present in our system (projection of the estimated arm position in the virtual scene) provides a way to recover from tracking failure: in such cases the user can detect tracking failures and ask the tracking system to reinitialize the articulated body model. Current work consists of integrating a single-frame pose estimator to automatically detect tracking failure and reinitialize the articulated body model.</p><p>This paper describes a multimodal recognition approach with a small set of actions. Some preliminary experiments have been performed on a similar system containing about ten actions and show that the performance of the system is comparable to the results reported in this paper.</p><p>There are many avenues of future work for this research, including the recognition of natural gestures, the use of machine learning techniques to make the interface adaptive to individual users and the integration of natural language dialog systems to allow conversational interaction. Yet another great challenge for future work is to study multiuser interaction and navigation in virtual environments. We believe that in multiuser situations passive and untethered sensing of users’ pose and gesture becomes even more valuable.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Bazaraa, H. Sherali, C. Shetty, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Bazaraa M, Sherali H, Shetty C (1993) Nonlinear programming: theory and algorithms. Wiley, London" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Bazaraa M, Sherali H, Shetty C (1993) Nonlinear programming: theory and algorithms. Wiley, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nonlinear%20programming%3A%20theory%20and%20algorithms&amp;publication_year=1993&amp;author=Bazaraa%2CM&amp;author=Sherali%2CH&amp;author=Shetty%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Besl, N. MacKay, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Besl P, MacKay N (1992) A method for registration of 3-d shapes. IEEE Trans Pattern Analysis Mach Intell 14:23" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Besl P, MacKay N (1992) A method for registration of 3-d shapes. IEEE Trans Pattern Analysis Mach Intell 14:239–256</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.121791" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20method%20for%20registration%20of%203-d%20shapes&amp;journal=IEEE%20Trans%20Pattern%20Analysis%20Mach%20Intell&amp;volume=14&amp;pages=239-256&amp;publication_year=1992&amp;author=Besl%2CP&amp;author=MacKay%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Breazeal, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Breazeal C (2003) Towards sociable robots. Robot Auton Syst 42(3–4):167–175" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Breazeal C (2003) Towards sociable robots. Robot Auton Syst 42(3–4):167–175</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0921-8890%2802%2900373-1" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20sociable%20robots&amp;journal=Robot%20Auton%20Syst&amp;volume=42&amp;issue=3%E2%80%934&amp;pages=167-175&amp;publication_year=2003&amp;author=Breazeal%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bregler C, Malik J (1998) Tracking people with twists and exponential maps. In: Proceedings of computer vision" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Bregler C, Malik J (1998) Tracking people with twists and exponential maps. In: Proceedings of computer vision and pattern recognition (CVPR’98)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Burges, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Burges C (1998) A tutorial on support vector machines for pattern recognition. Data Min Knowl Disc 2(2):121–16" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Burges C (1998) A tutorial on support vector machines for pattern recognition. Data Min Knowl Disc 2(2):121–167</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1009715923555" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial%20on%20support%20vector%20machines%20for%20pattern%20recognition&amp;journal=Data%20Min%20Knowl%20Disc&amp;volume=2&amp;issue=2&amp;pages=121-167&amp;publication_year=1998&amp;author=Burges%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Cassell, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Cassell J (2000) Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agen" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Cassell J (2000) Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents. In: Cassell J, Prevost S, Sullivan J, Churchill E (eds) Embodied conversational agents. MIT Press, cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Embodied%20conversational%20agents&amp;publication_year=2000&amp;author=Cassell%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Collobert R, Bengio S, MariTthoz J (2002) Torch: a modular machine learning software library. Technical Report" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Collobert R, Bengio S, MariTthoz J (2002) Torch: a modular machine learning software library. Technical Report IDIAP-RR 02-46,IDIAP(2002)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing." /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Corradini A, Wesson R, Cohen P (2002) A map-based system using speech and 3D gestures for pervasive computing. In : Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA, pp 191–196</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darrell T, Demirdjian D, Checka N, Felzenszwalb P (2001) Plan-view trajectory estimation with dense stereo bac" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Darrell T, Demirdjian D, Checka N, Felzenszwalb P (2001) Plan-view trajectory estimation with dense stereo background models. In: Proceedings of international conference on computer vision (ICCV’01). Vancouver, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darrell T, Maes P, Blumberg B, Pentland A (1994) A novel environment for situated vision and behavior. In: IEE" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Darrell T, Maes P, Blumberg B, Pentland A (1994) A novel environment for situated vision and behavior. In: IEEE workshop on visual behaviors</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Davis, AF. Bobick, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Davis JW, Bobick AF (2001) The recognition of human movement using temporal templates. IEEE Trans Patt Anal Ma" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Davis JW, Bobick AF (2001) The recognition of human movement using temporal templates. IEEE Trans Patt Anal Mach intell 23(3):257–267</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.910878" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20recognition%20of%20human%20movement%20using%20temporal%20templates&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20intell&amp;volume=23&amp;issue=3&amp;pages=257-267&amp;publication_year=2001&amp;author=Davis%2CJW&amp;author=Bobick%2CAF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Delamarre Q, Faugeras OD (1999) 3D articulated models and multi-view tracking with silhouettes. In:Proceedings" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Delamarre Q, Faugeras OD (1999) 3D articulated models and multi-view tracking with silhouettes. In:Proceedings of international conference on computer vision (ICCV’99), pp 716–721</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Demirdjian D.(2003) Enforcing constraints for human body tracking. In: Proceedings of workshop on multi-object" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Demirdjian D.(2003) Enforcing constraints for human body tracking. In: Proceedings of workshop on multi-object tracking, Madison, Wisconsin</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings " /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Demirdjian D, Darrell T (2002) 3D articulated pose tracking for untethered deictic reference. In: Proceedings of international conference on multimodal interfaces (ICMI’02), Pittsburgh, PA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fua P, Brechbuhler C (1996) Imposing hard constraints on soft snakes. In: Proceedings of european conference o" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Fua P, Brechbuhler C (1996) Imposing hard constraints on soft snakes. In: Proceedings of european conference on computer vision (ECCV’96), pp 495–506</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gavrila D, Davis L (1996) 3D model-based tracking of humans in action: A multi-view approach. In:Proceedings o" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Gavrila D, Davis L (1996) 3D model-based tracking of humans in action: A multi-view approach. In:Proceedings of computer vision and pattern recognition (CVPR’96)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hall D, Le Gal C, Martin J, Chomat O, Crowley JL (2001) Magicboard: a contribution to an intelligent office en" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Hall D, Le Gal C, Martin J, Chomat O, Crowley JL (2001) Magicboard: a contribution to an intelligent office environment. In: Intelligent robotic systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Isard M, Blake A (1998) Icondensation: unifying low-level and high-level tracking in a stochastic framework. I" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Isard M, Blake A (1998) Icondensation: unifying low-level and high-level tracking in a stochastic framework. In: Proceedings of european conference on computer vision (ECCV’98)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YA. Ivanov, AF. Bobick, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Tran" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(8):852–872</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.868686" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20visual%20activities%20and%20interactions%20by%20stochastic%20parsing&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=22&amp;issue=8&amp;pages=852-872&amp;publication_year=2000&amp;author=Ivanov%2CYA&amp;author=Bobick%2CAF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of internati" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Johnston M, Bangalore S (2000) Finite-state multimodal parsing and understanding. In: Proceedings of international conference on computational linguisitics, pp 369–375</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International confer" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Jojic N, Turk M, Huang T (1999) Tracking articulated objects in dense disparity maps. In: International conference on computer vision, pp 123–130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Feiner S, Cohen P (2003) Mutual disambiguation of 3d m" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Feiner S, Cohen P (2003) Mutual disambiguation of 3d multimodal interaction in augmented and virtual reality. In: Proceedings of international conference on multimodal interfaces (ICMI’03). Vancouver, BC, pp 12–19</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Kakadiaris, D. Metaxas, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Kakadiaris I, Metaxas D (1998) 3D human body model acquisition from multiple views. Int Jf Comput Vis 30(3):19" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Kakadiaris I, Metaxas D (1998) 3D human body model acquisition from multiple views. Int Jf Comput Vis 30(3):191-218</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1008071332753" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20human%20body%20model%20acquisition%20from%20multiple%20views&amp;journal=Int%20Jf%20Comput%20Vis&amp;volume=30&amp;issue=3&amp;pages=191-218&amp;publication_year=1998&amp;author=Kakadiaris%2CI&amp;author=Metaxas%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koons D, Sparrell C, Thrisson K (1993) Integrating simultaneous input from speech, gaze and hand gestures. Int" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Koons D, Sparrell C, Thrisson K (1993) Integrating simultaneous input from speech, gaze and hand gestures. Intell Multimedia Interfaces, pp 257–276</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krahnstoever N, Kettebekov S, Yeasin M, Sharma R (2002) A real-time framework for natural multimodal interacti" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Krahnstoever N, Kettebekov S, Yeasin M, Sharma R (2002) A real-time framework for natural multimodal interaction with large screen displays. In: Proceedings of international conference on multimodal interfaces (ICMI’02). Pittsburgh, PA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented " /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: IEEE international conference on automatic face and gesture recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Rabiner, B. Juang, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Rabiner L, Juang B (1986) An introduction to hidden markov models. IEEE ASSP Mag 3(1):4–16" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Rabiner L, Juang B (1986) An introduction to hidden markov models. IEEE ASSP Mag 3(1):4–16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20hidden%20markov%20models&amp;journal=IEEE%20ASSP%20Mag&amp;volume=3&amp;issue=1&amp;pages=4-16&amp;publication_year=1986&amp;author=Rabiner%2CL&amp;author=Juang%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Scholkopf, C. Burges, A. Smola, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Scholkopf B, Burges C, Smola A (1998) Advances in kernel methods. MIT Press, Cambridge" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Scholkopf B, Burges C, Smola A (1998) Advances in kernel methods. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20kernel%20methods&amp;publication_year=1998&amp;author=Scholkopf%2CB&amp;author=Burges%2CC&amp;author=Smola%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversationa" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Seneff S, Hurley E, Lau R, Pao C, Schmid P, Zue V (1998) Galaxy-ii: a reference architecture for conversational system development. In: ICSLP, vol 3. Sydney, Australia, pp 931–934</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sidenbladh H, Black MJ, Fleet DJ (2000) Stochastic tracking of 3D human figures using 2d image motion. In:Proc" /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Sidenbladh H, Black MJ, Fleet DJ (2000) Stochastic tracking of 3D human figures using 2d image motion. In:Proceedings of European conference on computer vision (ECCV’00), pp 702–718</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sminchisescu C, Triggs B (2001) Covariance scaled sampling for monocular 3D body tracking. In:Proceedings of t" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Sminchisescu C, Triggs B (2001) Covariance scaled sampling for monocular 3D body tracking. In:Proceedings of the conference on computer vision and pattern recognition (CVPR’01), Kauai, Hawaii</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vogler C, Metaxas D (1999) Parallel hidden markov models for american sign language recognition. In:Internatio" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Vogler C, Metaxas D (1999) Parallel hidden markov models for american sign language recognition. In:International conference on computer vision, Kerkyra, Greece</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Wilson, A. Bobick, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Wilson A, Bobick A (1999) Parametric hidden markov models for gesture recognition. IEEE Trans Pattern Anal Mac" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Wilson A, Bobick A (1999) Parametric hidden markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.790429" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parametric%20hidden%20markov%20models%20for%20gesture%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=21&amp;issue=9&amp;pages=884-900&amp;publication_year=1999&amp;author=Wilson%2CA&amp;author=Bobick%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Wren, A. Azarbayejani, T. Darrell, A. Pentland, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Wren C, Azarbayejani A, Darrell T, Pentland A (1997) Pfinder: Real-time tracking of the human body. IEEE Trans" /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Wren C, Azarbayejani A, Darrell T, Pentland A (1997) Pfinder: Real-time tracking of the human body. IEEE Trans Pattern Anal and Mach Intell 19(7):780–785</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598236" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pfinder%3A%20Real-time%20tracking%20of%20the%20human%20body&amp;journal=IEEE%20Trans%20Pattern%20Anal%20and%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=780-785&amp;publication_year=1997&amp;author=Wren%2CC&amp;author=Azarbayejani%2CA&amp;author=Darrell%2CT&amp;author=Pentland%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yamamoto M, Yagishita K (2000) Scene constraints-aided tracking of human body. In:Proceedings of computer visi" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Yamamoto M, Yagishita K (2000) Scene constraints-aided tracking of human body. In:Proceedings of computer vision and pattern recognition (CVPR’00)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0155-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA</p><p class="c-article-author-affiliation__authors-list">David Demirdjian, Teresa Ko &amp; Trevor Darrell</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-David-Demirdjian"><span class="c-article-authors-search__title u-h3 js-search-name">David Demirdjian</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;David+Demirdjian&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David+Demirdjian" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David+Demirdjian%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Teresa-Ko"><span class="c-article-authors-search__title u-h3 js-search-name">Teresa Ko</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Teresa+Ko&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Teresa+Ko" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Teresa+Ko%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Trevor-Darrell"><span class="c-article-authors-search__title u-h3 js-search-name">Trevor Darrell</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Trevor+Darrell&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Trevor+Darrell" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Trevor+Darrell%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                David Demirdjian.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Untethered%20gesture%20acquisition%20and%20recognition%20for%20virtual%20world%20manipulation&amp;author=David%20Demirdjian%20et%20al&amp;contentID=10.1007%2Fs10055-005-0155-3&amp;publication=1359-4338&amp;publicationDate=2005-07-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Demirdjian, D., Ko, T. &amp; Darrell, T. Untethered gesture acquisition and recognition for virtual world manipulation.
                    <i>Virtual Reality</i> <b>8, </b>222–230 (2005). https://doi.org/10.1007/s10055-005-0155-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0155-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-12">12 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-09">September 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0155-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0155-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Support Vector Machine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hide Markov Model</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual World</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Support Vector Machine Classifier</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gesture Recognition</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0155-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=155;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

