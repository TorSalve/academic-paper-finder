<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An integrated virtual environment for feasibility studies and implemen"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This work presents a complete framework of an integrated aerial virtual environment (IAVE), which could effectively help implementing MonoSLAM (single-camera simultaneous localization and mapping)..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM"/>

    <meta name="dc.source" content="Virtual Reality 2011 16:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-09-20"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This work presents a complete framework of an integrated aerial virtual environment (IAVE), which could effectively help implementing MonoSLAM (single-camera simultaneous localization and mapping) on an aerial vehicle. The developed system allows investigating different flight conditions without using any preloaded maps or predefined features. A 3D graphical engine integrated with a full 6 DOF aircraft dynamic simulator together with its trajectory generator completes the package. The 3D engine generates and accumulates real-time images of a general camera installed on the aerial vehicle. We effectively exploit C++ to develop the 3D graphics engine (3DGE) and all its associated visual effects, including different types of lighting, climate conditions, and moving objects. The existing 3DGE exploits the so-called Frenet Adapted Frames (FAF) with constrained angular velocities that is very effective in motion modeling of both ground and aerial moving objects. An in-house-developed MATLAB GUI puts into service the offline MonoSLAM system, which is very user friendly. The current version of IAVE effectively employs the so-called Inverse Depth Parameterization notions for features&#8217; depth estimation in monocular SLAM, where different case studies show its dependable results for low-cost aerial navigation of a general aviation low-speed aircraft."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-09-20"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="215"/>

    <meta name="prism.endingPage" content="232"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0197-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0197-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0197-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0197-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2012/09"/>

    <meta name="citation_online_date" content="2011/09/20"/>

    <meta name="citation_firstpage" content="215"/>

    <meta name="citation_lastpage" content="232"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0197-7"/>

    <meta name="DOI" content="10.1007/s10055-011-0197-7"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0197-7"/>

    <meta name="description" content="This work presents a complete framework of an integrated aerial virtual environment (IAVE), which could effectively help implementing MonoSLAM (single-came"/>

    <meta name="dc.creator" content="M. A. Amiri Atashgah"/>

    <meta name="dc.creator" content="S. M. B. Malaek"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran"/>

    <meta name="citation_reference" content="Berndt J (2004) JSBSim: an open source flight dynamics model in C++. In: AIAA Modeling and simulation technologies conference and exhibit, Providence, Rhode Island"/>

    <meta name="citation_reference" content="Berndt JS, The JSBSim Development Team (2009) JSBSim An open source, platform-independent, flight dynamics model in C++, [Online]. 
                    http://jsbsim.sourceforge.net/JSBSimReferenceManual.pdf2009
                    
                  
                "/>

    <meta name="citation_reference" content="Blanco JL (2010) The mobile robot programming toolkit. [Online]. 
                    http://babel.isa.uma.es/mrpt/index.php/Main_Page
                    
                  
                "/>

    <meta name="citation_reference" content="Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. 
                    http://www.vision.caltech.edu/bouguetj/calib_doc
                    
                  
                "/>

    <meta name="citation_reference" content="Brooks FP (1999) What&#8217;s real about virtual reality. University of North Carolina, Chapel Hill"/>

    <meta name="citation_reference" content="Cesetti A, Frontoni E, Mancini A, Zingaretti P, Longhi S (2009) Vision-based autonomous navigation and landing of an unmanned aerial vehicle using natural landmarks. In: 17th mediterranean conference on control &amp; automation, Makedonia Palace, Thessaloniki, Greece, pp 910&#8211;915"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robotics; citation_title=Inverse depth parametrization for monocular SLAM; citation_author=J Civera, JM Montiel, AJ Davison; citation_volume=24; citation_issue=5; citation_publication_date=2008; citation_pages=932-945; citation_id=CR7"/>

    <meta name="citation_reference" content="Conte G, Doherty P (2009) Vision-based unmanned aerial vehicle navigation using geo-referenced information. EURASIP Jo Advances Signal Process vol. 2009, [Online] 
                    http://downloads.hindawi.com/journals/asp/2009/387308.pdf
                    
                  
                "/>

    <meta name="citation_reference" content="Davison AJ, Cid YG, Kita N (2004) Real-time 3D SLAM with wide-angle vision. In: IFAC Symp Intell Autonomous Vehicles 2004, Lisbon, Portugal"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intelligence; citation_title=MonoSLAM: real-time single camera SLAM; citation_author=AJ Davison, ID Reid, ND Molton, O Stasse; citation_volume=29; citation_issue=6; citation_publication_date=2007; citation_pages=1052-1067; citation_id=CR10"/>

    <meta name="citation_reference" content="Diosi A, Kleeman L (2004) Advanced sonar and laser range finder fusion for simultaneous localization and mapping. In: Proceedings of 2004 IEEE/RSJ international conference on intelligent robots and systems, Sept 28&#8211;Oct 2, 2004, Sendai, Japan "/>

    <meta name="citation_reference" content="Ditchburn K (2010) 
                    http://www.Toymaker.Info
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Anim Virtual Worlds; citation_title=Spatial camera orientation control by rotation-minimizing directed frames; citation_author=RT Farouki, C Giannelli; citation_volume=20; citation_publication_date=2009; citation_pages=457-472; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Autonomous Robots; citation_title=A discussion of simultaneous localization and mapping; citation_author=U FRESE; citation_volume=20; citation_publication_date=2006; citation_pages=25-42; citation_id=CR12"/>

    <meta name="citation_reference" content="Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 2009, [Online] 
                    www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf
                    
                  )"/>

    <meta name="citation_reference" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of the 4th Alvey vision conference, Manchester, pp. 147&#8211;151"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Aerosp Electronic Syst; citation_title=Visual search automation for unmanned aerial vehicles; citation_author=EN Johnson, AA Proctor, AR Tannenbaum; citation_volume=41; citation_issue=1; citation_publication_date=2005; citation_pages=2007-2854; citation_id=CR15"/>

    <meta name="citation_reference" content="Johnson AE, Ansar A, Matthies LH, Trawny N, Mourikis AI, Roumeliotis SI (2007) A general approach to terrain relative navigation for planetary landing. In: Aiaa Conference and exhibit, 2007"/>

    <meta name="citation_reference" content="JSBSim Project (2010) [Online]. 
                    www.jsbsim.org
                    
                  
                "/>

    <meta name="citation_reference" content="Jung I, Lacroix S (2003) High resolution terrain mapping using low altitude aerial stereo imagery. In: Proceedings of ninth International Conference Computer Vision, Nice, France"/>

    <meta name="citation_reference" content="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int&#8217;l Conf. Robotics and Automation 1:406&#8211;411"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IME Part G: J Aerosp Eng; citation_title=Decentralized approach to UAV navigation: without the use of GPS and preloaded maps; citation_author=J Kim, S Ong, E Nettleton, S Sukkarieh; citation_volume=218; citation_issue=6; citation_publication_date=2006; citation_pages=399-416; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robotics Automation; citation_title=Mobile robot localization by tracking geometric beacons; citation_author=JJ Leonard, HF Durrant-Whyte; citation_volume=7; citation_issue=3; citation_publication_date=1991; citation_pages=376-382; citation_id=CR20"/>

    <meta name="citation_reference" content="Luna F (2006) Introduction to 3D game programming with direct X 9.0c: a shader approach: wordware game and graphics library. Wordware Publishing, Inc., Plano"/>

    <meta name="citation_reference" content="Montiel JMM, Civera J, Davison AJ (2006) Unified inverse depth parametrization for monocular SLAM. In: Robotics: science and systems, Philadelphia"/>

    <meta name="citation_reference" content="Mourikis AI, Trawny N, Roumeliotis SI, Johnson A, Matthies L (2007) Vision-aided inertial navigation for precise planetary landing: analysis and experiments. In: AIAA conference paper, 2007, Atlanta, GA"/>

    <meta name="citation_reference" content="Rolfe JM, Staples KJ (1986) Flight simulation. Cambridge University Press, Cambridge"/>

    <meta name="citation_reference" content="Roskam J (1987) Airplane design part VI: DAR corporation"/>

    <meta name="citation_reference" content="Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif University of Technology, Tehran, MS Dissertation"/>

    <meta name="citation_reference" content="Sim DG, Jeong SY, Park RH, Kim RC, Lee3 SU, Kim IC (1996) Navigation parameter estimation from sequential aerial images. In: Proceedings 1996 IEEE international conference image processing, vol. II, pp. 629&#8211;632, Lausanne, Switzerland"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Image Process; citation_title=Hybrid estimation of navigation parameters from aerial image sequence; citation_author=D Sim, SY Jeong, D Lee, R Park, R Kim, SU Lee, I Kim; citation_volume=8; citation_issue=3; citation_publication_date=1999; citation_pages=429-435; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Robotics Res; citation_title=On the representation of spatial uncertainty; citation_author=R Smith, P Cheeseman; citation_volume=5; citation_issue=4; citation_publication_date=1987; citation_pages=56-68; citation_id=CR27"/>

    <meta name="citation_reference" content="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse"/>

    <meta name="citation_reference" content="Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York"/>

    <meta name="citation_reference" content="Sunderhauf N, Lange S, Protzel P (2007) Using the unscented Kalman filter in mono-SLAM with inverse depth parametrization for autonomous airship control. In: IEEE international workshop on safety security and rescue robotics, Rome"/>

    <meta name="citation_author" content="M. A. Amiri Atashgah"/>

    <meta name="citation_author_email" content="atashgah@ae.sharif.edu"/>

    <meta name="citation_author_institution" content="Department of Aerospace Engineering, Sharif University of Technology, Tehran, Iran"/>

    <meta name="citation_author" content="S. M. B. Malaek"/>

    <meta name="citation_author_email" content="malaek@sharif.edu"/>

    <meta name="citation_author_institution" content="Department of Aerospace Engineering, Sharif University of Technology, Tehran, Iran"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0197-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0197-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM"/>
        <meta property="og:description" content="This work presents a complete framework of an integrated aerial virtual environment (IAVE), which could effectively help implementing MonoSLAM (single-camera simultaneous localization and mapping) on an aerial vehicle. The developed system allows investigating different flight conditions without using any preloaded maps or predefined features. A 3D graphical engine integrated with a full 6 DOF aircraft dynamic simulator together with its trajectory generator completes the package. The 3D engine generates and accumulates real-time images of a general camera installed on the aerial vehicle. We effectively exploit C++ to develop the 3D graphics engine (3DGE) and all its associated visual effects, including different types of lighting, climate conditions, and moving objects. The existing 3DGE exploits the so-called Frenet Adapted Frames (FAF) with constrained angular velocities that is very effective in motion modeling of both ground and aerial moving objects. An in-house-developed MATLAB GUI puts into service the offline MonoSLAM system, which is very user friendly. The current version of IAVE effectively employs the so-called Inverse Depth Parameterization notions for features’ depth estimation in monocular SLAM, where different case studies show its dependable results for low-cost aerial navigation of a general aviation low-speed aircraft."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0197-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"3D graphics engine, Virtual environment, MonoSLAM, General aviation, Aerial navigation","kwrd":["3D_graphics_engine","Virtual_environment","MonoSLAM","General_aviation","Aerial_navigation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0197-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0197-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=197;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0197-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0197-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0197-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-09-20" itemprop="datePublished">20 September 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-M__A_-Amiri_Atashgah" data-author-popup="auth-M__A_-Amiri_Atashgah" data-corresp-id="c1">M. A. Amiri Atashgah<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sharif University of Technology" /><meta itemprop="address" content="grid.412553.4, 0000000107409747, Department of Aerospace Engineering, Sharif University of Technology, Tehran, Iran" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-S__M__B_-Malaek" data-author-popup="auth-S__M__B_-Malaek">S. M. B. Malaek</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sharif University of Technology" /><meta itemprop="address" content="grid.412553.4, 0000000107409747, Department of Aerospace Engineering, Sharif University of Technology, Tehran, Iran" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">215</span>–<span itemprop="pageEnd">232</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">311 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0197-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This work presents a complete framework of an integrated aerial virtual environment (IAVE), which could effectively help implementing MonoSLAM (single-camera simultaneous localization and mapping) on an aerial vehicle. The developed system allows investigating different flight conditions without using any preloaded maps or predefined features. A 3D graphical engine integrated with a full 6 DOF aircraft dynamic simulator together with its trajectory generator completes the package. The 3D engine generates and accumulates real-time images of a general camera installed on the aerial vehicle. We effectively exploit C++ to develop the 3D graphics engine (3DGE) and all its associated visual effects, including different types of lighting, climate conditions, and moving objects. The existing 3DGE exploits the so-called Frenet Adapted Frames (FAF) with constrained angular velocities that is very effective in motion modeling of both ground and aerial moving objects. An in-house-developed MATLAB GUI puts into service the offline MonoSLAM system, which is very user friendly. The current version of IAVE effectively employs the so-called Inverse Depth Parameterization notions for features’ depth estimation in monocular SLAM, where different case studies show its dependable results for low-cost aerial navigation of a general aviation low-speed aircraft.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In recent years, much progress has been made in the field of “Autonomous Aerial Robot Navigation” and more specifically, simultaneous localization and mapping (SLAM). The latter has become an important research topic for different researchers working in the field of low-cost-dependable Aerial Navigation Systems. However, one of the major challenging issues in the arena of camera-based SLAM (Visual SLAM or in brief, VSLAM) is the ability to conduct low-cost feasibility studies on the performance of the existing algorithms and methods. This work is a comprehensive attempt to bring the state-of-the-art subject matters in the field of VSLAM by developing a complete framework to assess SLAM algorithms performance for aerial applications (Funke and Informatik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 2009, [Online] &#xA;                    www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf&#xA;                    &#xA;                  )" href="/article/10.1007/s10055-011-0197-7#ref-CR13" id="ref-link-section-d9282e301">2009</a>). SLAM was first introduced by Smith and Cheeseman in <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Smith R, Cheeseman P (1987) On the representation of spatial uncertainty. Int J Robotics Res 5(4):56–68" href="/article/10.1007/s10055-011-0197-7#ref-CR27" id="ref-link-section-d9282e304">1987</a>. Since then, the technique has been evolved from indoor robotics applications to other areas, such as air and space. It has also been proposed for exploring unknown environments, where dependable information is not available (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kim J, Ong S, Nettleton E, Sukkarieh S (2006) Decentralized approach to UAV navigation: without the use of GPS and preloaded maps. Proc IME Part G: J Aerosp Eng 218(6):399–416" href="/article/10.1007/s10055-011-0197-7#ref-CR19" id="ref-link-section-d9282e307">2006</a>). SLAM has also been consistently employed in experimental robots, which finally paved the way for its algorithms to being implemented in some practical applications (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Johnson AE, Ansar A, Matthies LH, Trawny N, Mourikis AI, Roumeliotis SI (2007) A general approach to terrain relative navigation for planetary landing. In: Aiaa Conference and exhibit, 2007" href="/article/10.1007/s10055-011-0197-7#ref-CR16" id="ref-link-section-d9282e310">2007</a>). Despite the vast reliance on computer vision, however, not just until very recently, the use of cameras has not been at the center of these research works. This work aims also to expand the SLAM application to the aerial vehicles, which show growing demand to use robotic vision for unknown environment. There are some valuable works in the field of robotics which have been quite helpful in aerial application, which is our primary domain of interest. For example, Davison et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e313">2007</a>) discuss the possibility of moving a sensor platform to construct a proper representation of the environment with a real-time estimation of its motion states. Besides cameras, some applications rely heavily on other type of sensors such as laser range finders and sonar (Leonard and Durrant-Whyte <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Leonard JJ, Durrant-Whyte HF (1991) Mobile robot localization by tracking geometric beacons. IEEE Trans Robotics Automation 7(3):376–382" href="/article/10.1007/s10055-011-0197-7#ref-CR20" id="ref-link-section-d9282e317">1991</a>; Diosi and Kleeman), which are not of direct interest in our work. Nonetheless, we have carefully reviewed all for possible usage.</p><p>It is well noted that current trend of increase in design and development cost of a modern aircraft together with the ever increasing complexity of its operating environment has encouraged broader use of integrated virtual environments (IVE). This approach is expected to be effective to lower the number of tests and to help control the development, training, and deployment cost of new fleets. In fact, developing an IVE is considered to be the first step to control the test and development cost through decreasing the need for extensive flight testing in the actual operating environment (Brooks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Brooks FP (1999) What’s real about virtual reality. University of North Carolina, Chapel Hill" href="/article/10.1007/s10055-011-0197-7#ref-CR5" id="ref-link-section-d9282e323">1999</a>). This is accomplished through proper combination of science and technology together with exploiting proper art to create an artificial realism for the real-world mission planning and pilot training (Rolfe and Staples <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Rolfe JM, Staples KJ (1986) Flight simulation. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-011-0197-7#ref-CR23" id="ref-link-section-d9282e326">1986</a>). Obviously, recent advances in simulation techniques have been quite helpful in this regard.</p><p>An IAVE presented in this work is specifically tailored to help implement MonoSLAM system for general aviation aircraft (GA). It basically consists of two major parts: (1) A flight simulator that replicates the laws of flying with an aerial vehicle as closely as possible coupled with a 3D engine for generating visual images and (2) a MonoSLAM toolbox that exploits the visual images and estimates the navigational parameters of the flying vehicle. The first part is designed to replicate the experience of flying, while the second part helps pilots to fly from one given position to another. The essential segments of each part are briefly explained throughout the paper.</p><h3 class="c-article__sub-heading" id="Sec2">Aerial VSLAM</h3><p>Considering what SLAM offers and our definition of VSLAM, an Aerial VSLAM (A-VSLAM) is therefore a method to construct maps that are suitable for any arbitrary flight over an unknown environment without a priori knowledge. The method could also be used to update the existing map of a known environment with some incomplete information. Simultaneously, the method helps estimate the current navigational data of the vehicle (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Davison AJ, Cid YG, Kita N (2004) Real-time 3D SLAM with wide-angle vision. In: IFAC Symp Intell Autonomous Vehicles 2004, Lisbon, Portugal" href="/article/10.1007/s10055-011-0197-7#ref-CR9" id="ref-link-section-d9282e338">2004</a>; Blanco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e341">2010</a>; Kim and Sukkarieh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics and Automation 1:406–411" href="/article/10.1007/s10055-011-0197-7#ref-CR18" id="ref-link-section-d9282e344">2003</a>; Sola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e347">2007</a>). SLAM as technique consists of numerous steps, such as landmark/feature extraction, feature matching, data association, state estimation by motion or transition model, state update using a predefined algorithm, and finally landmarks update (Blanco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e350">2010</a>). Obviously, one needs to address all these steps for aerial applications for an A-VSLAM. The measurement device for the determination of landmarks’ information as well as other features could be sonar, RADAR, a laser scanner, or even cameras. Nonetheless, sensor and sensor fusion very much depend on the flying environment or precision of the navigation task. Having selected a proper sensor, other steps are followed as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig1">1</a>. With Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig1">1</a>, the vehicle starts navigating in an arbitrary position in an unknown environment and through limited steps identifies the necessary parameters to fly through the region. In this work, the vehicle navigates with just using a single camera mounted on a known position on the aircraft lower skin. As the features are detected from the surroundings, landmark states are augmented by the SLAM methods to a single map to predict both the vehicle and the map states with successive observations. The reason we need to update both the vehicle states and the map is due to the fact that there exist a statistical associations between predictions of the positions of the aircraft and landmarks and simultaneously among the landmarks themselves (Kim and Sukkarieh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics and Automation 1:406–411" href="/article/10.1007/s10055-011-0197-7#ref-CR18" id="ref-link-section-d9282e360">2003</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Depiction of the general structure of Aerial VSLAM (Kim and Sukkarieh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics and Automation 1:406–411" href="/article/10.1007/s10055-011-0197-7#ref-CR18" id="ref-link-section-d9282e373">2003</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec3">Contributions of the current work</h3><p>This work describes all steps to develop a state of the art of VSLAM (Funke and Informatik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 2009, [Online] &#xA;                    www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf&#xA;                    &#xA;                  )" href="/article/10.1007/s10055-011-0197-7#ref-CR13" id="ref-link-section-d9282e394">2009</a>) for aerial applications (A-VSLAM). It shows how to exploit the contemporaneous methods and algorithms in the field of computer graphics. The contributions of this work starts by describing how the idea of MonoSLAM could be extended to cover aerial applications while flying over an unknown region. We also examine the performance of A-VSLAM algorithms with the help of a virtual environment, specifically developed for this purpose. Through some examples, we show how a vision-based navigation systems (VNS) for low-speed general aviation works. The virtual environment, itself, is an improved version of the aerial 3D engine (A3DGE) described by authors Amiri-Atashgah and Malaek in <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e397">2009</a>. The second contribution is in fact the implementation of the so-called Frenet Adapted Frames (FAF) through proper restriction of angular velocities used in path generation of ground and aerial moving objects to enhance the realism of the sceneries.</p><p>The developed MonoSLAM toolset exploits active matching that is critical for real-time operations. It basically includes a Harris feature extraction (Harris and Stephens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of the 4th Alvey vision conference, Manchester, pp. 147–151" href="/article/10.1007/s10055-011-0197-7#ref-CR14" id="ref-link-section-d9282e403">1988</a>), together with a constant velocity motion model (Sola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e406">2007</a>) for smooth 3D camera movements to estimate the navigational parameters of the flying vehicle. By means of the developed toolset, we are able to implement passive probabilistic estimation of the vehicle states during the flight. The attempt is to provide support for safe flights over unknown area with a single low-cost camera installed on the aircraft. The work could well be extended for space robots, although increasing cost would not be a primary concern for space applications.</p><p>As a prerequisite to develop a new 3D engine for aerial applications, authors have examined all available open sources such as OSG, Delta3D, and VTerrain. Some of existing engines are in fact effective to generate terrain and atmospheric effects. Nonetheless, undue effort is usually needed to link such engines to the flight dynamic model and/or vision-in-the-loop improvements that are the ultimate objective. While the former is described here, the latter is still in progress.</p><p>It is quite well known that proper integration of JSBSim and MRPT (Blanco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e414">2010</a>) to open source engines is usually a very challenging task, by itself. Besides, the diversities and levels of complexity of the actual environment influence the user–system interaction (Rolfe and Staples <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Rolfe JM, Staples KJ (1986) Flight simulation. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-011-0197-7#ref-CR23" id="ref-link-section-d9282e417">1986</a>; Stevens and Lewis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York" href="/article/10.1007/s10055-011-0197-7#ref-CR29" id="ref-link-section-d9282e420">2003</a>; Saghafi and Amiri-Atashgah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif University of Technology, Tehran, MS Dissertation" href="/article/10.1007/s10055-011-0197-7#ref-CR24" id="ref-link-section-d9282e423">2000</a>). Nevertheless, using the developed engine, we are able to generate and save the sequential images to be employed in MonoSLAM toolbox or generate Rawlogs (Blanco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e426">2010</a>) for MRPT-based version of the project. We find Rawlogs extremely useful to form binary file formats. These files that come with “Rawlogs” suffix could store robotic datasets and serve as input of many MRPT applications for future offline processing. Such capability is a must for aerial applications, where safety and ability to exchange experiences are essential. The extension of this work, also, aims to bring real-time scenarios to the existing IAVE using the MRPT and C++.</p><h3 class="c-article__sub-heading" id="Sec4">Related works</h3><p>In this section, we briefly overview the related works. Here, we limit ourselves to those specifically important for aerial applications. Aerial VSLAM could be found in quite a few references. Nonetheless, most of them come short to consider aerial vehicles dynamics or otherwise consider a very well-behaved aerial vehicle. For example, Jung and Lacroix (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Jung I, Lacroix S (2003) High resolution terrain mapping using low altitude aerial stereo imagery. In: Proceedings of ninth International Conference Computer Vision, Nice, France" href="/article/10.1007/s10055-011-0197-7#ref-CR17" id="ref-link-section-d9282e437">2003</a>) present a VSLAM system using a downward-looking stereo camera to localize a robotic airship. The method is offline and does not support real-time operation. “Features range measurements” also is performed directly using the stereo camera. On the other hand, Kim and Sukharieh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics and Automation 1:406–411" href="/article/10.1007/s10055-011-0197-7#ref-CR18" id="ref-link-section-d9282e440">2003</a>) use monocular vision infused with a precise inertial sensing for a maneuvering UAV. Here, the landmarks are synthetically placed on the scene, and estimation of their locations is made easier by assuming that they lie on a flat surface.</p><p>There are some works specifically on decentralized approach for UAV navigation that use bearing-only methods (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kim J, Ong S, Nettleton E, Sukkarieh S (2006) Decentralized approach to UAV navigation: without the use of GPS and preloaded maps. Proc IME Part G: J Aerosp Eng 218(6):399–416" href="/article/10.1007/s10055-011-0197-7#ref-CR19" id="ref-link-section-d9282e446">2006</a>). Such works would not make use of GPS data or preloaded maps. Davison et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e449">2007</a>) present a real-time single-camera SLAM that describes the main framework of this line of research. However, they limit themselves to work with just a 3D feature states method. This simply implies that the effects related to updating depths of the features within the process of the SLAM algorithm are ignored. Nonetheless, the important part of their work is that the camera model is in pixel basis that is more sensible for vision-based activities. Montiel et al. in Civera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e452">2008</a>) describe the so-called Inverse Depth Parameterization that uses an active feature depth estimation with a specific camera model. Their approach serves as a clue, which encourages one to use a mixed method using both Davison et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e455">2007</a>) and Civera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e458">2008</a>). The latter, in fact, forms the basis of our work in developing the MonoSLAM toolbox in MATLAB. In brief, we take a pixel-based camera model from Davison et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e462">2007</a>) and the “feature depth estimation” algorithms from Civera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e465">2008</a>) to handle the issues for aerial applications. The details are presented in the next sections.</p><p>There are other resources that revolve around SLAM as a general technique. For example, FRESE (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="FRESE U (2006) A discussion of simultaneous localization and mapping. Autonomous Robots 20:25–42" href="/article/10.1007/s10055-011-0197-7#ref-CR12" id="ref-link-section-d9282e471">2006</a>) describes the structure of the SLAM in general. It also provides a good discussion that is based on both informal studies and mathematical derivations of the method. There are other references that concentrate on space systems and rely on multisensor methods that are costly enough to fall beyond the scope of this work. Here, we describe those that observe “cost” as a factor.</p><p>Amiri-Atashgah and Malaek (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e477">2009</a>) investigate the possibility of augmenting an Unmanned Aerial Vehicle (UAV) navigation system with a passive video camera to cope with long-term GPS outages. It proposes a vision-based navigation architecture that combines inertial sensors, visual odometers, and registration of the onboard video to form a geo-referenced aerial image. Autonomous flights of an airship by means of SLAM are discussed in Sunderhauf et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sunderhauf N, Lange S, Protzel P (2007) Using the unscented Kalman filter in mono-SLAM with inverse depth parametrization for autonomous airship control. In: IEEE international workshop on safety security and rescue robotics, Rome" href="/article/10.1007/s10055-011-0197-7#ref-CR30" id="ref-link-section-d9282e480">2007</a>). Airships enjoy a very slow dynamics and fly in low speed. Nonetheless, it is quite useful for very low-speed aircraft. On the other hand, both Sim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Sim D, Jeong SY, Lee D, Park R, Kim R, Lee SU, Kim I (1999) Hybrid estimation of navigation parameters from aerial image sequence. IEEE Trans Image Process 8(3):429–435" href="/article/10.1007/s10055-011-0197-7#ref-CR25" id="ref-link-section-d9282e483">1999</a>) and (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Sim DG, Jeong SY, Park RH, Kim RC, Lee3 SU, Kim IC (1996) Navigation parameter estimation from sequential aerial images. In: Proceedings 1996 IEEE international conference image processing, vol. II, pp. 629–632, Lausanne, Switzerland" href="/article/10.1007/s10055-011-0197-7#ref-CR26" id="ref-link-section-d9282e486">1996</a>) describe a hybrid method for navigational parameters estimation using sequential aerial stereo images and DEM data. The proposed parameter estimation includes both relative and absolute position but not the relative orientation with respect to the Earth.</p><p>Some recent NASA works are also related to the “Terrain Relative Navigation” for the planetary landing (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Johnson AE, Ansar A, Matthies LH, Trawny N, Mourikis AI, Roumeliotis SI (2007) A general approach to terrain relative navigation for planetary landing. In: Aiaa Conference and exhibit, 2007" href="/article/10.1007/s10055-011-0197-7#ref-CR16" id="ref-link-section-d9282e493">2007</a>; Mourikis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mourikis AI, Trawny N, Roumeliotis SI, Johnson A, Matthies L (2007) Vision-aided inertial navigation for precise planetary landing: analysis and experiments. In: AIAA conference paper, 2007, Atlanta, GA" href="/article/10.1007/s10055-011-0197-7#ref-CR22" id="ref-link-section-d9282e496">2007</a>). They concentrate on a hybrid navigation method using an inertial sensor in addition to a series of terrain-related data of elevation and aerial images. As they demand a very high “Factor of Safety,” their method is effective in known environments. In this line of works, Johnson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Johnson EN, Proctor AA, Tannenbaum AR (2005) Visual search automation for unmanned aerial vehicles. IEEE Trans Aerosp Electronic Syst 41(1), Rohnert Park, CA AIAA, 2007–2854" href="/article/10.1007/s10055-011-0197-7#ref-CR15" id="ref-link-section-d9282e499">2005</a>) present a hybrid GPS/INS/Vision-based navigation for an Unmanned Aerial Vehicle (UAV) with automated capabilities. It describes how to search a prescribed area and to identify a specific building within the area.</p><p>More recently, Cesetti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Cesetti A, Frontoni E, Mancini A, Zingaretti P, Longhi S (2009) Vision-based autonomous navigation and landing of an unmanned aerial vehicle using natural landmarks. In: 17th mediterranean conference on control &amp; automation, Makedonia Palace, Thessaloniki, Greece, pp 910–915" href="/article/10.1007/s10055-011-0197-7#ref-CR6" id="ref-link-section-d9282e505">2009</a>) present the design and implementation of a vision-based navigation as well as landing algorithm for an autonomous helicopter. In the simulation section of the work, a VRML-based virtual environment is described, which is very simple and lacks some real-world conditions such as lakes, rivers, and different terrain texturing. Regarding the works on SLAM algorithms, the most critical part is the 3D graphics engine (Funke and Informatik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 2009, [Online] &#xA;                    www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf&#xA;                    &#xA;                  )" href="/article/10.1007/s10055-011-0197-7#ref-CR13" id="ref-link-section-d9282e508">2009</a>); therefore, we have paid a careful attention to have what is needed for a thorough preflight briefing. The current work, while using the basics of the existing open source engines, attempts to create enough potential for real-time vision-in-the-loop scenarios. The next sections provide the details.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Role of aerial vehicle dynamics in IAVE</h2><div class="c-article-section__content" id="Sec5-content"><p>The fundamental objective of a standard IAVE is essentially to manage a group of flying aircraft over a predefined aerodrome, a field known as Air Traffic Management (ATM). The cornerstone of ATM relies on navigating each individual aircraft in the given airspace with respect to the constraints imposed by other flying aircraft in the same region (Amiri-Atashgah and Malaek <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e520">2009</a>). Incorporating the effects of aircraft speed, altitude and their existing paths and maneuvering capabilities, therefore, contribute to the building blocks of the IAVE. Obviously, understanding the role of vehicle dynamics is the key, which is highlighted in this work. However, aircrafts come with very vast dynamic capabilities from very low speed to supersonic and for each specific speed range and maneuvering capabilities, suitable hardware is needed. Here, we consider a general aviation aircraft at low flight speeds below the transonic speed to be able to use low-cost cameras.</p><p>The developed six Degrees-of-Freedom (DOF) dynamic model is based on the study by Roskam and is modified in accordance with IAVE enhancements. Following subsection describes the steps, in brief.</p><p>With Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig2">2</a>, and knowing the forces and moments acting on a vehicle, one might use the Newton’s second law to develop the equations of motion of an aircraft (Roskam <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Roskam J (1987) Airplane design part VI: DAR corporation" href="/article/10.1007/s10055-011-0197-7#ref-CR33" id="ref-link-section-d9282e531">1987</a>). This approach requires, also, a suitable model for the rotation of the Earth (or any other planet we are flying over) to account for the gravity-related forces. A proper set of axes is also needed to describe the relative motion of the vehicle with respect to the Earth, itself. We need the frames of references to describe the forces as well as objects. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig2">2</a>, these frames are (1) <i>Body axes/frame</i>: This is a right-hand system (RHS) that is located at the aircraft C.G. with positive direction of the <i>x</i>-axis toward nose of aircraft and the <i>z</i>-axis is positive downward; (2) <i>Stability axes/frame</i>: This frame is similar to that of the body frame, but the <i>x</i>-axis points into the relative wind vector projected onto the XY plane of symmetry for the aircraft. The <i>y</i>-axis points out to the right wing and with the <i>z</i>-axis complete a right-hand system; (3) <i>Wind axes/frame</i>: This frame is similar to that of stability frame, except that the <i>x</i>-axis points directly into the relative wind; (4) <i>Structural axes/frame</i>: Again a RHS frame with <i>x</i>-axis from nose toward the tail and a <i>z</i>-axis with positive upwards. The selection of its origin, however, demands some experience, but if normally close to the frontal part of the aircraft. The <i>x</i>-axis is expected to remain coincident with the fuselage centerline and passes through the propeller hub (thrust axis). This frame is important as it is used to define all massy items on the aircraft, such as locations of the gears, engines, pilot and pilot’s eye, and the aircraft center of gravity (C.G.); (5) <i>Camera axes/frame</i>: This is a RHS with its origin at the camera lens and with <i>x</i>-axis pointing outward; and (6) <i>World or Inertial axes/frame</i>: This is a RHS that is fixed in space in such a way that Newton’s second law is applicable within its domain.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Depiction of the overall description of <b>a</b> structural frame, <b>b</b> wind and body axes, <b>c</b> aerodynamic moments, and <b>d</b> aerodynamic forces applied on the craft</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The coordinate systems in which we represent the equations of motion are the so-called wind and body-axis systems (Roskam <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Roskam J (1987) Airplane design part VI: DAR corporation" href="/article/10.1007/s10055-011-0197-7#ref-CR33" id="ref-link-section-d9282e622">1987</a>). To get the simulated navigation parameters of an aircraft and compare with those of vision-based navigation results, we use JSBSim that is a multiplatform open source simulation package (Berndt and The JSBSim Development Team <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Berndt JS, The JSBSim Development Team (2009) JSBSim An open source, platform-independent, flight dynamics model in C++, [Online]. &#xA;                    http://jsbsim.sourceforge.net/JSBSimReferenceManual.pdf2009&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR3" id="ref-link-section-d9282e625">2009</a>). JSBSim comes with a flight dynamics model (FDM) framework, which is written in C++ and supports simulation an arbitrary aircraft without the need for specific compiled and linked program code. However, the model is relatively simple written in an extensible markup language (XML) format (Berndt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Berndt J (2004) JSBSim: an open source flight dynamics model in C++. In: AIAA Modeling and simulation technologies conference and exhibit, Providence, Rhode Island" href="/article/10.1007/s10055-011-0197-7#ref-CR2" id="ref-link-section-d9282e628">2004</a>). JSBSim enjoys configurable simulation functionalities. It incorporates inertia and mass balance, aircraft geometry, flight control system, aerodynamics, propulsion, landing gear, and contact point arrangements. Effects of the Earth rotation on the equations of motion in terms of centrifugal accelerations are included in the model. Further enhancement is quite possible to have vision-in-the-loop and distributed simulation functionalities. In our work, the related C++ frameworks are integrated into our 3D engine to enhance flying in an unknown environment.</p><h3 class="c-article__sub-heading" id="Sec6">Aircraft equations of motion</h3><p>The rigid body equations of motion are in fact a set of differential equations that describe how states of an aircraft change during a flight. The aircraft states are generally the scalar components of its linear and angular velocities, its C.G. position vector, and finally the Quaternion vector that all defines the orientation of the aircraft relative to the Earth. With (Stevens and Lewis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York" href="/article/10.1007/s10055-011-0197-7#ref-CR29" id="ref-link-section-d9282e639">2003</a>; Sunderhauf et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sunderhauf N, Lange S, Protzel P (2007) Using the unscented Kalman filter in mono-SLAM with inverse depth parametrization for autonomous airship control. In: IEEE international workshop on safety security and rescue robotics, Rome" href="/article/10.1007/s10055-011-0197-7#ref-CR30" id="ref-link-section-d9282e642">2007</a>; Conte and Doherty <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Conte G, Doherty P (2009) Vision-based unmanned aerial vehicle navigation using geo-referenced information. EURASIP Jo Advances Signal Process vol. 2009, [Online] &#xA;                    http://downloads.hindawi.com/journals/asp/2009/387308.pdf&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR8" id="ref-link-section-d9282e645">2009</a>) the aircraft translational acceleration vector, is expressed in the body axis as follows</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {\dot{\mathbf{V}}}_{B}^{b} &amp; = \frac{1}{m}\;{\mathbf{F}}_{B}^{b} - ([{\varvec{\omega}}_{B}^{b} ] + [{\text{DCM}}]^{bi} [{\varvec{\omega}}_{E}^{i} ][{\text{DCM}}]^{bi\;T} ){\mathbf{V}}_{B}^{b} \\ &amp; \quad + [{\text{DCM}}]^{bi} ({\mathbf{g}}^{i} - [{\varvec{\omega}}_{E}^{i} ][{\varvec{\omega}}_{E}^{i} ]{\mathbf{P}}^{i} ) \\ \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (2.1)
                </div></div><p>where <b>F</b>
                  <span class="c-stack">
                    <sup><i>b</i></sup><sub>
                      <i>B</i>
                    </sub>
                    
                  </span> is the total force vector due to the surrounding atmosphere, propulsion system plus any aerodynamic control devices acting on the aircraft. <b>V</b>
                  <span class="c-stack">
                    <sup><i>b</i></sup><sub>
                      <i>B</i>
                    </sub>
                    
                  </span> and <b>P</b>
                  <sup><i>i</i></sup> are, respectively, the linear velocity and the position vectors, while <b>ω</b>
                  <span class="c-stack">
                    <sup><i>i</i></sup><sub>
                      <i>E</i>
                    </sub>
                    
                  </span> and <span class="mathjax-tex">\( {\varvec{\omega}}_{B}^{b} \)</span> are the angular velocities of Earth, and [DCM]<sup><i>bi</i></sup> is the rotation matrix from Earth-Centered Inertial (ECI) to the body-axis system, and finally, <b>g</b>
                  <sup><i>i</i></sup> is the gravity acceleration vector.</p><p>Similarly, for angular acceleration, one can write</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\dot{\mathbf{\omega }}} = - [J^{b} ]^{ - 1} [\omega_{B}^{b} ][J^{b} ]{\varvec{\omega}}_{B}^{b} + [J^{b} ]^{ - 1} {\mathbf{M}}_{B}^{b} . $$</span></div><div class="c-article-equation__number">
                    (2.2)
                </div></div><p>This is in fact the time derivative of the angular velocity of the aircraft body relative to the inertial frame, where <span class="mathjax-tex">\( {\mathbf{M}}_{B}^{b} \)</span> represents the non-gravitational body forces and moments and [<i>J</i>
                  <sup><i>b</i></sup>] is the cross-product inertia matrix together with <span class="mathjax-tex">\( {\varvec{\omega}}_{B}^{b} \)</span> as the angular velocity of the body relative to the inertial frame measured in the body-axes system.</p><p>Next, we need a suitable mathematical description of acting forces and moments on the aircraft. These forces and moments fall into three different categories of (1) gravitational forces, (2) propulsion forces, and (3) aerodynamic forces. The latter, with the help of the science of aerodynamics, gives us the aerodynamic forces and moments acting on the aircraft in terms of Lift, Drag and Longitudinal and Lateral Moments. A simplified representation of these aerodynamic forces and moments are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig2">2</a>. Lift is a vector that is perpendicular to the direction of the velocity vector acting against the Weight. Similarly, Drag is a vector that is perpendicular to the direction of the Lift, acting against the Thrust (Saghafi and Amiri-Atashgah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif University of Technology, Tehran, MS Dissertation" href="/article/10.1007/s10055-011-0197-7#ref-CR24" id="ref-link-section-d9282e791">2000</a>). Other forces and moments coming from propulsion system could easily be modeled based on available experimental data. Finally, the gravitational forces are extracted based on Earth gravitational field that is normally constant for low-altitude flights. However, we still need some forces and moments to control the direction of flight, which comes from aircraft control system through changing the aerodynamic forces and moments.</p><p>As an input device to guide and control the direction of flight, we use a joystick control tool. An autopilot accepts successive commands and helps the aircraft follow the predefined waypoints defined in the mission planner subroutine. User can either select a full-control mode or let the autopilot follow the predefined path. It should be noted that user commands need to be normalized to respect the maximum control surface deflection limits, which is essential to prevent any control surface saturation or violating validity of the aerodynamic model.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Aerial monocular SLAM</h2><div class="c-article-section__content" id="Sec7-content"><p>To observe practical flight scenarios, we only model an outward-looking sensor for the vision algorithm. The key concept in this work comes from combining the methods presented in Blanco (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e805">2010</a>), Stevens and Lewis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York" href="/article/10.1007/s10055-011-0197-7#ref-CR29" id="ref-link-section-d9282e808">2003</a>), and (Civera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e811">2008</a>). Its core is based on the so-called Probabilistic Feature-based Map that stands for a snapshot of the current predictions of the state of the camera and all features of interest at any moment. It also handles the uncertainty in these estimates, which regularly occurs in the aerial applications. The relevant maps are initialized at the system start-up and remain active until the whole operation ends. However, such maps are updated by the extended Kalman filter (EKF) to cover for any possible changes. In fact, the probabilistic state estimates of the camera and features are updated during aircraft motion and features observation. As soon as new features are added, the map is expanded with the help of new states. The probabilistic character of the map lies in its ability to propagate over time.</p><p>In EKF-SLAM, the map is accurately represented by a state vector <span class="mathjax-tex">\( \hat{x} \)</span> and a covariance matrix <b>P</b>. State vector <span class="mathjax-tex">\( \hat{x} \)</span> consists of a random state vector containing the aircraft existing position in addition to the currently mapped landmark positions, so</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\hat{\mathbf{X}}} = \left( {\begin{array}{*{20}c} {{\hat{\mathbf{X}}}_{A} } \\ {{\hat{\mathbf{X}}}_{M} } \\ \end{array} } \right),\;{\mathbf{P}} = \left( {\begin{array}{*{20}c} {{\mathbf{P}}_{AA} } &amp; {{\mathbf{P}}_{AM} } \\ {{\mathbf{P}}_{MA} } &amp; {{\mathbf{P}}_{MM} } \\ \end{array} } \right) $$</span></div><div class="c-article-equation__number">
                    (3.1)
                </div></div><p>where <span class="mathjax-tex">\( {\mathbf{X}}_{A} = (\begin{array}{*{20}c} {P_{B}^{W} } &amp; {{\mathbf{v}}_{B}^{W} } &amp; {a_{B}^{b} } &amp; {q_{B}^{b} } &amp; {{\varvec{\omega}}_{B}^{b} } \\ \end{array} ), \)</span> the aircraft state is a vector containing position <span class="mathjax-tex">\( (P_{B}^{W} ), \)</span> velocity <span class="mathjax-tex">\( ({\mathbf{v}}_{B}^{W} ), \)</span> acceleration <span class="mathjax-tex">\( (a_{B}^{W} ), \)</span> a quaternion defining orientation <span class="mathjax-tex">\( (q_{B}^{b} ), \)</span> and angular velocity <span class="mathjax-tex">\( ({\varvec{\omega}}_{B}^{b} ). \)</span> These states and the related covariance are updated during the movement of the aircraft, visiting already mapped landmarks while adding new ones during the flight.</p><p>The evolution of the aircraft position during one time step is expressed by a transition/motion model as follows:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\frown}$}}{X} (k + 1) &amp; = f(\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\frown}$}}{X} (k),u(k)) \\ &amp; = \left( {\begin{array}{*{20}c} {P_{B}^{W} (k + 1)} \\ {\begin{array}{*{20}c} {{\mathbf{v}}_{B}^{W} (k + 1)} \\ {a_{B}^{W} (k + 1)} \\ \end{array} } \\ {q_{B}^{b} (k + 1)} \\ {{\varvec{\omega}}_{B}^{b} (k + 1)} \\ \end{array} } \right) = \left( {\begin{array}{*{20}c} {P_{B}^{W} (k) + {\mathbf{v}}_{B}^{W} (k)\Updelta t + \tfrac{1}{2}a_{B}^{W} (k)\Updelta t^{2} } \\ {\begin{array}{*{20}c} {{\mathbf{v}}_{B}^{W} (k) + a_{B}^{W} (k)\Updelta t} \\ {a_{B}^{W} (k) + \upsilon_{a} } \\ \end{array} } \\ {q_{B}^{b} (k) \times {\mathbf{q}}_{B}^{b} ((\omega_{B}^{b} + \upsilon_{\omega } )\Updelta t)} \\ {{\varvec{\omega}}_{B}^{b} (k) + \upsilon_{\omega } } \\ \end{array} } \right) \\ \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (3.2)
                </div></div><p>From the EKF formulation, one needs to calculate following steps, where related Jacobians are calculated using the method proposed in the study by Sola (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e938">2007</a>):</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \eqalign{P_{{XX}} (k + 1) &amp;  = F_{X} (k)P_{{XX}} (k)F_{X} (k) + F_{u} (k)U(k){\text{ }}F_{u} (k) \cr P_{{XM}} (k + 1) &amp;  = F_{X} (k)P_{{XM}} (k) \cr P_{{MM}} (k + 1) &amp;  = P_{{MM}} (k) \cr} $$</span></div><div class="c-article-equation__number">
                    (3.3)
                </div></div>
              <p>The Inverse Depth Parameterization (IDP) introduced by Montiel et al. (Civera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e957">2008</a>) comes quite handy to fully initialize a landmark after it has been observed for the first time, and its unknown depth needs to be estimated. In this section, we give a summary of IDP technique. With Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig3">3</a>b, any point <i>i</i> on the 3D scene is defined by a state vector as</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ X_{{\mathbf{i}}} = (\begin{array}{*{20}c} {x_{i} } &amp; {y_{i} } &amp; {z_{i} } &amp; {\theta_{i} } &amp; {\phi_{i} } &amp; {\rho_{i} } \\ \end{array} )^{T} $$</span></div><div class="c-article-equation__number">
                    (3.4)
                </div></div><p>with </p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ X_{i}^{b} - \,\tfrac{1}{\rho }m(\theta_{i} ,\varphi_{i} ) $$</span></div><div class="c-article-equation__number">
                    (3.5)
                </div></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Depiction of <b>a</b> camera position on the aircraft and transformation components and <b>b</b> feature parameterization and measurement equation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Observing a point <i>X</i>
                <sub>
                  <i>i</i>
                </sub> from camera location defines a ray expressed in the camera frame (Civera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e1031">2008</a>) as</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{h}}^{C} = \left( {\begin{array}{*{20}c} {h_{x} } &amp; {h_{y} } &amp; {h{}_{z}} \\ \end{array} } \right) $$</span></div><div class="c-article-equation__number">
                    (3.6)
                </div></div>
                <div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{h}}^{C} = [{\text{DCM}}]_{W}^{C} \left( {\rho_{i} \left( {X_{i}^{W} - P_{C}^{W} } \right) + {\mathbf{m}}(\begin{array}{*{20}c} {\theta_{i} } &amp; {\phi_{i} } \\ \end{array} )^{T} } \right) $$</span></div><div class="c-article-equation__number">
                    (3.7)
                </div></div><p>where <span class="mathjax-tex">\( [{\text{DCM}}]_{W}^{C} \)</span> is a rotation matrix from world (inertial frame) to the camera frame. We use a pinhole model for camera. In this approach, the projection of any landmark in the image frame (Civera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e1072">2008</a>) is observed through projection on the normalized retina as (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e1075">2007</a>)</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \xi = {{h_{x} } \mathord{\left/ {\vphantom {{h_{x} } {h{}_{z}}}} \right. \kern-\nulldelimiterspace} {h{}_{z}}},\quad \varsigma = {{h_{y} } \mathord{\left/ {\vphantom {{h_{y} } {h{}_{z}}}} \right. \kern-\nulldelimiterspace} {h{}_{z}}} $$</span></div><div class="c-article-equation__number">
                    (3.8)
                </div></div>
              <p>Then, it is applied for the camera calibration to produce the pixel coordinates for the observed point (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e1094">2007</a>):</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{h}} = \left( {\begin{array}{*{20}c} u \\ v \\ \end{array} } \right) = \left( {\begin{array}{*{20}c} {u_{0} - f_{{k_{u} }} \xi } \\ {v_{0} - f_{{k_{v} }} \varsigma } \\ \end{array} } \right) $$</span></div><div class="c-article-equation__number">
                    (3.9)
                </div></div>
              <p>In (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0197-7#Equ11">3.9</a>), <span class="mathjax-tex">\( f_{{k_{u} }} ,\;f_{{k_{v} }} ,\;u_{0} ,\;{\text{and}}\;v_{0} \)</span> are the four standard camera calibration parameters. Nevertheless, we still need a camera-distortion model to account for camera lenses behavior in real environments. In this work, we use the standard two parameters distortion model, which is common in vision-based SLAM activities.</p><h3 class="c-article__sub-heading" id="Sec8">Frame transformations</h3><p>During simulations, we constantly need to conduct transformation among different frames; especially, among world (inertia frame), body frame, and camera frame. As a matter of fact, all produced images need to be in the sense of camera frame. In general, the position vector of the vehicle in world frame <span class="mathjax-tex">\( ({\mathbf{X}}_{B}^{W} ), \)</span> the rotation matrix of the vehicle with respect to the world frame <span class="mathjax-tex">\( ([{\text{DCM}}]_{W}^{B} ), \)</span> the rotation matrix of the camera with respect to the body frame <span class="mathjax-tex">\( ([{\text{DCM}}]_{C}^{B} ), \)</span> and relative position of the camera on the aircraft <span class="mathjax-tex">\( ({\mathbf{X}}_{C}^{B} ) \)</span> are related through (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig3">3</a>a)</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{X}}_{C}^{W} = {\mathbf{X}}_{B}^{W} + {\mathbf{X}}_{C}^{B} $$</span></div><div class="c-article-equation__number">
                    (3.10)
                </div></div>
                  <div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ [{\text{DCM}}]_{W}^{C} = [{\text{DCM}}]_{B}^{C} [{\text{DCM}}]_{W}^{B} $$</span></div><div class="c-article-equation__number">
                    (3.11)
                </div></div><p>where <span class="mathjax-tex">\( {\mathbf{X}}_{C}^{W} \;{\text{and}}\;[{\text{DCM}}]_{W}^{C} \)</span> are, respectively, the position vector and rotation matrix of the camera with respect to the world frame.</p><h3 class="c-article__sub-heading" id="Sec9">Camera model</h3><p>For the MonoSLAM camera model, we use focus distances in pixels, which is more common (Bouguet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. &#xA;                    http://www.vision.caltech.edu/bouguetj/calib_doc&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR34" id="ref-link-section-d9282e1227">2010</a>). This approach, however, demands the ordinary approach to compute the Jacobians to be modified accordingly and must remain consistent throughout the work. This is very important, as in this work we are able to use different fields of views for selected camera. On the other hand, we note that the accuracy of SLAM is significantly improved as the field of view is increased (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Davison AJ, Cid YG, Kita N (2004) Real-time 3D SLAM with wide-angle vision. In: IFAC Symp Intell Autonomous Vehicles 2004, Lisbon, Portugal" href="/article/10.1007/s10055-011-0197-7#ref-CR9" id="ref-link-section-d9282e1230">2004</a>). However, increasing field of view causes significant distortion in the image and must be handled properly, as straight lines in the 3D world do not appear as straight lines in the image (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e1233">2007</a>). For aerial applications, we find it more effective to perform “feature matching” on these raw images as oppose to un-distorting techniques. However, we need to calibrate the virtual camera(s) using different images from diverse viewpoints and directions.</p><p>There are several toolboxes in MATLAB and C++ to exploit as a camera calibration gadget that can be found on Blanco (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e1239">2010</a>) and Bouguet (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. &#xA;                    http://www.vision.caltech.edu/bouguetj/calib_doc&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR34" id="ref-link-section-d9282e1242">2010</a>). Different case studies conducted by the authors show that using the MATLAB toolbox (Bouguet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. &#xA;                    http://www.vision.caltech.edu/bouguetj/calib_doc&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR34" id="ref-link-section-d9282e1245">2010</a>) is quite handy and very effective for aerial applications when it comes to camera models.</p><h3 class="c-article__sub-heading" id="Sec10">IAVE structure and building blocks</h3><p>The main components of IAVE are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig4">4</a>a, and the details are described in Amiri-Atashgah and Malaek (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e1259">2009</a>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig5">5</a>). Here, we briefly go over the parts needed for aerial applications. As shown, we first need to plan a flight mission profile (FMP) using available tools (Amiri-Atashgah and Malaek <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e1265">2009</a>). Once the selected FMP is approved, we run the flight gadget. This leads to the series of images that correspond to the flight together with their navigation parameters of the aircraft, collected separately. All the necessary information is stored in some preselected files (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig6">6</a>b–f, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig7">7</a>a–f). At this stage, we are ready to embark on the MATLAB tools, by running the core script. The main GUI is started, and the estimation process begins with indexing the starting point and setting the initial values of the navigational parameters together with their related uncertainties. The outcomes, as estimated, are drawn with the help of appropriate routines of GUI (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig8">8</a>a–c). The description of the framework is discussed in the following subsection.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>
                          <b>a</b> Schematic presentation of the main components of IAVE and <b>b</b> detailed architecture of the IAVE and its main building blocks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>A flight path generated by spatial curves (Frenet frame) for a Flying Vehicle</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>
                          <b>a</b> A typical mission profile (<i>red line</i>) generated in the Mission Design Package, <b>b</b>–<b>f</b> Snapshots of generated images at different flight/climatic conditions for demonstration of precipitation transition effects at the same scene. <b>a</b> A use-selected flight profile, <b>b</b> A rainy weather, <b>c</b> Rain effects on the terrain textures of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig7">7</a>b, <b>d</b> A clean semi cloudy weather, <b>e</b> A snowy weather, <b>f</b> Snow effects on the terrain textures of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig7">7</a>e</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Snapshots of the generated image sequences at some adopted flight times. <b>a</b> flight time = 0 s, <b>b</b> flight time = 30 s, <b>c</b> flight time = 100 s, <b>d</b> flight time = 500 s, <b>e</b> flight time = 1,200 s, <b>f</b> flight time = 1,250 s</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>
                          <b>a</b> An overall depiction of the user interface in MonoSLAM toolbox in MATLAB, <b>b</b> 2D map, and <b>c</b> 3D map of observed and matched landmarks and their uncertainties in the image plane</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">Description of the virtual framework</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig4">4</a>b demonstrates the overall architecture of the IAVE of this work. The tasks associated with main building blocks are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">(1)</span>
                      
                        <p>3DGE (Amiri-Atashgah and Malaek <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e1467">2009</a>): This module handles different initializations needed for a flight session. This includes time (local as well as GMT), weather condition and sky climatic patterns, and finally the “Terrain Texturing.” Final rendering, however, takes place in the 3D terrain engine (3DTE), with the support of “Weather-Engine” and “Effects-Manager.”</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(2)</span>
                      
                        <p>JSBSim (Berndt and The JSBSim Development Team <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Berndt JS, The JSBSim Development Team (2009) JSBSim An open source, platform-independent, flight dynamics model in C++, [Online]. &#xA;                    http://jsbsim.sourceforge.net/JSBSimReferenceManual.pdf2009&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR3" id="ref-link-section-d9282e1481">2009</a>): This module handles the core operations related to the aerial simulation and Waypoint Navigation System (WNS). It initiates itself with loading the XML-based aircraft configuration files together with other necessary data to initiate flying the aircraft. Two steps are then executed: (a) aircraft trimming for both longitudinal as well as lateral directions and (b) incorporating the terrain engine to estimate aircraft height above the ground. As soon as these steps are completed, the simulation loop starts by receiving preselected commands from WNS. At each step of calculations, the aircraft position and orientation, as viewed by the mounted camera on the vehicle, are reported to the 3DGE block to produce the related images.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(3)</span>
                      
                        <p>MRPT (Blanco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Blanco JL (2010) The mobile robot programming toolkit. [Online]. &#xA;                    http://babel.isa.uma.es/mrpt/index.php/Main_Page&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR4" id="ref-link-section-d9282e1495">2010</a>): This module uses the aircraft states and image sequences, produced by JSBSim and 3DGE, to generate a consistent set of data for MonoSLAM toolbox and its associated navigational purposes.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(4)</span>
                      
                        <p>MonoSLAM (Sola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e1509">2007</a>; Civera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e1512">2008</a>) and Toolbox (Sola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e1515">2007</a>): This module, as the final block, uses the MRPT input datasets to evaluate the performance of the SLAM and vision Toolboxes.</p>
                      
                    </li>
                  </ol>
                <p>Details of aforementioned modules are further discussed in the following subsections.</p><h3 class="c-article__sub-heading" id="Sec12">The graphics engine (GE) toolsets</h3><p>In constructing all virtual models from a simple cube up to sophisticated ones, a common rule is the attempt to depict the objects with limited but definite data leading to unique and unambiguous model. This simple rule requires an efficient programming platform and toolset for the 3DGE to generate images that are deterministic in flight. We first investigate some of the well-known open source packages in the scientific communities;</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">(1)</span>
                      
                        <p>
                          <i>FlightGear</i>: This is an open source flight simulator; by means of which, one might exploit a preimplemented JSBSim FDM to estimate an aerial vehicle navigation parameters. However, the package is rather weak when it comes to the quality of scene rendering. It uses large terrain datasets with a low-precession data in their DEMs (30 Arc Sec).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(2)</span>
                      
                        <p>OSG, Delta3D, and VTerrain: These 3D engines, although with a strong scientific background in their structure and database handling techniques, are quite similar in the sense that they are considerably poor in their graphical toolsets. To be more specific, OSG has limited tools for terrain modeling, and both Delta3D and VTerrain are poor in data management necessary for an aerial vehicle.</p>
                      
                    </li>
                  </ol>
                <p>To overcome the existing shortcomings, we use C++ with DirectX<sup>®</sup>, HLSL together with the MRPT to create an efficient image handling gadget (Luna <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Luna F (2006) Introduction to 3D game programming with direct X 9.0c: a shader approach: wordware game and graphics library. Wordware Publishing, Inc., Plano" href="/article/10.1007/s10055-011-0197-7#ref-CR36" id="ref-link-section-d9282e1564">2006</a>). Our in-house 3DGE and its architecture are described in Amiri-Atashgah and Malaek (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran" href="/article/10.1007/s10055-011-0197-7#ref-CR1" id="ref-link-section-d9282e1567">2009</a>). In this manuscript, however, we concentrate on its operational capabilities for Aerial MonoSLAM. We start with the “Mission Design Package.” Advanced virtual environments usually need a preprocessing program to facilitate user–program interactions. Such preprocessors help users interact with the environment the way they expect. For aerial and space applications, this is a significant step as there are industry conventions that must be observed. With Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig3">3</a>, the basic duties of this IAVE are the following: (1) saving and loading the predesigned missions while providing the tools for their modifications; (2) adding and removing static objects such as buildings, light sources, mass bushes, and trees; (3) terrain settings including field type, texture, and dimensions; (4) climatic tunings such as visibility depth; (5) weather conditions and precipitation intensities; (6) sky texture settings; and finally, (7) presentation of the user-selected flight profile (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig6">6</a>a).</p><p>Once “Mission” is selected by the user, he needs to see what would actually happen for the selected mission and to make necessary changes. For aerial applications, in addition to have attractive sceneries of the environment, we also need to observe industry standards, which is primarily “safety.” This is in fact the first critical step for proper navigation of an aircraft, and the second critical step is related to the proper modeling of the vehicle dynamics itself. These two form the cornerstone of a successful Virtual Flight (VF), which comes in the “VF Package.” In brief, the following steps are completed: (1) User-selected mission is loaded, including the associated terrains, sky, and other existing 3D objects in the area and (2) A proper FDM of the aircraft is prepared.</p><p>At this stage, we are ready to fly the aircraft in the selected region. Throughout the flight, the corresponding images together with their associated navigation parameters of the aerial vehicle is collected and saved for later checks. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig6">6</a>b–f and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig7">7</a>a–f show examples of flying through rain and snow. The VF environment also conducts (1) collision detection with existing terrains and objects defined in the environment; (2) gathering and saving rendered images in a predefined frame rate and resolution; and (3) generation of “Rawlogs” files to be used in MRPT.</p><h3 class="c-article__sub-heading" id="Sec13">Modeling objects motion</h3><p>To consider the dynamic effects of moving objects in the generated images, we use Frenet Adapted Frames (FAF). A Frenet frame is a moving reference frame of <i>n</i> orthonormal vectors <b><i>e</i></b>
                  <sub>
                    <b><i>i</i></b>
                  </sub>(<b><i>t</i></b>) that are used to describe a curve locally at each point of <b><i>r</i></b>(<b><i>t</i></b>). It is the main tool in the differential geometric treatment of curves, as it is far easier and more natural to describe local properties (e.g., curvature, torsion) in terms of a local reference system as oppose to that of using a global Euclidean coordinates (Farouki and Giannelli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Farouki RT, Giannelli C (2009) Spatial camera orientation control by rotation-minimizing directed frames. Comput Anim Virtual Worlds 20:457–472" href="/article/10.1007/s10055-011-0197-7#ref-CR11" id="ref-link-section-d9282e1617">2009</a>). However, to get better rotational motions, we need to properly constrain the vehicle angular motions. It so appears that indiscriminant conditions mostly occur in the rolling motions. In fact, our IAVE enjoys proper orientation estimations in both yaw and pitch angles, and to estimate the roll angle (bank), we use heuristic parameters to overcome singularities within calculations. In the following section, we give a brief review of the method.</p><p>In general, for a parametric curve expressed by <span class="mathjax-tex">\( {\mathbf{r}}(t) = (x(t),\;y(t),\;z(t)), \)</span> the curve length <i>s</i> and its tangent vector are determined respectively by</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ s = \int\limits_{C} {{\text{d}}s} \, = \int\limits_{a}^{b} {\sqrt {x^{\prime 2} (\zeta ) + \, y^{\prime 2} (\zeta ) + \, z^{\prime 2} (\zeta )} {\text{d}}t} $$</span></div><div class="c-article-equation__number">
                    (4.1)
                </div></div><p>where <span class="mathjax-tex">\( {\dot{\mathbf{r}}}(t) = (\dot{x}(t),\;\dot{y}(t),\;\dot{z}(t)), \)</span> with arc element length <span class="mathjax-tex">\( {\text{d}}s = \left| {{\text{d}}{\mathbf{r}}} \right| = \left| {{\dot{\mathbf{r}}}} \right|{\text{d}}t = v{\text{d}}t. \)</span>
                </p><p>Motion of the camera is then characterized by its velocity on the aircraft path, with</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{v}} = {\dot{\mathbf{r}}} = vt,\;v \, = \left| {{\dot{\mathbf{r}}}} \right| = \dot{s},\;{\mathbf{a}} = {\mathbf{\ddot{r}}} = a_{t} {\mathbf{t}} + a_{n} {\mathbf{n}} \, $$</span></div><div class="c-article-equation__number">
                    (4.2)
                </div></div><p>and </p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ a_{n} = kv^{2} = \frac{{\left| {{\mathbf{v}} \times {\mathbf{a}}} \right|}}{v}\quad a_{t} = \dot{v} = \frac{{{\mathbf{v}} \cdot {\mathbf{a}}}}{v} $$</span></div><div class="c-article-equation__number">
                    (4.3)
                </div></div>
                <p>With Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig5">5</a>, the parameters are <span class="mathjax-tex">\( {\mathbf{v}}: \)</span> position vector, <span class="mathjax-tex">\( {\mathbf{t}}: \)</span> unit tangent vector, <span class="mathjax-tex">\( {\mathbf{n}}: \)</span> unit principal normal vector, <span class="mathjax-tex">\( a_{t} : \)</span> tangential component, and <span class="mathjax-tex">\( a_{n} : \)</span> normal component.</p><p>Here, the unit tangent vector is specified as one of the three frame vectors, and the other two vectors span the <i>normal plane</i> at each point. A <i>Frenet frame</i> on a space curve is then defined by</p><div id="Equ17" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${{\mathbf{t}} = \frac{{{\mathbf{r}}^{\prime } }}{{\left| {{\mathbf{r}}^{\prime } } \right|}}} \quad{{\mathbf{n}} = \frac{{{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } }}{{\left| {{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } } \right|}} \times {\mathbf{t}}}\quad{{\mathbf{b}} = \frac{{{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } }}{{\left| {{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } } \right|}}}$$</span></div><div class="c-article-equation__number">
                    (4.4)
                </div></div>
                <p>At each location, the principal normal (<b>n</b>) points toward the <i>center of curvature</i>, while the binormal <b>b</b> = <b>t</b> × <b>n</b> complements <b>t</b> and <b>n</b> so that (<b>t</b>, <b>n</b>, <b>b</b>) comprise a right-handed frame. The Frenet frame orientation is thus determined by the <i>intrinsic curve geometry</i>. The three mutually perpendicular planes spanned by the vector pairs (<b>n</b>, <b>b</b>), (<b>t</b>, <b>n</b>), and (<b>b</b>, <b>t</b>) at each point of a curve <b>r</b>(<i>t</i>) are known as the <i>normal</i>, <i>osculating</i>, and <i>rectifying</i> planes. The variation of the frame vectors (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0197-7#Equ16">4.3</a>) on <b>r</b>(<i>t</i>) can be described in terms of the <i>curvature</i> and <i>torsion</i> functions, defined by</p><div id="Equ18" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \kappa = \frac{{\left| {{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } } \right|}}{{\left| {{\mathbf{r}}^{\prime } } \right|^{3} }}\;{\text{and}}\;\tau = \, \frac{{({\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } ) \times {\mathbf{r}}^{\prime \prime \prime } }}{{\left| {{\mathbf{r}}^{\prime } \times {\mathbf{r}}^{\prime \prime } } \right|^{2} }} $$</span></div><div class="c-article-equation__number">
                    (4.5)
                </div></div>
                <p>Namely, the derivatives of vectors (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0197-7#Equ17">4.4</a>) with respect to the arc-length <i>s</i> are given by</p><div id="Equ19" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\frac{{{\text{d}}{\mathbf{t}}}}{{{\text{d}}s}} = \kappa {\mathbf{n}}},\quad{\frac{{{\text{d}}{\mathbf{n}}}}{{{\text{d}}s}} = - \kappa {\mathbf{t}} + \tau {\mathbf{b}}},\quad{\frac{{{\text{d}}{\mathbf{b}}}}{{{\mathbf{d}}s}} = - \tau {\mathbf{n}}}$$</span></div><div class="c-article-equation__number">
                    (4.6)
                </div></div>
                <p>The transformation matrix from the Frenet frame to the world frame and is as follows:</p><div id="Equ20" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{R}}^{WF} = \left[ {\begin{array}{*{20}c} {\mathbf{t}} &amp; {\mathbf{n}} &amp; {\mathbf{b}} \\ \end{array} } \right] \, . $$</span></div><div class="c-article-equation__number">
                    (4.7)
                </div></div>
                <p>To compute the corresponding Euler angles, a series of trigonometric relations are utilized in which the related equations are given by</p><div id="Equ21" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \psi = \text{atan} 2\left( {{\mathbf{R}}^{WF} \left( {1,2} \right),{\mathbf{R}}^{WF} \left( {1,1} \right)} \right) $$</span></div><div class="c-article-equation__number">
                    (4.8)
                </div></div>
                  <div id="Equ22" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {{\uptheta}} = \text{atan} \left( { - {\mathbf{R}}^{WF} \left( {1,3} \right)} \right) $$</span></div><div class="c-article-equation__number">
                    (4.9)
                </div></div>
                  <div id="Equ23" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \varphi = \text{atan} 2\left( {{\mathbf{R}}^{WF} \left( {2,3} \right),{\mathbf{R}}^{WF} \left( {3,3} \right)} \right) $$</span></div><div class="c-article-equation__number">
                    (4.10)
                </div></div>
                <h3 class="c-article__sub-heading" id="Sec14">MonoSLAM toolsets</h3><p>The first attempt to construct the MonoSLAM Toolset was based on a series of MATLAB scripts presented on VSLAM as described in Civera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e1975">2008</a>). However, the toolbox needed much improvement to be applicable for practical applications. There have been other attempts, however, to cover the shortcomings, which (Sola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e1978">2007</a>; Civera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945" href="/article/10.1007/s10055-011-0197-7#ref-CR7" id="ref-link-section-d9282e1981">2008</a>; Montiel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Montiel JMM, Civera J, Davison AJ (2006) Unified inverse depth parametrization for monocular SLAM. In: Robotics: science and systems, Philadelphia" href="/article/10.1007/s10055-011-0197-7#ref-CR21" id="ref-link-section-d9282e1984">2006</a>) offer. Sola (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse" href="/article/10.1007/s10055-011-0197-7#ref-CR28" id="ref-link-section-d9282e1987">2007</a>) suggests to implement “Inverse Depth Parameterization” for some simulated landmarks without any vision implementation. We use this reference as our starting point and add all vision tools (such as, image handling, feature extraction and handlings, matching, and data association facilities) to make it effective for aerial navigations. In this approach, we start MonoSLAM toolset by initializing the aircraft states and uncertainties. With provided facilities, user can monitor the outcomes at any time (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig8">8</a>a–c). This includes updating aircraft states as well as features in the flying environment. During the observation process, 2D Harris features are detected and added to the landmarks database. It is worth noting that if the number of observed features is less that a threshold (usually 14, according to Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067" href="/article/10.1007/s10055-011-0197-7#ref-CR10" id="ref-link-section-d9282e1994">2007</a>), the toolset automatically adds new features to prevent instabilities in calculations.</p><p>We need to emphasize that previous works such as Ditchburn (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ditchburn K (2010) &#xA;                    http://www.Toymaker.Info&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR35" id="ref-link-section-d9282e2000">2010</a>), Luna (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Luna F (2006) Introduction to 3D game programming with direct X 9.0c: a shader approach: wordware game and graphics library. Wordware Publishing, Inc., Plano" href="/article/10.1007/s10055-011-0197-7#ref-CR36" id="ref-link-section-d9282e2003">2006</a>) and Saghafi and Amiri-Atashgah (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif University of Technology, Tehran, MS Dissertation" href="/article/10.1007/s10055-011-0197-7#ref-CR24" id="ref-link-section-d9282e2006">2000</a>) have been quite useful to enhance current IAVE development process. Nevertheless, runtime complexities depended on the square of the state vector dimension (<i>n*n</i>). That is, keeping the state vector small is essential to achieve a high frame-rate. As we conduct a short-term SLAM, we do not have to keep a large number of landmarks in the map. Landmarks that are not seen any more can be ignored, as no loop closing is required due to the global localization.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">IAVE simulation results</h2><div class="c-article-section__content" id="Sec15-content"><p>Vision-based navigation for aerial applications is, in fact, a cumbersome multidimensional task. Its difficulties are associated with three major factors: (a) aircraft states, including height above the ground, position, and orientation in different phases of the mission; (b) time of the day and its associated shadings, especially during sunset and sunrise where the shadows have fast dynamics; and (c) climatic conditions and precipitations that affects visibility.</p><p>Different case studies conducted by authors provide enough evidence to claim that developed 3DGE is sufficiently capable of addressing all aspects of a safe navigation of a GA aircraft. With the help of developed IAVE and its MonoSLAM navigation tool, we are able to investigate different types of aircraft in different climatic and flight conditions. Here, we study some examples. As the first case, we study an aircraft flying at 1,000 m above the ground with the speed of 300 km/h (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig9">9</a>a–d). Such a case describes a normal cruising flight that could be used for validating MonoSLAM tool. The second case describes a turning flight on a circular path (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig10">10</a>a–d). The altitude is kept constant, which represents a loitering over a predefined region. The third case belongs to a free flight of a Cessna 172X (JSBSim Project <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="JSBSim Project (2010) [Online]. &#xA;                    www.jsbsim.org&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0197-7#ref-CR32" id="ref-link-section-d9282e2029">2010</a>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig11">11</a>a–d). This case shows the dynamic effects in both longitudinal and lateral directions of the aircraft. Examining the figures reveals that we are able to navigate the aircraft over an unknown environment, without having any knowledge of the states of the aircraft.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A comparison of position results between MonoSLAM and ground truth data in a cruising flight with 1,000 m flight altitude <b>a</b> orientation toward north, <b>b</b> position in east direction, <b>c</b> velocity in north direction, and <b>d</b> velocity in east direction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>A comparison of position results between MonoSLAM and ground truth data in a curved path with 1,000 m flight altitude <b>a</b> position in north direction, <b>b</b> position in east direction, <b>c</b> velocity in north direction, and <b>d</b> velocity in east direction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>A comparison of position results between MonoSLAM and ground truth data in a high dynamics path with 1,000 m flight altitude <b>a</b> position in north direction, <b>b</b> position in east direction, <b>c</b> velocity in north direction, and <b>d</b> velocity in east direction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In general, the related outcomes in the case of a cruising flight are quite acceptable (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig9">9</a>a–d). However, for maneuvering flights, there are some discrepancies in the predicted position in the form of uncertainties, which occur after 10 to 15 s from the starting point of the flight. Fortunately, such discrepancies are bounded and remain relatively constant in time. Therefore, one might still conclude that the method is effective in cases where precise navigation is not a must. On the other hand, we expect this approach to be implemented for SATS projects, where most of the time aircraft follows specific landmarks such as, roads, railroads, and highways. For other cases such as aerial patrolling and surveillance missions, the precision is not that restricting. It is well noted that results for estimating velocities are acceptable as we only rely on camera vision. Obviously, in cases where a better approximation is required, one might add predefined maps or features to enhance the precision. Next, we present systematic studies to demonstrate the effects of different landscapes, weather conditions or flight speed and altitude on the navigational parameters to demonstrate other system capabilities.</p><p>In the first set, we present the effects of different landscapes, including (1) a barren desert, (2) a mountain with scattered snow, and (3) a suburb area. For each case, “Mission Design Package” is used to develop the mission profile and then “Virtual Flight Gadget” is used to produce images as seen by camera. The flight speed is selected to be 400 km/h, and the flight altitude is kept at 500 m above the ground level (AGL). In all cases, the visibility depth is assumed to be infinite. The related results are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig12">12</a>a, b. With careful examination, we notice the following:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>
                        <b>a</b> Position error and <b>b</b> velocity error in different terrain types</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <ul class="u-list-style-bullet">
                  <li>
                    <p>Comparing the error history in both transient and the steady-state conditions suggests better steady-state results come with relatively worse transient behaviors.</p>
                  </li>
                  <li>
                    <p>The minimum steady-state error is associated with the suburban areas, and the worst one is for the desert landscape. This is obviously coincides with SATS applications.</p>
                  </li>
                  <li>
                    <p>Comparing the steady-state errors in mountainous areas with that of the suburban areas reveals the criticality of the snow coverage uniformity, which might cause some mismatching while dealing with the features.</p>
                  </li>
                </ul>
              <p>In the second set, the effects of different weather conditions are studied. Here, the flight speed is kept at 400 km/h and flight altitude at 500 m (AGL), and we vary visibility depth from infinite down to 5,000 m. We then proceed to a rainy weather with the visibility depth of 2,000 m and finally a snowy weather with a visibility depth of 1,000 m. With results shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig13">13</a>a, b, we see only a subtle position error as little as 3%. The less the visibility, the more would be the error in the position. In fact, the worse is for the snowy weather.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>a Position error and <b>b</b> velocity error in different weather conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In the third set, we study the effects of the speed and altitude on the resulting navigational data. Varying aircraft speed from 100 to 400 km/h and flight altitude from 300 to 1,000 m (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig14">14</a>a through <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig14">14</a>f), we notice changing aircraft altitude exhibits some strong non-linear effects at each different flight speed. In fact, by decreasing speed, the effects of altitude diminish. At relatively lower speeds, the results are similar for all altitudes. However, the relatively better results for lower altitudes are mainly due to the faster predictions of the range of landmarks. Obviously, the new studies are based on assumptions that no blur or distortion occurs in the captured images. This is equivalent to that of enjoying relatively high frame-rate image acquisition capabilities. Obviously, effects of motion-blurring by themselves are an interesting subject for any further enhancement in the existing algorithms.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0197-7/MediaObjects/10055_2011_197_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Velocity and position error in different mission cases (Altitude and Velocities). <b>a</b>
                        <i>V</i> = 100 km/h, <b>b</b>
                        <i>V</i> = 100 km/h, <b>c</b>
                        <i>V</i> = 250 km/h, <b>d</b>
                        <i>V</i> = 250 km/h, <b>e</b>
                        <i>V</i> = 400 km/h, <b>f</b>
                        <i>V</i> = 400 km/h</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0197-7/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions and discussion</h2><div class="c-article-section__content" id="Sec16-content"><p>In this paper, we illustrate how the enhancement to the existing tools could be used to cover aerial applications, especially vision-based navigation concept. We show how a GA aircraft could properly be navigated in quite different flight condition by means of monocular SLAM techniques. We further show how MonoSLAM, Inverse Depth Parameterization, and EKF are integrated to form an effective low-cost vision-based navigation. Comprehensive case studies and simulation results show the overall feasibility of the approach for the projects similar to SATA. Nevertheless, to examine the degree of precision and safety demands proper experimental flights, which in fact is the next phase of the current work, and we expect the enhanced engine introduced in this work help reduce the need for such experiments.</p><p>Obviously, the current work could still be improved to include virtual implementation of other types of sensors, such as, multimonocular cameras, LASER range finders, and infrared sensors for night flights. There are other possible enhancements for aerospace applications. For example, to improve the results of navigation, one might incorporate either Global Positioning System (GPS) or Inertial Navigation System (INS) measurements to enhance the vision capabilities.</p><p>To investigate whether we are able to navigate an aircraft in different climatic conditions, we have effectively developed proper tools. For example, a dynamic scene for a rainy weather is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig6">6</a>b, c, and a dynamic scene for a snowy weather is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0197-7#Fig6">6</a>e, f.</p><p>In order to boost the capabilities of the developed IAVE, one might work on new improvements such as (1) fusion of the different sensors in the vision system, (2) incorporation of MonoSLAM with 3D engine by means of C++ to cover vision-in-the-loop experiments for aerial navigation, and the last but not the least (3) investigation of blur or distortion of the images on the MonoSLAM results.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system" /><p class="c-article-references__text" id="ref-CR1">Amiri-Atashgah MA, Malaek SMB (2009) A 3D engine for image generation in an aerial single camera vision system. In: Flight simulation conference, Tehran</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berndt J (2004) JSBSim: an open source flight dynamics model in C++. In: AIAA Modeling and simulation technolo" /><p class="c-article-references__text" id="ref-CR2">Berndt J (2004) JSBSim: an open source flight dynamics model in C++. In: AIAA Modeling and simulation technologies conference and exhibit, Providence, Rhode Island</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berndt JS, The JSBSim Development Team (2009) JSBSim An open source, platform-independent, flight dynamics mod" /><p class="c-article-references__text" id="ref-CR3">Berndt JS, The JSBSim Development Team (2009) JSBSim An open source, platform-independent, flight dynamics model in C++, [Online]. <a href="http://jsbsim.sourceforge.net/JSBSimReferenceManual.pdf2009">http://jsbsim.sourceforge.net/JSBSimReferenceManual.pdf2009</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blanco JL (2010) The mobile robot programming toolkit. [Online]. http://babel.isa.uma.es/mrpt/index.php/Main_P" /><p class="c-article-references__text" id="ref-CR4">Blanco JL (2010) The mobile robot programming toolkit. [Online]. <a href="http://babel.isa.uma.es/mrpt/index.php/Main_Page">http://babel.isa.uma.es/mrpt/index.php/Main_Page</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. http://w" /><p class="c-article-references__text" id="ref-CR34">Bouguet J (2010) Camera calibration toolbox for matlab. California Institute of Technology. [Online]. <a href="http://www.vision.caltech.edu/bouguetj/calib_doc">http://www.vision.caltech.edu/bouguetj/calib_doc</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brooks FP (1999) What’s real about virtual reality. University of North Carolina, Chapel Hill" /><p class="c-article-references__text" id="ref-CR5">Brooks FP (1999) What’s real about virtual reality. University of North Carolina, Chapel Hill</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cesetti A, Frontoni E, Mancini A, Zingaretti P, Longhi S (2009) Vision-based autonomous navigation and landing" /><p class="c-article-references__text" id="ref-CR6">Cesetti A, Frontoni E, Mancini A, Zingaretti P, Longhi S (2009) Vision-based autonomous navigation and landing of an unmanned aerial vehicle using natural landmarks. In: 17th mediterranean conference on control &amp; automation, Makedonia Palace, Thessaloniki, Greece, pp 910–915</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Civera, JM. Montiel, AJ. Davison, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics " /><p class="c-article-references__text" id="ref-CR7">Civera J, Montiel JM, Davison AJ (2008) Inverse depth parametrization for monocular SLAM. IEEE Trans Robotics 24(5):932–945</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Inverse%20depth%20parametrization%20for%20monocular%20SLAM&amp;journal=IEEE%20Trans%20Robotics&amp;volume=24&amp;issue=5&amp;pages=932-945&amp;publication_year=2008&amp;author=Civera%2CJ&amp;author=Montiel%2CJM&amp;author=Davison%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Conte G, Doherty P (2009) Vision-based unmanned aerial vehicle navigation using geo-referenced information. EU" /><p class="c-article-references__text" id="ref-CR8">Conte G, Doherty P (2009) Vision-based unmanned aerial vehicle navigation using geo-referenced information. EURASIP Jo Advances Signal Process vol. 2009, [Online] <a href="http://downloads.hindawi.com/journals/asp/2009/387308.pdf">http://downloads.hindawi.com/journals/asp/2009/387308.pdf</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Davison AJ, Cid YG, Kita N (2004) Real-time 3D SLAM with wide-angle vision. In: IFAC Symp Intell Autonomous Ve" /><p class="c-article-references__text" id="ref-CR9">Davison AJ, Cid YG, Kita N (2004) Real-time 3D SLAM with wide-angle vision. In: IFAC Symp Intell Autonomous Vehicles 2004, Lisbon, Portugal</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Davison, ID. Reid, ND. Molton, O. Stasse, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Ana" /><p class="c-article-references__text" id="ref-CR10">Davison AJ, Reid ID, Molton ND, Stasse O (2007) MonoSLAM: real-time single camera SLAM. IEEE Trans Pattern Anal Mach Intelligence 29(6):1052–1067</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=MonoSLAM%3A%20real-time%20single%20camera%20SLAM&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intelligence&amp;volume=29&amp;issue=6&amp;pages=1052-1067&amp;publication_year=2007&amp;author=Davison%2CAJ&amp;author=Reid%2CID&amp;author=Molton%2CND&amp;author=Stasse%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Diosi A, Kleeman L (2004) Advanced sonar and laser range finder fusion for simultaneous localization and mappi" /><p class="c-article-references__text" id="ref-CR31">Diosi A, Kleeman L (2004) Advanced sonar and laser range finder fusion for simultaneous localization and mapping. In: Proceedings of 2004 IEEE/RSJ international conference on intelligent robots and systems, Sept 28–Oct 2, 2004, Sendai, Japan </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ditchburn K (2010) http://www.Toymaker.Info&#xA;                " /><p class="c-article-references__text" id="ref-CR35">Ditchburn K (2010) <a href="http://www.Toymaker.Info">http://www.Toymaker.Info</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RT. Farouki, C. Giannelli, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Farouki RT, Giannelli C (2009) Spatial camera orientation control by rotation-minimizing directed frames. Comp" /><p class="c-article-references__text" id="ref-CR11">Farouki RT, Giannelli C (2009) Spatial camera orientation control by rotation-minimizing directed frames. Comput Anim Virtual Worlds 20:457–472</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20camera%20orientation%20control%20by%20rotation-minimizing%20directed%20frames&amp;journal=Comput%20Anim%20Virtual%20Worlds&amp;volume=20&amp;pages=457-472&amp;publication_year=2009&amp;author=Farouki%2CRT&amp;author=Giannelli%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="U. FRESE, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="FRESE U (2006) A discussion of simultaneous localization and mapping. Autonomous Robots 20:25–42" /><p class="c-article-references__text" id="ref-CR12">FRESE U (2006) A discussion of simultaneous localization and mapping. Autonomous Robots 20:25–42</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20discussion%20of%20simultaneous%20localization%20and%20mapping&amp;journal=Autonomous%20Robots&amp;volume=20&amp;pages=25-42&amp;publication_year=2006&amp;author=FRESE%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 200" /><p class="c-article-references__text" id="ref-CR13">Funke J, Informatik F (2009) A framework for evaluating visual SLAM. In British machine vision conference, 2009, [Online] <a href="http://www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf">www.bmva.org/bmvc/2009/Papers/Paper396/Paper396.pdf</a>)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of the 4th Alvey vision confe" /><p class="c-article-references__text" id="ref-CR14">Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of the 4th Alvey vision conference, Manchester, pp. 147–151</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EN. Johnson, AA. Proctor, AR. Tannenbaum, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Johnson EN, Proctor AA, Tannenbaum AR (2005) Visual search automation for unmanned aerial vehicles. IEEE Trans" /><p class="c-article-references__text" id="ref-CR15">Johnson EN, Proctor AA, Tannenbaum AR (2005) Visual search automation for unmanned aerial vehicles. IEEE Trans Aerosp Electronic Syst 41(1), Rohnert Park, CA AIAA, 2007–2854</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20search%20automation%20for%20unmanned%20aerial%20vehicles&amp;journal=IEEE%20Trans%20Aerosp%20Electronic%20Syst&amp;volume=41&amp;issue=1&amp;pages=2007-2854&amp;publication_year=2005&amp;author=Johnson%2CEN&amp;author=Proctor%2CAA&amp;author=Tannenbaum%2CAR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson AE, Ansar A, Matthies LH, Trawny N, Mourikis AI, Roumeliotis SI (2007) A general approach to terrain r" /><p class="c-article-references__text" id="ref-CR16">Johnson AE, Ansar A, Matthies LH, Trawny N, Mourikis AI, Roumeliotis SI (2007) A general approach to terrain relative navigation for planetary landing. In: Aiaa Conference and exhibit, 2007</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="JSBSim Project (2010) [Online]. www.jsbsim.org&#xA;                " /><p class="c-article-references__text" id="ref-CR32">JSBSim Project (2010) [Online]. <a href="http://www.jsbsim.org">www.jsbsim.org</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jung I, Lacroix S (2003) High resolution terrain mapping using low altitude aerial stereo imagery. In: Proceed" /><p class="c-article-references__text" id="ref-CR17">Jung I, Lacroix S (2003) High resolution terrain mapping using low altitude aerial stereo imagery. In: Proceedings of ninth International Conference Computer Vision, Nice, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics " /><p class="c-article-references__text" id="ref-CR18">Kim JH, Sukkarieh S (2003) Airborne simultaneous localization and map building. In: IEEE Int’l Conf. Robotics and Automation 1:406–411</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kim, S. Ong, E. Nettleton, S. Sukkarieh, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Kim J, Ong S, Nettleton E, Sukkarieh S (2006) Decentralized approach to UAV navigation: without the use of GPS" /><p class="c-article-references__text" id="ref-CR19">Kim J, Ong S, Nettleton E, Sukkarieh S (2006) Decentralized approach to UAV navigation: without the use of GPS and preloaded maps. Proc IME Part G: J Aerosp Eng 218(6):399–416</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Decentralized%20approach%20to%20UAV%20navigation%3A%20without%20the%20use%20of%20GPS%20and%20preloaded%20maps&amp;journal=Proc%20IME%20Part%20G%3A%20J%20Aerosp%20Eng&amp;volume=218&amp;issue=6&amp;pages=399-416&amp;publication_year=2006&amp;author=Kim%2CJ&amp;author=Ong%2CS&amp;author=Nettleton%2CE&amp;author=Sukkarieh%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Leonard, HF. Durrant-Whyte, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Leonard JJ, Durrant-Whyte HF (1991) Mobile robot localization by tracking geometric beacons. IEEE Trans Roboti" /><p class="c-article-references__text" id="ref-CR20">Leonard JJ, Durrant-Whyte HF (1991) Mobile robot localization by tracking geometric beacons. IEEE Trans Robotics Automation 7(3):376–382</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mobile%20robot%20localization%20by%20tracking%20geometric%20beacons&amp;journal=IEEE%20Trans%20Robotics%20Automation&amp;volume=7&amp;issue=3&amp;pages=376-382&amp;publication_year=1991&amp;author=Leonard%2CJJ&amp;author=Durrant-Whyte%2CHF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Luna F (2006) Introduction to 3D game programming with direct X 9.0c: a shader approach: wordware game and gra" /><p class="c-article-references__text" id="ref-CR36">Luna F (2006) Introduction to 3D game programming with direct X 9.0c: a shader approach: wordware game and graphics library. Wordware Publishing, Inc., Plano</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Montiel JMM, Civera J, Davison AJ (2006) Unified inverse depth parametrization for monocular SLAM. In: Robotic" /><p class="c-article-references__text" id="ref-CR21">Montiel JMM, Civera J, Davison AJ (2006) Unified inverse depth parametrization for monocular SLAM. In: Robotics: science and systems, Philadelphia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mourikis AI, Trawny N, Roumeliotis SI, Johnson A, Matthies L (2007) Vision-aided inertial navigation for preci" /><p class="c-article-references__text" id="ref-CR22">Mourikis AI, Trawny N, Roumeliotis SI, Johnson A, Matthies L (2007) Vision-aided inertial navigation for precise planetary landing: analysis and experiments. In: AIAA conference paper, 2007, Atlanta, GA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rolfe JM, Staples KJ (1986) Flight simulation. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR23">Rolfe JM, Staples KJ (1986) Flight simulation. Cambridge University Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roskam J (1987) Airplane design part VI: DAR corporation" /><p class="c-article-references__text" id="ref-CR33">Roskam J (1987) Airplane design part VI: DAR corporation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif Uni" /><p class="c-article-references__text" id="ref-CR24">Saghafi F, Amiri-Atashgah MA (2000) Developing a flight simulation software for unmanned aircrafts. Sharif University of Technology, Tehran, MS Dissertation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sim DG, Jeong SY, Park RH, Kim RC, Lee3 SU, Kim IC (1996) Navigation parameter estimation from sequential aeri" /><p class="c-article-references__text" id="ref-CR26">Sim DG, Jeong SY, Park RH, Kim RC, Lee3 SU, Kim IC (1996) Navigation parameter estimation from sequential aerial images. In: Proceedings 1996 IEEE international conference image processing, vol. II, pp. 629–632, Lausanne, Switzerland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Sim, SY. Jeong, D. Lee, R. Park, R. Kim, SU. Lee, I. Kim, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Sim D, Jeong SY, Lee D, Park R, Kim R, Lee SU, Kim I (1999) Hybrid estimation of navigation parameters from ae" /><p class="c-article-references__text" id="ref-CR25">Sim D, Jeong SY, Lee D, Park R, Kim R, Lee SU, Kim I (1999) Hybrid estimation of navigation parameters from aerial image sequence. IEEE Trans Image Process 8(3):429–435</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hybrid%20estimation%20of%20navigation%20parameters%20from%20aerial%20image%20sequence&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=8&amp;issue=3&amp;pages=429-435&amp;publication_year=1999&amp;author=Sim%2CD&amp;author=Jeong%2CSY&amp;author=Lee%2CD&amp;author=Park%2CR&amp;author=Kim%2CR&amp;author=Lee%2CSU&amp;author=Kim%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Smith, P. Cheeseman, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Smith R, Cheeseman P (1987) On the representation of spatial uncertainty. Int J Robotics Res 5(4):56–68" /><p class="c-article-references__text" id="ref-CR27">Smith R, Cheeseman P (1987) On the representation of spatial uncertainty. Int J Robotics Res 5(4):56–68</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20representation%20of%20spatial%20uncertainty&amp;journal=Int%20J%20Robotics%20Res&amp;volume=5&amp;issue=4&amp;pages=56-68&amp;publication_year=1987&amp;author=Smith%2CR&amp;author=Cheeseman%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric " /><p class="c-article-references__text" id="ref-CR28">Sola J (2007) Towards visual localization, mapping and moving objects tracking by a mobile Robot: a geometric and probabilistic approach. PhD thesis, Institut National Politechnique de Toulouse</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York" /><p class="c-article-references__text" id="ref-CR29">Stevens BL, Lewis FL (2003) Aircraft control and simulation. Wiley, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sunderhauf N, Lange S, Protzel P (2007) Using the unscented Kalman filter in mono-SLAM with inverse depth para" /><p class="c-article-references__text" id="ref-CR30">Sunderhauf N, Lange S, Protzel P (2007) Using the unscented Kalman filter in mono-SLAM with inverse depth parametrization for autonomous airship control. In: IEEE international workshop on safety security and rescue robotics, Rome</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0197-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Aerospace Engineering, Sharif University of Technology, Tehran, Iran</p><p class="c-article-author-affiliation__authors-list">M. A. Amiri Atashgah &amp; S. M. B. Malaek</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-M__A_-Amiri_Atashgah"><span class="c-article-authors-search__title u-h3 js-search-name">M. A. Amiri Atashgah</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;M. A.+Amiri Atashgah&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=M. A.+Amiri Atashgah" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22M. A.+Amiri Atashgah%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-S__M__B_-Malaek"><span class="c-article-authors-search__title u-h3 js-search-name">S. M. B. Malaek</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;S. M. B.+Malaek&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=S. M. B.+Malaek" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22S. M. B.+Malaek%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0197-7/email/correspondent/c1/new">M. A. Amiri Atashgah</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20integrated%20virtual%20environment%20for%20feasibility%20studies%20and%20implementation%20of%20aerial%20MonoSLAM&amp;author=M.%20A.%20Amiri%20Atashgah%20et%20al&amp;contentID=10.1007%2Fs10055-011-0197-7&amp;publication=1359-4338&amp;publicationDate=2011-09-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Amiri Atashgah, M.A., Malaek, S.M.B. An integrated virtual environment for feasibility studies and implementation of aerial MonoSLAM.
                    <i>Virtual Reality</i> <b>16, </b>215–232 (2012). https://doi.org/10.1007/s10055-011-0197-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0197-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-09-09">09 September 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-08-03">03 August 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-09-20">20 September 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-09">September 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0197-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0197-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">3D graphics engine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">MonoSLAM</span></li><li class="c-article-subject-list__subject"><span itemprop="about">General aviation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Aerial navigation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0197-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=197;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

