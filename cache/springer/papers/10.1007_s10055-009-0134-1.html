<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Multisensory VR exploration for computer fluid dynamics in the CoRSAIR"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In the last 30&#160;years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/13/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project"/>

    <meta name="dc.source" content="Virtual Reality 2009 13:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2009-09-16"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In the last 30&#160;years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed the emergence of the discipline known as computational fluid dynamics or CFD. More recently, virtual reality (VR) systems have proven an interesting alternative to conventional user interfaces, in particular, when exploring complex and massive datasets, such as those encountered in scientific visualization applications. Unfortunately, all too often, VR technologies have proven unsatisfactory in providing a true added value compared to standard interfaces, mostly because insufficient attention was given to the activity and needs of the intended user audience. The present work focuses on the design of a multimodal VR environment dedicated to the analysis of non-stationary flows in CFD. Specifically, we report on the identification of relevant strategies of CFD exploration coupled to adapted VR data representation and interaction techniques. Three different contributions will be highlighted. First, we show how placing the CFD expert user at the heart of the system is accomplished through a formalized analysis of work activity and through system evaluation. Second, auditory outputs providing analysis of time-varying phenomena in a spatialized virtual environment are introduced and evaluated. Finally, specific haptic feedbacks are designed and evaluated to enhance classical visual data exploration of CFD simulations."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2009-09-16"/>

    <meta name="prism.volume" content="13"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="257"/>

    <meta name="prism.endingPage" content="271"/>

    <meta name="prism.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-009-0134-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-009-0134-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-009-0134-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-009-0134-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project"/>

    <meta name="citation_volume" content="13"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2009/12"/>

    <meta name="citation_online_date" content="2009/09/16"/>

    <meta name="citation_firstpage" content="257"/>

    <meta name="citation_lastpage" content="271"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-009-0134-1"/>

    <meta name="DOI" content="10.1007/s10055-009-0134-1"/>

    <meta name="citation_doi" content="10.1007/s10055-009-0134-1"/>

    <meta name="description" content="In the last 30&#160;years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simul"/>

    <meta name="dc.creator" content="J. M. V&#233;zien"/>

    <meta name="dc.creator" content="B. M&#233;n&#233;las"/>

    <meta name="dc.creator" content="J. Nelson"/>

    <meta name="dc.creator" content="L. Picinali"/>

    <meta name="dc.creator" content="P. Bourdot"/>

    <meta name="dc.creator" content="M. Ammi"/>

    <meta name="dc.creator" content="B. F. G. Katz"/>

    <meta name="dc.creator" content="J. M. Burkhardt"/>

    <meta name="dc.creator" content="L. Pastur"/>

    <meta name="dc.creator" content="F. Lusseyran"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: IEEE virtual reality annual international symposium &#8217;95, Research Triangle Park, pp 203&#8211;210"/>

    <meta name="citation_reference" content="Andr&#233; E (2000) Handbook of natural language processing. In: Chapter &#8220;The generation of multimedia presentations&#8221;, pp 305&#8211;327"/>

    <meta name="citation_reference" content="Annett J (2003) Handbook of cognitive task design. In: Chapter Hierarchical Task Analysis, pp 17&#8211;35"/>

    <meta name="citation_reference" content="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: 7th conference on visualization, pp 197&#8211;204. doi:
                    10.1109/VISUAL.1996.568108
                    
                  
                        "/>

    <meta name="citation_reference" content="Bryson S, Levit C (1991) The virtual windtunnel: an environment for the exploration of three-dimensional unsteady flows. In: IEEE Visualization, pp 17&#8211;24"/>

    <meta name="citation_reference" content="citation_title=Force and touch feedback for VR; citation_publication_date=1996; citation_id=CR6; citation_author=G Burdea; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="Chen K-W, Heng P-A, Sun H (2000) Direct haptic rendering of isosurface by intermediate representation. In: ACM symposium on virtual reality software and technology VRST, pp 188&#8211;194"/>

    <meta name="citation_reference" content="citation_journal_title=Rev Educ Res; citation_title=The role of anomalous data in knowledge acquisition: a theoretical framework and implications for science instruction; citation_author=CA Chinn, WF Brewer; citation_volume=63; citation_issue=1; citation_publication_date=1993; citation_pages=1-49; citation_id=CR8"/>

    <meta name="citation_reference" content="Crawfis RA, Shen H-W, Max N (2000) Flow visualization techniques for CFD using volume rendering. In: 9th International symposium on flow visualization"/>

    <meta name="citation_reference" content="Donker H, Klante P, Gorny P (2002) The design of auditory user interfaces for blind users. In: 2nd Nordic conference on human-computer interaction (NordiCHI &#8217;02), vol 31. ACM, New York, pp 149&#8211;156"/>

    <meta name="citation_reference" content="Dudas R (2002) Spectral envelope correction for real-time transposition: proposal of a &#8220;floating-formant&#8221; method. In: International computer music conference (ICMC &#8217;02), Goteborg, pp 126&#8211;129"/>

    <meta name="citation_reference" content="Ericsson KA, Simon, HA (1984/1993) Protocol analysis: Verbal reports as data (revised edition)"/>

    <meta name="citation_reference" content="Fauvet N, Ammi M, Bourdot P (2007) Experiments of haptic perception techniques for computational fluid dynamics. In: Cyber world 2007, Hannover, pp 322&#8211;329"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Phase vocoder; citation_author=JL Flanagan; citation_volume=28; citation_issue=5; citation_publication_date=1965; citation_pages=939-940; citation_doi=10.1121/1.1939800; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Numer Methods Fluids; citation_title=A general methodology for investigating flow instability in complex geometries: application to natural convection in enclosures; citation_author=E Gadouin, P Qu&#233;r&#233;, O Daube; citation_volume=37; citation_issue=2; citation_publication_date=2001; citation_pages=175-208; citation_doi=10.1002/fld.173; citation_id=CR16"/>

    <meta name="citation_reference" content="Gherbi R, Bourdot B, Vezien JM, Herisson J, Fauvet N, Ferey N (2006) Le trait&#233; de la r&#233;alit&#233; virtuelle, vol 4: Les applications de la R&#233;alit&#233; Virtuelle. In: Fuchs P, Moreau G, Papin J-P (eds) Chapter 4: Explorations de donn&#233;es scientifiques et exp&#233;rimentations virtuelles. Presses de l&#8217;Ecoles de Mines de Paris"/>

    <meta name="citation_reference" content="Ghiglione R, Landr&#233; A, Bromberg M, Molette P (1998) L&#8217;analyse automatique des contenus [Automatic content analysis]. Dunod Ed"/>

    <meta name="citation_reference" content="Hoppe H (1996) Progressive Meshes ACM SIGGRAPH 96. In: Computer graphics proceedings, annual conference series, pp 99&#8211;108"/>

    <meta name="citation_reference" content="Katz BFG, Warusfel O, Bourdot P, Vezien J-M (2007) CoRSAIRe&#8212;Combination of sensori-motor rendering for the immersive analysis of results. In: 2nd International workshop on interactive sonification (ISon 2007), York"/>

    <meta name="citation_reference" content="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference of auditory display (ICAD 2008), Paris"/>

    <meta name="citation_reference" content="citation_title=Auditory display: sonification, audification, and auditory interfaces; citation_publication_date=1994; citation_id=CR22; citation_publisher=Santa Fe Institute Studies in the Sciences of Complexity, Westview Press"/>

    <meta name="citation_reference" content="Krner O, Schill M, Wagner C, Bender H-J, Mnner R (1999) Haptic volume rendering with an intermediate local representation. In: 1st International workshop on the haptic devices in medical applications, pp 79&#8211;84"/>

    <meta name="citation_reference" content="Levoy M, Rusinkiewicz S (2000) QSplat: a multiresolution point rendering system for large meshes. In: SIGGRAPH&#8217;2000, Computer graphics proceedings, annual conference series, pp 343&#8211;352"/>

    <meta name="citation_reference" content="Lorensen WE, Cline HE (1987) Marching cubes: a high resolution 3D surface construction algorithm. In: ACM SIGGRAPH 87, computer graphics proceedings, annual conference series 21(4):163&#8211;169"/>

    <meta name="citation_reference" content="citation_title=The image of the city; citation_publication_date=1960; citation_id=CR25; citation_author=K Lynch; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Methods to support human-centred design; citation_author=M Maguire; citation_volume=55; citation_issue=4; citation_publication_date=2001; citation_pages=587-634; citation_doi=10.1006/ijhc.2001.0503; citation_id=CR26"/>

    <meta name="citation_reference" content="Mark W, Randolph S, Finch M, Verth JV, Taylor RM (1996) Adding force feedback to graphics systems: issues and solutions. In: 23rd annual conference on computer graphics and interactive techniques, pp 447&#8211;452"/>

    <meta name="citation_reference" content="Menelas B, Ammi M, Bourdot P (2008) A flexible method for haptic rendering of isosurface from volumetric data. In: Lecture Notes In Computer Science, vol 5024, Proceedings of the 6th international conference on haptics: perception, devices and scenarios, pp 687&#8211;693"/>

    <meta name="citation_reference" content="Menelas B, Ammi M, Pastur L, Bourdot P (2009) Haptic exploration of an unsteady flow. In: Symposium on haptic interfaces for virtual environment and teleoperator systems (WorldHaptics 2009), pp 232&#8211;237"/>

    <meta name="citation_reference" content="Menelas B, Ammi M, Bourdot P, Richir S (2009) Survey on haptic rendering of data sets: exploration of scalar and vector. J Virtual Real Broadcast (in press)"/>

    <meta name="citation_reference" content="Nesbitt KV (2003) Designing multi-sensory displays for abstract data. PhD thesis, School of Information Technology, University of Sydney"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Fluids; citation_title=Quantifying the nonlinear mode competition in the flow over an open cavity at medium Reynolds number; citation_author=L Pastur, F Lusseyran, TM Faure, Y Fraigneau, R Pethieu, P Debesse; citation_volume=44; citation_issue=4; citation_publication_date=2008; citation_pages=597-608; citation_doi=10.1007/s00348-007-0419-7; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=J Fluids Eng; citation_title=A reconstruction method for the flow past an open cavity; citation_author=B Podvin, Y Fraigneau, F Lusseyran, P Gougat; citation_volume=128; citation_issue=3; citation_publication_date=2006; citation_pages=531-540; citation_doi=10.1115/1.2175159; citation_id=CR33"/>

    <meta name="citation_reference" content="Schnell N, Peeters G, Lemouton S, Manoury P, Rodet X (2000) Synthesizing a choir in real-time using pitch synchronous overlap add (PSOLA). In: International computer music conference, Berlin"/>

    <meta name="citation_reference" content="Touraine D, Bourdot P (2001) VEserver : a manager for input and haptic multi-sensorial device. In: IEEE international workshop on robot-human interactive communication (IEEE ROMAN 2001), Bordeaux and Paris"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graphics Appl; citation_title=Immersive virtual reality for scientific visualization: a progress report; citation_author=A Dam, AS Forsberg, DH Laidlaw, JJ LaViola, RM Simpson; citation_volume=128; citation_issue=3; citation_publication_date=2000; citation_pages=531-540; citation_id=CR37"/>

    <meta name="citation_reference" content="Wright M, Freed A, Momeni A (2003) Open sound control: state of the art 2003. In: International conference on new interfaces for musical expression, Montreal, pp 153&#8211;159"/>

    <meta name="citation_reference" content="Ziegeler S, Moorhead RJ, Croft PJ, Lu D (2001) The MetVR case study: meteorological visualization in an immersive virtual environment. In: IEEE 12th conference on visualization (VIS&#8217; 2001)"/>

    <meta name="citation_author" content="J. M. V&#233;zien"/>

    <meta name="citation_author_email" content="vezien@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="B. M&#233;n&#233;las"/>

    <meta name="citation_author_email" content="bob@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="J. Nelson"/>

    <meta name="citation_author_email" content="julien.nelson@paris.ensam.fr"/>

    <meta name="citation_author_institution" content="Laboratoire Conception de Produits et Innovation, Arts et M&#233;tiers ParisTech, Paris, France"/>

    <meta name="citation_author" content="L. Picinali"/>

    <meta name="citation_author_email" content="picinali@ircam.fr"/>

    <meta name="citation_author_institution" content="Institut de Recherche et de Coordination Acoustique/Musique, UMR CNRS 9912, Paris, France"/>

    <meta name="citation_author" content="P. Bourdot"/>

    <meta name="citation_author_email" content="bourdot@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="M. Ammi"/>

    <meta name="citation_author_email" content="ammi@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="B. F. G. Katz"/>

    <meta name="citation_author_email" content="katz@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="J. M. Burkhardt"/>

    <meta name="citation_author_email" content="jean-marie.burkhardt@univ-paris5.fr"/>

    <meta name="citation_author_institution" content="ECI, Universit&#233; Paris V, Paris, France"/>

    <meta name="citation_author" content="L. Pastur"/>

    <meta name="citation_author_email" content="pastur@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_author" content="F. Lusseyran"/>

    <meta name="citation_author_email" content="lussey@limsi.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Informatique et de M&#233;canique pour les Sciences de l&#8217;Ing&#233;nieur, Centre National de la Recherche Scientifique, Orsay Cedex, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-009-0134-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2009/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-009-0134-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project"/>
        <meta property="og:description" content="In the last 30&amp;nbsp;years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed the emergence of the discipline known as computational fluid dynamics or CFD. More recently, virtual reality (VR) systems have proven an interesting alternative to conventional user interfaces, in particular, when exploring complex and massive datasets, such as those encountered in scientific visualization applications. Unfortunately, all too often, VR technologies have proven unsatisfactory in providing a true added value compared to standard interfaces, mostly because insufficient attention was given to the activity and needs of the intended user audience. The present work focuses on the design of a multimodal VR environment dedicated to the analysis of non-stationary flows in CFD. Specifically, we report on the identification of relevant strategies of CFD exploration coupled to adapted VR data representation and interaction techniques. Three different contributions will be highlighted. First, we show how placing the CFD expert user at the heart of the system is accomplished through a formalized analysis of work activity and through system evaluation. Second, auditory outputs providing analysis of time-varying phenomena in a spatialized virtual environment are introduced and evaluated. Finally, specific haptic feedbacks are designed and evaluated to enhance classical visual data exploration of CFD simulations."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-009-0134-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Computer fluid dynamics, Sonification, Haptics, Multimodal virtual environment","kwrd":["Virtual_reality","Computer_fluid_dynamics","Sonification","Haptics","Multimodal_virtual_environment"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-009-0134-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-009-0134-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=134;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-009-0134-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0134-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0134-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2009-09-16" itemprop="datePublished">16 September 2009</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J__M_-V_zien" data-author-popup="auth-J__M_-V_zien" data-corresp-id="c1">J. M. Vézien<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-B_-M_n_las" data-author-popup="auth-B_-M_n_las">B. Ménélas</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J_-Nelson" data-author-popup="auth-J_-Nelson">J. Nelson</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Arts et Métiers ParisTech" /><meta itemprop="address" content="grid.434207.6, 0000000121946047, Laboratoire Conception de Produits et Innovation, Arts et Métiers ParisTech, Paris, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-L_-Picinali" data-author-popup="auth-L_-Picinali">L. Picinali</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institut de Recherche et de Coordination Acoustique/Musique, UMR CNRS 9912" /><meta itemprop="address" content="grid.425206.7, 0000000406413041, Institut de Recherche et de Coordination Acoustique/Musique, UMR CNRS 9912, 1 Place Igor-Stravinsky, 75004, Paris, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-P_-Bourdot" data-author-popup="auth-P_-Bourdot">P. Bourdot</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-M_-Ammi" data-author-popup="auth-M_-Ammi">M. Ammi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-B__F__G_-Katz" data-author-popup="auth-B__F__G_-Katz">B. F. G. Katz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J__M_-Burkhardt" data-author-popup="auth-J__M_-Burkhardt">J. M. Burkhardt</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="ECI, Université Paris V" /><meta itemprop="address" content="grid.10992.33, 0000000121880914, ECI, Université Paris V, Paris, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-L_-Pastur" data-author-popup="auth-L_-Pastur">L. Pastur</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-F_-Lusseyran" data-author-popup="auth-F_-Lusseyran">F. Lusseyran</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre National de la Recherche Scientifique" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 13</b>, Article number: <span data-test="article-number">257</span> (<span data-test="article-publication-year">2009</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">186 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-009-0134-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In the last 30 years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed the emergence of the discipline known as computational fluid dynamics or CFD. More recently, virtual reality (VR) systems have proven an interesting alternative to conventional user interfaces, in particular, when exploring complex and massive datasets, such as those encountered in scientific visualization applications. Unfortunately, all too often, VR technologies have proven unsatisfactory in providing a true added value compared to standard interfaces, mostly because insufficient attention was given to the activity and needs of the intended user audience. The present work focuses on the design of a multimodal VR environment dedicated to the analysis of non-stationary flows in CFD. Specifically, we report on the identification of relevant strategies of CFD exploration coupled to adapted VR data representation and interaction techniques. Three different contributions will be highlighted. First, we show how placing the CFD expert user at the heart of the system is accomplished through a formalized analysis of work activity and through system evaluation. Second, auditory outputs providing analysis of time-varying phenomena in a spatialized virtual environment are introduced and evaluated. Finally, specific haptic feedbacks are designed and evaluated to enhance classical visual data exploration of CFD simulations.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction: issues and bottlenecks</h2><div class="c-article-section__content" id="Sec1-content"><p>Many scientific disciplines produce massive sets of complex data in a routine way. The process of extracting hidden patterns from such data is generically referred to as “data mining.” It often requires human interaction to further exploit the perceptive and cognitive abilities of users, so as to focus studies on underlying phenomena of interest. Computational fluid dynamics (CFD) is a strong field of application because the study of the characteristics of 3D dynamic structures generated within a flow is of growing importance, especially for flow control. The fields of application are numerous: automotive and aviation design (aerodynamic optimization and trail analysis), urban environment (circulation of air and pollutants), meteorology, oceanography, solar dynamics, planetary magnetism, etc.</p><h3 class="c-article__sub-heading" id="Sec2">A virtual wind tunnel?</h3><p>In the last 30 years, evolution of numerical data processing in terms of computing power, data storage capacity and algorithmic efficiency in modeling and simulating physical phenomena have led CFD experts to envision the creation of a “virtual wind tunnel”: the numerical data (generated in real time by simulation code) would be uploaded to a 3D interactive environment, thus making it possible for the experts to analyze the evolution of a non-stationary flow “live,” using various interaction tools (Bryson and Levit <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Bryson S, Levit C (1991) The virtual windtunnel: an environment for the exploration of three-dimensional unsteady flows. In: IEEE Visualization, pp 17–24" href="/article/10.1007/s10055-009-0134-1#ref-CR5" id="ref-link-section-d54791e504">1991</a>). The application expert could also directly interact with the CFD code by modifying simulation parameters directly, controlling the numerical experiment with natural interfaces (e.g., changing the geometry of a simulated foil shape and being able to visualize the effects immediately).</p><p>With the current state of available technology, we are still far from this objective and several obstacles hinders us from reaching it. First is the sheer amount of data required, which involves the need to store, transmit, and represent several gigabytes of information per second, even for the simplest of 3D flows. There currently exist a number of powerful techniques for structuring and compressing data, making it possible to consider the transfer of such simulation results in real time from a distant machine (the simulation server) to the user interface. However, interactive computation of these data, as well as their transfer to rendering servers to produce visual, audio, or haptic feedbacks appears unattainable for the time being. However, existing rendering and interaction devices suggest promising possibilities for future CFD tools. It is expected that interactive virtual reality (VR) simulations should drastically improve the research conditions of physicists, providing a wealth of possibilities to perceive and analyze complex phenomena. They should also provide highly favorable environments for the training of future engineers and researchers.</p><p>The most complex CFD simulations, where turbulence is fully developed and conditions the phenomenon of interest, can only be approximated using purely statistical approaches. Relying on VR is probably not essential in that case. On the other hand, the time–space organization of coherent structures of a flow and its temporal variability are key points for the interpretation and control of the main properties of the simulation. This organization is three-dimensional in space, in addition to being non-stationary. Traditional tools for 3D viewing and volume rendering using lighting, 2D slabs, etc., although very useful, remain of limited effectiveness relative to this 4D problem. Immersive VR makes aids in achieving a better understanding of the various physical variables within the CFD results (such as speed vectors, density, energy, pressure, temperature, rotational speed, vorticity). Since the visualization space is larger, more data can be displayed in order to fill the user’s field of view. Moreover, during the exploration stage, which involves locating and isolating relevant areas of the flow, this task is facilitated by interactive VR navigation. In addition, the analysis of a given simulation often requires users to visualize several fields simultaneously, using various tools (e.g., pressure isosurfaces combined with velocity streamlines). This type of complex visual analysis may be facilitated by an immersive approach (Ziegeler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Ziegeler S, Moorhead RJ, Croft PJ, Lu D (2001) The MetVR case study: meteorological visualization in an immersive virtual environment. In: IEEE 12th conference on visualization (VIS’ 2001)" href="/article/10.1007/s10055-009-0134-1#ref-CR39" id="ref-link-section-d54791e513">2001</a>). Following this, the fundamental rationale of the present work is to study the conditions under which multimodal VR interfaces may complement conventional desktop usage when exploring complex and massive CFD datasets.</p><h3 class="c-article__sub-heading" id="Sec3">The perception challenge</h3><p>The sheer size of typical CFD simulations makes it impossible to visualize them, as they are, on current graphics processors, without performing a significant reduction of the amount of data to be displayed, so that interactive rendering does not suffer from any significant latency. This operation is called adaptive visualization and can be achieved with the help of several data reduction techniques. Two-dimensional surfaces can be efficiently visualized with traditional algorithms, such as polygonal reduction (Hoppe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Hoppe H (1996) Progressive Meshes ACM SIGGRAPH 96. In: Computer graphics proceedings, annual conference series, pp 99–108" href="/article/10.1007/s10055-009-0134-1#ref-CR18" id="ref-link-section-d54791e524">1996</a>) or point-based rendering (Levoy and Rusinkiewicz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Levoy M, Rusinkiewicz S (2000) QSplat: a multiresolution point rendering system for large meshes. In: SIGGRAPH’2000, Computer graphics proceedings, annual conference series, pp 343–352" href="/article/10.1007/s10055-009-0134-1#ref-CR23" id="ref-link-section-d54791e527">2000</a>). More specialized techniques include particle-based approaches or volume rendering (Crawfis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Crawfis RA, Shen H-W, Max N (2000) Flow visualization techniques for CFD using volume rendering. In: 9th International symposium on flow visualization" href="/article/10.1007/s10055-009-0134-1#ref-CR9" id="ref-link-section-d54791e530">2000</a>). Another useful method is to carry out “on-demand” data loading, by visualizing only the data that the user perceives or is likely to perceive in the near future, taking into account the context of interaction. In this way, only the data contained in the (culled) volume viewed by an immersed user will be loaded in the graphic memory. The most relevant approach (but also the most complex one), consists in structuring 3D information specifically for VR exploration. As an example, we developed a specific fast visualization technique dedicated to immersive viewing of isosurfaces computed on rectilinear, non-uniform grids. The technique is based on a hierarchical, octree-based partitioning computed in a single pre-processing stage from the input data. Based on this structure, fast isosurface computation restricted to the volume of view can be achieved by quickly rejecting 95% of the octree nodes. Details can be found in Gherbi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gherbi R, Bourdot B, Vezien JM, Herisson J, Fauvet N, Ferey N (2006) Le traité de la réalité virtuelle, vol 4: Les applications de la Réalité Virtuelle. In: Fuchs P, Moreau G, Papin J-P (eds) Chapter 4: Explorations de données scientifiques et expérimentations virtuelles. Presses de l’Ecoles de Mines de Paris" href="/article/10.1007/s10055-009-0134-1#ref-CR36" id="ref-link-section-d54791e533">2006</a>).</p><p>Despite numerous and highly desirable efforts to quickly and efficiently visualize rapidly evolving volumes of data, the exploration of complex flow structures often requires the simultaneous representation of many different mathematical values at the same location. This need rapidly leads to a saturation of the user’s visual channel. Moreover, vision is not always best suited for the perception of intermittent time-dependent phenomena or highly local phenomena (intermittency of turbulence, structure breaks resulting from stretching in swirls, etc.). Therefore, the importance of adopting a multisensory VR approach to CFD logically arises.</p><p>For example, the audio channel may provide a natural means of perceiving the dynamics of phenomena, such as their periodic, chaotic, or turbulent character. It should be noted that these features cannot easily be directly perceived visually unless (1) they are in the user’s field of view and (2) the visualization parameters are correctly tuned (e.g., correct isovalues). Spatial auditory perception can be exploited to detect and analyze this type of phenomenon within the volume of flow, also providing the user with a guide to their spatial region location. The sense of touch can also be perceived in a VR setup, through the use of specific devices such as haptic arms or vibro-tactile gloves (Burdea <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Burdea G (1996) Force and touch feedback for VR. Wiley, Chichester" href="/article/10.1007/s10055-009-0134-1#ref-CR6" id="ref-link-section-d54791e542">1996</a>). Nevertheless, the use of haptics for flow exploration calls for very specific considerations. One can exploit haptic feedback to “feel” selected physical characteristics of the flow, such as gradients, pressure fields. It is also possible to guide the hand of the user toward certain areas of interest. This is very different from the common use of haptics for feeling the hard surfaces of virtual objects in a 3D environment. In addition, the very nature of CFD data, with rapidly varying (and sometimes hard to predict) properties in space and time, calls for specific developments in order to provide useful and stable sensorimotor feedbacks to a CFD expert immersed in a VR simulation.</p><h3 class="c-article__sub-heading" id="Sec4">Identifying user tasks and needs</h3><p>Overall, research in the field of VR-assisted exploration of scientific data has led to the design of rich and powerful systems from a strictly technical point of view (Van Dam et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Van Dam A, Forsberg AS, Laidlaw DH, LaViola JJ, Simpson RM (2000) Immersive virtual reality for scientific visualization: a progress report. IEEE Comput Graphics Appl 20(6):26–52" href="/article/10.1007/s10055-009-0134-1#ref-CR37" id="ref-link-section-d54791e553">2000</a>). Nevertheless, the resulting platforms are not judged very usable and, at the very least, have not led to a general acceptance by the CFD community. One of the reasons for the lack of widespread acceptance of VR technologies is a simplistic and somewhat naive view of the activity and real needs of CFD users. The techno-centric view focusing on data management and rendering, while necessary, has also led to a relative neglect of user tasks and needs. A state of the art review of the VR-CFD domain quickly reveals that very little exists concerning the evaluation of previous efforts, either of the underlying design methodology or the actual impact on potential users. One manner to overcome this recurring issue is by explicitly adopting a methodology based on user-centered design (Maguire <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Maguire M (2001) Methods to support human-centred design. Int J Hum Comput Stud 55(4):587–634" href="/article/10.1007/s10055-009-0134-1#ref-CR26" id="ref-link-section-d54791e556">2001</a>). This design must be founded in an understanding of both the object of study (e.g., specificities of a CFD simulation) and from how experts study the object or are likely to study it with the future VR interfaces.</p><h3 class="c-article__sub-heading" id="Sec5">Focus and contribution of CoRSAIRe/CFD</h3><p>The present work outlines the main advances of the CoRSAIRe/CFD project, a 3-year government-funded effort gathering experts in VR, CFD analysis, and multimodal supervision. CoRSAIRe set out to create, in two standard cases,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> a truly usable VR exploration environment by formalizing the relationships between the analysis performed by expert users and the numerical data which they rely on. The focus of the present study is thus the identification of relevant strategies of CFD exploration coupled to adapted data representation and interaction techniques.</p><p>The contributions of the work are (a) providing a systematic methodology to promote user needs in the VR design loop, through a formal task description leading to the identification of current issues in CFD analysis and (b) implementation of several adapted multisensory interaction techniques to match the foreseen activity of CFD experts. In particular, new 3D audio and haptic feedbacks have been designed and implemented to aid in the analysis of fluid flow.</p><p>The remaining sections of the paper are organized as follows:</p><p>Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec6">2</a> presents the task analysis process, whose goal is to model the activity of a CFD expert investigating a typical dataset, providing the necessary bridge between tasks and display modalities. This analysis led to several useful observations on the nature of existing investigation methods, anticipating needs that may not be immediately perceived (or at least explicitly formulated) by users. It also provides a means to give recommendations on how an immersive virtual environment should allocate the available modalities.</p><p>Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec13">3</a> describes the technical issues faced in the process of setting up an immersive flow simulator. It first describes the complete architecture integrating the different VR components into a flexible and efficient experimental test bed. Then, auralization techniques adapted to provide CFD experts with meaningful auditory feedbacks are presented. It then focuses on designing efficient haptic rendering for flow dynamics, based on structural observations. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec20">4</a> describes ongoing evaluation experiments being carried out to assess the potential benefits of multimodal VR exploration for CFD.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">An initial step for design: clarifying field practices and user needs</h2><div class="c-article-section__content" id="Sec6-content"><h3 class="c-article__sub-heading" id="Sec7">Methods</h3><p>Our study first focused on the use of existing desktop-based tools commonly used to explore large numerical simulations of fluid flows. Interviews were carried out in order to gauge the impact, as perceived by users, of introducing innovative technologies (i.e., VR and multimodal interfaces) in a field already replete with working tools and practices. Five researchers in CFD, average age of 43.5 years (SD = 6.8 years) took part in the study. Each had a mean experience of 10.5 years in the use of CFD software. Two separate techniques were used for the analysis of user needs: semi-directed interviews and observation of work sessions. In the first stage, we carried out six semi-directed interviews with the subjects in the workplace, recorded them, and transcribed them verbatim. Interviews, both confidential and anonymous, focused on three major points: ongoing research, experience and habits in the use of flow simulations, and possible future uses of VR and consequences of its use on everyday work. Interview corpora were subjected to a cognitive discursive analysis (Ghiglione et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ghiglione R, Landré A, Bromberg M, Molette P (1998) L’analyse automatique des contenus [Automatic content analysis]. Dunod Ed" href="/article/10.1007/s10055-009-0134-1#ref-CR17" id="ref-link-section-d54791e609">1998</a>) using the Tropes program developed by Acetic Software. This analysis focused on identifying what properties were deemed relevant (e.g., physical, mathematical) in the study of a CFD simulation, and what strategies were used in the navigation of these complex datasets. In the second stage, we recorded and analyzed four work sessions in which individual subjects used their existing tools (AVS or TecPlot) to generate and explore visual renderings of numerical flow simulations. The simulations used varied between subjects, since they were representative of problems with which they were very familiar and which were part of their ongoing work. Video recordings served as a basis to analyze and model the exploration task. Verbal protocols (Ericsson and Simon 1984/<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Ericsson KA, Simon, HA (1984/1993) Protocol analysis: Verbal reports as data (revised edition)" href="/article/10.1007/s10055-009-0134-1#ref-CR13" id="ref-link-section-d54791e612">1993</a>) were collected throughout, taking explicit verbalization of the tasks carried out as a unit of analysis.</p><p>Coding the subjects’ actions and verbalizations allowed us to construct a task model using the hierarchical task analysis methodology (HTA, see Annett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Annett J (2003) Handbook of cognitive task design. In: Chapter Hierarchical Task Analysis, pp 17–35" href="/article/10.1007/s10055-009-0134-1#ref-CR3" id="ref-link-section-d54791e618">2003</a>). HTA is based on the premise that tasks may be described following a hierarchy of goals and subgoals. This allows for the close examination of what user goals are and, more importantly to design, what the user needs in order to achieve each goal in terms of information, product functions, etc.</p><h3 class="c-article__sub-heading" id="Sec8">Results</h3><p>The user and task analysis led to the construction of three elements to assist the design of a multimodal VR application for the exploration of flow simulations: (1) a model of the tasks carried out in the exploration of existing simulations, chosen by subjects as representative of their ongoing work; (2) a model of user needs; and (3) a set of principles to guide choices in terms of modal allocation to the various kinds of data used by subjects and the choice of relevant interaction techniques.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Use case: the cavity simulation</h4><p>To serve as an example of a typical CFD analysis task, it was decided to focus on the simulation of an incompressible flow inside an open cavity.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> This cavity flow was exploited throughout the CoRSAIRe/CFD project to experiment with the new multimodal immersive schemes described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec13">3</a>. The setup is displayed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig1">1</a>. In all subsequent discussion, we will respectively denote <i>x</i>, <i>y</i>, <i>and</i>
                              <i>z</i>, the longitudinal, vertical, and transverse directions of the flow, and <i>V</i> = (<i>v</i>
                              <sub>
                      <i>x</i>
                    </sub>,<i>v</i>
                              <sub>
                      <i>y</i>
                    </sub>,<i>v</i>
                              <sub>
                      <i>z</i>
                    </sub>) the corresponding velocity vector. The cavity is 100 mm long and 50 mm high. The total height of the domain is 125 mm, while the total length is 410 mm. For the boundary conditions at the outlet, the longitudinal velocity component was computed using mass conservation over the domain. The gradient of the other two velocity components was set to zero and no-slip conditions were used at the walls. A discretization of 256 cells were used in the longitudinal direction and 128 in the spanwise and normal directions. The mesh was refined near the walls and over the cavity in order to obtain a fine resolution on structures generated by instabilities. To minimize numerical inaccuracy, the greatest size variation between successive cells was 3% over the cavity (5% elsewhere) and the dimensional ratio for one cell was of the order of 1.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The geometry of the CFD simulation used as a test case</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The flow is simulated by numerically solving over time the Navier–Stokes equations for incompressible flow:
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \nabla \cdot V = 0 \quad (\hbox{Mass equation}) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                              <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \frac{\partial{V}} {\partial{t}}+ \nabla \cdot V^tV = -\frac{1} {\rho_0}\nabla{P} + \nabla \cdot \nu \nabla V \quad (\hbox{Momentum equations}) $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>
                              <i>V</i> is the (eulerian) particle speed, <i>t</i> is the time, ρ<sub>0</sub> the uniform and constant density, <i>P</i> the pressure, and ν the kinematic viscosity (constant).</p><p>The reader should refer to Gadouin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Gadouin E, Le Quéré P, Daube O (2001) A general methodology for investigating flow instability in complex geometries: application to natural convection in enclosures. Int J Numer Methods Fluids 37(2):175–208" href="/article/10.1007/s10055-009-0134-1#ref-CR16" id="ref-link-section-d54791e754">2001</a>) for details on the numerical solving of these equations in the special case of the cavity of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig1">1</a>.</p><p>Several days of super-computer time were necessary to run the simulated experiment, whose actual duration is on the order of one minute. Velocity vector data resulting from the simulation was stored in two separate sets: (a) each sample point of the complete domain with a frequency of 30 Hz and (b) 21 special interest points, in or near the cavity, with a measuring frequency of 400 Hz. Each step of the simulation occupies about 50 MB of storage space, resulting in a total of about 100 GB.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">A task-oriented hierarchical model of current task</h4><p>The HTA task tree (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig2">2</a>) describes the task of exploring the flow simulation as a hierarchy of tasks and subtasks. This model was extracted by combining results from interviews and observations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overall view of the task structure for processing the simulation data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Much of the CFD expert’s work (task 1) rests on formalizing the problem at hand in order to construct a relevant protocol to generate flow data. This task involves exploring scientific literature, to identify relevant physical properties, and also mathematical tools, that may be adapted from existing work, to the problem at hand.</p><p>For example, interviews with one subject and analysis of related literature (Podvin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Podvin B, Fraigneau Y, Lusseyran F, Gougat P (2006) A reconstruction method for the flow past an open cavity. J Fluids Eng. 128(3):531–540" href="/article/10.1007/s10055-009-0134-1#ref-CR33" id="ref-link-section-d54791e798">2006</a>; Pastur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pastur L, Lusseyran F, Faure TM, Fraigneau Y, Pethieu R, Debesse P (2008) Quantifying the nonlinear mode competition in the flow over an open cavity at medium Reynolds number. Exp Fluids 44(4):597–608" href="/article/10.1007/s10055-009-0134-1#ref-CR32" id="ref-link-section-d54791e801">2008</a>) showed that building this simulation involved four subtasks. First, the subject chose (with colleagues) to model the flow inside an open cavity as an incompressible flow. This term alludes to a well-known set of physical properties, as well as to a widely accepted mathematical model, derived from the Navier–Stokes equation. Secondly, simplification of this model was carried out through the strategic choice of specific schemes of the numerical simulation, allowing cancelation of some of the equation’s terms, and therefore faster solving of the simulation equations. Thirdly, this mathematical model is solved for given boundary conditions reflecting the geometrical and dynamical conditions of the cavity flow. In short, the custom model was built by specifying unusual and little-known approximations and initial conditions to a well-known general simulation problem.</p><p>Running the simulation then allows one to save snapshots of the flow. The first instants computed represent the simulation response to the given initial conditions and do not represent the “natural” flow behavior, but rather a transient stage. Some subjects referred to this process as “letting structures grow,” suggesting that the desired state of the flow was one where specific structures would be apparent. In the case described here, subjects used their knowledge of domain literature, past experiments, and visual signatures typical of specific structures in numerical simulations to identify precisely which structures the simulation yielded, i.e., structures known as “Kelvin–Helmholtz rolls,” “pulsating vortices” and “Taylor–Görtler vortices.” This new knowledge allowed researchers to further their knowledge of flow behavior inside the cavity, thus validating their hypotheses and strengthening and revising their mental model of flow behavior (Chinn and Brewer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Chinn CA, Brewer WF (1993) The role of anomalous data in knowledge acquisition: a theoretical framework and implications for science instruction. Rev Educ Res 63(1):1–49" href="/article/10.1007/s10055-009-0134-1#ref-CR8" id="ref-link-section-d54791e807">1993</a>).</p><p>The HTA model highlights current working practices as highly constrained by the characteristics of GUI-based desktop tools. In particular, CFD experts construct mental models of dynamic 3D structures based on the sequential exploration of flow slabs and instants. In contrast, VR-based multimodal environments may overturn existing tools by making the whole set of relevant information accessible to users “in one sitting” through the use of multiple sensory modalities. Although this task model only formalizes existing exploration strategies, we expect VR to fundamentally change its structure by allowing new behaviors to emerge in an immersive environment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Users’ informational needs</h4><p>Cognitive discursive analysis results highlighted the importance of pattern recognition in the study of flow properties. For example, analysis results highlighted the concept of a “vortex” as particularly important and related to a specific physical behavior, mathematical formula, and graphical signature. In 3D view, vortices appeared as tubular structures; in 2D view, as a range of elliptical structures. Recognizing such patterns relies on several types of information and mathematical parameters form the basis of this reasoning. Although the parameters used are highly problem-specific, velocity and vorticity were shown to be frequently used by all subjects. Spatial (2D and 3D) and temporal (4D) distribution of these parameters allows identification of the underlying structures of the flow (e.g., vortices) and their dynamic properties. Data visualization techniques allowed construction of standard representations such as isosurfaces and isocontours to facilitate pattern recognition by the user. Pattern recognition also relies on the fact that the CFD specialist already knows “what to look for” in terms of graphical signatures, since he is often responsible for building the simulation as well as analyzing its results.</p><p>One apparent limitation of existing visualization software is that it only supports detailed visualization of flow properties on 2D “slabs.” In contrast, 3D views only provide information about the overall topology of the flow. Time-dependent properties are therefore accessed through the sequential examination of slabs and 3D views.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Recommendations for modal allocation</h4><p>André (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="André E (2000) Handbook of natural language processing. In: Chapter “The generation of multimedia presentations”, pp 305–327" href="/article/10.1007/s10055-009-0134-1#ref-CR2" id="ref-link-section-d54791e834">2000</a>) used the term “modal allocation” to describe the use of specific sensory modalities to present information. This is a necessity when designing a multimodal user interfaces, since designers are confronted with a particularly large design space. In particular, the questions posed are “What is the most relevant sensory channel to convey the various pieces of information necessary to the exploration task?” and “What is the most relevant way to present this information in this particular channel?”</p><p>Several criteria are involved when proposing a modal allocation scheme, notably (1) hardware and software limitations, (2) task-related information semantics, and (3) user characteristics, e.g., perceptual and cognitive characteristics. Although existing tools provide a wealth of possibilities for data presentation, our findings suggest that relatively few of these are used in the exploration stage itself. Clear identification of user needs thus allows a minimalist approach to product design, thus simplifying the design process. This means that the development of a VR prototype could limit itself to a few key modalities. Proposing principles for modal allocation implies giving more weight to specific information-modality pairings. Few guidelines exist to guide this process, and none of them can be described as universal, though some provide significant guidance in designing interfaces to explore abstract, numerical, rather than concrete and realistic data (Nesbitt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nesbitt KV (2003) Designing multi-sensory displays for abstract data. PhD thesis, School of Information Technology, University of Sydney" href="/article/10.1007/s10055-009-0134-1#ref-CR31" id="ref-link-section-d54791e840">2003</a>). However, task analysis may help in providing specific guidance depending on information semantics, i.e., the properties of the displayed information that can be viewed as directly relevant to the task at hand. Specifically, the properties identified were as follows: variables dependent on space and time need to be superimposed against the more stable elements of the experimental setup (i.e., the flow’s “surroundings”). Visual feedback may be used to display these invariant elements. Within the flow, scientific reasoning is structured around a limited number of dynamic objects (e.g., vortices, jets, plumes) who have a distinctive spatial and temporal signature. Thus, the use of visual, audio, and temporal channels needs to account for these objects’ shape and temporal variations thereof (e.g., through audio or haptic feedbacks). Beyond identification of these structures, CFD scientists’ work also involves navigation between them in order to reconstruct the global topology and physical behavior of the flow. This implies the use of landmarks—in Lynch’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1960" title="Lynch K (1960) The image of the city. MIT Press, Cambridge, MA" href="/article/10.1007/s10055-009-0134-1#ref-CR25" id="ref-link-section-d54791e843">1960</a>) sense—to help speed up navigational tasks, which may be presented in any modality, such as Donker et al.’s “torch metaphor” (Donker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Donker H, Klante P, Gorny P (2002) The design of auditory user interfaces for blind users. In: 2nd Nordic conference on human-computer interaction (NordiCHI ’02), vol 31. ACM, New York, pp 149–156" href="/article/10.1007/s10055-009-0134-1#ref-CR11" id="ref-link-section-d54791e846">2002</a>) or systems for haptic guidance.</p><p>The term ‘landmark’, however, only partially reflects the reality of flow exploration. Indeed, flow structures are also time dependent, and CFD experts are more interested in dynamic events than they are in static landmarks. Finally, a logical consequence of this is that users do not access information regarding the behavior of the whole flow at any one time, but need to piece it together by analyzing specific events one by one. This essentially removes the risk of perceptual masking between several sources of information.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Multimodal interaction for immersive CFD exploration</h2><div class="c-article-section__content" id="Sec13-content"><p>Using VR environments for scientific data examination ultimately means that user needs meet technical implementations of VR systems, with the current possibilities and limitations of hardware and software. A user-centered approach should prevent the design process from being exclusively pushed by technological factors, but also pulled by users’ work-related needs.</p><p>This section presents how relevant observations and evaluations on realistic test cases helped in the design of helpful immersive, multimodal feedbacks in the CoRSAIRe/CFD framework.</p><h3 class="c-article__sub-heading" id="Sec14">VR architecture</h3><p>Early on, several off-the-shelf pieces of CFD simulation software were identified as potential test beds for the CoRSAIRe experiments. The requirements were
</p><ul class="u-list-style-dash">
                    <li>
                      <p>management of classic graphical objects for CFD simulations (streamlines, isosurfaces, stream ribbons, cutting planes, etc.). This was crucial to provide potential users with a familiar environment where standard 3D representations could be easily invoked.</p>
                    </li>
                    <li>
                      <p>distributed multiscreen visualization along with stereo display capabilities: all experiments were to take place in a large immersive room;</p>
                    </li>
                    <li>
                      <p>management of classic VR functionalities (e.g., 3D tracking is mandatory to match the visual and auditory viewpoint with the user’s position or to interact with the simulation with a 3D pointer);</p>
                    </li>
                    <li>
                      <p>easy-to-use API for extending the input/output capabilities according to needs (in our case, extend visual channel with haptics and audio);</p>
                    </li>
                    <li>
                      <p>reasonable cost;</p>
                    </li>
                    <li>
                      <p>a stable platform that could be reused and extended as the project and its offsprings unfold.</p>
                    </li>
                  </ul>
                        <p>Taking in account these requirements, the final choice was amiraVR, commercialized by Mercury Systems.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> The global hardware and software configuration are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The VR architecture for the CoRSAIRe/CFD experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Our VR setup consists of two large (2 m × 2 m) screens in an L-shape configuration, each displaying the output of a powerful workstation hosting an Nvidia FX 3000G quadro graphic card. Outputs are genlocked and framelocked to ensure that stereo frames are properly synchronized at 100 Hz. A third identical workstation serves as a central console and hosts the communication modules between the central CFD applications and the various inputs and outputs of the system. Viewpoint tracking and 3D pointing are performed by an ART-track2 infrared optical tracking system
<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> with a refresh rate of 60 Hz. The haptic interface consists of a large 6-DOF VIRTUOSE haptic arm provided by the HAPTION company, controlled through the Virtuose API and running on a dedicated PC. The audio rendering is performed on a separate PC equipped with a sound processing card. Sonification is performed within Max/MSP, a graphical development environment for music and multimedia (Website: <a href="http://www.cycling74.com">http://www.cycling74.com</a>). Communications between the different components (inputs, processing, and outputs) are managed through a dedicated client-server architecture developed by one of the CoRSAIRe partners codenamed VEserver/EVI3d (Touraine and Bourdot <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Touraine D, Bourdot P (2001) VEserver : a manager for input and haptic multi-sensorial device. In: IEEE international workshop on robot-human interactive communication (IEEE ROMAN 2001), Bordeaux and Paris" href="/article/10.1007/s10055-009-0134-1#ref-CR35" id="ref-link-section-d54791e976">2001</a>). Communications themselves rely on message-passing using the UDP protocol. For example, a custom protocol was developed, based on the OSC specifications (Open Sound Control, see Wright et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wright M, Freed A, Momeni A (2003) Open sound control: state of the art 2003. In: International conference on new interfaces for musical expression, Montreal, pp 153–159" href="/article/10.1007/s10055-009-0134-1#ref-CR38" id="ref-link-section-d54791e979">2003</a>), to provide communication between the sonification modules and the core amiraVR<sup>TM</sup> application. This protocol allows one to easily manage the association between the chosen variables and the sonification parameters while removing many configuration details from the amiraVR<sup>TM</sup> platform.</p><p>Inevitable compromises resulted from combining some specialized, home-made VR components with a commercial application (trading flexibility and source access vs. visualization power). Despite these compromises, the resulting architecture proved fruitful and permitted the integration of audio and haptic modalities into existing visual CFD exploration scenarios explored in early evaluation sessions (Cf. Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec6">2</a>).</p><h3 class="c-article__sub-heading" id="Sec15">Audio modalities</h3><p>Sonification refers to the use of non-speech audio to convey information. Due to the high temporal resolution and wide bandwidth, the use of auditory stimuli is highly suitable for the display of time-varying parameters (when compared to other modalities such as video and haptics), concurrent streams (the superposition of multiple audio renderings for various parameters is possible and easily comprehensible if properly designed), and spatial information (lower definition if compared to visual stimuli, but possible over the 360° sphere, therefore true full space three-dimensional rendering).</p><p>One study was recently performed within this framework with regards to sound spatialization, which aimed to examine the effect of sound spatialization on a specific sonification and sound exploration task (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference of auditory display (ICAD 2008), Paris" href="/article/10.1007/s10055-009-0134-1#ref-CR20" id="ref-link-section-d54791e1004">2008</a>). Subjects were asked to virtually navigate, using a pointing and tracking device, a two-dimensional topological function mapped onto the surface of a sphere surrounding the user (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig4">4</a>). The data function was sonified with a modified beep sound; the task was to find the maximum of the function, the point with the highest frequency sound and to validate its position. The experiment was repeated with and without the use of sound spatialization techniques. While spatialization was not required to perform the task, the precision of target selection appears to improve with the addition of spatialization. This simple test case platform of a single sound source can be used for the investigation of basic principles of auditory spatial sonification exploration. The CFD data sonification task presents a much more complicated dataset, where multiple regions of interests exist and must be explored and understood.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The virtual sphere around the subject (<i>left</i>), and an example of the two-dimensional function mapped on its surface, represented as a planisphere (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference of auditory display (ICAD 2008), Paris" href="/article/10.1007/s10055-009-0134-1#ref-CR20" id="ref-link-section-d54791e1023">2008</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>When dealing with CFD simulations, a considerable importance is given to the perception of phenomena characterized by their intermittent nature and, most of all, strongly localizable within space (i.e., intermittence of a turbulence structure, breaking-up of the structure). Sonification can be employed to detect these phenomena within the cavity simulation test bed (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Katz BFG, Warusfel O, Bourdot P, Vezien J-M (2007) CoRSAIRe—Combination of sensori-motor rendering for the immersive analysis of results. In: 2nd International workshop on interactive sonification (ISon 2007), York" href="/article/10.1007/s10055-009-0134-1#ref-CR19" id="ref-link-section-d54791e1038">2007</a>), and auditory spatialization can be used to segregate concurrent streams and to guide the user toward a specific position of interest, allowing one to follow future development of the specified parameter or structure in that location.</p><p>The data resulting from the cavity simulation described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec6">2</a> (velocity components <i>v</i>
                           <sub>
                    <i>x</i>
                  </sub>, <i>v</i>
                           <sub>
                    <i>y</i>
                  </sub>, <i>v</i>
                           <sub>
                    <i>z</i>
                  </sub>) have only been slightly pre-processed for sonification. The DC component has been removed from the different parameter values for each position, removing the constant flow component and focusing only on variations around the median value. This results in a dataset which is well suited to audio rendering. The real-time duration of the simulated experiment is on the order of one minute, with the temporal region of expressed interest lasting on the order of tens of seconds.</p><p>Two software platforms have then been developed for the sonification of these data inside the cavity: the first platform treats the sonification of the 21 discrete points at a raw sample rate of 400 Hz, and the second sonification platform is for any point within the data volume recorded with the sample rate of 30 Hz. The sonification was developed using the Max/MSP platform (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec14">3.1</a>). An example of a user scenario for the sonification configuration of the 400 Hz dataset consists in selecting a number of monitoring/observation points within the cavity and choosing data parameters for sonification (such as the velocity of particles along the <i>X</i>-axis). The rendered audio streams are spatialized in real time, coherent with the visual display according to the current position and orientation of the experimenter using 3D tracking information received from the central system.</p><p>A large variety of sonification techniques exist and are used in various applications (Kramer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Kramer G (ed) (1994) Auditory display: sonification, audification, and auditory interfaces. Santa Fe Institute Studies in the Sciences of Complexity, Westview Press, Santa Fe" href="/article/10.1007/s10055-009-0134-1#ref-CR22" id="ref-link-section-d54791e1088">1994</a>). The technique that was selected and implemented for CoRSAIRe/CFD is termed as “audification.” Audification is based on the transformation of a generic time varying signal into an audible signal. This method is well suited to the CFD data, which can be regarded as a low-sample-rate time-varying data. Expert users currently use frequency analysis transforms of the data for numerical analysis, which is another method well suited to acoustic data. Rather than performing detailed preprocessing analysis of the data streams before the actual audio rendering, in this case the low-sample-rate data streams are transformed into audio streams with auditory information which is legible by the user. No a priori understanding of the data content is needed. The user is in a direct sense “listening” to the output of the parameter probe in the cavity.</p><p>Four sonification–audification metaphors have been developed and implemented within the two platforms (i.e., 30 and 400 Hz). The metaphors are described here, using the axial particle velocity parameters as a reference. It should be noted that the different modules listed here can actually accept any type of time-varying parameter, such as the turbulence or the vorticity particle parameters.
</p><ul class="u-list-style-dash">
                    <li>
                      <p>FM: a simple frequency modulation based on the formula:
</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f = f_0 + (\alpha * v(\hbox{centered})) $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <i>f</i> is the frequency of the output signal. A carrier wave <i>f</i>
                                    <sub>0</sub>, in this case a sinusoid, is modulated in frequency by the velocity parameter data values at the given monitoring position (time fluctuations of <i>v</i>
                                    <sub>
                          <i>x</i>
                        </sub>, <i>v</i>
                                    <sub>
                          <i>y</i>
                        </sub>, or <i>v</i>
                                    <sub>
                          <i>z</i>
                        </sub> around their respective mean). The user can then manipulate in real time the frequency of the carrier signal and the weight of the modulation α.</p>
                    </li>
                    <li>
                      <p>GIZMO: a spectral domain pitch shifter based on the GIZMO method (Dudas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Dudas R (2002) Spectral envelope correction for real-time transposition: proposal of a “floating-formant” method. In: International computer music conference (ICMC ’02), Goteborg, pp 126–129" href="/article/10.1007/s10055-009-0134-1#ref-CR12" id="ref-link-section-d54791e1155">2002</a>). According to the GIZMO algorithm, the signal is split into smaller “grains.” Each grain is then transposed using a spectral shift method, and the signal itself is then re-assembled. In this manner, it is possible to shift the pitch of the signal without changing its duration, allowing the user to investigate the evolution of the parameters at the real-time scale of the turbulent flow if desired. The goal is to directly render, for example, the velocity data stream for one of the three axes into an audible audio stream. Due to the fact that its original sample rate and frequency content are too low to be audible, it is transposed in frequency following the formula:
</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f = T_f + v(\hbox{centered}) $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>f</i> is the frequency of the output signal. The transposition factor <i>T</i>
                                    <sub>
                          <i>f</i>
                        </sub> can be manipulated by the user in real time.</p>
                    </li>
                    <li>
                      <p>PHASE VOCODER: this metaphor is very similar to the GIZMO method, except for the fact that the pitch shift is performed through a phase vocoder (Flanagan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1965" title="Flanagan JL (1965) Phase vocoder. J Acoust Soc Am 28(5):939–940" href="/article/10.1007/s10055-009-0134-1#ref-CR15" id="ref-link-section-d54791e1189">1965</a>). A vocoder is an analysis/synthesis system, mainly used for speech, in which a control signal is divided into frequency bands and, for each band, passed through an envelope follower that will then control the signal to be processed. A “phase” vocoder is a further modification on the principle, where the signal to be processed can be scaled both in the frequency and time domains by using phase information.</p>
                    </li>
                    <li>
                      <p>PITCH SHIFT: a particular pitch shifter based on the PSOLA algorithm (Schnell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Schnell N, Peeters G, Lemouton S, Manoury P, Rodet X (2000) Synthesizing a choir in real-time using pitch synchronous overlap add (PSOLA). In: International computer music conference, Berlin" href="/article/10.1007/s10055-009-0134-1#ref-CR34" id="ref-link-section-d54791e1198">2000</a>). One of the difficulties of using pitch-shifting algorithms is the creation of audible artifacts. The presence of artifacts is more prominent when the frequency of the signal to be shifted is not known beforehand. The PSOLA algorithm uses knowledge of the fundamental frequency of the signal, dividing the signal into frames with length of a period of the fundamental frequency itself. The frames are then played back sequentially at different speed rates depending of the shifting parameter (in this case, the centered velocity of the particles). From prior analysis of the simulated cavity, the fundamental resonance (<i>F</i>), and therefore the fundamental frequency of the velocity oscillation, has been estimated at <i>F</i> = 13.5 Hz. The user can define the weight of the shifting α following the formula:
</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f = F + (\alpha * v(\hbox{centered})) $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>f</i> is the frequency of the output signal.</p>
                    </li>
                  </ul>
                        <p>It can easily be noticed that the last three sonification metaphors are based on a constant transposition (pitch shifting) of the centered velocity oscillation: the audible differences between the three methods are then given by the artifacts generated by the specific algorithm used to perform the transposition, and different degrees of pitch shifting would bring about different variations at different audible frequencies. This user controlled variability is crucial for the use of the system. As an example scenario, consider a 400 Hz sampled data stream (valid data from 0 to 200 Hz according to sampling theory) and an “unknown” region of interest around 150 Hz. This stream could be directly frequency scaled by a factor of 100, producing an audible stream covering the frequency range from 0 to 20,000 Hz, which extends slightly beyond the entire audible spectrum (20–20,000 Hz), with the region of interest now scaled to 15,000 Hz. But humans are not sensitive to all frequencies to the same degree. In addition, while some frequencies are perceptible, they are not always pleasant to listen to for long durations. Very high frequencies are good example of this. An alternative would be a pitch shift (rather than stretch) where the data is “shifted” by 1,000 Hz, for example. This would result in the entire simulation information being concentrated between 1,000 and 1,200 Hz, a rather fine and limited use of the audible range. To deal with these conditions, the user can adjust and combine different metaphor parameters to create both an informative and usable sonification where they can focus their attention.</p><p>Preliminary evaluations of the different audification metaphors with CFD experts, working with their own CFD data results, have been recently carried out. For researchers who do not as yet have the habit of actually listening to their data, the results are encouraging even though the task was not directly obvious. The FM method was reported to be the most “intuitive” in terms of perception of known events, such as oscillation of the mixing layer boundary. This phenomenon was perceived as beat frequencies around the cavity resonance frequency of 13.5 Hz. Such frequency phenomena below the lower frequency limit of human hearing can be perceived as beatings. Slight additional pitch shifting can result in the frequency of the phenomenon increasing a few tens of Hertz, being conversely perceived as low frequency tone oscillations. This simply depends on the exact transformation or adjustment of the sonification parameters. While the fundamental CFD phenomenon of this effect remains the same, the auditory perception is quite different. It has been decided that subsequent evaluations will require a learning phase approach in order to better demonstrate to the expert users the functionality of each metaphor, with its perceptual counterpart, before parameter adjustments are made.</p><p>Future work in the sonification of the CFD data will diverge from direct audification and will consider the use of pre-processing algorithms to extract features from the dataset. For example, spatial transformations could be used, sonifying the data along the non-temporal dimensions to identify spatial periodicity of turbulent structures.</p><h3 class="c-article__sub-heading" id="Sec16">Haptic rendering of CFD datasets</h3><p>As analyzed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec6">2</a>, the general objective of a CFD specialist is to locate interesting structures (e.g., a vortex core), based on visual cues. But the intrinsic complexity of the topology of CFD structures makes the task of precise positioning (say, for future annotation) even more difficult. Also, one should note that the more cluttered the visual space is with simultaneous renderings; the more difficult it is to pinpoint specific 3D structures.</p><p>In this context, haptic perception has been investigated for the last two decades, with significant achievements (Menelas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009b" title="Menelas B, Ammi M, Bourdot P, Richir S (2009) Survey on haptic rendering of data sets: exploration of scalar and vector. J Virtual Real Broadcast (in press)" href="/article/10.1007/s10055-009-0134-1#ref-CR30" id="ref-link-section-d54791e1250">2009b</a>). In the framework of the CoRSAIRe project, we have introduced novel haptic renderings and associated metaphors to complement visual and auditory feedbacks and to present information that is otherwise difficult to perceive, so as to improve the interaction process. Specifically, we investigated the use of the haptic feedback (a) to facilitate and speed up the positioning in a scene (magnetic metaphor), (b) to enable the CFD user to haptically perceive isosurfaces (isosurface haptic rendering), and (c) to provide a new tool for critical point analysis.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Magnetic metaphor</h4><p>In the magnetic metaphor, the target point acts like a magnet attracting the haptic probe.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> In the implemented version, the force feedback is computed via the function represented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig5">5</a>. Whenever the distance between the haptic probe and the target is less than a threshold <i>D</i>, the hand of the user is attracted with a quadratic force by the virtual magnet until threshold <i>d</i> is reached. Since at this position the user is very close to the target, the attraction force is then progressively diminished until it vanishes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The distance–force mapping function used for the magnetic metaphor</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>This metaphor has been evaluated through a psychophysical study in a targeting task (a common task in data analysis). This experiment consisted in reaching some specific points of a Q-factor isosurface
<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> computed on the cavity flow simulation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig6">6</a>). Only haptic-enhanced conditions were tested, and three haptic paradigms were compared: the proposed magnetic metaphor versus two standard kinesthetic force feedbacks (a polygonal and a volumetric one). Ten users participated in the experiment using the immersive VR simulation described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec14">3.1</a>. For each trial, the trajectory described by the user as well as the required time are logged.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>A Q-Factor isosurface displayed inside the cavity along with pre-defined targets (<i>dark spots</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Experiments showed that the magnetic metaphor provided a much better feedback to perform the task: user trajectories were smoother, displayed less hesitation, targeting precision was improved, and time-to-target was reduced, compared to more standard kinesthetic force feedbacks (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig7">7</a>). The reader may refer to Fauvet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Fauvet N, Ammi M, Bourdot P (2007) Experiments of haptic perception techniques for computational fluid dynamics. In: Cyber world 2007, Hannover, pp 322–329" href="/article/10.1007/s10055-009-0134-1#ref-CR14" id="ref-link-section-d54791e1336">2007</a>) for details on protocol implementation and error measurement results. The magnetic metaphor proved interesting to signal points of interest once these had been identified, allowing the hand of the CFD expert to be attracted to interesting locations in subsequent exploration sessions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>An example of targeting trajectory. <i>Top</i> a trajectory using the proposed method. <i>Bottom-left</i> using the polygonal kinesthetic force feedback method. <i>Bottom-right</i> using the volumetric kinesthetic force feedback method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Isosurface rendering for haptic feedback</h4><p>The literature on isosurface rendering follows indirect and direct rendering approaches. The first category aims to extract a polygonal representation from volume data. Algorithms adapted from the initial Marching Cubes approach (Lorensen and Cline <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Lorensen WE, Cline HE (1987) Marching cubes: a high resolution 3D surface construction algorithm. In: ACM SIGGRAPH 87, computer graphics proceedings, annual conference series 21(4):163–169" href="/article/10.1007/s10055-009-0134-1#ref-CR24" id="ref-link-section-d54791e1375">1987</a>) can be used for computation of such surfaces. Producing a surface-based representation offers the advantage of providing a stable feedback for a subsequent haptic interaction. However, due to the computation time required by the surface estimation step, real-time surface update (as required in a VR environment) is inherently difficult to achieve. These limitations were overcome in Adachi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: IEEE virtual reality annual international symposium ’95, Research Triangle Park, pp 203–210" href="/article/10.1007/s10055-009-0134-1#ref-CR1" id="ref-link-section-d54791e1378">1995</a>), and later by Mark et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Mark W, Randolph S, Finch M, Verth JV, Taylor RM (1996) Adding force feedback to graphics systems: issues and solutions. In: 23rd annual conference on computer graphics and interactive techniques, pp 447–452" href="/article/10.1007/s10055-009-0134-1#ref-CR27" id="ref-link-section-d54791e1381">1996</a>) and Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Chen K-W, Heng P-A, Sun H (2000) Direct haptic rendering of isosurface by intermediate representation. In: ACM symposium on virtual reality software and technology VRST, pp 188–194" href="/article/10.1007/s10055-009-0134-1#ref-CR7" id="ref-link-section-d54791e1384">2000</a>) by the introduction of an intermediate representation approximating the surface.</p><p>Concerning direct rendering, a well-known approach was exhibited by Avila and Sobierajski (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: 7th conference on visualization, pp 197–204. doi:&#xA;                    10.1109/VISUAL.1996.568108&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-009-0134-1#ref-CR4" id="ref-link-section-d54791e1390">1996</a>). Their work addressed the haptic exploration of the complete data volume, or a sub-volume, such as an isosurface. For isosurface rendering, this method does not require any intermediate representation of the surface. The generated feedback is expressed as a combination of a retarding and a stiffness force directly approximated by the penetration distance to the virtual surface, proportional to a gradient computed on the field value. This approach works well with standard data volumes, providing a very fast haptic loop (hence a satisfactory sensorimotor feedback) without any surface representation. However, some undesirable vibrations can occur in regions exhibiting high frequency data. In such regions, high gradients in the field value result in a poor approximation of the penetration distance of the probe into the isosurface. This shortcoming was previously underlined in Fauvet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Fauvet N, Ammi M, Bourdot P (2007) Experiments of haptic perception techniques for computational fluid dynamics. In: Cyber world 2007, Hannover, pp 322–329" href="/article/10.1007/s10055-009-0134-1#ref-CR14" id="ref-link-section-d54791e1393">1996</a>).</p><p>For these reasons, we introduced in Menelas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Menelas B, Ammi M, Bourdot P (2008) A flexible method for haptic rendering of isosurface from volumetric data. In: Lecture Notes In Computer Science, vol 5024, Proceedings of the 6th international conference on haptics: perception, devices and scenarios, pp 687–693" href="/article/10.1007/s10055-009-0134-1#ref-CR28" id="ref-link-section-d54791e1399">2008</a>) a flexible method based on a more generic approach. By casting rays emanating from the probe in several directions, positions are computed the where the probe would be if it were constrained by a virtual isosurface, i.e., the proxy position (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig8">8</a>). Once this position is determined, this information is haptically conveyed to the user through the haptic channel using a penalty-based method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Computation of the proxy position in Menelas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Menelas B, Ammi M, Bourdot P (2008) A flexible method for haptic rendering of isosurface from volumetric data. In: Lecture Notes In Computer Science, vol 5024, Proceedings of the 6th international conference on haptics: perception, devices and scenarios, pp 687–693" href="/article/10.1007/s10055-009-0134-1#ref-CR28" id="ref-link-section-d54791e1415">2008</a>). <b>a</b> Six rays are cast from the probe position <i>A</i>. Here, three intersection points are found. <b>b</b>
                                          <i>A</i>′ is the projection of the probe on the plane defined by the three intersection points. <b>c</b> The proxy position <i>P</i> is on the isosurface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>We have experimented with this approach in a task consisting of path-following along an isosurface using the same cavity flow simulation with and VR setup. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig9">9</a> exhibits the proposed route on the surface. The new method (referred to as M3) was compared with two other methods, namely the volumic approach of Avila and Sobierajski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: 7th conference on visualization, pp 197–204. doi:&#xA;                    10.1109/VISUAL.1996.568108&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-009-0134-1#ref-CR4" id="ref-link-section-d54791e1452">1996</a> mentioned earlier (M1) and an intermediate representation computation model (M2) proposed in Krner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Krner O, Schill M, Wagner C, Bender H-J, Mnner R (1999) Haptic volume rendering with an intermediate local representation. In: 1st International workshop on the haptic devices in medical applications, pp 79–84" href="/article/10.1007/s10055-009-0134-1#ref-CR21" id="ref-link-section-d54791e1455">1999</a>). Ten participants were randomly allocated into three groups. We measured the accuracy (precision of the haptic interaction) in the tracking task, the haptic rendering quality (users’ preference) of each method and the computation load required by each method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Experimented isosurface. The <i>line</i> going from point A to point B represents the recommended path</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Performance measurements carried out on the three algorithms confirm the fact that the indirect rendering (M2) requires significantly more computing time than the direct (volumetric) rendering methods (M1, M3). It was noted that the haptic loop frequency of M2 depends on the amount of data (i.e., the more data there is, the lower the haptic loop frequency). On the contrary, data count does not significantly affect the frequency of the haptic loop in either M1 or M3 (direct rendering algorithms). Moreover, participants highlighted the fact that a better haptic rendering was provided with the new flexible method M3, which allowed users to perceive all the isosurface details, even weak undulations (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig9">9</a>).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig10">10</a> presents a typical scenario that takes advantage of the proposed haptic metaphors, whereby the user has in addition a 2D volumetric cutting plane attached to the haptic probe which then follows the isosurface. In such a situation, in addition to the haptic feedback of the isosurface, the user can simultaneously access additional meaningful information situated in the transverse plane. Moreover, some quantities such as vorticity or Q-Factor may be directly mapped onto the haptic feedback as a viscosity drag.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Accurate positioning of a colored cutting plane during a CFD immersive exploration session</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Haptic characterization of critical points</h4><p>Among feature-based flow visualization methods, topology-based approaches aim to detect and classify critical points of the flow.<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> Such points are of primary importance as they structure the overall flow features. To this effect, we investigated the characterization of critical points by means of haptic feedback. As outlined by the HTA task tree, the building of a mental model of the analyzed flow is carried out through a step by step construction of the solution. The work presented here addresses the analysis of one instant of an unsteady flow (we are currently investigating the extension of our solution to a time sequence). This approach can be divided into two main steps; <i>detection</i> followed by <i>characterization</i> of critical points.</p><p>In the detection step, the CFD expert starts with an empty visual scene and is invited to freely explore the flow domain. During this exploration process, the presence of critical points located in the cuboid that surrounds the haptic probe position is rendered via a vibration feedback in addition to the visual display (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0134-1#Fig11">11</a>). Critical points are detected “on the fly” within the local environment explored by the user. Thus, pre-existing expertise and previously discovered critical points both serve to guide the exploration process throughout various areas of interest. The expert can construct his own mind map of the flow at her/his own pace.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0134-1/MediaObjects/10055_2009_134_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Representation of all the critical points located in the volume surrounding the probe position</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0134-1/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Once the critical points are detected, the characterization step serves to identify how the flow enters and leave these points. To this effect, the velocity of the flow field is now directly mapped as a force feedback to the user in the neighborhood of the critical points. In the current implementation of the system, the mapping function is defined by <b>F</b> = α·<b>V</b>, where α is the coefficient of the mapping and <b>V</b> is the velocity of the flow. With this metaphor, the haptic probe tends to follow the path of a particle injected inside the flow.</p><p>The proposed concept has been evaluated through several psychophysical experiments in Menelas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009a" title="Menelas B, Ammi M, Pastur L, Bourdot P (2009) Haptic exploration of an unsteady flow. In: Symposium on haptic interfaces for virtual environment and teleoperator systems (WorldHaptics 2009), pp 232–237" href="/article/10.1007/s10055-009-0134-1#ref-CR29" id="ref-link-section-d54791e1571">2009a</a>). In summary, all participants emphasized that vibration cues provided real assistance for them to rapidly detect areas of interest. Conversely, with pure visual feedback, they seemed to randomly explore the cavity in search of critical points. Regarding the characterization step, some participants noted that being able to perceive the velocity of the flow through the haptic modality and to manually follow the trajectory of a fluid particle allowed for an interactive analysis of critical points (as opposed to the static display of a streamline). On the other hand, these users noted difficulties faced while attempting to understand the temporal evolution of some serpentine (sinuous) streamlines.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusion and future work</h2><div class="c-article-section__content" id="Sec20-content"><p>The virtual wind tunnel paradigm echoes a pervasive need in the CFD community for more interactive and intuitive means to explore large flow-related datasets. VR and multimodal interaction have long been viewed as “holy grails” of CFD in that they provide the basis of a multisensory, immersive work experience. Although numerous attempts had been made in the past, no existing tools implement VR today in such a way as to replace, or even complement, existing desktop-based solutions. This is deemed to be due to both a lack of formal knowledge regarding work-related user needs, and technological issues underlying the design of workable solutions.</p><p>The innovative character of our contribution in the field is that it simultaneously tackles both these issues by proposing a framework to guide the relevant use of available techniques (e.g., modal allocation schemes) as well as innovative research taken on interaction and rendering techniques.</p><p>By means of a formal task analysis approach, we were able to obtain verbal data regarding the steps followed by CFD researchers when exploring a typical flow simulation. From there, recommendations were issued on the multimodal presentation of the structures to be identified within the simulation. An immersive VR-CFD simulator was presented, in which several dedicated interaction experiments led to the development of new, customized audification and haptic interaction techniques. These methods take into account the specificities of CFD investigations (e.g., high gradients, turbulent flows), while being tuned to the experts’ needs and feedbacks, in order to provide a clear advantage for users as compared to traditional desktop-based approaches.</p><p>We found early on in the user analysis that there is no consensus on the objects to be identified: mathematical operators reveal some properties of the flow, but they must be combined and coincide with previous user experience in past work; a process which can be highly individual. Therefore, while the general exploration procedure is clear, the expert user must retain maximum freedom to select the data that will be relevant in the analysis. The use of 3D perception proved a clear advantage, along with the possibility of interactively setting visualization parameters (such as isovalues). Audio feedback clearly showed potential value, but even with a limited set of parameters the range of transformations must be refined, reduced, and adjusted to optimize the exploration of possible perceptions. User interaction and a learning phase are essential in the early stages of use with such a flexible approach. In addition, experiments clearly showed that critical point localization was made easier by choosing relevant haptic metaphors.</p><p>For these reasons, we are currently preparing a new set of experiments, focusing on very localized simulation properties, where the user will interactively tune rendering parameters to minimize or maximize some perceived stimuli corresponding to phenomena previously identified by direct, “blind” computation. For example, the cavity flow described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0134-1#Sec6">2</a> exhibits some clear symmetries in the transverse direction, i.e., points that are on both sides of a vertical sagittal plane which have correlated flow values. We expect the user to be able to discriminate and characterize these symmetries better and more quickly with immersive feedback using multiple sensorimotor modalities, compared to standard monoscopic visual observation. In this manner, we will move gradually from human factors studies dedicated to observing VR experiments to the design of actual working environments where substantial progress in the CFD domain can be made.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Another application, molecular docking, is presented in a separate article.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>Cavity flows are classical objects of study among CFD experts. Typical applications range from minimizing noise and turbulences in fast-moving objects to the study of atmospheric distribution of urban pollution.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>The software is now commercialized under the name Avizo<span class="mathjax-tex">\( ^{\circledR}\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>
                                 <a href="http://www.ar-tracking.de/">http://www.ar-tracking.de/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>The probe is the representation of the haptic device in the virtual world.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>The Q-factor is defined as the vorticity from which the shear component is subtracted. An isosurface of Q-factor thus highlights specific tubular microstructures in the flow.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>In CFD, critical points are defined as points where the velocity vanishes.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: IEEE virtual rea" /><p class="c-article-references__text" id="ref-CR1">Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: IEEE virtual reality annual international symposium ’95, Research Triangle Park, pp 203–210</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="André E (2000) Handbook of natural language processing. In: Chapter “The generation of multimedia presentation" /><p class="c-article-references__text" id="ref-CR2">André E (2000) Handbook of natural language processing. In: Chapter “The generation of multimedia presentations”, pp 305–327</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Annett J (2003) Handbook of cognitive task design. In: Chapter Hierarchical Task Analysis, pp 17–35" /><p class="c-article-references__text" id="ref-CR3">Annett J (2003) Handbook of cognitive task design. In: Chapter Hierarchical Task Analysis, pp 17–35</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: 7th conference on vi" /><p class="c-article-references__text" id="ref-CR4">Avila RS, Sobierajski LM (1996) A haptic interaction method for volume visualization. In: 7th conference on visualization, pp 197–204. doi:<a href="https://doi.org/10.1109/VISUAL.1996.568108">10.1109/VISUAL.1996.568108</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bryson S, Levit C (1991) The virtual windtunnel: an environment for the exploration of three-dimensional unste" /><p class="c-article-references__text" id="ref-CR5">Bryson S, Levit C (1991) The virtual windtunnel: an environment for the exploration of three-dimensional unsteady flows. In: IEEE Visualization, pp 17–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Burdea, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Burdea G (1996) Force and touch feedback for VR. Wiley, Chichester" /><p class="c-article-references__text" id="ref-CR6">Burdea G (1996) Force and touch feedback for VR. Wiley, Chichester</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Force%20and%20touch%20feedback%20for%20VR&amp;publication_year=1996&amp;author=Burdea%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen K-W, Heng P-A, Sun H (2000) Direct haptic rendering of isosurface by intermediate representation. In: ACM" /><p class="c-article-references__text" id="ref-CR7">Chen K-W, Heng P-A, Sun H (2000) Direct haptic rendering of isosurface by intermediate representation. In: ACM symposium on virtual reality software and technology VRST, pp 188–194</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CA. Chinn, WF. Brewer, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Chinn CA, Brewer WF (1993) The role of anomalous data in knowledge acquisition: a theoretical framework and im" /><p class="c-article-references__text" id="ref-CR8">Chinn CA, Brewer WF (1993) The role of anomalous data in knowledge acquisition: a theoretical framework and implications for science instruction. Rev Educ Res 63(1):1–49</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20anomalous%20data%20in%20knowledge%20acquisition%3A%20a%20theoretical%20framework%20and%20implications%20for%20science%20instruction&amp;journal=Rev%20Educ%20Res&amp;volume=63&amp;issue=1&amp;pages=1-49&amp;publication_year=1993&amp;author=Chinn%2CCA&amp;author=Brewer%2CWF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Crawfis RA, Shen H-W, Max N (2000) Flow visualization techniques for CFD using volume rendering. In: 9th Inter" /><p class="c-article-references__text" id="ref-CR9">Crawfis RA, Shen H-W, Max N (2000) Flow visualization techniques for CFD using volume rendering. In: 9th International symposium on flow visualization</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Donker H, Klante P, Gorny P (2002) The design of auditory user interfaces for blind users. In: 2nd Nordic conf" /><p class="c-article-references__text" id="ref-CR11">Donker H, Klante P, Gorny P (2002) The design of auditory user interfaces for blind users. In: 2nd Nordic conference on human-computer interaction (NordiCHI ’02), vol 31. ACM, New York, pp 149–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dudas R (2002) Spectral envelope correction for real-time transposition: proposal of a “floating-formant” meth" /><p class="c-article-references__text" id="ref-CR12">Dudas R (2002) Spectral envelope correction for real-time transposition: proposal of a “floating-formant” method. In: International computer music conference (ICMC ’02), Goteborg, pp 126–129</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ericsson KA, Simon, HA (1984/1993) Protocol analysis: Verbal reports as data (revised edition)" /><p class="c-article-references__text" id="ref-CR13">Ericsson KA, Simon, HA (1984/1993) Protocol analysis: Verbal reports as data (revised edition)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fauvet N, Ammi M, Bourdot P (2007) Experiments of haptic perception techniques for computational fluid dynamic" /><p class="c-article-references__text" id="ref-CR14">Fauvet N, Ammi M, Bourdot P (2007) Experiments of haptic perception techniques for computational fluid dynamics. In: Cyber world 2007, Hannover, pp 322–329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JL. Flanagan, " /><meta itemprop="datePublished" content="1965" /><meta itemprop="headline" content="Flanagan JL (1965) Phase vocoder. J Acoust Soc Am 28(5):939–940" /><p class="c-article-references__text" id="ref-CR15">Flanagan JL (1965) Phase vocoder. J Acoust Soc Am 28(5):939–940</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1939800" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Phase%20vocoder&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=28&amp;issue=5&amp;pages=939-940&amp;publication_year=1965&amp;author=Flanagan%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Gadouin, P. Quéré, O. Daube, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Gadouin E, Le Quéré P, Daube O (2001) A general methodology for investigating flow instability in complex geom" /><p class="c-article-references__text" id="ref-CR16">Gadouin E, Le Quéré P, Daube O (2001) A general methodology for investigating flow instability in complex geometries: application to natural convection in enclosures. Int J Numer Methods Fluids 37(2):175–208</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Ffld.173" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20general%20methodology%20for%20investigating%20flow%20instability%20in%20complex%20geometries%3A%20application%20to%20natural%20convection%20in%20enclosures&amp;journal=Int%20J%20Numer%20Methods%20Fluids&amp;volume=37&amp;issue=2&amp;pages=175-208&amp;publication_year=2001&amp;author=Gadouin%2CE&amp;author=Qu%C3%A9r%C3%A9%2CP&amp;author=Daube%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gherbi R, Bourdot B, Vezien JM, Herisson J, Fauvet N, Ferey N (2006) Le traité de la réalité virtuelle, vol 4:" /><p class="c-article-references__text" id="ref-CR36">Gherbi R, Bourdot B, Vezien JM, Herisson J, Fauvet N, Ferey N (2006) Le traité de la réalité virtuelle, vol 4: Les applications de la Réalité Virtuelle. In: Fuchs P, Moreau G, Papin J-P (eds) Chapter 4: Explorations de données scientifiques et expérimentations virtuelles. Presses de l’Ecoles de Mines de Paris</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ghiglione R, Landré A, Bromberg M, Molette P (1998) L’analyse automatique des contenus [Automatic content anal" /><p class="c-article-references__text" id="ref-CR17">Ghiglione R, Landré A, Bromberg M, Molette P (1998) L’analyse automatique des contenus [Automatic content analysis]. Dunod Ed</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hoppe H (1996) Progressive Meshes ACM SIGGRAPH 96. In: Computer graphics proceedings, annual conference series" /><p class="c-article-references__text" id="ref-CR18">Hoppe H (1996) Progressive Meshes ACM SIGGRAPH 96. In: Computer graphics proceedings, annual conference series, pp 99–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Warusfel O, Bourdot P, Vezien J-M (2007) CoRSAIRe—Combination of sensori-motor rendering for the imm" /><p class="c-article-references__text" id="ref-CR19">Katz BFG, Warusfel O, Bourdot P, Vezien J-M (2007) CoRSAIRe—Combination of sensori-motor rendering for the immersive analysis of results. In: 2nd International workshop on interactive sonification (ISon 2007), York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration" /><p class="c-article-references__text" id="ref-CR20">Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference of auditory display (ICAD 2008), Paris</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Kramer G (ed) (1994) Auditory display: sonification, audification, and auditory interfaces. Santa Fe Institute" /><p class="c-article-references__text" id="ref-CR22">Kramer G (ed) (1994) Auditory display: sonification, audification, and auditory interfaces. Santa Fe Institute Studies in the Sciences of Complexity, Westview Press, Santa Fe</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20display%3A%20sonification%2C%20audification%2C%20and%20auditory%20interfaces&amp;publication_year=1994">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krner O, Schill M, Wagner C, Bender H-J, Mnner R (1999) Haptic volume rendering with an intermediate local rep" /><p class="c-article-references__text" id="ref-CR21">Krner O, Schill M, Wagner C, Bender H-J, Mnner R (1999) Haptic volume rendering with an intermediate local representation. In: 1st International workshop on the haptic devices in medical applications, pp 79–84</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levoy M, Rusinkiewicz S (2000) QSplat: a multiresolution point rendering system for large meshes. In: SIGGRAPH" /><p class="c-article-references__text" id="ref-CR23">Levoy M, Rusinkiewicz S (2000) QSplat: a multiresolution point rendering system for large meshes. In: SIGGRAPH’2000, Computer graphics proceedings, annual conference series, pp 343–352</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lorensen WE, Cline HE (1987) Marching cubes: a high resolution 3D surface construction algorithm. In: ACM SIGG" /><p class="c-article-references__text" id="ref-CR24">Lorensen WE, Cline HE (1987) Marching cubes: a high resolution 3D surface construction algorithm. In: ACM SIGGRAPH 87, computer graphics proceedings, annual conference series 21(4):163–169</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="K. Lynch, " /><meta itemprop="datePublished" content="1960" /><meta itemprop="headline" content="Lynch K (1960) The image of the city. MIT Press, Cambridge, MA" /><p class="c-article-references__text" id="ref-CR25">Lynch K (1960) The image of the city. MIT Press, Cambridge, MA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20image%20of%20the%20city&amp;publication_year=1960&amp;author=Lynch%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Maguire, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Maguire M (2001) Methods to support human-centred design. Int J Hum Comput Stud 55(4):587–634" /><p class="c-article-references__text" id="ref-CR26">Maguire M (2001) Methods to support human-centred design. Int J Hum Comput Stud 55(4):587–634</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0984.68616" aria-label="View reference 26 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.2001.0503" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Methods%20to%20support%20human-centred%20design&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=55&amp;issue=4&amp;pages=587-634&amp;publication_year=2001&amp;author=Maguire%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mark W, Randolph S, Finch M, Verth JV, Taylor RM (1996) Adding force feedback to graphics systems: issues and " /><p class="c-article-references__text" id="ref-CR27">Mark W, Randolph S, Finch M, Verth JV, Taylor RM (1996) Adding force feedback to graphics systems: issues and solutions. In: 23rd annual conference on computer graphics and interactive techniques, pp 447–452</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Menelas B, Ammi M, Bourdot P (2008) A flexible method for haptic rendering of isosurface from volumetric data." /><p class="c-article-references__text" id="ref-CR28">Menelas B, Ammi M, Bourdot P (2008) A flexible method for haptic rendering of isosurface from volumetric data. In: Lecture Notes In Computer Science, vol 5024, Proceedings of the 6th international conference on haptics: perception, devices and scenarios, pp 687–693</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Menelas B, Ammi M, Pastur L, Bourdot P (2009) Haptic exploration of an unsteady flow. In: Symposium on haptic " /><p class="c-article-references__text" id="ref-CR29">Menelas B, Ammi M, Pastur L, Bourdot P (2009) Haptic exploration of an unsteady flow. In: Symposium on haptic interfaces for virtual environment and teleoperator systems (WorldHaptics 2009), pp 232–237</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Menelas B, Ammi M, Bourdot P, Richir S (2009) Survey on haptic rendering of data sets: exploration of scalar a" /><p class="c-article-references__text" id="ref-CR30">Menelas B, Ammi M, Bourdot P, Richir S (2009) Survey on haptic rendering of data sets: exploration of scalar and vector. J Virtual Real Broadcast (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nesbitt KV (2003) Designing multi-sensory displays for abstract data. PhD thesis, School of Information Techno" /><p class="c-article-references__text" id="ref-CR31">Nesbitt KV (2003) Designing multi-sensory displays for abstract data. PhD thesis, School of Information Technology, University of Sydney</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Pastur, F. Lusseyran, TM. Faure, Y. Fraigneau, R. Pethieu, P. Debesse, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Pastur L, Lusseyran F, Faure TM, Fraigneau Y, Pethieu R, Debesse P (2008) Quantifying the nonlinear mode compe" /><p class="c-article-references__text" id="ref-CR32">Pastur L, Lusseyran F, Faure TM, Fraigneau Y, Pethieu R, Debesse P (2008) Quantifying the nonlinear mode competition in the flow over an open cavity at medium Reynolds number. Exp Fluids 44(4):597–608</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00348-007-0419-7" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantifying%20the%20nonlinear%20mode%20competition%20in%20the%20flow%20over%20an%20open%20cavity%20at%20medium%20Reynolds%20number&amp;journal=Exp%20Fluids&amp;volume=44&amp;issue=4&amp;pages=597-608&amp;publication_year=2008&amp;author=Pastur%2CL&amp;author=Lusseyran%2CF&amp;author=Faure%2CTM&amp;author=Fraigneau%2CY&amp;author=Pethieu%2CR&amp;author=Debesse%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Podvin, Y. Fraigneau, F. Lusseyran, P. Gougat, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Podvin B, Fraigneau Y, Lusseyran F, Gougat P (2006) A reconstruction method for the flow past an open cavity. " /><p class="c-article-references__text" id="ref-CR33">Podvin B, Fraigneau Y, Lusseyran F, Gougat P (2006) A reconstruction method for the flow past an open cavity. J Fluids Eng. 128(3):531–540</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1115%2F1.2175159" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20reconstruction%20method%20for%20the%20flow%20past%20an%20open%20cavity&amp;journal=J%20Fluids%20Eng&amp;volume=128&amp;issue=3&amp;pages=531-540&amp;publication_year=2006&amp;author=Podvin%2CB&amp;author=Fraigneau%2CY&amp;author=Lusseyran%2CF&amp;author=Gougat%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schnell N, Peeters G, Lemouton S, Manoury P, Rodet X (2000) Synthesizing a choir in real-time using pitch sync" /><p class="c-article-references__text" id="ref-CR34">Schnell N, Peeters G, Lemouton S, Manoury P, Rodet X (2000) Synthesizing a choir in real-time using pitch synchronous overlap add (PSOLA). In: International computer music conference, Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Touraine D, Bourdot P (2001) VEserver : a manager for input and haptic multi-sensorial device. In: IEEE intern" /><p class="c-article-references__text" id="ref-CR35">Touraine D, Bourdot P (2001) VEserver : a manager for input and haptic multi-sensorial device. In: IEEE international workshop on robot-human interactive communication (IEEE ROMAN 2001), Bordeaux and Paris</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Dam, AS. Forsberg, DH. Laidlaw, JJ. LaViola, RM. Simpson, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Van Dam A, Forsberg AS, Laidlaw DH, LaViola JJ, Simpson RM (2000) Immersive virtual reality for scientific vis" /><p class="c-article-references__text" id="ref-CR37">Van Dam A, Forsberg AS, Laidlaw DH, LaViola JJ, Simpson RM (2000) Immersive virtual reality for scientific visualization: a progress report. IEEE Comput Graphics Appl 20(6):26–52</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20virtual%20reality%20for%20scientific%20visualization%3A%20a%20progress%20report&amp;journal=IEEE%20Comput%20Graphics%20Appl&amp;volume=128&amp;issue=3&amp;pages=531-540&amp;publication_year=2000&amp;author=Dam%2CA&amp;author=Forsberg%2CAS&amp;author=Laidlaw%2CDH&amp;author=LaViola%2CJJ&amp;author=Simpson%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wright M, Freed A, Momeni A (2003) Open sound control: state of the art 2003. In: International conference on " /><p class="c-article-references__text" id="ref-CR38">Wright M, Freed A, Momeni A (2003) Open sound control: state of the art 2003. In: International conference on new interfaces for musical expression, Montreal, pp 153–159</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ziegeler S, Moorhead RJ, Croft PJ, Lu D (2001) The MetVR case study: meteorological visualization in an immers" /><p class="c-article-references__text" id="ref-CR39">Ziegeler S, Moorhead RJ, Croft PJ, Lu D (2001) The MetVR case study: meteorological visualization in an immersive virtual environment. In: IEEE 12th conference on visualization (VIS’ 2001)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-009-0134-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work is currently supported by the <i>ANR</i> (French National Agency for Research) through the <i>CoRSAIRe</i> project of <i>ARA-MDMSA</i> and by the <i>RTRA</i> (french Thematic Network of Advanced Research) <i>DIGITEO</i>, through the <i>SIMCoD</i> project.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Laboratoire d’Informatique et de Mécanique pour les Sciences de l’Ingénieur, Centre National de la Recherche Scientifique, 91403, Orsay Cedex, France</p><p class="c-article-author-affiliation__authors-list">J. M. Vézien, B. Ménélas, P. Bourdot, M. Ammi, B. F. G. Katz, L. Pastur &amp; F. Lusseyran</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Laboratoire Conception de Produits et Innovation, Arts et Métiers ParisTech, Paris, France</p><p class="c-article-author-affiliation__authors-list">J. Nelson</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Institut de Recherche et de Coordination Acoustique/Musique, UMR CNRS 9912, 1 Place Igor-Stravinsky, 75004, Paris, France</p><p class="c-article-author-affiliation__authors-list">L. Picinali</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">ECI, Université Paris V, Paris, France</p><p class="c-article-author-affiliation__authors-list">J. M. Burkhardt</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-J__M_-V_zien"><span class="c-article-authors-search__title u-h3 js-search-name">J. M. Vézien</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J. M.+V%C3%A9zien&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J. M.+V%C3%A9zien" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J. M.+V%C3%A9zien%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-B_-M_n_las"><span class="c-article-authors-search__title u-h3 js-search-name">B. Ménélas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;B.+M%C3%A9n%C3%A9las&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=B.+M%C3%A9n%C3%A9las" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22B.+M%C3%A9n%C3%A9las%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-J_-Nelson"><span class="c-article-authors-search__title u-h3 js-search-name">J. Nelson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J.+Nelson&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J.+Nelson" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J.+Nelson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-L_-Picinali"><span class="c-article-authors-search__title u-h3 js-search-name">L. Picinali</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;L.+Picinali&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=L.+Picinali" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22L.+Picinali%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-P_-Bourdot"><span class="c-article-authors-search__title u-h3 js-search-name">P. Bourdot</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;P.+Bourdot&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=P.+Bourdot" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22P.+Bourdot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-M_-Ammi"><span class="c-article-authors-search__title u-h3 js-search-name">M. Ammi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;M.+Ammi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=M.+Ammi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22M.+Ammi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-B__F__G_-Katz"><span class="c-article-authors-search__title u-h3 js-search-name">B. F. G. Katz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;B. F. G.+Katz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=B. F. G.+Katz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22B. F. G.+Katz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-J__M_-Burkhardt"><span class="c-article-authors-search__title u-h3 js-search-name">J. M. Burkhardt</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J. M.+Burkhardt&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J. M.+Burkhardt" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J. M.+Burkhardt%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-L_-Pastur"><span class="c-article-authors-search__title u-h3 js-search-name">L. Pastur</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;L.+Pastur&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=L.+Pastur" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22L.+Pastur%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-F_-Lusseyran"><span class="c-article-authors-search__title u-h3 js-search-name">F. Lusseyran</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;F.+Lusseyran&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=F.+Lusseyran" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22F.+Lusseyran%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-009-0134-1/email/correspondent/c1/new">J. M. Vézien</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Multisensory%20VR%20exploration%20for%20computer%20fluid%20dynamics%20in%20the%20CoRSAIRe%20project&amp;author=J.%20M.%20V%C3%A9zien%20et%20al&amp;contentID=10.1007%2Fs10055-009-0134-1&amp;publication=1359-4338&amp;publicationDate=2009-09-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Vézien, J.M., Ménélas, B., Nelson, J. <i>et al.</i> Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project.
                    <i>Virtual Reality</i> <b>13, </b>257 (2009). https://doi.org/10.1007/s10055-009-0134-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-009-0134-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-03-11">11 March 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-08-25">25 August 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-09-16">16 September 2009</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-009-0134-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-009-0134-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Computer fluid dynamics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sonification</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal virtual environment</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0134-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=134;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

