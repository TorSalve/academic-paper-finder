<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Document search support by making physical documents transparent in pr"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents Limpid Desk that supports document search on a physical desktop by making the upper layer of a document stack transparent in a projection-based mixed reality environment. A user..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Document search support by making physical documents transparent in projection-based mixed reality"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-03-31"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents Limpid Desk that supports document search on a physical desktop by making the upper layer of a document stack transparent in a projection-based mixed reality environment. A user can visually access a lower-layer document without physically removing the upper documents. This is accomplished by superimposition of cover textures of lower-layer documents on the upper documents by projected imagery. This paper introduces a method of generating projection images that make physical documents transparent. Furthermore, a touch sensing method based on thermal image processing is proposed for the system&#8217;s input interface. Areas touched by a user on physical documents can be detected without any user-worn or handheld devices. This interface allows a user to select a stack to be made transparent by a simple touch gesture. Three document search support techniques are realized using the system. User studies are conducted, and the results show the effectiveness of the proposed techniques."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-03-31"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="147"/>

    <meta name="prism.endingPage" content="160"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0159-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0159-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0159-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0159-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Document search support by making physical documents transparent in projection-based mixed reality"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2010/03/31"/>

    <meta name="citation_firstpage" content="147"/>

    <meta name="citation_lastpage" content="160"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0159-5"/>

    <meta name="DOI" content="10.1007/s10055-010-0159-5"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0159-5"/>

    <meta name="description" content="This paper presents Limpid Desk that supports document search on a physical desktop by making the upper layer of a document stack transparent in a projecti"/>

    <meta name="dc.creator" content="Daisuke Iwai"/>

    <meta name="dc.creator" content="Kosuke Sato"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented reality (ISAR &#8217;01), pp 207&#8211;216"/>

    <meta name="citation_reference" content="Baudisch P, Gutwin C (2004) Multiblending: displaying overlapping windows simultaneously without the drawbacks of alpha blending. In: Proceedings of of ACM conference on human factors in computing systems (CHI &#8217;04), pp 367&#8211;374"/>

    <meta name="citation_reference" content="citation_title=Spatial augmented reality: merging real and virtual worlds; citation_publication_date=2005; citation_id=CR3; citation_author=O Bimber; citation_author=R Raskar; citation_publisher=A. K. Peters Ltd."/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=The visual computing of projector-camera systems; citation_author=O Bimber, D Iwai, G Wetzstein, A Grundh&#246;fer; citation_volume=27; citation_issue=8; citation_publication_date=2008; citation_pages=2219-2254; citation_doi=10.1111/j.1467-8659.2008.01175.x; citation_id=CR4"/>

    <meta name="citation_reference" content="Bonanni L, Lee CH, Selker T (2005) Attention-based design of augmented reality interfaces. In: Proceedings of ACM conference on human factors in computing systems (CHI &#8217;05) Extended Abstracts, pp 1228&#8211;1231"/>

    <meta name="citation_reference" content="Dietz P, Leigh D (2001) DiamondTouch: a multi-user touch technology. In: Proceedings of ACM symposium on user interface software and technology (UIST &#8217;01), pp 219&#8211;226"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Real-time adaptive radiometric compensation; citation_author=A Grundh&#246;fer, O Bimber; citation_volume=14; citation_issue=1; citation_publication_date=2008; citation_pages=97-108; citation_doi=10.1109/TVCG.2007.1052; citation_id=CR7"/>

    <meta name="citation_reference" content="Ho HN, Amemiya T, Ando H (2007) Revealing invisible traces of hand-object interactions with thermal vision. In: Proceedings of the 2007 inframation conference, pp 431&#8211;438"/>

    <meta name="citation_reference" content="Inami M, Kawakami N, Tachi S (2003) Optical camouflage using retro-reflective projection technology. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR &#8217;03), pp 348&#8211;349"/>

    <meta name="citation_reference" content="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of ACM international conference on advances in computer entertainment technology (ACE &#8217;05), pp 213&#8211;216"/>

    <meta name="citation_reference" content="Iwai D, Sato K (2006) Limpid desk: see-through access to disorderly desktop in projection-based mixed reality. In: Proceedings of ACM symposium on virtual reality software and technology (VRST &#8217;06), pp 112&#8211;115"/>

    <meta name="citation_reference" content="Iwai D, Hanatani S, Horii C, Sato K (2006) Limpid desk: transparentizing documents on real desk in projection-based mixed reality. In: Proceedings of IEEE workshop on emerging display technologies (EDT &#8217;06), pp 30&#8211;31"/>

    <meta name="citation_reference" content="Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic desktops. In: Proceedings of ACM symposium on user interface software and technology (UIST &#8217;04), pp 99&#8211;107"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Integrating paper and digital information on enhanceddesk: a method for realtime finger tracking on an augmented desk system; citation_author=H Koike, Y Sato, K Yoshinori; citation_volume=8; citation_issue=4; citation_publication_date=2001; citation_pages=307-322; citation_doi=10.1145/504704.504706; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Keypoint recognition using randomized trees; citation_author=V Lepetit, P Fua; citation_volume=28; citation_issue=9; citation_publication_date=2006; citation_pages=1465-1479; citation_doi=10.1109/TPAMI.2006.188; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Distinctive image features from scale-invariant keypoints; citation_author=DG Lowe; citation_volume=60; citation_issue=2; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR16"/>

    <meta name="citation_reference" content="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision (ICCV &#8217;87), pp 657&#8211;661"/>

    <meta name="citation_reference" content="Seetzen H, Heidrich W, Stuerzlinger W, Ward G, Whitehead L, Trentacoste M, Ghosh A, Vorozcovs A (2004) High dynamic range display systems. In: Proceedings of ACM international conference on computer graphics and interactive techniques (SIGGRAPH &#8217;04), pp 760&#8211;768"/>

    <meta name="citation_reference" content="Siio I, Rawan J, Mynatt E (2003) Finding objects in &#8220;strata drawer&#8221;. In: Proceedings of ACM conference on human factors in computing systems (CHI &#8217;03) Extended Abstracts, pp 982&#8211;983"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Color indexing; citation_author=MJ Swain, DH Ballard; citation_volume=7; citation_issue=1; citation_publication_date=1991; citation_pages=11-32; citation_doi=10.1007/BF00130487; citation_id=CR20"/>

    <meta name="citation_reference" content="Wilson AD (2005) PlayAnywhere: a compact interactive tabletop projection-vision system. In: Proceedings of ACM symposium on user interface software and technology (UIST &#8217;05), pp 83&#8211;92"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Thermo-Key: human region segmentation from video; citation_author=K Yasuda, T Naemura, H Harashima; citation_volume=24; citation_issue=1; citation_publication_date=2004; citation_pages=26-30; citation_doi=10.1109/MCG.2004.1255805; citation_id=CR22"/>

    <meta name="citation_reference" content="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia (VSMM &#8217;03), pp 161&#8211;168"/>

    <meta name="citation_reference" content="Zollmann S, Bimber O (2007) Imperceptible calibration for radiometric compensation. In: Proceedings of eurographics (short paper), pp 61&#8211;64"/>

    <meta name="citation_author" content="Daisuke Iwai"/>

    <meta name="citation_author_email" content="daisuke.iwai@sys.es.osaka-u.ac.jp"/>

    <meta name="citation_author_institution" content="Graduate School of Engineering Science, Osaka University, Osaka, Japan"/>

    <meta name="citation_author" content="Kosuke Sato"/>

    <meta name="citation_author_institution" content="Graduate School of Engineering Science, Osaka University, Osaka, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0159-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0159-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Document search support by making physical documents transparent in projection-based mixed reality"/>
        <meta property="og:description" content="This paper presents Limpid Desk that supports document search on a physical desktop by making the upper layer of a document stack transparent in a projection-based mixed reality environment. A user can visually access a lower-layer document without physically removing the upper documents. This is accomplished by superimposition of cover textures of lower-layer documents on the upper documents by projected imagery. This paper introduces a method of generating projection images that make physical documents transparent. Furthermore, a touch sensing method based on thermal image processing is proposed for the system’s input interface. Areas touched by a user on physical documents can be detected without any user-worn or handheld devices. This interface allows a user to select a stack to be made transparent by a simple touch gesture. Three document search support techniques are realized using the system. User studies are conducted, and the results show the effectiveness of the proposed techniques."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Document search support by making physical documents transparent in projection-based mixed reality | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0159-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Projection-based mixed reality, Document search support, Making documents transparent, Thermal image processing, Thermal trace, Touch sensing","kwrd":["Projection-based_mixed_reality","Document_search_support","Making_documents_transparent","Thermal_image_processing","Thermal_trace","Touch_sensing"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0159-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0159-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=159;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0159-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Document search support by making physical documents transparent in projection-based mixed reality
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0159-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0159-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-03-31" itemprop="datePublished">31 March 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Document search support by making physical documents transparent in projection-based mixed reality</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daisuke-Iwai" data-author-popup="auth-Daisuke-Iwai" data-corresp-id="c1">Daisuke Iwai<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Graduate School of Engineering Science, Osaka University, Machikaneyama 1-3, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kosuke-Sato" data-author-popup="auth-Kosuke-Sato">Kosuke Sato</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Graduate School of Engineering Science, Osaka University, Machikaneyama 1-3, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">147</span>–<span itemprop="pageEnd">160</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">215 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">15 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0159-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents <i>Limpid Desk</i> that supports document search on a physical desktop by making the upper layer of a document stack transparent in a projection-based mixed reality environment. A user can visually access a lower-layer document without physically removing the upper documents. This is accomplished by superimposition of cover textures of lower-layer documents on the upper documents by projected imagery. This paper introduces a method of generating projection images that make physical documents transparent. Furthermore, a touch sensing method based on thermal image processing is proposed for the system’s input interface. Areas touched by a user on physical documents can be detected without any user-worn or handheld devices. This interface allows a user to select a stack to be made transparent by a simple touch gesture. Three document search support techniques are realized using the system. User studies are conducted, and the results show the effectiveness of the proposed techniques.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction and motivation</h2><div class="c-article-section__content" id="Sec1-content"><p>Physical documents (e.g., magazines, books, and papers) are usually scattered and piled chaotically on a desktop. The disorder on the desktop makes searching for a desired document burdensome. When all the documents are organized, the desired document can easily be found, and consequently, work can be handled efficiently. However, most people are generally too busy to clean up their desktops.</p><p>Several window management tools have been developed with which either windows or file icons can be efficiently searched on a personal computer (PC) desktop. For example, <i>Exposé</i>
                        <sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> allows a user to quickly locate an open window without the need to click through many windows to find a specific target. In particular, the <i>All windows</i> feature shows all open windows, shrinking their appearance so that they all fit on a single screen. <i>Windows 7</i>
                        <sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> has a feature where all the open windows are made transparent to make file icons on a desktop visible. These tools can be activated by a single action, such as pressing a key or moving the mouse cursor to a corner of the desktop.</p><p>This paper presents a novel document search support system in which documents are made transparent in situ on a physical desktop. Usually, the upper-layer documents in a stack are opaque, and users have to remove them to look at lower-layer documents. If the upper-layer documents become transparent, the lower-layer document is visible. In such an environment, users can easily find a specific target in a document stack without removing the upper documents.</p><p><i>Limpid Desk</i> uses projection-based mixed reality (MR) technology to change the appearances of physical documents so that they look transparent in real space (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig1">1</a>). First, the system combines surface textures of the lower-layer documents and the desktop under the upper-layer documents to be made transparent. Then, geometric registration and radiometric compensation are applied to the image. The generated image is projected onto the upper-layer documents with additional visual effects so that a user can feel that the documents are made transparent and easily count the number of documents that are already made transparent. These effects allow the user to remove a target lower-layer document from the stack immediately after the target document is virtually exposed. A novel touch sensing method using thermal image processing is also proposed. Areas that a user touches on physical documents can be detected without any user-worn or handheld devices.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Concept of <i>Limpid Desk</i>
                                 </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Based on these basic technologies, this paper proposes three document search support techniques. A user can search for a desired document by directly touching a document stack in real space. Conventional interfaces, such as computer monitors, keyboards, and mice, are not needed. Because all the interaction spaces are unified on a physical desktop, an efficient document search is realized.</p><p>The remainder of this paper is organized as follows. Related studies are briefly described in the subsequent section. Detailed information about how to make physical documents transparent and the touch sensing method is provided in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec3">3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec12">4</a>, respectively. System configuration, document search support techniques, and user studies are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec15">5</a>. The advantages and limitations of the proposed approach are discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec25">6</a>. Finally, the conclusion of the paper is provided in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec26">7</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Projection-based MR focuses on changing attributes (e.g., color, texture, and shape) of a physical object by projected imagery (Bimber and Raskar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters Ltd, Natick" href="/article/10.1007/s10055-010-0159-5#ref-CR3" id="ref-link-section-d69876e405">2005</a>). In recent years, this has been exploited for making physical objects transparent (Inami et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Inami M, Kawakami N, Tachi S (2003) Optical camouflage using retro-reflective projection technology. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR ’03), pp 348–349" href="/article/10.1007/s10055-010-0159-5#ref-CR9" id="ref-link-section-d69876e408">2003</a>; Bonanni et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bonanni L, Lee CH, Selker T (2005) Attention-based design of augmented reality interfaces. In: Proceedings of ACM conference on human factors in computing systems (CHI ’05) Extended Abstracts, pp 1228–1231" href="/article/10.1007/s10055-010-0159-5#ref-CR5" id="ref-link-section-d69876e411">2005</a>). However, these objects were limited to those suitable for image projection, such as a retro-reflective material and a refrigerator’s simple white door. Physical documents, such as magazines and photos, have spatially varying reflectance properties on their surfaces, which disturb the appearance of projected images.</p><p>Several researchers focus on controlling a physical object’s appearance by a projection image even when the object has complex reflectance properties. This can be achieved by a radiometric compensation process (Bimber et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector-camera systems. Comput Graph Forum 27(8):2219–2254" href="/article/10.1007/s10055-010-0159-5#ref-CR4" id="ref-link-section-d69876e417">2008</a>). We apply one of these techniques (Yoshida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia (VSMM ’03), pp 161–168" href="/article/10.1007/s10055-010-0159-5#ref-CR23" id="ref-link-section-d69876e420">2003</a>) to make physical documents with complex textures transparent.</p><p>Previous studies try to support a user to search a physical document with a PC interface. Kim et al. proposed tracking physical documents over time using an overhead camera (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic desktops. In: Proceedings of ACM symposium on user interface software and technology (UIST ’04), pp 99–107" href="/article/10.1007/s10055-010-0159-5#ref-CR13" id="ref-link-section-d69876e426">2004</a>). The tracked documents are automatically linked to the corresponding electronic documents, which are stored on a PC. The user can easily locate the desired document in physical stacks by performing a keyword search. <i>Strata Drawer</i> supports document search in a physical drawer (Siio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Siio I, Rawan J, Mynatt E (2003) Finding objects in “strata drawer”. In: Proceedings of ACM conference on human factors in computing systems (CHI ’03) Extended Abstracts, pp 982–983" href="/article/10.1007/s10055-010-0159-5#ref-CR19" id="ref-link-section-d69876e432">2003</a>). A camera is mounted facing downward in the drawer. When a user places a document in it, a photograph is automatically taken. Then, the user can browse pictures of documents in the drawer with PC interfaces.</p><p>In these systems, a user has to move to a place where the desired document exists after searching for it on a PC. A spatial seam exists between the manipulation-and-display space (PC) and the search space (desktop). In contrast, <i>Limpid Desk</i> is designed to unify these spaces on a physical desktop to realize more efficient document search. In addition, we assume that documents targeted by <i>Limpid Desk</i> include those that cannot be located by a keyword search, such as photos or handwritten notes.</p><p>A touching action is used as a trigger operation in <i>Limpid Desk</i> to support a user’s direct manipulation of physical documents. Several methods have been developed to measure user actions, such as a magnetic tracker (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented reality (ISAR ’01), pp 207–216" href="/article/10.1007/s10055-010-0159-5#ref-CR1" id="ref-link-section-d69876e451">2001</a>), sensors embedded in the projection surface (Dietz and Leigh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Dietz P, Leigh D (2001) DiamondTouch: a multi-user touch technology. In: Proceedings of ACM symposium on user interface software and technology (UIST ’01), pp 219–226" href="/article/10.1007/s10055-010-0159-5#ref-CR6" id="ref-link-section-d69876e454">2001</a>), and vision-based finger gesture recognition (Koike et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Koike H, Sato Y, Yoshinori K (2001) Integrating paper and digital information on enhanceddesk: a method for realtime finger tracking on an augmented desk system. ACM Trans Comput Hum Interact 8(4):307–322" href="/article/10.1007/s10055-010-0159-5#ref-CR14" id="ref-link-section-d69876e457">2001</a>).</p><p>In the first solution, a user has to hold a magnetic tracker to interact with the system. The device interrupts the natural document search interaction. The second solution is unsuitable for the proposed system because it cannot detect a user’s touch actions on a physical document stack. The vision-based solution can robustly recognize a user’s finger gestures. However, it also has difficulties with detecting a user’s touch action. In contrast, Wilson proposed using the shadows of a user’s fingers to detect touch points on the surface (Wilson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wilson AD (2005) PlayAnywhere: a compact interactive tabletop projection-vision system. In: Proceedings of ACM symposium on user interface software and technology (UIST ’05), pp 83–92" href="/article/10.1007/s10055-010-0159-5#ref-CR21" id="ref-link-section-d69876e463">2005</a>). But this shadow-based algorithm cannot work well on a desktop where various objects are scattered. This paper proposes a novel touch sensing method using thermal images. In this method, touched areas can be detected on physical documents that are scattered and piled chaotically on a desktop. This approach is developed on the basis of a previous touch-detection method where areas that a user touches can be detected only on a thin paper screen (Iwai and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of ACM international conference on advances in computer entertainment technology (ACE ’05), pp 213–216" href="/article/10.1007/s10055-010-0159-5#ref-CR10" id="ref-link-section-d69876e466">2005</a>).</p><p>The basic concept of <i>Limpid Desk</i> and several related technological principles were preliminarily proposed in short reports (Iwai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Iwai D, Hanatani S, Horii C, Sato K (2006) Limpid desk: transparentizing documents on real desk in projection-based mixed reality. In: Proceedings of IEEE workshop on emerging display technologies (EDT ’06), pp 30–31" href="/article/10.1007/s10055-010-0159-5#ref-CR12" id="ref-link-section-d69876e475">2006</a>; Iwai and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Iwai D, Sato K (2006) Limpid desk: see-through access to disorderly desktop in projection-based mixed reality. In: Proceedings of ACM symposium on virtual reality software and technology (VRST ’06), pp 112–115" href="/article/10.1007/s10055-010-0159-5#ref-CR11" id="ref-link-section-d69876e478">2006</a>). This paper extends these technologies and furthermore conducts user studies to confirm the effectiveness of the proposed document search support techniques.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Making physical documents transparent in projection-based MR</h2><div class="c-article-section__content" id="Sec3-content"><p>This section introduces a fundamental optical model of making a physical document transparent by projected imagery. Additional visual effects are also designed specifically to support document search.</p><h3 class="c-article__sub-heading" id="Sec4">Fundamental optical model</h3><p>We apply a simple optical model to describe the appearance of a document stack where the upper-layer documents are transparent. A radiometric compensation technique is also applied to display the desired appearance on the textured surfaces of the documents. A proof-of-concept experiment is conducted to confirm the validity of the model.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Principle</h4><p>This paper applies the alpha-blending approach to realize virtually transparent documents (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig2">2</a>). Although the proposed model does not follow real physics of reflection and transmission, we found that it works quite well to display transparent effect. First, the appearance of a transparent document (spectral reflectance <i>M</i>
                              <sub>1</sub>(λ), transparency α) that covers an opaque document (spectral reflectance <i>M</i>
                              <sub>2</sub>(λ)) under environment light <i>E</i>(λ) is considered (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig2">2</a>a). The observed spectral distribution of the reflection <i>R</i>(λ) is computed by
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ R(\lambda) = E(\lambda)\{\alpha M_1(\lambda) + (1-\alpha)M_2(\lambda)\}, $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where λ represents the wavelength of light. <i>R</i>(λ) is the desired appearance to be displayed on the surface by projection-based superimposition.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Optical model: <b>a</b> upper document is transparent under environment light and <b>b</b> upper document is opaque under projection light</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Second, the <i>Limpid Desk</i> configuration is considered. In this case, the environment light is replaced with a projector, and the upper document is opaque. The observed reflection <i>R</i>(λ) of a projection light <i>P</i>(λ|<i>I</i>
                              <sub>
                      <i>R</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>G</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>B</i>
                    </sub>) is computed by
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ R(\lambda) = P(\lambda| I_R, I_G, I_B)M_1(\lambda), $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <i>I</i>
                              <sub>
                      <i>R</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>G</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>B</i>
                    </sub> are the input intensity values of the projector (0≤<i>I</i>
                              <sub>
                      <i>R</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>G</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>B</i>
                    </sub> ≤1). <i>P</i>(λ|<i>I</i>
                              <sub>
                      <i>R</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>G</i>
                    </sub>, <i>I</i>
                              <sub>
                      <i>B</i>
                    </sub>) contains both the projector-to-surface form-factor and the black-level of the projector. The fraction of light that arrives at a surface patch depends on the geometric relationship between the light source and the surface, particularly the angle between the light ray and the surface normal, and the distance between the light source and the surface. The form-factor is the simplest way to approximate this fraction. The black-level represents the projector’s minimum intensity level.</p><p>As the environment light of the system, the projector illuminates the desktop with half of the maximum intensity:
</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ E(\lambda) = P(\lambda| 0.5, 0.5, 0.5). $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                           <p>This was chosen to avoid clipping artifacts in the following radiometric compensation process. Because a dim environment light makes it difficult for users to read documents, the light should be as bright as possible. However, if the maximum intensity of the projector is applied as the environment light, correct colors cannot be displayed when the system tries to display a brighter color than the reflectance color.</p><p>Every time, a new document is placed on the stack, its appearance under the environment light <i>E</i>(λ)<i>M</i>(λ) is captured by an overhead visible camera. The target appearance can be calculated from the stored appearances on the basis of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0159-5#Equ1">1</a>). Once the target appearance is defined, the per-pixel projection color is calculated through a radiometric compensation technique that we previously proposed (Yoshida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia (VSMM ’03), pp 161–168" href="/article/10.1007/s10055-010-0159-5#ref-CR23" id="ref-link-section-d69876e736">2003</a>). The detailed principle of the radiometric compensation technique is described in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec27">Appendix</a>. Because the method cancels complex textures on the projection surface, a better transparent appearance is realized.</p><p>The applied radiometric compensation requires color and geometric calibration processes. The overhead visible camera that captures the reflection of the projection light is used for them. In color calibration, uniform colored patterns are projected to calculate color mixing matrices for each camera pixel, which are required for radiometric compensation.</p><p>In geometric calibration, structured light patterns (particularly graycode patterns) are projected onto the document stack to obtain camera-to-projector pixel correspondences (<i>C2P map</i>) (Sato and Inokuchi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision (ICCV ’87), pp 657–661" href="/article/10.1007/s10055-010-0159-5#ref-CR17" id="ref-link-section-d69876e750">1987</a>). If intrinsic and extrinsic parameters of the visible camera and the projector are calibrated, 3-D shape of the document stack can be measured through the C2P map. Consequently, the desired appearance can be rendered for an arbitrarily viewpoint by a 2-pass rendering method described in (Bimber and Raskar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters Ltd, Natick" href="/article/10.1007/s10055-010-0159-5#ref-CR3" id="ref-link-section-d69876e753">2005</a>). In the following experiments and pilot study, we fix the user’s viewpoint. In a practical application, an eye tracking sensor can be integrated into the proposed system.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Proof-of-concept experiment</h4><p>A fundamental experiment was conducted to confirm the effectiveness of the proposed principle. Two documents were placed one by one on a desktop under the environment light provided by the projector, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig3">3</a>a. When each document was placed, a color image was captured to obtain <i>E</i>(λ)<i>M</i>
                              <sub>1</sub>(λ) and <i>E</i>(λ)<i>M</i>
                              <sub>2</sub>(λ). We calculated the target appearance by using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0159-5#Equ1">1</a>) with α (i.e., the transparency of the upper-layer document) of 1.0 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig3">3</a>b). Note that <i>R</i>(λ) represents the target appearance in this case. Therefore, the color of the upper-layer document does not remain in the target appearance. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig3">3</a>c shows a projection result based on the proposed principle. It is confirmed that the target appearance is almost reproduced on the textured surface. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig3">3</a>d shows the result without radiometric compensation. We confirmed that the radiometric compensation technique improved the appearance of transparency.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Results of fundamental experiment: <b>a</b> a document stack under projected environment light, <b>b</b> target appearance, <b>c</b> projection result, and <b>d</b> projection result without radiometric compensation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Limitation</h4><p>A major drawback of the current implementation is that geometric and radiometric calibration needs to be done every time there is a change in the scene. For the calibration, multiple pattern lights have to be projected. Because it takes at least a few seconds, this process disturbs a user’s natural document search process. On the other hand, several researchers have focused on fast and imperceptible calibration methods (Zollmann and Bimber <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Zollmann S, Bimber O (2007) Imperceptible calibration for radiometric compensation. In: Proceedings of eurographics (short paper), pp 61–64" href="/article/10.1007/s10055-010-0159-5#ref-CR24" id="ref-link-section-d69876e839">2007</a>). We believe that such a method can solve this problem.</p><p>Limited dynamic range of projection is another issue. In some cases, real documents cannot be completely made transparent due to this limitation. Several researchers have attempted to solve the issue by adjusting the desired appearance before radiometric compensation is carried out (Grundhöfer and Bimber <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Grundhöfer A, Bimber O (2008) Real-time adaptive radiometric compensation. IEEE Trans Vis Comput Graph 14(1):97–108" href="/article/10.1007/s10055-010-0159-5#ref-CR7" id="ref-link-section-d69876e845">2008</a>). This reduces perceived visual artifacts while simultaneously preserving a maximum of luminance and contrast. Furthermore, high dynamic range projection technologies have also been developed in research fields of computer graphics and projection display (Seetzen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Seetzen H, Heidrich W, Stuerzlinger W, Ward G, Whitehead L, Trentacoste M, Ghosh A, Vorozcovs A (2004) High dynamic range display systems. In: Proceedings of ACM international conference on computer graphics and interactive techniques (SIGGRAPH ’04), pp 760–768" href="/article/10.1007/s10055-010-0159-5#ref-CR18" id="ref-link-section-d69876e848">2004</a>). We believe that these technologies can solve the issue.</p><h3 class="c-article__sub-heading" id="Sec8">Additional visual effects</h3><p>In addition to the fundamental optical model, two types of visual effects are applied. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig3">3</a>, the system can change the appearance of the upper-layer document so that the lower-layer document becomes visible to the user. However, the user may not feel that the document is made transparent because the visualization is unrealistic. Thus, one of the effects is designed to allow a user to feel that the upper-layer document becomes transparent.</p><p>Because the purpose of the system is to support document search, users must be able to pick up a desired document immediately after it is visually exposed. Therefore, the other effect is designed to allow users know how many documents are already made transparent.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Enhancement of transparent effect</h4><p>Fadeout is applied to the first visual effect, which allows a user to feel that the upper-layer document becomes transparent. This visual effect gradually reduces the opacity of the upper-layer document.</p><p>A questionnaire survey was conducted to evaluate the effect. Seven participants were recruited from a local university. In the experiment, two papers were piled on a desktop. The participants saw that the upper one was made transparent with the fadeout effect by projected imagery. They also saw that the upper one was made transparent without the effect. All participants answered that they were able to feel that the upper-layer document was made transparent with the effect more than without the effect. The result indicates the effectiveness of the proposed effect.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Indication of layer number of visually exposed document</h4><p>In document search scenarios, it is important for users to comprehend the layer number of the visually exposed lower-layer document. This allows users to take a document from the stack immediately after it is exposed. It is important to provide users with information about the number of upper-layer documents that are already made transparent. A straightforward solution is alpha blending of all the invisible documents. However, simple alpha blending considerably decreases readability of the blended image contents because they visually interfere with each other (Baudisch and Gutwin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baudisch P, Gutwin C (2004) Multiblending: displaying overlapping windows simultaneously without the drawbacks of alpha blending. In: Proceedings of of ACM conference on human factors in computing systems (CHI ’04), pp 367–374" href="/article/10.1007/s10055-010-0159-5#ref-CR2" id="ref-link-section-d69876e881">2004</a>). Instead, we propose an effect that makes upper-layer documents transparent one by one while retaining all disappeared documents’ frames.</p><p>The analytic hierarchy process (AHP) is conducted to prove that the proposed effect is efficient. Six visual effects (A, B, …, F) including the proposed one are compared. Effect A makes the upper-layer documents transparent without any additional effects. Effect B makes them transparent with alpha blending. Effect C makes them transparent while retaining all disappeared documents’ frames. In these effects, the upper-layer documents disappear at once. Effects D, E, and F are the same as A, B, and C, respectively, except that they make the upper-layer documents transparent one by one. Thus, effect F is the one that is proposed.</p><p>Fifteen participants were recruited from a local university. Each pair of objective visual effects was displayed on a PC monitor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig4">4</a>a). The participants saw the effects and compared them. They assigned 1 to the effect that makes it easier to understand the layer number of the exposed lower-layer document and 0 for the other effect of each pair. The result shows the relative scores of the compared effects (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig4">4</a>b). This result indicates that the proposed effect is effective for helping a user understand the layer number of the visually exposed lower-layer document.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>AHP to validate the proposed visual effect: <b>a</b> view of the experiment and <b>b</b> result</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Combination of two visual effects</h4><p>The aforementioned visual effects are combined and implemented in the proposed system. The combined effect is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig5">5</a> where the upper-layer documents are gradually made transparent one by one while retaining the frames of the disappeared documents.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Example of combination of two visual effects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Touched area sensing</h2><div class="c-article-section__content" id="Sec12-content"><p>This section describes a touch sensing method using thermography. The proposed method can detect an instantaneous touch action on a physical document placed at various heights on the desktop. In general, when a hand touches an object, body heat is transferred to the object. After the hand releases the object, the body heat stays for a while. This residual body heat is called a <i>thermal trace</i> in this paper. The proposed method extracts the thermal trace of a user’s instantaneous touch on a physical document. Note that the following image processing is performed on 8-bit images.</p><h3 class="c-article__sub-heading" id="Sec13">Method</h3><p>An overhead thermo-infrared (IR) camera is used to capture the temperature distribution of a desktop scene. A warm area in the captured IR image is detected as a touched area through a thresholding process. However, besides the touched area, other warm objects such as a user’s hand and a coffee cup on the desktop are also detected. Therefore, it is necessary to detect only the user’s touch area from warm areas. To achieve this, an additional visible camera is used, which captures the scene from the same perspective as the IR camera.</p><p>We assume that the thermal trace caused by a user’s instantaneous touch action is static in the visible image and dynamic in the IR image. If an area in an image does not change for at least ten seconds, it is regarded as static; it is considered dynamic otherwise. Warm areas are classified into four categories, as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0159-5#Tab1">1</a>.
</p><ul class="u-list-style-none">
                    <li>
                      <p><i>Case 1</i>: A warm area that is static in both the visible and IR images (e.g., a coffee cup or a user’s hand that is placed on the desktop or a document and does not move for more than ten seconds)</p>
                    </li>
                    <li>
                      <p><i>Case 2</i>: A warm area that is static in the visible image and dynamic in the IR image (a touched area based on our assumption)</p>
                    </li>
                    <li>
                      <p><i>Case 3</i>: A warm area that is dynamic in the visible image and static in the IR image (e.g., a small TV screen that is placed on the desktop)</p>
                    </li>
                    <li>
                      <p><i>Case 4</i>: A warm area that is dynamic in both the visible and IR images (e.g., a coffee cup moved by a user, a moving hand, or a thermal trace caused when an object of Case 1, such as a static coffee cup, is released from the desktop or the document)</p>
                    </li>
                  </ul>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Categories of warm areas</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0159-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig6">6</a> shows the process flow of the proposed method. This extracts only a warm area of Case 2, which is static in the visible image and dynamic in the IR image. First, dynamic areas in the visible image such as a user’s moving hand are removed by foreground subtraction followed by a mask process. Second, warm areas are extracted from the masked IR image by a thresholding process. Third, background subtraction is applied to the IR image to remove static warm areas such as a coffee cup on the desktop. Finally, an additional noise cancelation process enables detection of only the touched area. This process is performed at 15 frames per second with an experimental system that will be introduced in the following section. Note that the proposed method updates background images by taking new visible and thermal images when there is no change in the captured scene for a certain time (10 s).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Process flow of touch sensing method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>This technique can selectively detect a warm area of Case 2 as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0159-5#Tab1">1</a>. However, if a warm object such as a coffee mug is placed on a desktop for a couple of seconds and then moved away from the desktop, it belongs to Case 2 and is detected as a user’s hand touch. The failure can be solved by applying an image-based object recognition technique to distinguish a user’s hand from other objects (Swain and Ballard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Swain MJ, Ballard DH (1991) Color indexing. Int J Comput Vis 7(1):11–32" href="/article/10.1007/s10055-010-0159-5#ref-CR20" id="ref-link-section-d69876e1213">1991</a>).</p><h3 class="c-article__sub-heading" id="Sec14">Investigation into thermal trace transition</h3><p>Heat conduction is generally considered a slow phenomenon. We investigate the transition of thermal traces on a document. The transitions on copper and thermo-plastic acrylonitrile butadiene styrene (ABS) have been studied (Ho et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ho HN, Amemiya T, Ando H (2007) Revealing invisible traces of hand-object interactions with thermal vision. In: Proceedings of the 2007 inframation conference, pp 431–438" href="/article/10.1007/s10055-010-0159-5#ref-CR8" id="ref-link-section-d69876e1224">2007</a>). However, to our knowledge, such previous research has not well investigated the thermal trace transition on paper.</p><p>We evaluate how long it takes to transfer body heat from a user to a document. We also evaluate how long the heat stays on the touched area after the hand is released. A piece of paper is used as a touch surface. The paper is thin enough so that the same temperature distribution occurs on both sides of the paper simultaneously. In the experiment, a hand touches the paper for about one second. The thermo-IR camera measures the temperature transition of the touched area from the other side of the paper.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig7">7</a>a shows that the temperature reaches the predetermined threshold (pixel value: 130) from room temperature (pixel value: 90) in 0.15 s after the hand touches the paper. This result confirms that the transfer time is short enough so that a user can manipulate the system by a daily touching action. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig7">7</a>b shows that the temperature falls below the threshold 1.05 s after the hand is released from the paper. This result confirms that the user has to wait for about one second to perform another manipulation on the same document stack. Considering this transition time, we designed the document search support techniques introduced in the following section.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Thermal transition on paper with one second of hand contact: <b>a</b> hand touches the paper and <b>b</b> it is released</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Document search support system</h2><div class="c-article-section__content" id="Sec15-content"><p>This section describes how the proposed document search support system works. First, the proposed system configuration is introduced. Then, three document search support techniques are proposed. Finally, user studies are conducted to evaluate how the proposed techniques improve document search.</p><h3 class="c-article__sub-heading" id="Sec16">System configuration</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig8">8</a> shows a view of the experimental setup of <i>Limpid Desk</i>. The system consists of a video projector, two visible cameras, and a thermo-IR camera, which are mounted overhead and facing the desktop. This system runs on a PC (Intel Pentium-4 2.5 [GHz]). A video projector (NEC MT1075J) renders document images directly on the stacks. As explained in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec4">3.1</a>, this is also used as the environment light on the scene. A visible camera (Point Grey Research Scorpion) that is next to the projector is used for geometric and radiometric image correction. Note that gamma characteristics of the devices are already corrected.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>View of experimental setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Another visible camera (Sony EVI-G20) is used for the proposed touched area sensing. A thermo-IR camera (Mitsubishi IR-SC1) takes a thermal image that is also used for touched area sensing. The optics of the thermo-IR camera and the visible camera are coaxial. A dichroic mirror is used, which passes visible light and reflects thermo-IR light. Images are geometrically registered via homography, as proposed in a previous study (Yasuda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Yasuda K, Naemura T, Harashima H (2004) Thermo-Key: human region segmentation from video. IEEE Comput Graph Appl 24(1):26–30" href="/article/10.1007/s10055-010-0159-5#ref-CR22" id="ref-link-section-d69876e1308">2004</a>). This setup allows users to move their hands freely on and over the desktop.</p><h3 class="c-article__sub-heading" id="Sec17">Document search support techniques</h3><p>We implement three different document search support techniques in which a user can see through stacked documents only by touching them without any user-worn or handheld devices.</p><p>In the first technique, after the user touches the top of a document stack, documents in the stack are made transparent one by one to the bottom (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig9">9</a>). The documents disappear according to the visual effect proposed in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec8">3.2</a>. As a result, the user can browse all the documents in the stack by one direct manipulation without physically removing the upper-layer documents. Furthermore, if the desired document is found, the user can immediately take it from the stack as a result of the visual effect. The technique is referred to as <b>Browse Search</b>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Browse Search: <b>a</b> a user touches the top document, <b>b</b> the touched area is detected, <b>c</b> documents are gradually made transparent, and <b>d</b> the bottom document is visually exposed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the second technique, after a user touches the top of a document stack, thumbnail images of all documents in the stack are aligned and projected onto the top (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig10">10</a>). If the desired document appears, the user touches its thumbnail image. Then, the overlying documents are made transparent according to the visual effect proposed in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec8">3.2</a>. The technique allows the user to comprehend all the documents in the stack at a glance. The technique is referred to as <b>Thumbnail Search</b>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Thumbnail Search: <b>a</b> a user touches the top document, <b>b</b> thumbnail images are exposed, <b>c</b> user touches one of the thumbnails, and <b>d</b> the selected document is exposed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the third technique, a user can directly select a desired document by touching its edge (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig11">11</a>). All the upper-layer documents disappear, and the target document is exposed immediately after the user touches its edge. The technique is referred to as <b>Direct Selection</b>. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Direct Select: <b>a</b> a user touches a document, <b>b</b> magnified view, <b>c</b> the touched area is detected, and <b>d</b> the selected document is exposed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The first two techniques are designed to be used in a scenario where a user searches for a document by seeing all documents in a stack. On the other hand, the last technique is designed to be used not in a document search scenario, but in a scenario where the user is interested in a document in a stack and would like to quickly check what it is.</p><h3 class="c-article__sub-heading" id="Sec18">Evaluation of Browse Search and Thumbnail Search</h3><p>A user study was conducted to address the issue of whether or not the proposed document search techniques are any better than searching for documents by hand in terms of document search time. Document search only by hand is referred to as <b>Hand Search</b>. We compared the first two interaction techniques, Browse Search and Thumbnail Search, with Hand Search.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Document search time model</h4><p>First, we propose a model of document search time. In general, multiple document stacks are placed on a desktop. Therefore, it is more reasonable to consider a situation in which a person searches for a desired document from multiple stacks rather than from a single stack. When the person searches from <i>L</i> document stacks and finds it in the <i>l</i>th stack (i.e., <i>l</i> &lt; <i>L</i>), the total amount of document search time <i>T</i>(<i>l</i>|<i>L</i>) is modeled as:
</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ T(l|L) =\left\{ \begin{array}{ll} t_e(l) &amp; (l=1)\\ t_e(l) + \sum_{i=1}^{l-1}t_n(i) &amp; (l &gt;1) \end{array} \right. $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>l</i>) represents the time required for the person to find the document in the <i>l</i>th stack. On the other hand, <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>i</i>) represents the time required for the person to realize that the document does not exist in the <i>i</i>th stack.</p><p>Because there are generally multiple document stacks on a desktop (i.e., <i>L</i> &gt; 1) as noted above, the person will find the desired document after searching through more than one stack in most cases (i.e., <i>l</i> &gt; 1). Therefore, it is more significant to make <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>i</i>) shorter than to make <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>l</i>) shorter.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Method</h4><p>We prepared a document stack <i>x</i> and measured <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>x</i>) and <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>) through the user study. Each time was measured in the following three different document search techniques: Hand Search, Browse Search, and Thumbnail Search. Consequently, we measured six different document search times as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0159-5#Tab2">2</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Measured document search time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0159-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>A document stack, a copy of a target document, and a timer were prepared in each trial. First, a participant was allowed to look at the copy and recognized that he (or she) would try to find this document in the trial. Then, the participant started the document search. Simultaneously, the timer was activated. In cases where the target document did not exist in the stack, the participant stopped the timer when he (or she) realized it. When the document existed in the stack, the participant stopped the timer after finding and taking it. Note that the participant did not know whether the document existed in the stack at the beginning of each trial.</p><p>Six different document stacks were prepared for the six conditions. Each document stack consisted of twenty documents (ten paper documents, eight magazines, one bookbinder, and one file folder). It took 0.5 s to make each document transparent in Browse Search and Thumbnail Search. The target was a paper document inserted at the sixteenth layer of the stack when we measured <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>|<i>HS</i>), <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>|<i>BS</i>), and <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>|<i>TS</i>). Sixteen participants were recruited from a local university. Each performed the trials under all six conditions. Before the study, they used the proposed system to know how to use it.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Result</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>a and b show the average document search time in a case where target documents existed and in the other case where they did not exist, respectively. A one-way analysis of variance (ANOVA) with repeated measures showed statistically significant differences of all the respective times in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>a (<i>F</i>
                              <sub>2, 30</sub> = 6.9, <i>p</i> &lt; 0.01) and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>b (<i>F</i>
                              <sub>2, 30</sub> = 41.8, <i>p</i> &lt; 0.01). Post-hoc analysis was then performed using a Student-Newman–Keuls test for pairwise comparison. It showed statistically significant differences between each pair of conditions in both cases except for between <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>|<i>HS</i>) and <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>x</i>|<i>BS</i>) as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Average document search time with standard deviation: <b>a</b> in the case where target documents existed and <b>b</b> in the case where they did not exist (**<i>p</i> &lt; 0.01, *<i>p</i> &lt; 0.05)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>a shows that Browse Search did not significantly improve the document search time over Hand Search when the target document existed in the stack. Furthermore, Hand Search was about 7 s faster than Thumbnail Search. On the other hand, when the target document did not exist in the stack, the document search time could be significantly shortened by both Browse Search (about 16 s) and Thumbnail Search (about 22 s) over Hand Search (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig12">12</a>b).</p><p>In summary, Browse Search could shorten <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>i</i>) over Hand Search. On the other hand, longer <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>l</i>) was needed in Thumbnail Search than in Hand Search, although <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>i</i>) could be significantly shortened in Thumbnail Search. The amount of time shortened in <i>t</i>
                              <sub>
                      <i>n</i>
                    </sub>(<i>i</i>) (22 s) was more than thrice as great as the amount of time increased in <i>t</i>
                              <sub>
                      <i>e</i>
                    </sub>(<i>l</i>) (7 s). Therefore, according to the proposed model in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0159-5#Sec19">5.3.1</a>, both the proposed techniques can shorten a document search time when a person searches for a document from at least two document stacks. So, when <i>l</i> = 1,
</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ T(l|L,HS) = T(l|L, BS) &lt; T(l|L, TS), $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>and when <i>l</i> &gt; 1,
</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ T(l|L,HS) &gt; T(l|L, BS) &gt; T(l|L, TS), $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>T</i>(<i>l</i>|<i>L</i>, <i>HS</i>), <i>T</i>(<i>l</i>|<i>L</i>, <i>BS</i>), and <i>T</i>(<i>l</i>|<i>L</i>, <i>TS</i>) represent the total amounts of time required in Hand Search, Browse Search, and Thumbnail Search, respectively. Because there are generally multiple document stacks on a desktop, we believe that the proposed techniques can effectively support document search.</p><h3 class="c-article__sub-heading" id="Sec22">Evaluation of direct select</h3><p>We carried out another user study to evaluate the last interaction technique, Direct Select. The technique might be useful when a user is interested in a document in a stack and would like to check what it is. Without the technique, a person normally has to remove the upper documents by his (or her) hands only, even if they are quite heavy to move. We refer to this manipulation as <b>Hand Select</b>. This study was conducted to confirm whether or not Direct Select is more effective than Hand Select in such usage scenario.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Method</h4><p>We compared the task completion time that was needed to recognize the month and year of publication of a magazine in a document stack by Direct Select, <i>t</i>
                              <sub>
                      <i>r</i>
                    </sub>(<i>DS</i>), and by Hand Select, <i>t</i>
                              <sub>
                      <i>r</i>
                    </sub>(<i>HS</i>). A document stack, a timer, a pen, and a sheet of paper were placed on a desktop. The document stack consisted of the same kinds of documents used in the previous study. The sixteenth document was a monthly magazine on which the month and year of publication were printed.</p><p>First, we told the participant to check the sixteenth document and to write down its month and year of publication. Then, the participant started to do the task by either Direct Select or Hand Select. Simultaneously, the timer was activated. After the participant recognized the document and wrote the information on the paper, he (or she) stopped the timer.</p><p>Each participant performed two different trials: one was by Direct Select and the other was by Hand Select. We prepared a different document stack for each trial. Sixteen participants were recruited from a local university.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Result</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0159-5#Fig13">13</a> shows the average task completion time in each condition. A paired <i>T</i> test between them (<i>t</i>
                              <sub>15</sub> = 13.98, <i>p</i> &lt; 0.01) showed that Direct Select could significantly shorten the time over Hand Select. Therefore, we reached the conclusion that Direct Select is more effective than Hand Select in a usage scenario where people would like to check a document in a document stack if its edge shows from the stack.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0159-5/MediaObjects/10055_2010_159_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Average task completion time with standard deviation in recognition of month and year of publication of magazine (**<i>p</i> &lt; 0.01)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0159-5/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Discussion</h2><div class="c-article-section__content" id="Sec25-content"><p>The proposed touch sensing method can be applied to other projection-based MR systems where users interact with computer systems in the real world. A possible application is a projection-based real world painting system such as (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented reality (ISAR ’01), pp 207–216" href="/article/10.1007/s10055-010-0159-5#ref-CR1" id="ref-link-section-d69876e2248">2001</a>). In the previous study, a user must have an unusual input device to draw on a physical object. But the proposed method realizes that the user can paint physical objects directly by touching them. The system projects colored images onto the surface where touched areas are detected. Users can use their own hands, fingers, bodies, and even breath to paint on physical objects. They can also use a tool in their immediate environment, such as a paintbrush, an airbrush with hot water, and a hairdryer, as in (Iwai and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of ACM international conference on advances in computer entertainment technology (ACE ’05), pp 213–216" href="/article/10.1007/s10055-010-0159-5#ref-CR10" id="ref-link-section-d69876e2251">2005</a>). Furthermore, the painting results can be unique because the hand shape or air flow appears on the physical object. This cannot be achieved by normal digital painting tools.</p><p>As used in (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic desktops. In: Proceedings of ACM symposium on user interface software and technology (UIST ’04), pp 99–107" href="/article/10.1007/s10055-010-0159-5#ref-CR13" id="ref-link-section-d69876e2257">2004</a>), several vision-based methods can recognize and track documents on a desktop in real time based on feature point matching techniques (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110" href="/article/10.1007/s10055-010-0159-5#ref-CR16" id="ref-link-section-d69876e2260">2004</a>; Lepetit and Fua <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Lepetit V, Fua P (2006) Keypoint recognition using randomized trees. IEEE Trans Pattern Anal Mach Intell 28(9):1465–1479" href="/article/10.1007/s10055-010-0159-5#ref-CR15" id="ref-link-section-d69876e2263">2006</a>). By exploiting these methods, the proposed system can automatically recognize all documents in a stack even when documents are piled and removed dynamically on a desktop, as shown in (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic desktops. In: Proceedings of ACM symposium on user interface software and technology (UIST ’04), pp 99–107" href="/article/10.1007/s10055-010-0159-5#ref-CR13" id="ref-link-section-d69876e2266">2004</a>).</p><p>We believe that the proposed framework can be used for applications other than document search. For example, the proposed system can be applied to an educational interface where students can look inside various living things (e.g., plants, animals, and human body) and machines (e.g., printers, PCs, TVs, audio players, air-conditioners, and cars) with an intuitive touch interface.</p><p>This paper presents a proof-of-concept system. Although the current configuration is large and expensive, all the devices it uses have been getting cheaper and smaller recently. Therefore, we believe that the system will be accepted in a typical office in the near future.</p></div></div></section><section aria-labelledby="Sec26"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Conclusion</h2><div class="c-article-section__content" id="Sec26-content"><p>This paper has presented <i>Limpid Desk</i>, which supports document search on a physical desktop by making the upper layer of a document stack transparent in a projection-based MR environment. A user can visually access a lower-layer document without physically removing the upper documents. This is accomplished by superimposition of cover textures of lower-layer documents on the upper documents by projected imagery. This paper has introduced a method of generating projection images that make physical documents appear transparent. Furthermore, a touch sensing method based on thermal trace detection has been proposed for the system’s input interface. User’s touched areas on physical documents can be detected without any user-worn or handheld devices. This interface allows a user to select a stack to be made transparent by a simple touching gesture. Three document search support techniques have been proposed, and the results of a user study showed the effectiveness of the proposed technique.</p><p>In future, we will try to extend the proposed framework to seamlessly combine the virtual and physical desktop spaces.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Apple Inc., <a href="http://www.apple.com/">http://www.apple.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>Microsoft Corporation, <a href="http://www.microsoft.com/">http://www.microsoft.com/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR1">Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented reality (ISAR ’01), pp 207–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baudisch P, Gutwin C (2004) Multiblending: displaying overlapping windows simultaneously without the drawbacks" /><p class="c-article-references__text" id="ref-CR2">Baudisch P, Gutwin C (2004) Multiblending: displaying overlapping windows simultaneously without the drawbacks of alpha blending. In: Proceedings of of ACM conference on human factors in computing systems (CHI ’04), pp 367–374</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="O. Bimber, R. Raskar, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters Ltd, Natick" /><p class="c-article-references__text" id="ref-CR3">Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters Ltd, Natick</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20augmented%20reality%3A%20merging%20real%20and%20virtual%20worlds&amp;publication_year=2005&amp;author=Bimber%2CO&amp;author=Raskar%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Bimber, D. Iwai, G. Wetzstein, A. Grundhöfer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector-camera systems. Comput Gr" /><p class="c-article-references__text" id="ref-CR4">Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector-camera systems. Comput Graph Forum 27(8):2219–2254</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2008.01175.x" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20computing%20of%20projector-camera%20systems&amp;journal=Comput%20Graph%20Forum&amp;volume=27&amp;issue=8&amp;pages=2219-2254&amp;publication_year=2008&amp;author=Bimber%2CO&amp;author=Iwai%2CD&amp;author=Wetzstein%2CG&amp;author=Grundh%C3%B6fer%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bonanni L, Lee CH, Selker T (2005) Attention-based design of augmented reality interfaces. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR5">Bonanni L, Lee CH, Selker T (2005) Attention-based design of augmented reality interfaces. In: Proceedings of ACM conference on human factors in computing systems (CHI ’05) Extended Abstracts, pp 1228–1231</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dietz P, Leigh D (2001) DiamondTouch: a multi-user touch technology. In: Proceedings of ACM symposium on user " /><p class="c-article-references__text" id="ref-CR6">Dietz P, Leigh D (2001) DiamondTouch: a multi-user touch technology. In: Proceedings of ACM symposium on user interface software and technology (UIST ’01), pp 219–226</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Grundhöfer, O. Bimber, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Grundhöfer A, Bimber O (2008) Real-time adaptive radiometric compensation. IEEE Trans Vis Comput Graph 14(1):9" /><p class="c-article-references__text" id="ref-CR7">Grundhöfer A, Bimber O (2008) Real-time adaptive radiometric compensation. IEEE Trans Vis Comput Graph 14(1):97–108</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2007.1052" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20adaptive%20radiometric%20compensation&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=14&amp;issue=1&amp;pages=97-108&amp;publication_year=2008&amp;author=Grundh%C3%B6fer%2CA&amp;author=Bimber%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ho HN, Amemiya T, Ando H (2007) Revealing invisible traces of hand-object interactions with thermal vision. In" /><p class="c-article-references__text" id="ref-CR8">Ho HN, Amemiya T, Ando H (2007) Revealing invisible traces of hand-object interactions with thermal vision. In: Proceedings of the 2007 inframation conference, pp 431–438</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Inami M, Kawakami N, Tachi S (2003) Optical camouflage using retro-reflective projection technology. In: Proce" /><p class="c-article-references__text" id="ref-CR9">Inami M, Kawakami N, Tachi S (2003) Optical camouflage using retro-reflective projection technology. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR ’03), pp 348–349</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of ACM internation" /><p class="c-article-references__text" id="ref-CR10">Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of ACM international conference on advances in computer entertainment technology (ACE ’05), pp 213–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iwai D, Sato K (2006) Limpid desk: see-through access to disorderly desktop in projection-based mixed reality." /><p class="c-article-references__text" id="ref-CR11">Iwai D, Sato K (2006) Limpid desk: see-through access to disorderly desktop in projection-based mixed reality. In: Proceedings of ACM symposium on virtual reality software and technology (VRST ’06), pp 112–115</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iwai D, Hanatani S, Horii C, Sato K (2006) Limpid desk: transparentizing documents on real desk in projection-" /><p class="c-article-references__text" id="ref-CR12">Iwai D, Hanatani S, Horii C, Sato K (2006) Limpid desk: transparentizing documents on real desk in projection-based mixed reality. In: Proceedings of IEEE workshop on emerging display technologies (EDT ’06), pp 30–31</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic deskto" /><p class="c-article-references__text" id="ref-CR13">Kim J, Seitz SM, Agrawala M (2004) Video-based document tracking: unifying your physical and electronic desktops. In: Proceedings of ACM symposium on user interface software and technology (UIST ’04), pp 99–107</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Koike, Y. Sato, K. Yoshinori, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Koike H, Sato Y, Yoshinori K (2001) Integrating paper and digital information on enhanceddesk: a method for re" /><p class="c-article-references__text" id="ref-CR14">Koike H, Sato Y, Yoshinori K (2001) Integrating paper and digital information on enhanceddesk: a method for realtime finger tracking on an augmented desk system. ACM Trans Comput Hum Interact 8(4):307–322</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F504704.504706" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrating%20paper%20and%20digital%20information%20on%20enhanceddesk%3A%20a%20method%20for%20realtime%20finger%20tracking%20on%20an%20augmented%20desk%20system&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=8&amp;issue=4&amp;pages=307-322&amp;publication_year=2001&amp;author=Koike%2CH&amp;author=Sato%2CY&amp;author=Yoshinori%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Lepetit, P. Fua, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Lepetit V, Fua P (2006) Keypoint recognition using randomized trees. IEEE Trans Pattern Anal Mach Intell 28(9)" /><p class="c-article-references__text" id="ref-CR15">Lepetit V, Fua P (2006) Keypoint recognition using randomized trees. IEEE Trans Pattern Anal Mach Intell 28(9):1465–1479</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2006.188" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Keypoint%20recognition%20using%20randomized%20trees&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=28&amp;issue=9&amp;pages=1465-1479&amp;publication_year=2006&amp;author=Lepetit%2CV&amp;author=Fua%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DG. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110" /><p class="c-article-references__text" id="ref-CR16">Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int%20J%20Comput%20Vis&amp;volume=60&amp;issue=2&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE " /><p class="c-article-references__text" id="ref-CR17">Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision (ICCV ’87), pp 657–661</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seetzen H, Heidrich W, Stuerzlinger W, Ward G, Whitehead L, Trentacoste M, Ghosh A, Vorozcovs A (2004) High dy" /><p class="c-article-references__text" id="ref-CR18">Seetzen H, Heidrich W, Stuerzlinger W, Ward G, Whitehead L, Trentacoste M, Ghosh A, Vorozcovs A (2004) High dynamic range display systems. In: Proceedings of ACM international conference on computer graphics and interactive techniques (SIGGRAPH ’04), pp 760–768</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Siio I, Rawan J, Mynatt E (2003) Finding objects in “strata drawer”. In: Proceedings of ACM conference on huma" /><p class="c-article-references__text" id="ref-CR19">Siio I, Rawan J, Mynatt E (2003) Finding objects in “strata drawer”. In: Proceedings of ACM conference on human factors in computing systems (CHI ’03) Extended Abstracts, pp 982–983</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Swain, DH. Ballard, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Swain MJ, Ballard DH (1991) Color indexing. Int J Comput Vis 7(1):11–32" /><p class="c-article-references__text" id="ref-CR20">Swain MJ, Ballard DH (1991) Color indexing. Int J Comput Vis 7(1):11–32</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00130487" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color%20indexing&amp;journal=Int%20J%20Comput%20Vis&amp;volume=7&amp;issue=1&amp;pages=11-32&amp;publication_year=1991&amp;author=Swain%2CMJ&amp;author=Ballard%2CDH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wilson AD (2005) PlayAnywhere: a compact interactive tabletop projection-vision system. In: Proceedings of ACM" /><p class="c-article-references__text" id="ref-CR21">Wilson AD (2005) PlayAnywhere: a compact interactive tabletop projection-vision system. In: Proceedings of ACM symposium on user interface software and technology (UIST ’05), pp 83–92</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Yasuda, T. Naemura, H. Harashima, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Yasuda K, Naemura T, Harashima H (2004) Thermo-Key: human region segmentation from video. IEEE Comput Graph Ap" /><p class="c-article-references__text" id="ref-CR22">Yasuda K, Naemura T, Harashima H (2004) Thermo-Key: human region segmentation from video. IEEE Comput Graph Appl 24(1):26–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2004.1255805" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Thermo-Key%3A%20human%20region%20segmentation%20from%20video&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=24&amp;issue=1&amp;pages=26-30&amp;publication_year=2004&amp;author=Yasuda%2CK&amp;author=Naemura%2CT&amp;author=Harashima%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projectio" /><p class="c-article-references__text" id="ref-CR23">Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia (VSMM ’03), pp 161–168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zollmann S, Bimber O (2007) Imperceptible calibration for radiometric compensation. In: Proceedings of eurogra" /><p class="c-article-references__text" id="ref-CR24">Zollmann S, Bimber O (2007) Imperceptible calibration for radiometric compensation. In: Proceedings of eurographics (short paper), pp 61–64</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0159-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Graduate School of Engineering Science, Osaka University, Machikaneyama 1-3, Toyonaka, Osaka, 560-8531, Japan</p><p class="c-article-author-affiliation__authors-list">Daisuke Iwai &amp; Kosuke Sato</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Daisuke-Iwai"><span class="c-article-authors-search__title u-h3 js-search-name">Daisuke Iwai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daisuke+Iwai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daisuke+Iwai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daisuke+Iwai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kosuke-Sato"><span class="c-article-authors-search__title u-h3 js-search-name">Kosuke Sato</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kosuke+Sato&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kosuke+Sato" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kosuke+Sato%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0159-5/email/correspondent/c1/new">Daisuke Iwai</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix: Radiometric compensation</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix: Radiometric compensation</h3><p>In the real world, most objects have spatially varying reflectance properties that disturb the appearance of a projected image. We apply a radiometric compensation method (Yoshida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia (VSMM ’03), pp 161–168" href="/article/10.1007/s10055-010-0159-5#ref-CR23" id="ref-link-section-d69876e2301">2003</a>), so as not to decrease the visibility of the appearance content of the projected thermal image. Note that the method assumes the spectral response of the visible camera is same as one of the human eye.</p><p>The method uses the visible camera of the system to obtain an affine correlation in color space between the projection and the reflectance captured by the camera for each camera pixel. This affine correlation can transform the desired color appearance on a physical objects surface into a projected color value. When the input RGB value for the projector is represented as (<i>I</i>
                           <sub>
                    <i>R</i>
                  </sub>, <i>I</i>
                           <sub>
                    <i>G</i>
                  </sub>, <i>I</i>
                           <sub>
                    <i>B</i>
                  </sub>) and the captured RGB value of the visible camera is represented as (<i>C</i>
                           <sub>
                    <i>R</i>
                  </sub>, <i>C</i>
                           <sub>
                    <i>G</i>
                  </sub>, <i>C</i>
                           <sub>
                    <i>B</i>
                  </sub>), the correlation between them can be represented by the following equation in the affine transformation.
</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[
\begin{array}{ccc} I_R &amp; I_G &amp; I_B\\ \end{array}\right]^{t}= {\bf
K} \left[ \begin{array}{cccc} C_R &amp; C_G &amp; C_B &amp; 1\\ \end{array}
\right]^t. $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                        <p><b>K</b> is a 3 × 4 matrix that transforms a camera’s color space to that of a projector. Therefore, <b>K</b> is called a color mixing matrix that takes into account the projector’s spectral characteristics, the camera’s spectral sensitivity, and the spectral reflectance of the object’s surface. <b>K</b> has to be calibrated for each camera pixel. Once at least four correspondences between (<i>C</i><sub>
                    <i>R</i>
                  </sub>, <i>C</i><sub>
                    <i>G</i>
                  </sub>, <i>C</i><sub>
                    <i>B</i>
                  </sub>) and (<i>I</i><sub>
                    <i>R</i>
                  </sub>, <i>I</i><sub>
                    <i>G</i>
                  </sub>, <i>I</i><sub>
                    <i>B</i>
                  </sub>) are obtained, <b>K</b> is calculated by a least-squares method. In the calibration process, more than four simple color patterns (e.g., red, green, blue, yellow, magenta, and cyan) are projected, and the reflectance of each projected pattern is captured. After this color calibration, images of the desired color can be displayed on surfaces under consideration of their reflectance. The color of the compensated projection image (<i>I</i><sub>
                    <i>R</i>
                  </sub>, <i>I</i><sub>
                    <i>G</i>
                  </sub>, <i>I</i><sub>
                    <i>B</i>
                  </sub>) is calculated by (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0159-5#Equ7">7</a>) for each pixel of the projection image where RGB values of the desired color are assigned to (<i>C</i><sub>
                    <i>R</i>
                  </sub>, <i>C</i><sub>
                    <i>G</i>
                  </sub>, <i>C</i><sub>
                    <i>B</i>
                  </sub>). As described above, the color mixing matrix <b>K</b> can be calibrated without any prior information about the spectral characteristics of the projector, the camera, and the object’s surface.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Document%20search%20support%20by%20making%20physical%20documents%20transparent%20in%20projection-based%20mixed%20reality&amp;author=Daisuke%20Iwai%20et%20al&amp;contentID=10.1007%2Fs10055-010-0159-5&amp;publication=1359-4338&amp;publicationDate=2010-03-31&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Iwai, D., Sato, K. Document search support by making physical documents transparent in projection-based mixed reality.
                    <i>Virtual Reality</i> <b>15, </b>147–160 (2011). https://doi.org/10.1007/s10055-010-0159-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0159-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-06-30">30 June 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-03-15">15 March 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-03-31">31 March 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0159-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0159-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Projection-based mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Document search support</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Making documents transparent</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Thermal image processing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Thermal trace</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Touch sensing</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0159-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=159;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

