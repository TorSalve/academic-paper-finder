<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="In-Situ interactive image-based model building for Augmented Reality f"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present a system that is designed for in-situ..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="In-Situ interactive image-based model building for Augmented Reality from a handheld device"/>

    <meta name="dc.source" content="Virtual Reality 2011 17:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-01-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present a system that is designed for in-situ modeling using interactive techniques for two generic versions of handheld devices equipped with cameras. The system allows online building of 3D wireframe models through a combination of user interaction and automated methods. In particular, we concentrate in rigorous evaluation of the two devices and interaction methods in the context of 3D feature selection. We present the key components of our system, discuss our findings and results and identify design recommendations."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-01-05"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="137"/>

    <meta name="prism.endingPage" content="146"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0206-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0206-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0206-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0206-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="In-Situ interactive image-based model building for Augmented Reality from a handheld device"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2013/06"/>

    <meta name="citation_online_date" content="2012/01/05"/>

    <meta name="citation_firstpage" content="137"/>

    <meta name="citation_lastpage" content="146"/>

    <meta name="citation_article_type" content="SI: Mixed and Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0206-x"/>

    <meta name="DOI" content="10.1007/s10055-011-0206-x"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0206-x"/>

    <meta name="description" content="Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present "/>

    <meta name="dc.creator" content="Pished Bunnun"/>

    <meta name="dc.creator" content="Sriram Subramanian"/>

    <meta name="dc.creator" content="Walterio W. Mayol-Cuevas"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: Proceedings of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;10), Seoul, Korea"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces theory and practice; citation_publication_date=2005; citation_id=CR2; citation_author=D Bowman; citation_author=E Kruijff; citation_author=JL Violla; citation_author=I Poupyrev; citation_publisher=Addison Wesley"/>

    <meta name="citation_reference" content="Brown M, Drummond T, Cipolla R (2000) 3d model acquisition by tracking 2d wireframes. In: British Machine Vision Conference BMVC, pp 11&#8211;14"/>

    <meta name="citation_reference" content="Bunnun P, Mayol-Cuevas W (2008) Outlinar: an assisted interactive model building system with reduced computational effort. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;08)"/>

    <meta name="citation_reference" content="Chekhlov D, Gee AP, Calway A, Mayol-Cuevas W (2007) Ninja on a plane: automatic discovery of physical planes for Augmented Reality using visual slam. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;07)"/>

    <meta name="citation_reference" content="Comport AI, Marchand E, Pressigout M, Chaumette F (2006) Real-time markerless tracking for Augmented Reality: the virtual visual servoing framework. IEEE Trans Vis Comput Graph 12(4)"/>

    <meta name="citation_reference" content="Davison AJ, Mayol W, Murray D (2003) Real-time localisation and mapping with wearable active vision. In: International Symposium on Mixed and Augmented Reality"/>

    <meta name="citation_reference" content="Debevec PE, Taylor CJ, Malik J (1996) Modeling and rendering architecture from photographs. In: SIGGRAPH96"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans on PAMI; citation_title=Real-time visual tracking of complex structures; citation_author=T Drummond, R Cipolla; citation_volume=24; citation_issue=7; citation_publication_date=2002; citation_pages=932-946; citation_doi=10.1109/TPAMI.2002.1017620; citation_id=CR9"/>

    <meta name="citation_reference" content="Faugeras O, Lustman F (1988) Motion and structure from motion in a piecewise planar environment. Technical report, INRIA"/>

    <meta name="citation_reference" content="Freeman R, Steed A (2006) Interactive modelling and tracking for mixed and Augmented Reality. In: Proceedings of ACM Virtual Reality Software and Technology, Cyprus, pp 61&#8211;64"/>

    <meta name="citation_reference" content="Gee AP, Mayol-Cuevas W (2006) Real-time model-based slam using line segments. In: International Symposium on Visual Computing"/>

    <meta name="citation_reference" content="Grossman T, Balakrishnan R (2006) The design and evaluation of selection techniques for 3D volumetric displays. In: Proceedings of the 19th Annual ACM Symposium on User interface Software and Technology"/>

    <meta name="citation_reference" content="van den Hengel A, Dick A, Thormahlen T, Ward B, Torr PHS (2007) Videotrace: rapid interactive scene modelling from video. In: Proceedings of SIGGRAPH"/>

    <meta name="citation_reference" content="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the  IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;09)"/>

    <meta name="citation_reference" content="Kim K, Lepetit V, Woo W (2010) Keyframe-based modeling and tracking of multiple 3D objects. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;10), Seoul, Korea"/>

    <meta name="citation_reference" content="Kim S, DiVerdi S, Chang JS, Kang T, Iltis R, H&#246;llerer T (2007) Implicit 3D modeling and tracking for anywhere augmentation. In: VRST &#8217;07: proceedings of the 2007 ACM symposium on virtual reality software and technology, ACM, pp 19&#8211;28"/>

    <meta name="citation_reference" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;07), Japan"/>

    <meta name="citation_reference" content="Lepetit V, Vacchetti L, Thalmann D, Fua P (2003) Fully automated and stable registration for Augmented Reality applications. In: Proceedings of the Second IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;03), Tokyo, Japan"/>

    <meta name="citation_reference" content="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;07), Nara, Japan"/>

    <meta name="citation_reference" content="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, pp 1498&#8211;1505"/>

    <meta name="citation_reference" content="Pan Q, Reitmayr G, Drummond T (2009) ProFORMA: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British Machine Vision Conference (BMVC), London"/>

    <meta name="citation_reference" content="Reitmayr G, Eade E, Drummond T (2007) Semi-automatic annotations in unknown environments. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR&#8217;07), Nara, Japan"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Grabcut: interactive foreground extraction using iterated graph cuts; citation_author=C Rother, V Kolmogorov, A Blake; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=309-314; citation_doi=10.1145/1015706.1015720; citation_id=CR24"/>

    <meta name="citation_reference" content="Simon G (2006) Automatic online walls detection for immediate use in AR tasks. In: International Symposium on Mixed and Augmented Reality"/>

    <meta name="citation_reference" content="citation_title=Explortary Data Analysis; citation_publication_date=1977; citation_id=CR26; citation_author=JW Tukey; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="Vacchetti L, Lepetit V, Fua P (2004) Combining edge and texture information for real-time accurate 3D camera tracking. In: International Symposium on Mixed and Augmented Reality"/>

    <meta name="citation_reference" content="citation_title=Lie groups, Lie algebras, and their representations; citation_publication_date=1974; citation_id=CR28; citation_author=V Varadarajan; citation_publisher=Prentice-Hall, Inc."/>

    <meta name="citation_author" content="Pished Bunnun"/>

    <meta name="citation_author_email" content="pbunnun@cs.bris.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Bristol, Bristol, UK"/>

    <meta name="citation_author" content="Sriram Subramanian"/>

    <meta name="citation_author_email" content="sriram@cs.bris.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Bristol, Bristol, UK"/>

    <meta name="citation_author" content="Walterio W. Mayol-Cuevas"/>

    <meta name="citation_author_email" content="wmayol@cs.bris.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Bristol, Bristol, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0206-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0206-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="In-Situ interactive image-based model building for Augmented Reality from a handheld device"/>
        <meta property="og:description" content="Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present a system that is designed for in-situ modeling using interactive techniques for two generic versions of handheld devices equipped with cameras. The system allows online building of 3D wireframe models through a combination of user interaction and automated methods. In particular, we concentrate in rigorous evaluation of the two devices and interaction methods in the context of 3D feature selection. We present the key components of our system, discuss our findings and results and identify design recommendations."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>In-Situ interactive image-based model building for Augmented Reality from a handheld device | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0206-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Model building, Augmented Reality, Handheld device","kwrd":["Model_building","Augmented_Reality","Handheld_device"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0206-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0206-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=206;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0206-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            In-Situ interactive image-based model building for Augmented Reality from a handheld device
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0206-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0206-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Mixed and Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-01-05" itemprop="datePublished">05 January 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">In-Situ interactive image-based model building for Augmented Reality from a handheld device</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Pished-Bunnun" data-author-popup="auth-Pished-Bunnun" data-corresp-id="c1">Pished Bunnun<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bristol" /><meta itemprop="address" content="grid.5337.2, 0000000419367603, Department of Computer Science, University of Bristol, Bristol, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sriram-Subramanian" data-author-popup="auth-Sriram-Subramanian">Sriram Subramanian</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bristol" /><meta itemprop="address" content="grid.5337.2, 0000000419367603, Department of Computer Science, University of Bristol, Bristol, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Walterio_W_-Mayol_Cuevas" data-author-popup="auth-Walterio_W_-Mayol_Cuevas">Walterio W. Mayol-Cuevas</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bristol" /><meta itemprop="address" content="grid.5337.2, 0000000419367603, Department of Computer Science, University of Bristol, Bristol, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">137</span>–<span itemprop="pageEnd">146</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">504 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0206-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present a system that is designed for in-situ modeling using interactive techniques for two generic versions of handheld devices equipped with cameras. The system allows online building of 3D wireframe models through a combination of user interaction and automated methods. In particular, we concentrate in rigorous evaluation of the two devices and interaction methods in the context of 3D feature selection. We present the key components of our system, discuss our findings and results and identify design recommendations.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Model building is necessary for most visual Augmented Reality (AR) systems. Conventionally, models used at the augmentation stage have been generated away from the object to be modeled, mainly from a video sequence captured off-line and subsequently processed in front of a workstation. An example of this well-developed approach is the pioneering work by Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Debevec PE, Taylor CJ, Malik J (1996) Modeling and rendering architecture from photographs. In: SIGGRAPH96" href="/article/10.1007/s10055-011-0206-x#ref-CR8" id="ref-link-section-d33377e313">1996</a>) and recently, with enhanced AR emphasis, the method described by van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="van den Hengel A, Dick A, Thormahlen T, Ward B, Torr PHS (2007) Videotrace: rapid interactive scene modelling from video. In: Proceedings of SIGGRAPH" href="/article/10.1007/s10055-011-0206-x#ref-CR14" id="ref-link-section-d33377e316">2007</a>). However, having off-line techniques does not fit all applications, in particular, those in wearable computing and <i>anywhere</i> AR where the user is, by definition, present where the augmentation or the modeling is needed and where the computational and editing resources are limited.</p><p>When we desire to perform AR tasks (either augmentations or model building) in previously unknown places, camera positioning relative to the objects of relevance is paramount. Techniques have been proposed that demonstrate online assistance to somewhat mobile users by developing systems that provide positioning with computationally demanding approaches. An increasingly popular example is by using visual simultaneous localization and mapping (SLAM) that builds a map of the environment concurrently with camera pose estimation. Early work demonstrated a backbone system to facilitate remote annotations helped by visual SLAM (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Davison AJ, Mayol W, Murray D (2003) Real-time localisation and mapping with wearable active vision. In: International Symposium on Mixed and Augmented Reality" href="/article/10.1007/s10055-011-0206-x#ref-CR7" id="ref-link-section-d33377e325">2003</a>). Some other more recent systems have shown how visual SLAM, or the more generic framework of structure from motion, can provide a base for augmentations in real time (Chekhlov et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Chekhlov D, Gee AP, Calway A, Mayol-Cuevas W (2007) Ninja on a plane: automatic discovery of physical planes for Augmented Reality using visual slam. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07)" href="/article/10.1007/s10055-011-0206-x#ref-CR5" id="ref-link-section-d33377e328">2007</a>; Simon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Simon G (2006) Automatic online walls detection for immediate use in AR tasks. In: International Symposium on Mixed and Augmented Reality" href="/article/10.1007/s10055-011-0206-x#ref-CR25" id="ref-link-section-d33377e331">2006</a>; Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Japan" href="/article/10.1007/s10055-011-0206-x#ref-CR18" id="ref-link-section-d33377e334">2007</a>; Reitmayr et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Reitmayr G, Eade E, Drummond T (2007) Semi-automatic annotations in unknown environments. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Nara, Japan" href="/article/10.1007/s10055-011-0206-x#ref-CR23" id="ref-link-section-d33377e337">2007</a>). These systems have visible applications in positioning the camera when the computational resources are available and for when the automatic feature selection has enough information to not need any user input.</p><p>Three-dimensional maps obtained from SLAM or the more generic structure from motion approach are regularly intended to map larger areas of the environment to support applications such as exploration or user positioning. However, a related approach to camera positioning traditionally used to track smaller areas or individual objects is model-based tracking (MBT), which has been researched by a number of works to improve to be more robust and efficient for real-time implementation (Comport et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Comport AI, Marchand E, Pressigout M, Chaumette F (2006) Real-time markerless tracking for Augmented Reality: the virtual visual servoing framework. IEEE Trans Vis Comput Graph 12(4)" href="/article/10.1007/s10055-011-0206-x#ref-CR6" id="ref-link-section-d33377e343">2006</a>; Drummond and Cipolla <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Drummond T, Cipolla R (2002) Real-time visual tracking of complex structures. IEEE Trans on PAMI 24(7):932–946" href="/article/10.1007/s10055-011-0206-x#ref-CR9" id="ref-link-section-d33377e346">2002</a>; Lepetit et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Lepetit V, Vacchetti L, Thalmann D, Fua P (2003) Fully automated and stable registration for Augmented Reality applications. In: Proceedings of the Second IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’03), Tokyo, Japan" href="/article/10.1007/s10055-011-0206-x#ref-CR19" id="ref-link-section-d33377e349">2003</a>; Vacchetti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Vacchetti L, Lepetit V, Fua P (2004) Combining edge and texture information for real-time accurate 3D camera tracking. In: International Symposium on Mixed and Augmented Reality" href="/article/10.1007/s10055-011-0206-x#ref-CR27" id="ref-link-section-d33377e352">2004</a>). In MBT, the user input happens at the model-building stage before the tracking starts, although some work has been aimed to relax this condition, so that elements of the model can grow automatically, and thus bridging the gap with SLAM (Gee and Mayol-Cuevas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gee AP, Mayol-Cuevas W (2006) Real-time model-based slam using line segments. In: International Symposium on Visual Computing" href="/article/10.1007/s10055-011-0206-x#ref-CR12" id="ref-link-section-d33377e355">2006</a>).</p><p>Considering the above techniques from the point of view of model building, Visual SLAM and related methods extract features that are usually inside rich texture regions or that have strong gradients. A notable recent exception is the work in Newcombe and Davison (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, pp 1498–1505" href="/article/10.1007/s10055-011-0206-x#ref-CR21" id="ref-link-section-d33377e361">2010</a>) where the emphasis is not on a map that supports camera pose computation but on high quality dense reconstruction. However, a key distinction to have is that the recovered models, either sparse or dense, may not correspond to what the user may want to model. In terms of MBT, those features present in the available model may not all be good for tracking and commonly such models cannot be edited to match the real object.</p><p>This calls for methods that allow the user to specify interactively what is to be modeled and what is not needed as well as enable the result to be tested and verified as soon as possible. Furthermore, independently from the model building engine to use, it is crucial to be aware of the effect of the interaction technique and device format within the interactive, in-situ, context.</p><p>In this paper, we extend from our previous work OutlinAR (Bunnun and Mayol-Cuevas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bunnun P, Mayol-Cuevas W (2008) Outlinar: an assisted interactive model building system with reduced computational effort. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’08)" href="/article/10.1007/s10055-011-0206-x#ref-CR4" id="ref-link-section-d33377e370">2008</a>), an interactive real-time model building system for the delineation of basic wireframe shapes with the help of the user and whenever possible, with assistance of automated methods and low computational tracking demand. In particular, we perform controlled experiments to determine performance of generic shapes of handheld devices and methods for spatial feature selection in 3D.</p><p>We built two devices (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig1">1</a>)—a standard tablet-like touch screen for two-handed usage and a small display mounted on a wand-like device with a scroll wheel for one-handed usage. We have also devised two interaction techniques for 3D point selection—one based on intersection of two rays (two-click) and another based on specifying a ray and moving a point along it (click and move). We are also interested in modeling 3D objects that can be defined by their outline edges and less so by their texture appearance. This is in contrast with the objects suitable for the point-based feature descriptors and modeling approaches used by some of the methods above mentioned.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Two devices used in the evaluation. <i>Left</i>: wand-like device with screen, camera, and wheel and buttons interface. <i>Right</i>: touch screen fitted with camera</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The main contributions of this article are (1) exploring new contexts for interaction techniques; (2) identifying differences between two handheld devices, a touch screen and a scroll-wheel wand; and (3) investigating selection techniques for aiding in-situ 3D model building.</p><p>For the rest of the article, we review related work on interactive model building primarily in the context of AR in the following Section. The system’s overview is briefly introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec3">3</a>. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec4">4</a> discusses about the model tracker. Interactive methods are explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec5">5</a>, the experiments and results from the user case studies are discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec11">6</a>, and a recommended architecture of the system is discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec14">7</a>. Finally, we conclude the article and future work in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec15">8</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">In-situ interactive model building for AR</h2><div class="c-article-section__content" id="Sec2-content"><p>As we have argued before, there is a gain in systems that allow interactive model building such that this can be done in-situ and where the user can verify that what is contained in the resulting model is what is needed. In this respect, we share the motivation of Freeman and Steed (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Freeman R, Steed A (2006) Interactive modelling and tracking for mixed and Augmented Reality. In: Proceedings of ACM Virtual Reality Software and Technology, Cyprus, pp 61–64" href="/article/10.1007/s10055-011-0206-x#ref-CR11" id="ref-link-section-d33377e436">2006</a>) where an interactive model building approach is used. In that work, an AR toolkit marker is in the same plane as the object to be modeled, and the live camera view is paused to allow the user to define a basic model that can later be independently tracked through its texture. The editing happens on video, potentially live, in the image plane and not in front of the object. Motivation is also shared with the work by Neubert et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Nara, Japan" href="/article/10.1007/s10055-011-0206-x#ref-CR20" id="ref-link-section-d33377e439">2007</a>) where video is fed into a visual SLAM system to obtain camera positioning to aid model delineation on automatically selected key frames. A different example is the one offered by Kim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kim S, DiVerdi S, Chang JS, Kang T, Iltis R, Höllerer T (2007) Implicit 3D modeling and tracking for anywhere augmentation. In: VRST ’07: proceedings of the 2007 ACM symposium on virtual reality software and technology, ACM, pp 19–28" href="/article/10.1007/s10055-011-0206-x#ref-CR17" id="ref-link-section-d33377e442">2007</a>) where a mobile user is enabled to build models outdoors by virtue of a range of onboard positioning sensors such as inertial and GPS and a database of aerial images.</p><p>An early example of interactive wireframe model building is Brown et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Brown M, Drummond T, Cipolla R (2000) 3d model acquisition by tracking 2d wireframes. In: British Machine Vision Conference BMVC, pp 11–14" href="/article/10.1007/s10055-011-0206-x#ref-CR3" id="ref-link-section-d33377e448">2000</a>) where images at different viewpoints are collected (in that case from a camera-mounted robot) and annotated by tracing as much as possible of the object via edges that are then optimized to accumulate information of the 3D object. As in our system, the partial model is tracked and can be used to assist the modeling process.</p><p>While some recent systems have explored other outlining methods for model building, for example, rough silhouette selection as explained by van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the  IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’09)" href="/article/10.1007/s10055-011-0206-x#ref-CR15" id="ref-link-section-d33377e454">2009</a>), fine feature modeling may still require interacting with individual 3D vertices or mesh points. By using PTAM (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Japan" href="/article/10.1007/s10055-011-0206-x#ref-CR18" id="ref-link-section-d33377e457">2007</a>) instead of an off-line structure from motion method as in their original work van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="van den Hengel A, Dick A, Thormahlen T, Ward B, Torr PHS (2007) Videotrace: rapid interactive scene modelling from video. In: Proceedings of SIGGRAPH" href="/article/10.1007/s10055-011-0206-x#ref-CR14" id="ref-link-section-d33377e460">2007</a>), the system in Bastian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: Proceedings of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’10), Seoul, Korea" href="/article/10.1007/s10055-011-0206-x#ref-CR1" id="ref-link-section-d33377e463">2010</a>) is capable of building a model in real-time with user interaction. In this case, GrabCut (Rother et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23(3):309–314" href="/article/10.1007/s10055-011-0206-x#ref-CR24" id="ref-link-section-d33377e466">2004</a>) segmentation is used to separate the object to be built from the background. Although, the user defines the boundary of the object only in the 2D image, the system maintains the 3D pose of the camera relative to the surrounding map. Combining both the 2D boundary of the object from many views and the 3D camera poses, the object model can be built.</p><p>Instead of using a map generated from PTAM only for tracking the environment as in Bastian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: Proceedings of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’10), Seoul, Korea" href="/article/10.1007/s10055-011-0206-x#ref-CR1" id="ref-link-section-d33377e472">2010</a>), a 3D point cloud is used to build and track 3D object models (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kim K, Lepetit V, Woo W (2010) Keyframe-based modeling and tracking of multiple 3D objects. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’10), Seoul, Korea" href="/article/10.1007/s10055-011-0206-x#ref-CR16" id="ref-link-section-d33377e475">2010</a>). A plane is built from cloud points, and a 3D box of objects is extruded from the plane by a similar technique as used in Bunnun and Mayol-Cuevas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bunnun P, Mayol-Cuevas W (2008) Outlinar: an assisted interactive model building system with reduced computational effort. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’08)" href="/article/10.1007/s10055-011-0206-x#ref-CR4" id="ref-link-section-d33377e478">2008</a>).</p><p>Another approach to build models with a degree of interaction is that the user manipulates a textured object in front of a stationary camera while a system tracks available point features (Pan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Pan Q, Reitmayr G, Drummond T (2009) ProFORMA: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British Machine Vision Conference (BMVC), London" href="/article/10.1007/s10055-011-0206-x#ref-CR22" id="ref-link-section-d33377e485">2009</a>). In this case, a bundle adjustment process is used to estimate the 3D pose of the tracked features and thus the object. The system also adds the ability to provide guidance to the user in terms of how to move the object to explore unseen regions.</p><p>One of the central components of any in-situ model building <i>using a handheld device</i> is indeed the way to point and select object features to be modeled. Pointing and selection techniques have been studied primarily in the context of Virtual Reality, and a number of approaches exist (see Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman D, Kruijff E, Violla JL, Poupyrev I (2005) 3D user interfaces theory and practice. Addison Wesley, Boston" href="/article/10.1007/s10055-011-0206-x#ref-CR2" id="ref-link-section-d33377e494">2005</a> work for a review). A conventional approach is to use image-plane techniques that use projections of the 3D objects into a 2D image from where the object or object features are selected.</p><p>While there is limited investigation into appropriate interaction mechanisms to aide in-situ 3D model building using cameras, there is related work that explores 3D feature selection on volumetric displays (Grossman and Balakrishnan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Grossman T, Balakrishnan R (2006) The design and evaluation of selection techniques for 3D volumetric displays. In: Proceedings of the 19th Annual ACM Symposium on User interface Software and Technology" href="/article/10.1007/s10055-011-0206-x#ref-CR13" id="ref-link-section-d33377e500">2006</a>). Grossman and Balakrishnan present a suite of interaction techniques that facilitate selection of digital objects on a 3D display. The tasks of selecting objects in a 3D display and in the real-world are, however, different, since the object cannot be enhanced by any graphical augmentation which can guide the selection process, something that would indeed need a previously acquired model of such object.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">System overview</h2><div class="c-article-section__content" id="Sec3-content"><p>The main components of an in-situ 3D model building system are the model tracking, the device that is used as the modeling tool, and the interaction techniques used to dynamically input model parameters as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overall system architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>To generate the model of the object iteractively, the camera pose relative to the object is essential.</p><p>The modeling work flow starts by the user initializing the tracker whether by aligning the camera with a known template or by a predefined shape. After the initialization phase, the tracker constantly feeds the camera pose to the interaction component. The system then uses information from the user to update the model of the object. Notice that while generating the model, the partial model itself is also used for tracking the camera pose inside the tracker.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Model-based tracker</h2><div class="c-article-section__content" id="Sec4-content"><p>To make the discussion self-contained, we describe the details for the model-based tracker we have used, including the incorporated few modifications. The MBT method we use is mainly based on the system developed by Drummond and Cipolla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Drummond T, Cipolla R (2002) Real-time visual tracking of complex structures. IEEE Trans on PAMI 24(7):932–946" href="/article/10.1007/s10055-011-0206-x#ref-CR9" id="ref-link-section-d33377e544">2002</a>) where further details can be found. The tracker is based on the estimation of partial motion of a 3D rigid object moving in 3D space, which is described by a 6D Lie group and associated algebra. There are 2 main steps in each iteration. Firstly, it computes an objective function based on a sum of square errors between the 3D model image map and its corresponding edges of the current image. Secondly, it computes motion parameters or the extrinsic camera parameters by minimizing the objective function via least-squares optimization using an M-estimator.</p><p>The finite projective camera model is used in the tracker. The camera model represents the relationship between the position of a point in the object coordinate and its corresponding image coordinate. Parameters of the model can be divided into two groups: extrinsic and intrinsic parameters. Extrinsic parameters <b>M</b>, composed of a rotational matrix <i>R</i> and a translational vector <i>t</i> written as <span class="mathjax-tex">\({\bf M}= \left[\begin{array}{ll} R &amp; t\\ {\bf 0} &amp; 1\end{array}\right],\)</span> transfer an object position <i>P</i>
                <sub>
                  <i>o</i>
                </sub>(<i>x</i>
                <sub>
                  <i>o</i>
                </sub>, <i>y</i>
                <sub>
                  <i>o</i>
                </sub>, <i>z</i>
                <sub>
                  <i>o</i>
                </sub>, 1) in the object coordinate into a position in the camera coordinate <i>P</i>
                <sub>
                  <i>c</i>
                </sub>(<i>x</i>
                <sub>
                  <i>c</i>
                </sub>, <i>y</i>
                <sub>
                  <i>c</i>
                </sub>, <i>z</i>
                <sub>
                  <i>c</i>
                </sub>, 1) by <i>P</i>
                <sub>
                  <i>c</i>
                </sub> = <b>M</b>
                <i>P</i>
                <sub>
                  <i>o</i>
                </sub>. Intrinsic parameters <i>K</i>, sometimes called internal parameters, can be represented in terms of the scale factor or focal length <i>f</i> = (<i>f</i>
                <sub>
                  <i>x</i>
                </sub>, <i>f</i>
                <sub>
                  <i>y</i>
                </sub>) in the <i>x</i> and <i>y</i> direction, respectively, principal point (<i>u</i>
                <sub>0</sub>,<i>v</i>
                <sub>0</sub>) indicating the point at which the optical axis <i>Z</i>
                <sub>
                  <i>c</i>
                </sub> passes through the image plane, and skew parameter <i>s</i> that depends on the angle between the <i>x</i> and <i>y</i> axes, as shown in <span class="mathjax-tex">\( K=\left[\begin{array}{lll}f_{x} &amp; s &amp; u_{0}\\ 0 &amp; f_{y} &amp; v_{0}\\ 0 &amp; 0 &amp; 1\end{array}\right]. \)</span> Generally, <i>s</i> for most cameras will be zero or <i>s</i>≈0, following which <i>K</i> can be rewritten in the simple form <span class="mathjax-tex">\( K=\left[\begin{array}{lll}f_{x} &amp; 0 &amp; u_{0}\\ 0 &amp; f_{y} &amp; v_{0}\\ 0 &amp; 0 &amp; 1\end{array}\right] \)</span> which will be used for the rest of the article. It maps a projection <span class="mathjax-tex">\(\bar{x}_{n}\)</span> in the normalized camera coordinate (<i>Z</i>
                <sub>
                  <i>c</i>
                </sub> = 1) defined by</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \bar{x}_{n}=proj(P_{c})=\left[\begin{array}{l}\frac{x_{c}}{z_{c}}\\ \frac{x_{c}}{z_{c}}\end{array}\right]=\left[\begin{array}{l}u_{n}\\ v_{n}\end{array}\right] $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>into the image position <i>p</i>(<i>u</i>, <i>v</i>). If <span class="mathjax-tex">\(\bar{x}_{n}\)</span> and <i>p</i> are written in homogeneous form, the relation between them is</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[\begin{array}{l}p\\ 1\end{array}\right]=K\left[\begin{array}{l}\bar{x}_{n}\\ 1\end{array}\right]. $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Lens distortion is now added into the camera model. Only radial distortion is considered up to the second order. The normalized image projection <span class="mathjax-tex">\(\bar{x}_{n}\)</span> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ2">2</a>) is now replaced by</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \bar{x}_{d} = D(\bar{x}_{n})\bar{x}_{n}=\left[\begin{array}{l}u_{d}\\ v_{d}\end{array}\right] $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ D(\bar{x}_{n}) = (1+k_{1}r^{2}+k_{2}r^{4}) $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>r</i>
                <sup>2</sup> = <i>u</i>
                <span class="c-stack">
                  <sup>2</sup><sub>
                    <i>n</i>
                  </sub>
                  
                </span> + <i>v</i>
                <span class="c-stack">
                  <sup>2</sup><sub>
                    <i>n</i>
                  </sub>
                  
                </span> and <i>k</i>
                <sub>1</sub> and <i>k</i>
                <sub>2</sub> are constant distortion coefficients.</p><p>Extrinsic camera parameter <b>M</b> can also be represented by the group <i>SE</i>(3) of rigid body motions in a 6D Lie Group (see Varadarajan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Varadarajan V (1974) Lie groups, Lie algebras, and their representations. Prentice-Hall Inc., Inglewood Cliffs" href="/article/10.1007/s10055-011-0206-x#ref-CR28" id="ref-link-section-d33377e924">1974</a> for an overview) with 6 generators for 6 <i>df</i>. The generators are taken to be translation in the <i>x</i>,  <i>y,</i> and <i>z</i> directions and rotations about <i>x</i>,  <i>y</i>, and <i>z</i>, axes respectively. They are represented by the following matrices</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ G_{1}=\left[\begin{array}{llll}0 0 &amp; 0 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right], G_{2}=\left[\begin{array}{llll}0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right], G_{3}=\left[\begin{array}{llll}0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right] \\ G_{4}=\left[\begin{array}{llll}0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; -1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right], G_{5}=\left[\begin{array}{llll}0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ -1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right], G_{6}=\left[\begin{array}{llll}0 &amp; -1 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right]. $$</span></div></div><p>The relationship between <b>M</b> and its generators is explained by the exponential map</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf M}=\left[\begin{array}{ll}R &amp; t\\ {\bf 0} &amp; 1\end{array}\right]=\exp\left(\sum_{i=1}^{6}\alpha_{i}G_{i}\right), $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>For small motion between each consecutive image frame, which is mostly true in the visual tracking system, <b>M</b>
                <sub>
                  <i>k</i>
                </sub> at frame <i>k</i> can be calculated from</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf M}_{k}=\exp\left(\sum_{i=1}^{6}\alpha_{i}G_{i}\right){\bf M}_{k-1}, $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <b>M</b>
                <sub>
                  <i>k</i>-1</sub> is a value from previous <i>k</i> − 1 frame. As a result, the problem to estimate <b>M</b>
                <sub>
                  <i>k</i>
                </sub> will be reduced to estimate <span class="mathjax-tex">\(\bar{\alpha}=[\begin{array}{llllll}\alpha_{1} &amp; \alpha_{2} &amp; \alpha_{3} &amp; \alpha_{4} &amp; \alpha_{5} &amp; \alpha_{6}\end{array}]^{T}\)</span> from a partial motion of the model relative to the camera. Let image position <span class="mathjax-tex">\(p(\bar{\alpha})\)</span> of 3D point <i>P</i>
                <sub>
                  <i>o</i>
                </sub> be a function of <span class="mathjax-tex">\(\bar{\alpha}_{i}\)</span> described by</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ p(\bar{\alpha})=\left[\begin{array}{ll}f_{x} &amp; 0\\ 0 &amp; f_{y}\end{array}\right]\bar{x}_{d}+\left[\begin{array}{ll}u_{0}\\ v_{0}\end{array}\right]. $$</span></div></div><p>The partial motion of point <i>P</i>
                <sub>
                  <i>o</i>
                </sub> in the image plane with respected to the <i>i</i>th generating motion is then computed by</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \frac{\partial p(\alpha_{i})}{\partial\alpha_{i}}=\left[\begin{array}{ll}f_{x} &amp; 0\\ 0 &amp; f_{y}\end{array}\right]\frac{\partial\bar{x}_{d}}{\partial\alpha_{i}}. $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Considering the partial derivative around α<sub>
                  <i>i</i>
                </sub> = 0 for <i>i</i> = 1, …, 6 a jacobian matrix <i>J</i> will be written in the form</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ J = \left[\begin{array}{llllll}J_{1} &amp; J_{2} &amp; J_{3} &amp; J_{4} &amp; J_{5} &amp; J_{6}\end{array}\right], \hbox{where} $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                <div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ J_{i} = \frac{\partial}{\partial\alpha_{i}}\left[\begin{array}{ll}u\\ v\end{array}\right]\bigg|_{\alpha_{i}=0}=\left[\begin{array}{ll}f_{x} &amp; 0\\ 0 &amp; f_{y}\end{array}\right]\frac{\partial\bar{x}_{d}}{\partial\alpha_{i}}\bigg|_{\alpha_{i}=0}. $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>Then (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ7">7</a>) can be rewritten as <span class="mathjax-tex">\(\frac{\partial p(\bar{\alpha})}{\partial\bar{\alpha}}=J\)</span>. Let the partial motion measured in the frame <i>k</i> be Δ<i>p</i>, we will get <span class="mathjax-tex">\(\Updelta p=J\bar{\alpha}\)</span>. However, only one partial motion is not enough to estimate <span class="mathjax-tex">\(\bar{\alpha},\)</span> and there is also measurement noise in Δ<i>p</i>. Therefore, if there are ξ 3D points whose partial motion can be measured, the objective function can be written as</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ S=\sum_{\xi}\left(\Updelta p^{\xi}-J^{\xi}\bar{\alpha}\right)^{T}\left(\Updelta p^{\xi}-J^{\xi}\bar{\alpha}\right). $$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>Finally, an estimation of <span class="mathjax-tex">\(\bar{\alpha}\)</span> can be found by minimizing the objective function in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ10">10</a>). To calculate <i>J</i>
                <sub>
                  <i>i</i>
                </sub> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ9">9</a>), we start with</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \frac{\partial\bar{x}_{d}}{\partial\alpha_{i}} = \frac{\partial}{\partial\alpha_{i}}(D(\bar{x}_{n})\bar{x}_{n}) $$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div>
                <div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ = \bar{x}_{n}\frac{\partial D(\bar{x}_{n})}{\partial\alpha_{i}}+D(\bar{x}_{n})\frac{\partial\bar{x}_{n}}{\partial\alpha_{i}} $$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
                <div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \frac{\partial\bar{x}_{n}}{\partial\alpha_{i}}\bigg|_{\alpha_{i}=0}  = \left[\begin{array}{l}\frac{\partial u_{n}}{\partial\alpha_{i}}\\ \frac{\partial v_{n}}{\partial\alpha_{i}}\end{array}\right]\bigg|_{\alpha_{i}=0}=\left[\begin{array}{l}\frac{\partial}{\partial\alpha_{i}}(\frac{x_{c}}{z_{c}})\\ \frac{\partial}{\partial\alpha_{i}}(\frac{yc}{z_{c}})\end{array}\right]\bigg|_{\alpha_{i}=0}\\ $$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
                <div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ =\left[\begin{array}{l}z_{c0}^{-1}\frac{\partial x_{c}}{\partial\alpha_{i}}-x_{c0}z_{c0}^{-2}\frac{\partial z_{c}}{\partial\alpha_{i}}\\ z_{c0}^{-1}\frac{\partial y_{c}}{\partial\alpha_{i}}-y_{c0}z_{c0}^{-2}\frac{\partial z_{c}}{\partial\alpha_{i}}\end{array}\right]. $$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>We consider only <span class="mathjax-tex">\(\frac{\partial\bar{x}_{n}}{\partial\alpha_{i}}\)</span> here because the term <span class="mathjax-tex">\(\frac{\partial D(\bar{x}_{n})}{\partial\alpha_{i}}\)</span> also contains <span class="mathjax-tex">\(\frac{\partial u_{n}}{\partial\alpha_{i}}\)</span> and <span class="mathjax-tex">\(\frac{\partial v_{n}}{\partial\alpha_{i}}\)</span>. To determine all parameters in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ14">14</a>), we take a partial derivative of <i>P</i>
                <sub>
                  <i>c</i>
                </sub> = <b>M</b>
                <sub>
                  <i>k</i>
                </sub>
                <i>P</i>
                <sub>
                  <i>o</i>
                </sub> with respect to α<sub>
                  <i>i</i>
                </sub> at α<sub>
                  <i>i</i>
                </sub> = 0 for <i> i</i> = 1, …, 6. As a result, we get</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P_{c}\bigg|_{\alpha_{i}=0} = \left[\begin{array}{l}x_{c0}\\ y_{c0}\\ z_{c0}\\ 1\end{array}\right]={\bf M}_{k-1}P_{o} $$</span></div></div>
                <div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \frac{\partial P_{c}}{\partial\alpha_{i}}\bigg|_{\alpha_{i}=0} = \left[\begin{array}{l}\frac{\partial x_{c}}{\partial\alpha_{i}}\\ \frac{\partial y_{c}}{\partial\alpha_{i}}\\ \frac{\partial z_{c}}{\partial\alpha_{i}}\\ 0\end{array}\right]\bigg|_{\alpha_{i}=0}=G_{i}{\bf M}_{k-1}P_{o}. $$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>Note that, if the lens distortion is ignored, <i>J</i>
                <sub>
                  <i>i</i>
                </sub> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ9">9</a>) will be similar to the equation used by Drummond and Cipolla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Drummond T, Cipolla R (2002) Real-time visual tracking of complex structures. IEEE Trans on PAMI 24(7):932–946" href="/article/10.1007/s10055-011-0206-x#ref-CR9" id="ref-link-section-d33377e1410">2002</a>), with a change in the order between <i>G</i>
                <sub>
                  <i>i</i>
                </sub> and <i>M</i>
                <sub>
                  <i>k</i>-1</sub> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ15">15</a>). Finding correspondences between a projected edge and an edge in the image is not straightforward because most edge points in the same edge line look similar. The objective function in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ10">10</a>) thus needs to be amended. We choose to search for a corresponding edge by searching only in a normal direction to the edge as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig3">3</a>. Let <i>d</i> be a normal distance between the projected point <span class="mathjax-tex">\([\begin{array}{ll}u &amp; v\end{array}]^{T}\)</span> and its corresponding edge point. If there is no measurement noise, then <i>d</i> must be close to ∑<span class="c-stack">
                  <sup>6</sup><sub>
                    <i>i</i>=1</sub>
                  
                </span> α<sub>
                  <i>i</i>
                </sub>
                <i>f</i>
                <sub>
                  <i>i</i>
                </sub> for all sample points where <i>f</i>
                <sub>
                  <i>i</i>
                </sub> is a normal component of <i>J</i>
                <sub>
                  <i>i</i>
                </sub> or <span class="mathjax-tex">\(f_{i}=\hat{n}.J_{i}\)</span>, provided <span class="mathjax-tex">\(\hat{n}\)</span> is a normal vector at the point [<i>u</i>, <i>v</i>]<sup><i>T</i></sup>. If there are ξ sampled points along the 3D edge model, the objective function can be defined by</p><div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ S=\sum_{\xi}(d^{\xi}-\bar{\alpha}^{T}f^{\xi})^{2} $$</span></div></div><p>where <span class="mathjax-tex">\( f^{\xi } = \left[f_{1}^{\xi }, \, f_{2}^{\xi }, \, \ldots, \,f_{6}^{\xi } \right]^{T}\)</span>. A standard least-squares algorithm can be used to minimize the objective function as follows</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ v = \sum_{\xi}d^{\xi}f^{\xi}, $$</span></div><div class="c-article-equation__number">
                    (16)
                </div></div>
                <div id="Equ17" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ C = \sum_{\xi}f^{\xi}f^{\xi T}, $$</span></div><div class="c-article-equation__number">
                    (17)
                </div></div>
                <div id="Equ18" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \bar{\alpha} = C^{-1}v. $$</span></div><div class="c-article-equation__number">
                    (18)
                </div></div><p>After getting <span class="mathjax-tex">\(\bar{\alpha},\,{\bf M}_{k}\)</span> can be estimated from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>
                        <i>d</i> and its normal component <i>J</i>
                        <sub>
                          <i>i</i>
                        </sub>
                      </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>To deal with outliers, the robust M-estimator for the least-squares estimator is used instead of the standard least-squares by replacing (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ16">16</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0206-x#Equ17">17</a>) with
</p><div id="Eque" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} v &amp;= \sum_{\xi}w(d^{\xi})d^{\xi}f^{\xi},\\ C &amp;= \sum_{\xi}w(d^{\xi})f^{\xi}f^{\xi T}, \end{aligned} $$</span></div></div><p>where <i>w</i>(<i>d</i>
                <sup>ξ</sup>) is a weighting function. In this algorithm, Tukey’s weighting and objective function Tukey (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1977" title="Tukey JW (1977) Explortary Data Analysis. Addison-Wesley, Boston" href="/article/10.1007/s10055-011-0206-x#ref-CR26" id="ref-link-section-d33377e1673">1977</a>) are used. They are defined by
</p><div id="Equf" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} w(d^{\xi}) &amp;= \left\{\begin{array}{ll}\left(1-\left(\frac{d^{\xi}}{c}\right)^{2}\right)^{2}, &amp; \hbox{if} |d^{\xi}|\leq c\\ 0, &amp; \hbox{elsewhere}\end{array}\right.\\ \rho(d^{\xi}) &amp; = \left\{\begin{array}{ll}1-\left(1-\left(\frac{d^{\xi}}{c}\right)^{2}\right)^{3}, &amp; \hbox{if} |d^{\xi}|\leq c\\ 0, &amp; \hbox{elsewhere}\end{array}\right. \end{aligned} $$</span></div></div><p>where the parameter <i>c</i> is chosen to be approximately 4.6851σ to give 95% efficiency in the case of Gaussian noise. Standard deviation of the inlying data σ can be approximated by using median absolute deviation (MAD). This estimator is known as iterative re-weighted least square(IRLS) since <i>w</i> is changed with each iteration due to <i>d</i>
                <sup>ξ</sup>.</p><p>To find corresponding edges in the image, firstly the image is smoothed by applying Gaussian filter. After projecting sample points along the 3D edge or boundary of the target model into the image, a one-directional search for each sample point along its normal direction is performed to find corresponding edges. The search range plays an important role here. On the one hand, if it is too short, the tracker will not be able to handle large camera movement. On the other hand, if it is too long, the tracker will need intensive computation to search for the corresponding edge.</p><p>A point will be accepted to be a candidate edge if its gradient is a local extrema and gradient orientation is similar to the edge normal. The gradient is obtained by applying Sobel filter along only a search path. As suggested by Vacchetti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Vacchetti L, Lepetit V, Fua P (2004) Combining edge and texture information for real-time accurate 3D camera tracking. In: International Symposium on Mixed and Augmented Reality" href="/article/10.1007/s10055-011-0206-x#ref-CR27" id="ref-link-section-d33377e1705">2004</a>), if there is more than one candidate edge, the candidate edge, given the lowest objective score, will be chosen. Furthermore, its weight <i>w</i>(<i>d</i>
                <sup>ξ</sup>) will also be divided by a value proportional to the number of candidate edges to make sure that the M-estimator is not too influenced by this measurement.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Devices and interaction methods</h2><div class="c-article-section__content" id="Sec5-content"><p>We built two devices (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig1">1</a>), the first being a wand-like device that contains the interface of a 3-button wheeled mouse arranged in a handheld case and a second device being a touch-screen tablet with stylus. Both of these have the same wide-angle lens camera and display. The camera has 80° of horizontal field of view that provides a pixel image stream at 30fps. We prefer a monocular system as an example of a non-radiating and currently ubiquitous feature on several kinds of mobiles. A single camera system also highlights the benefit of having an interactive approach with the person providing decisions that could pose significant challenges to fully automatic methods.</p><h3 class="c-article__sub-heading" id="Sec6">Wand-like device (WnD)</h3><p>This device is aimed for single-handed operation, with the thumb operating the wheel and buttons. The scroll wheel is placed at the center and one button is activated by pressing the wheel. The other two buttons are located left and right to the wheel. The camera is placed at its tip and the mouse interface inside.</p><h3 class="c-article__sub-heading" id="Sec7">Touch-screen device (TS)</h3><p>The touch screen is connected by cable to the VGA card of the computer and it is operated by a stylus. Its nature makes it a two-handed device with one hand used for holding it. Clicking is emulated by tapping the screen, while scrolling emulated by dragging the stylus. Dragging upward gives an upward scrolling and downwards gives a downward scrolling.</p><h3 class="c-article__sub-heading" id="Sec8">Basic interactive methods</h3><p>The most basic operation in generic model building is to specify a point in 3D space to serve as a models vertex or perhaps to serve as an anchor to where an already defined shape can be attached to. Given the camera pose as computed from the model-based tracking, the first step involves defining the first view of an objects vertex. This produces only a 3D ray in space that links the camera with the vertex in space. After defining this ray, there are two techniques that can be used for specifying the 3D vertex in space.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Two clicks (2Cs)</h4><p>The 2Cs method consists of taking the camera to another viewpoint from the one where the first ray was defined and one more click to define a second 3D ray in space as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig4">4</a>. These rays may not intersect perfectly, so the closest point between these rays is used instead.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Sketch of the two pointing techniques used to select 3D features. <i>Left Column</i>: (2Cs) where two rays in 3D intersect to define a vertex. <i>Right Column</i>: (CnM) where one ray in 3D is used to define the vertex along its length from a second viewpoint</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Click and move (CnM)</h4><p>The CnM method defines the vertex by computing the epipolar line based on the first 3D ray for any other viewpoint. The epipolar line is a ray in 3D that projects the first ray into the current view of the camera. Therefore, the positioning of the vertex is constrained to lie along the first ray and what is then needed is to specify where along this ray the vertex is. This is done by using a scroll motion until the vertex is positioned and finally selected by a click. This method is also sketched in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig4">4</a>.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">User study</h2><div class="c-article-section__content" id="Sec11-content"><p>The main experimental goal was to examine the differences in accuracy and speed of the different techniques and devices. The display used in both devices is a 7′′ VGA LCD touch screen providing touch input via USB interface. Both devices are connected via cable to an Intel Core2 Duo CPU 2.2 GHz with 2GB of RAM laptop. The image size processed in the tracker is reduced to 320 × 240 to make the laptop be able to save every image frames for analyzing later. The experimental software recorded trial completion time and accuracy in 3D. The completion time was the duration between the first and second clicks for defining 3D points. The accuracy was calculated as the mean-squared error between the user-estimated target location and the ground truth that was computed a priori.</p><h3 class="c-article__sub-heading" id="Sec12">Task and stimuli</h3><p>A 3D object as sketched in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig5">5</a> is used for the experiments. To ensure stable and accurate tracking, six known rectangular templates with high contrast features are placed around an office cabinet. The known coordinates of the templates were given to the tracker. There were 4 corner points labeled with numbers. These 1–4 corners offer a range of positions around the object and were at 70, 119, 119, and 132 cm from the ground, respectively, and in different planes. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig6">6</a> shows a user during doing the study of both devices. A video of the experimental setup is available on this article’s entry at <a href="http://www.cs.bris.ac.uk/Publications">http://www.cs.bris.ac.uk/Publications</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The object used during experiments. <i>Numbers</i> indicate 3D point ID. The <i>line</i> on the <i>ground</i> indicates the boundary the participants were asked not to cross</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>A user during the experimental study. <b>a</b> Using wand-like device. <b>b</b> Using touch-screen device</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec13">Procedure and design</h3><p>We carried out the experiment with 10 participants (3 women and 7 men) between the ages of 20 and 40. All participants were tested individually. Each experiment lasted approximately 30 minutes. We designed a within-subject experiment to evaluate the techniques and devices and used a 2 × 2 × 4 within-participants factorial design. The factors were as follows: Devices WnD and TS; Techniques 2Cs and CnM; Target locations, 4 locations as indicated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig5">5</a>. With a total of 8 trials per condition, we collected a total of 10 × 2 × 2 × 4 × 8 = 1,280 trials.</p><p>Participants were briefed on the way to operate the devices and techniques and given several practice trials to get familiar with the task, device, and techniques. The order of presentation of techniques and devices was counterbalanced between participants for learning effects using a partial latin square. They were instructed to do the task as fast as they could and told not to move beyond a border 70cm away from the front view of the cabinet, although this was not compulsory. At the end of the experiment, users were asked to rank-order the four conditions in terms of overall effort, speed, and performance.</p><h3 class="c-article__sub-heading" id="Sec14">Results and discussion</h3><p>The average trial completion time was 7.2s (SD 7.26s). There were 48 trials (3% of data) that were outside 3 times s.d. of the mean. These trials were considered outliers and excluded from further analysis. We did a univariate ANOVA on the aggregated data with time as the dependent variable, device and technique as fixed factors. The ANOVA did not show any significant effect of device (<i>F</i>(1, 36) = 0.408, <i>p</i> = 0.5), but showed a significant effect of technique (<i>F</i>(1, 36) = 13.8, <i>p</i> &lt; 0.01) on trial completion times. As can be seen from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig7">7</a>, the 2Cs technique was significantly faster than CnM. The average times for both TS and WnD were similar (7.27s and 7.28s, respectively). Univariate ANOVA revealed a significant effect of device (<i>F</i>(1, 36) = 17.14, <i>p</i> &lt; 0.01), but no effect of technique (<i>F</i>(1, 36) = 1.34, <i>p</i> = 0.25) on error rate. As can be seen on Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig7">7</a>a, the WnD resulted in significantly less mean errors (19.8 mm) versus the TS (32.5 mm). The average error for 2Cs was 23.9 against 28 mm for CnM. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig8">8</a> shows the users ranking of the different techniques and device combination. The TS combined with the CnM was the least preferred technique in terms of overall effort (7 of 10 ranked it 4th), speed (5 of 10), and performance (5 of 10).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Per device and per technique. <b>a</b> Error. <b>b</b> Average trial completion time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Average user ranking of the different techniques and devices in terms of effort, speed, and performance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Both the WnD-based techniques were well liked by the users. Overall users had a marginal preference for WnD with CnM over WnD with 2Cs. When we conceived the wand-based technique, one of the problems we faced was that there was no direct way to alter the point of interest on the device, this is that, the point of interest was always the center of the image, and the user had to move this center (identified by a cross-hair) to the right 3D vertex. This meant that even if the vertex of interest was within camera view, the user had to physically move the device to re-position the center of the display on the vertex. To overcome this problem, we built the TS.</p><p>In the TS, users can easily pick their point of interest by moving the cross-hair to a new position on the visual workspace. However, rather paradoxically, this meant that the device did not know what the point of interest of the user was anymore. This meant that the device could not provide cues about the reliability of the selected point until after the user had tapped on the screen. This resulted in an increased number of errors with the TS device. Moreover, for the second click in the 2Cs, when using the TS, it is not possible to give constant visual feedback based on the point of interest, and the user may have to move and click the device screen several times until the second ray is accepted. These conditions made users feel more frustrated.</p><p>For the WnD, the device always knows the user’s point of interest (it is along the ray passing the center of the image), and therefore it is possible to provide continuous and dynamic information about that point. This is akin to providing tooltip on a mouse when one hovers over an object of interest. We were able to leverage this, to let the user know before they select a vertex whether the highlighted point can potentially yield good results. Almost all users found this extremely useful, and it helped reduce the overall error rate of the wand-like device. It is important to consider errors relative to the device object distance and camera resolution. For example, although the average errors in this experiment were between 19.8 and 32.5 mm, this can also be measured as between 5.8 and 9.5 pixels if the camera is at 70 cm. Naturally, if the modeling tool is closer and is based on visual tracking, this has to be balanced with how reliable the tracking can be based on how much field of view the system has. Alternatively, it is expectable that these error figures can be improved if the resolution of the camera is increased. Overall, the 2Cs perform faster than click and move, and has less error in both devices. Our results suggest that the wand-like device along with the 2Cs is the best system to use for 3D vertex definition. The wand-like device resulted in significantly fewer errors, while the two-click was significantly faster than the click and move. It is, however, important to note that the 2Cs demand stable tracking, which can be achievable only by having sufficient features in any view where the device is taken to. This may not be the case, and in these situations, the CnM is an option since it allows for the definition of 3D points from narrower angles; thanks to the constraint imposed from the first ray. Even though the devices were purpose built for our experiment, we believe that the techniques and hardware can be easily included in emerging camera-enabled devices.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Further refinements</h2><div class="c-article-section__content" id="Sec15-content"><p>While the evaluation in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0206-x#Sec11">6</a> suggests to use the wand-like device, we are aware that the vast majority of current mobile devices use a touch screen, and therefore expanding our work to also work with these is important. Based on this, and if we select the 2Cs technique, some further refinements are possible. To start, we have, in addition to the original features, added the ability for users to pause an image and select many vertices from one view. With the pause ability, the system is also able to initialize the tracker without any known template with the compromise that the scale of the model will be arbitrary. In the beginning, the user needs to define a 3D plane and builds some parts of the object that belong to the plane. The system will then use this plane to initialize the tracker.</p><p>To define a 3D plane, four corners of a polygon are needed to be selected on a single view. The live camera is then paused and each corner is selected and dragged to its corresponding image point. A homography and then later a rotation and translation between two views will be estimated by using the method explained by Faugeras and Lustman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Faugeras O, Lustman F (1988) Motion and structure from motion in a piecewise planar environment. Technical report, INRIA" href="/article/10.1007/s10055-011-0206-x#ref-CR10" id="ref-link-section-d33377e1996">1988</a>). Finally, 3D positions of all four corners are determined by triangulation from which the polygon can be used as a template for building object features. This process is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Defining <i>rectangular plane</i> without a template. <b>a</b> Align corners in 1st view. <b>b</b> Align corners in 2nd view. <b>c</b> Define 3D plane. <b>d</b> Use polygon as template</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>To define a 3D circle on a plane, five points need to be placed at a circular boundary in the image. The system will use the 2D positions of these points to estimate a flat 3D circle and its plane. Note that this method will need only one view to model the circular plane as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig10">10</a>. Nonetheless, the circular plane remains ambiguous. The user needs to choose the correct plane by defining the plane’s normal direction.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Defining a circular plane without a template. <b>a</b> 5 points on a circle. <b>b</b> Estimate circular plane. <b>c</b> Extrude the circle. <b>d</b> Track a cylinder</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Now, the relative position between the camera and center of circle can be estimated, but not the orientation around the center’s axis.</p><p>To allow for extra tracking stability and by knowing the object surface after some initial model is built, it is possible to capture a “snapshot” of edges on the model’s surface from a given viewpoint. Inner edges will then be tracked as any other part of the model. This ability permits tracking of otherwise ambiguous objects if only the outline is considered, as is the case of mainly cylindrical shapes. An example of the captured inner edges is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0206-x#Fig11">11</a>a.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0206-x/MediaObjects/10055_2011_206_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>
                        <b>a</b>, <b>b </b>Inner edges allow for more stable tracking and better model representation. <b>c</b>, <b>d</b>) Extra tracking examples after object modeling. <b>a</b> Inner edges (<i>blue dots</i>). <b>b</b> Mug with Handle. <b>c</b> Screw driver. <b>d</b> Hammer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0206-x/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions and future work</h2><div class="c-article-section__content" id="Sec16-content"><p>In this article, we presented interaction techniques and devices aimed at supporting users for in-situ 3D model building. We investigated interaction issues surrounding 3D model building using mobile devices through an experimental evaluation. We are also working on integrating an object detector so that when tracking is lost, the system automatically reinitializes; however, we have found that in most cases, the models themselves can be roughly aligned by the user and this resumes tracking too. For further future work, if an object to be modeled is rich in point features, we can improve the stability of the tracker by considering to add point features into the tracker’s cost function. Extending this work to capture views for using an off-line optimization method such as bundle adjustment is feasible, but will have to be weighted by the availability of computational resources and level of interactivity.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: Proc" /><p class="c-article-references__text" id="ref-CR1">Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: Proceedings of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’10), Seoul, Korea</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Bowman, E. Kruijff, JL. Violla, I. Poupyrev, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bowman D, Kruijff E, Violla JL, Poupyrev I (2005) 3D user interfaces theory and practice. Addison Wesley, Bost" /><p class="c-article-references__text" id="ref-CR2">Bowman D, Kruijff E, Violla JL, Poupyrev I (2005) 3D user interfaces theory and practice. Addison Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%20theory%20and%20practice&amp;publication_year=2005&amp;author=Bowman%2CD&amp;author=Kruijff%2CE&amp;author=Violla%2CJL&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brown M, Drummond T, Cipolla R (2000) 3d model acquisition by tracking 2d wireframes. In: British Machine Visi" /><p class="c-article-references__text" id="ref-CR3">Brown M, Drummond T, Cipolla R (2000) 3d model acquisition by tracking 2d wireframes. In: British Machine Vision Conference BMVC, pp 11–14</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bunnun P, Mayol-Cuevas W (2008) Outlinar: an assisted interactive model building system with reduced computati" /><p class="c-article-references__text" id="ref-CR4">Bunnun P, Mayol-Cuevas W (2008) Outlinar: an assisted interactive model building system with reduced computational effort. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’08)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chekhlov D, Gee AP, Calway A, Mayol-Cuevas W (2007) Ninja on a plane: automatic discovery of physical planes f" /><p class="c-article-references__text" id="ref-CR5">Chekhlov D, Gee AP, Calway A, Mayol-Cuevas W (2007) Ninja on a plane: automatic discovery of physical planes for Augmented Reality using visual slam. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Comport AI, Marchand E, Pressigout M, Chaumette F (2006) Real-time markerless tracking for Augmented Reality: " /><p class="c-article-references__text" id="ref-CR6">Comport AI, Marchand E, Pressigout M, Chaumette F (2006) Real-time markerless tracking for Augmented Reality: the virtual visual servoing framework. IEEE Trans Vis Comput Graph 12(4)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Davison AJ, Mayol W, Murray D (2003) Real-time localisation and mapping with wearable active vision. In: Inter" /><p class="c-article-references__text" id="ref-CR7">Davison AJ, Mayol W, Murray D (2003) Real-time localisation and mapping with wearable active vision. In: International Symposium on Mixed and Augmented Reality</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec PE, Taylor CJ, Malik J (1996) Modeling and rendering architecture from photographs. In: SIGGRAPH96" /><p class="c-article-references__text" id="ref-CR8">Debevec PE, Taylor CJ, Malik J (1996) Modeling and rendering architecture from photographs. In: SIGGRAPH96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Drummond, R. Cipolla, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Drummond T, Cipolla R (2002) Real-time visual tracking of complex structures. IEEE Trans on PAMI 24(7):932–946" /><p class="c-article-references__text" id="ref-CR9">Drummond T, Cipolla R (2002) Real-time visual tracking of complex structures. IEEE Trans on PAMI 24(7):932–946</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2002.1017620" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20visual%20tracking%20of%20complex%20structures&amp;journal=IEEE%20Trans%20on%20PAMI&amp;volume=24&amp;issue=7&amp;pages=932-946&amp;publication_year=2002&amp;author=Drummond%2CT&amp;author=Cipolla%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Faugeras O, Lustman F (1988) Motion and structure from motion in a piecewise planar environment. Technical rep" /><p class="c-article-references__text" id="ref-CR10">Faugeras O, Lustman F (1988) Motion and structure from motion in a piecewise planar environment. Technical report, INRIA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Freeman R, Steed A (2006) Interactive modelling and tracking for mixed and Augmented Reality. In: Proceedings " /><p class="c-article-references__text" id="ref-CR11">Freeman R, Steed A (2006) Interactive modelling and tracking for mixed and Augmented Reality. In: Proceedings of ACM Virtual Reality Software and Technology, Cyprus, pp 61–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gee AP, Mayol-Cuevas W (2006) Real-time model-based slam using line segments. In: International Symposium on V" /><p class="c-article-references__text" id="ref-CR12">Gee AP, Mayol-Cuevas W (2006) Real-time model-based slam using line segments. In: International Symposium on Visual Computing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grossman T, Balakrishnan R (2006) The design and evaluation of selection techniques for 3D volumetric displays" /><p class="c-article-references__text" id="ref-CR13">Grossman T, Balakrishnan R (2006) The design and evaluation of selection techniques for 3D volumetric displays. In: Proceedings of the 19th Annual ACM Symposium on User interface Software and Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Hengel A, Dick A, Thormahlen T, Ward B, Torr PHS (2007) Videotrace: rapid interactive scene modelling " /><p class="c-article-references__text" id="ref-CR14">van den Hengel A, Dick A, Thormahlen T, Ward B, Torr PHS (2007) Videotrace: rapid interactive scene modelling from video. In: Proceedings of SIGGRAPH</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the  IEEE and" /><p class="c-article-references__text" id="ref-CR15">van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the  IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’09)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim K, Lepetit V, Woo W (2010) Keyframe-based modeling and tracking of multiple 3D objects. In: Proceeding of " /><p class="c-article-references__text" id="ref-CR16">Kim K, Lepetit V, Woo W (2010) Keyframe-based modeling and tracking of multiple 3D objects. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’10), Seoul, Korea</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim S, DiVerdi S, Chang JS, Kang T, Iltis R, Höllerer T (2007) Implicit 3D modeling and tracking for anywhere " /><p class="c-article-references__text" id="ref-CR17">Kim S, DiVerdi S, Chang JS, Kang T, Iltis R, Höllerer T (2007) Implicit 3D modeling and tracking for anywhere augmentation. In: VRST ’07: proceedings of the 2007 ACM symposium on virtual reality software and technology, ACM, pp 19–28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of the IEEE an" /><p class="c-article-references__text" id="ref-CR18">Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lepetit V, Vacchetti L, Thalmann D, Fua P (2003) Fully automated and stable registration for Augmented Reality" /><p class="c-article-references__text" id="ref-CR19">Lepetit V, Vacchetti L, Thalmann D, Fua P (2003) Fully automated and stable registration for Augmented Reality applications. In: Proceedings of the Second IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’03), Tokyo, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image" /><p class="c-article-references__text" id="ref-CR20">Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Nara, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: Computer Vision and Pa" /><p class="c-article-references__text" id="ref-CR21">Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, pp 1498–1505</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan Q, Reitmayr G, Drummond T (2009) ProFORMA: probabilistic feature-based on-line rapid model acquisition. In" /><p class="c-article-references__text" id="ref-CR22">Pan Q, Reitmayr G, Drummond T (2009) ProFORMA: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British Machine Vision Conference (BMVC), London</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reitmayr G, Eade E, Drummond T (2007) Semi-automatic annotations in unknown environments. In: Proceeding of th" /><p class="c-article-references__text" id="ref-CR23">Reitmayr G, Eade E, Drummond T (2007) Semi-automatic annotations in unknown environments. In: Proceeding of the Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’07), Nara, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Rother, V. Kolmogorov, A. Blake, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. A" /><p class="c-article-references__text" id="ref-CR24">Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23(3):309–314</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1015706.1015720" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grabcut%3A%20interactive%20foreground%20extraction%20using%20iterated%20graph%20cuts&amp;journal=ACM%20Trans%20Graph&amp;volume=23&amp;issue=3&amp;pages=309-314&amp;publication_year=2004&amp;author=Rother%2CC&amp;author=Kolmogorov%2CV&amp;author=Blake%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simon G (2006) Automatic online walls detection for immediate use in AR tasks. In: International Symposium on " /><p class="c-article-references__text" id="ref-CR25">Simon G (2006) Automatic online walls detection for immediate use in AR tasks. In: International Symposium on Mixed and Augmented Reality</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JW. Tukey, " /><meta itemprop="datePublished" content="1977" /><meta itemprop="headline" content="Tukey JW (1977) Explortary Data Analysis. Addison-Wesley, Boston" /><p class="c-article-references__text" id="ref-CR26">Tukey JW (1977) Explortary Data Analysis. Addison-Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Explortary%20Data%20Analysis&amp;publication_year=1977&amp;author=Tukey%2CJW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vacchetti L, Lepetit V, Fua P (2004) Combining edge and texture information for real-time accurate 3D camera t" /><p class="c-article-references__text" id="ref-CR27">Vacchetti L, Lepetit V, Fua P (2004) Combining edge and texture information for real-time accurate 3D camera tracking. In: International Symposium on Mixed and Augmented Reality</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="V. Varadarajan, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Varadarajan V (1974) Lie groups, Lie algebras, and their representations. Prentice-Hall Inc., Inglewood Cliffs" /><p class="c-article-references__text" id="ref-CR28">Varadarajan V (1974) Lie groups, Lie algebras, and their representations. Prentice-Hall Inc., Inglewood Cliffs</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lie%20groups%2C%20Lie%20algebras%2C%20and%20their%20representations&amp;publication_year=1974&amp;author=Varadarajan%2CV">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0206-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, University of Bristol, Bristol, UK</p><p class="c-article-author-affiliation__authors-list">Pished Bunnun, Sriram Subramanian &amp; Walterio W. Mayol-Cuevas</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Pished-Bunnun"><span class="c-article-authors-search__title u-h3 js-search-name">Pished Bunnun</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Pished+Bunnun&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Pished+Bunnun" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Pished+Bunnun%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Sriram-Subramanian"><span class="c-article-authors-search__title u-h3 js-search-name">Sriram Subramanian</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sriram+Subramanian&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sriram+Subramanian" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sriram+Subramanian%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Walterio_W_-Mayol_Cuevas"><span class="c-article-authors-search__title u-h3 js-search-name">Walterio W. Mayol-Cuevas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Walterio W.+Mayol-Cuevas&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Walterio W.+Mayol-Cuevas" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Walterio W.+Mayol-Cuevas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0206-x/email/correspondent/c1/new">Pished Bunnun</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=In-Situ%20interactive%20image-based%20model%20building%20for%20Augmented%20Reality%20from%20a%20handheld%20device&amp;author=Pished%20Bunnun%20et%20al&amp;contentID=10.1007%2Fs10055-011-0206-x&amp;publication=1359-4338&amp;publicationDate=2012-01-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Bunnun, P., Subramanian, S. &amp; Mayol-Cuevas, W.W. In-Situ interactive image-based model building for Augmented Reality from a handheld device.
                    <i>Virtual Reality</i> <b>17, </b>137–146 (2013). https://doi.org/10.1007/s10055-011-0206-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0206-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-03-14">14 March 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-19">19 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-01-05">05 January 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06">June 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0206-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0206-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Model building</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented Reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Handheld device</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0206-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=206;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

