<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Hand posture and gesture recognition techniques for virtual reality ap"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/21/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Hand posture and gesture recognition techniques for virtual reality applications: a survey"/>

    <meta name="dc.source" content="Virtual Reality 2016 21:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-11-17"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2016-11-17"/>

    <meta name="prism.volume" content="21"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="91"/>

    <meta name="prism.endingPage" content="107"/>

    <meta name="prism.copyright" content="2016 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-016-0301-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-016-0301-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-016-0301-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-016-0301-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Hand posture and gesture recognition techniques for virtual reality applications: a survey"/>

    <meta name="citation_volume" content="21"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2017/06"/>

    <meta name="citation_online_date" content="2016/11/17"/>

    <meta name="citation_firstpage" content="91"/>

    <meta name="citation_lastpage" content="107"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-016-0301-0"/>

    <meta name="DOI" content="10.1007/s10055-016-0301-0"/>

    <meta name="citation_doi" content="10.1007/s10055-016-0301-0"/>

    <meta name="description" content="Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand"/>

    <meta name="dc.creator" content="K. Martin Sagayam"/>

    <meta name="dc.creator" content="D. Jude Hemanth"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Aggarwal JK, Cai Q (1997) Human motion analysis: a review. In: Proceedings of the IEEE non-rigid and articulated motion workshop, pp 99&#8211;102"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Comput Surv; citation_title=Human activity analysis: a review; citation_author=J Aggarwal, MS Ryoo; citation_volume=43; citation_issue=3; citation_publication_date=2011; citation_pages=16; citation_doi=10.1145/1922649.1922653; citation_id=CR2"/>

    <meta name="citation_reference" content="Aggarwal JK, Cai Q, Liao W, Sabata B (1994) Articulated and elastic non-rigid motion: a review. In: Proceedings of the IEEE workshop on motion of non-rigid and articulated objects, pp 2&#8211;14"/>

    <meta name="citation_reference" content="Bansal M, Saxena S, Desale D, Jadhav D (2011) Dynamic gesture recognition using hidden Markov models in static background. Int J Comput Sci 8(6), no. 1, 391&#8211;398"/>

    <meta name="citation_reference" content="citation_journal_title=Eng Appl Artif Intell; citation_title=Recognition of facial expressions using Gabor wavelets and learning vector quantization; citation_author=S Bashyal, GK Venayagamoorthy; citation_volume=21; citation_publication_date=2008; citation_pages=10; citation_doi=10.1016/j.engappai.2007.11.010; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Geosci; citation_title=FCM: the fuzzy C-means clustering algorithm; citation_author=JC Bezdek, R Ehrlich, W Full; citation_volume=10; citation_issue=2&#8211;3; citation_publication_date=1984; citation_pages=191-203; citation_doi=10.1016/0098-3004(84)90020-7; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=SIGGRAPH Comput Graph; citation_title=Put that where? Voice and gesture at the graphics interface; citation_author=M Billinghurst; citation_volume=32; citation_issue=4; citation_publication_date=1998; citation_pages=60-63; citation_doi=10.1145/307710.307730; citation_id=CR7"/>

    <meta name="citation_reference" content="Bolt RA (1980) Put that-there: voice and gestures at the graphics interface. In: SIGGRAPH 80: 7th annual conference on computer graphics and interactive techniques. ACM Press, New York, pp 262&#8211;270"/>

    <meta name="citation_reference" content="citation_title=Principles for the design of performance-oriented interaction techniques; citation_inbook_title=Handbook of virtual environments: design, implementation, and applications; citation_publication_date=2002; citation_pages=201-207; citation_id=CR9; citation_author=D Bowman; citation_publisher=Lawerence Erlabum Associates"/>

    <meta name="citation_reference" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingAR-tips: gesture based direct manipulation in augmented reality. In: GRAPHITE&#8217;04, 2nd international conference on graphics and interactive techniques in Australis and South East Asia. ACM Press, New York, pp 212&#8211;221"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Intell Transp Syst; citation_title=Understanding transit scenes: a survey on human behavior recognition algorithms; citation_author=J Candamo, M Shreve, DB Goldgof, DB Sapper, R Kasturi; citation_volume=11; citation_issue=1; citation_publication_date=2010; citation_pages=206-224; citation_doi=10.1109/TITS.2009.2030963; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Motion based recognition: a survey; citation_author=C Cedras, M Shah; citation_volume=13; citation_issue=2; citation_publication_date=1995; citation_pages=129-155; citation_doi=10.1016/0262-8856(95)93154-K; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A survey of video datasets for human action and activity recognition; citation_author=JM Chaquet, EJ Carmona, A Fernandez-Caballero; citation_volume=117; citation_issue=6; citation_publication_date=2013; citation_pages=633-659; citation_doi=10.1016/j.cviu.2013.01.013; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Hand gesture recognition using a real-time tracking method and hidden Markov models; citation_author=FS Chen, CM Fu, CL Huang; citation_volume=21; citation_publication_date=2003; citation_pages=745-758; citation_doi=10.1016/S0262-8856(03)00070-2; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Instrum Meas; citation_title=Hand gesture recognition using Haar-like features and a stochastic context-free grammar; citation_author=Q Chen, ND Georganas, EM Petriu; citation_volume=57; citation_issue=8; citation_publication_date=2008; citation_pages=1562-1571; citation_doi=10.1109/TIM.2008.922070; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit Lett; citation_title=A survey of human motion analysis using depth imagery; citation_author=L Chen, H Wei, J Ferryman; citation_volume=34; citation_issue=15; citation_publication_date=2013; citation_pages=1995-2006; citation_doi=10.1016/j.patrec.2013.02.006; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=J Softw Eng Appl; citation_title=Hand Gesture recognition using appearance features based on 3D point cloud; citation_author=Y Chong, J Huang, S Pan; citation_volume=9; citation_publication_date=2016; citation_pages=103-111; citation_doi=10.4236/jsea.2016.94009; citation_id=CR17"/>

    <meta name="citation_reference" content="Chung KYC (2010) Facial expression recognition by using class mean gabor responses with kernel principal component analysis M.Sc Thesis, Russ College of Engineering and Technology, Ohio University, USA, pp 1&#8211;69"/>

    <meta name="citation_reference" content="Chung WK, Wu X, Xu Y (2009) A real-time hand gesture recognition based on haar wavelet representation. In: Proceedings of the IEEE international conference on robotics and biometrics (ROBIO&#8217;08), Bangkok, Thailand, pp 336&#8211;341"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Pattern Recognit Artif Intell; citation_title=Thirty years of graph matching in pattern recognition; citation_author=D Conte, P Foggia, C Sansone, M Vento; citation_volume=18; citation_issue=3; citation_publication_date=2004; citation_pages=265-298; citation_doi=10.1142/S0218001404003228; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Neuro Comput; citation_title=Human behavior analysis in video surveillance: a social signal processing perspective; citation_author=M Cristani, R Raghavendra, A Bue, V Murino; citation_volume=100; citation_publication_date=2013; citation_pages=86-97; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Instrum Meas; citation_title=Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques; citation_author=NH Dardas, ND Georganas; citation_volume=60; citation_issue=11; citation_publication_date=2011; citation_pages=3592-3607; citation_doi=10.1109/TIM.2011.2161140; citation_id=CR22"/>

    <meta name="citation_reference" content="Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recognition. Pattern Recognit Lett 101&#8211;111"/>

    <meta name="citation_reference" content="Elmezai M, Al-Hamadi A, Krell G, El-Etriby S, Michaelis B (2007) Gesture recognition for alphabets from hand motion trajectory using hidden markov models. In: Proceeding of IEEE international symposium on signal processing and information technologies"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw; citation_title=Glove-talk: a neural network interface between a data-glove and a speech synthesizer; citation_author=SS Fels, GE Hinton; citation_volume=4; citation_issue=1; citation_publication_date=1993; citation_pages=2-8; citation_doi=10.1109/72.182690; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw; citation_title=Glove-talk: a neural network interface which maps gestures to parallel formant speech synthesizer controls; citation_author=SS Fels, GE Hinton; citation_volume=9; citation_issue=1; citation_publication_date=1998; citation_pages=205-212; citation_doi=10.1109/72.655042; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_title=Deliang Zhu; citation_publication_date=2011; citation_id=CR27; citation_author=Zhiquan Feng; citation_author=Bo Yang; citation_author=Yuehui Chen; citation_author=Yanwei Zheng; citation_author=Xu Tao; citation_author=Yi Li; citation_author=Xu Ting; citation_publisher=Features extraction from hand images based on new detection operators"/>

    <meta name="citation_reference" content="citation_title=Motion tracking requirements and technologies; citation_inbook_title=Handbook of virtual environments: design, implementation, and applications; citation_publication_date=2002; citation_pages=163-210; citation_id=CR28; citation_author=E Foxlin; citation_publisher=Lawrence Erlbaum Associates"/>

    <meta name="citation_reference" content="Freeman WT, Roth M (1995) Orientation histograms for hand gesture recognition. In: IEEE international workshop on automatic face and gesture recognition, Zurich"/>

    <meta name="citation_reference" content="Gabbard J (1997) A taxonomy of usability characteristics in virtual environments. Master&#8217;s thesis, Department of Computer Science, University of Western Australia"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=The visual analysis of human movement: a survey; citation_author=DM Gavrila; citation_volume=73; citation_issue=1; citation_publication_date=1999; citation_pages=82-98; citation_doi=10.1006/cviu.1998.0716; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Hand gesture recognition and tracking based on distributed locally linear embedding; citation_author=SS Ge, Y Yang, TH Lee; citation_volume=26; citation_publication_date=2008; citation_pages=1607-1620; citation_doi=10.1016/j.imavis.2008.03.004; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=A survey on still image based human action recognition; citation_author=G Guo, A Lai; citation_volume=47; citation_publication_date=2014; citation_pages=3343-3361; citation_doi=10.1016/j.patcog.2014.04.018; citation_id=CR33"/>

    <meta name="citation_reference" content="Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd international conference on communication, computing and security, pp 98&#8211;107"/>

    <meta name="citation_reference" content="Heap T, Hogg D (1996) Towards 3D hand tracking using a deformable model. In: Proceeding IEEE 2nd international conference on automatic face and gesture recognition"/>

    <meta name="citation_reference" content="Holte MB, Tran C, Trivedi MM, Moeslund TB (2011) Human action recognition using multiple view: a comparative perspective on recent developments. In: Proceedings of the joint ACM workshop on human gesture and behavior understanding, pp 47&#8211;52"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern C Appl Rev; citation_title=A survey on visual surveillance of object motion and behaviors; citation_author=W Hu, T Tan, L Wangs, S Maybank; citation_volume=34; citation_issue=3; citation_publication_date=2004; citation_pages=334-352; citation_doi=10.1109/TSMCC.2004.829274; citation_id=CR37"/>

    <meta name="citation_reference" content="Huang Z et al (2010) Study of sign language recognition based on Gabor wavelet transforms. In: International conference on computer design and applications"/>

    <meta name="citation_reference" content="citation_journal_title=Expert Syst Appl; citation_title=Gabor filter-based hand pose angle estimation for hand gesture recognition under varying illumination; citation_author=DY Huang, WC Hu, SH Chang; citation_volume=38; citation_issue=5; citation_publication_date=2011; citation_pages=6031-6042; citation_doi=10.1016/j.eswa.2010.11.016; citation_id=CR39"/>

    <meta name="citation_reference" content="Ibarguren A, Maurtua I, Sierra B (2010) Layered architecture for real time sign recognition: hand gesture and movement. J Eng Appl Artif Intell 1216&#8211;1228"/>

    <meta name="citation_reference" content="Jain AK, Duta N (1999) Deformable matching of hand shapes for verification. In: Proceedings of international conference on image processing"/>

    <meta name="citation_reference" content="Jain AK, Ross A, Pankanti S (1999) A prototype hand geometry based verification system. In: Proceedings of 2nd international conference on audio and video based biometric person authentication, pp 166&#8211;171"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Sci Inf Secur; citation_title=Automatic local Gabor features extraction for face recognition; citation_author=YB Jemaa, S Khanfir; citation_volume=3; citation_publication_date=2009; citation_pages=1-7; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern C Appl Rev; citation_title=Advances in view-invariant human motion analysis: a review; citation_author=X Ji, H Liu; citation_volume=40; citation_issue=1; citation_publication_date=2010; citation_pages=13-24; citation_id=CR44"/>

    <meta name="citation_reference" content="Joshi A, Monnier C, Betke M, Sclaroff S (2016) Comparing random forest approaches to segmenting and classifying gestures. J Image Vis Comput 1&#8211;10. doi:
                    10.1016/j.imavis.2016.06.001
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A comparative study of two-state-of-the art sequence processing techniques for hand gesture recognition; citation_author=A Just, S Marcel; citation_volume=113; citation_issue=4; citation_publication_date=2009; citation_pages=532-543; citation_doi=10.1016/j.cviu.2008.12.001; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=Expert Syst Appl; citation_title=Persian sign language (PSL) recognition using wavelet transform and neural networks; citation_author=A Karami, B Zanj, AK Sarkaleh; citation_volume=38; citation_publication_date=2011; citation_pages=2661-2667; citation_doi=10.1016/j.eswa.2010.08.056; citation_id=CR47"/>

    <meta name="citation_reference" content="Keskin C, Erkan A, Akarun L (2003) Real time hand tracking and 3D gesture recognition for interactive interface using HMM. In: Proceedings of international conference on artificial neural networks"/>

    <meta name="citation_reference" content="citation_journal_title=J Math Probl Eng; citation_title=Hand gesture recognition using modified 1$ and background subtraction algorithms; citation_author=H Khaled, SG Sayed, ESM Saad, H Ali; citation_volume=2015; citation_publication_date=2015; citation_pages=1-8; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Commun Image Recognit; citation_title=A hand gesture recognition technique for human&#8211;computer interaction; citation_author=NC Kiliboz, U Gudukbay; citation_volume=28; citation_publication_date=2015; citation_pages=97-104; citation_doi=10.1016/j.jvcir.2015.01.015; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Simultaneous gesture segmentation and recognition based on forward spotting accumulative HMMs; citation_author=D Kim, J Song, D Kim; citation_volume=40; citation_issue=11; citation_publication_date=2007; citation_pages=3012-3026; citation_doi=10.1016/j.patcog.2007.02.010; citation_id=CR51"/>

    <meta name="citation_reference" content="Kohler M, Schroter S (1998) A survey of video-based gesture recognition: stereo and mono systems. Technical report no. 693/1998, Informatik VII, University of Dortmund"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Hum Comput Interact; citation_title=Integrating paper and digital information on enhanced desk: a method for real time finger tracking on an augmented desk system; citation_author=H Koike, Y Sato, Y Kobayashi; citation_volume=8; citation_issue=4; citation_publication_date=2001; citation_pages=307-322; citation_doi=10.1145/504704.504706; citation_id=CR53"/>

    <meta name="citation_reference" content="Kolsch M, Turk M (2004) Robust hand detection. In: 6th IEEE international conference on automatic face and gesture recognition, vol 614"/>

    <meta name="citation_reference" content="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: CHI&#8217;94: conference companion on human factors in computing systems. ACM Press, New York, pp 453&#8211;454"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Commun Surv Tutor; citation_title=A survey on human activity recognition using wearable sensors; citation_author=OD Lara, MA Labrador; citation_volume=15; citation_issue=3; citation_publication_date=2013; citation_pages=1192-1209; citation_doi=10.1109/SURV.2012.110112.00192; citation_id=CR56"/>

    <meta name="citation_reference" content="LaViola JJ Jr (1999) A survey of hand posture and gesture recognition and technology. Master thesis, NSF Science and Technology Center for Computer Graphics and Scientific Visualization, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Opt Laser Technol; citation_title=Hand shape recognition; citation_author=YL Lay; citation_volume=32; citation_issue=1; citation_publication_date=2000; citation_pages=1-5; citation_doi=10.1016/S0030-3992(99)00105-X; citation_id=CR58"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=An HMM based threshold model approach for gesture recognition; citation_author=KH Lee, JH Kim; citation_volume=21; citation_issue=10; citation_publication_date=1999; citation_pages=961-973; citation_doi=10.1109/34.799904; citation_id=CR59"/>

    <meta name="citation_reference" content="citation_journal_title=J Robot Auton Syst; citation_title=Motion recognition and recovery from occluded monocular observations; citation_author=D Lee, Y Nakamura; citation_volume=62; citation_publication_date=2014; citation_pages=818-832; citation_doi=10.1016/j.robot.2014.02.002; citation_id=CR60"/>

    <meta name="citation_reference" content="Lenman S, Bretzner L, Thuresson B (2002) Using marking menus to develop command sets for computer vision based hand gesture interfaces. In: NordiCHI&#8217;02: second nordic conference on human computer interaction. ACM Press, New York, pp 239&#8211;242"/>

    <meta name="citation_reference" content="Letessier J, Berard F (2004) Visual tracking of bare fingers for interactive surfaces. In: UIST&#8217;04: 17th annual ACM symposium on user interface software and technology. ACM Press, New York, pp 119&#8211;122"/>

    <meta name="citation_reference" content="Li X (2003) Gesture recognition based on fuzzy C-means clustering algorithm. Department of Computer Science, The University of Tennessee, Knoxville"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=HEGM: A hierarchical elastic graph matching for hand gesture recognition; citation_author=YT Li, JP Wachs; citation_volume=47; citation_issue=1; citation_publication_date=2014; citation_pages=80-88; citation_doi=10.1016/j.patcog.2013.05.028; citation_id=CR64"/>

    <meta name="citation_reference" content="Li C, Zhani P, Zheng S, Prabhakaran B (2004) Segmentation and recognition of multi-attribute motion sequences. In: Proceedings of the ACM multimedia conference, pp 836&#8211;843"/>

    <meta name="citation_reference" content="citation_journal_title=J Neuro Comput; citation_title=Feature learning based on SAE&#8211;PCA network for human gesture recognition in RGBD images; citation_author=S-Z Li, B Yu, W Wu, S-Z Su, R-R Ji; citation_volume=151; citation_publication_date=2015; citation_pages=565-573; citation_id=CR66"/>

    <meta name="citation_reference" content="citation_title=Dynamic training of hand gesture recognition system; citation_inbook_title=Proceedings of international conference on pattern recognition; citation_publication_date=2004; citation_pages=971-974; citation_id=CR67; citation_author=A Licsar; citation_author=T Sziranyi; citation_publisher=ICPR"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=User-adaptive hand gesture recognition system with interactive training; citation_author=A Licsar, T Sziranyi; citation_volume=23; citation_publication_date=2005; citation_pages=1102-1114; citation_doi=10.1016/j.imavis.2005.07.016; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_journal_title=J Pattern Recognit; citation_title=Fuzzy human motion analysis: a review; citation_author=CH Lim, E Vats, CS Chan; citation_volume=48; citation_publication_date=2015; citation_pages=1773-1796; citation_doi=10.1016/j.patcog.2014.11.016; citation_id=CR69"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Sign language recognition by combining statistical IDTW and independent classification; citation_author=JF Litchtenauer, EA Hendriks, MJT Reinders; citation_volume=30; citation_issue=11; citation_publication_date=2008; citation_pages=2040-2046; citation_doi=10.1109/TPAMI.2008.123; citation_id=CR70"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Gabor-Based Kernel PCA with fractional power polynomial models for face recognition; citation_author=C Liu; citation_volume=26; citation_publication_date=2004; citation_pages=10; citation_id=CR71"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=A survey of surgical simulation: applications, technology and education; citation_author=A Liu, F Tendick, K Clearly, C Kaufmann; citation_volume=12; citation_issue=6; citation_publication_date=2003; citation_pages=599-614; citation_doi=10.1162/105474603322955905; citation_id=CR72"/>

    <meta name="citation_reference" content="Malik S, Laszlo J (2004) Visual touchpad: a two-handed gestural input device. In: ICMI&#8217;04: 6th international conference on multimodal interfaces. ACM Press, New York, pp 289&#8211;296"/>

    <meta name="citation_reference" content="Maraqa M, Abu-Zaiter R (2008) Recognition of Arabic Sign Language (ArSL) using recurrent neural networks. In: IEEE 1st international conference on the applications of digital information and web technologies, pp 478&#8211;484. doi:
                    10.1109/ICADIWT.2008.4664396
                    
                  
                        "/>

    <meta name="citation_reference" content="Marcel S, Bernier O (1999) Hand posture recognition in bady faced centered space. In: Proceeding of the international gesture workshop, Gif-sur-Yvette, France"/>

    <meta name="citation_reference" content="Martin J, Devin V, Crowley JL (1998) Active hand tracking. In: FG&#8217;98: 3rd international conference on face &amp; gesture recognition. IEEE Computer Society, Washington, p 573"/>

    <meta name="citation_reference" content="citation_journal_title=World Acad Sci Eng Technol; citation_title=Real-time hand tracking and gesture recognition system using neural networks; citation_author=TH Maung; citation_volume=50; citation_publication_date=2009; citation_pages=466-470; citation_id=CR77"/>

    <meta name="citation_reference" content="Meena S (2011) A study on hand gesture recognition technique. Master thesis, Department of Electronics and Communication Engineering, National Institute of Technology, India"/>

    <meta name="citation_reference" content="citation_title=Data mining: multimedia, soft computing, and bioinformatics; citation_publication_date=2003; citation_id=CR79; citation_author=S Mitra; citation_author=T Acharya; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern C Appl Rev; citation_title=Gesture recognition: a survey; citation_author=S Mitra, T Acharya; citation_volume=37; citation_issue=3; citation_publication_date=2007; citation_pages=311-324; citation_doi=10.1109/TSMCC.2007.893280; citation_id=CR80"/>

    <meta name="citation_reference" content="Mo Z, Lewis JP, Neumann U (2005) Smartcanvas: a gesture-driven intelligent drawing desk system. In: IUI&#8217;05: 10th international conference on intelligent user interfaces. ACM Press, New York, pp 239&#8211;243"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A survey of computer vision-based human motion capture; citation_author=TB Moeslund, A Hilton, V Kruger; citation_volume=81; citation_issue=3; citation_publication_date=2001; citation_pages=231-268; citation_doi=10.1006/cviu.2000.0897; citation_id=CR82"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A survey of advances in vision-based human motion capture and analysis; citation_author=TB Moeslund, A Hilton, V Kruger; citation_volume=104; citation_issue=2; citation_publication_date=2006; citation_pages=90-126; citation_doi=10.1016/j.cviu.2006.08.002; citation_id=CR83"/>

    <meta name="citation_reference" content="Murakami K, Taguchi H (1999) Gesture recognition using recurrent neural networks. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp 237&#8211;242. doi:
                    10.1145/108844.108900
                    
                  
                        "/>

    <meta name="citation_reference" content="Mutha SS, Kinage K (2015) Hand Gesture recognition using LAB thresholding technique. In: 4th post graduate conference (iPGCON-2015), pp 1&#8211;5"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Real-time gesture recognition system and application; citation_author=CW Ng, S Ranganath; citation_volume=20; citation_publication_date=2002; citation_pages=993-1007; citation_doi=10.1016/S0262-8856(02)00113-0; citation_id=CR86"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Robot Syst; citation_title=Two-stage hidden Markov model in gesture recognition for human robot interaction; citation_author=N Nguyen-Duc-Thanh, S Lee, D Kim; citation_volume=9; citation_publication_date=2012; citation_pages=1-10; citation_doi=10.5772/50204; citation_id=CR87"/>

    <meta name="citation_reference" content="Nielsen M, Storring M, Moeslund TB, Granum E (2003) A procedure for developing intuitive and ergonomic gesture interfaces for HCI. In: 5th international gesture workshop, pp 409&#8211;420"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit Lett; citation_title=Combining implicit polynomials and geometric features for hand recognition; citation_author=C Oden, A Ercil, B Buke; citation_volume=24; citation_publication_date=2003; citation_pages=2145-2152; citation_doi=10.1016/S0167-8655(03)00087-4; citation_id=CR89"/>

    <meta name="citation_reference" content="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: FGR&#8217;02: 5th IEEE international conference on automatic face and gesture recognition. IEEE Computer Society, Washington, p 429"/>

    <meta name="citation_reference" content="Ong EJ, Bowden R (2004) A boosted classifier tree for hand shape detection. In: 6th IEEE international conference on automatic face and gesture recognition, pp 889&#8211;894"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Hand gesture modeling and recognition involving changing shapes and trajectories using a predictive eigen tracker; citation_author=KS Patwardhan, SD Roy; citation_volume=28; citation_publication_date=2007; citation_pages=329-334; citation_doi=10.1016/j.patrec.2006.04.002; citation_id=CR92"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Visual interpretation of hand gestures for human computer interaction; citation_author=VI Pavlovic, R Sharma, TS Huang; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=677-695; citation_doi=10.1109/34.598226; citation_id=CR93"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Looking at people: sensing for ubiquitous and wearable computing; citation_author=A Pentland; citation_volume=22; citation_issue=1; citation_publication_date=2000; citation_pages=107-119; citation_doi=10.1109/34.824823; citation_id=CR94"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Vis Image Underst; citation_title=Recent methods and databases in vision-based hand gesture recognition: a review; citation_author=PK Pisharady, M Saerbeck; citation_volume=141; citation_publication_date=2015; citation_pages=152-165; citation_doi=10.1016/j.cviu.2015.08.004; citation_id=CR95"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Humanoid Robot; citation_title=Hand posture and face recognition using a fuzzy-rough approach; citation_author=PK Pisharady, P Vadakkepat, AP Loh; citation_volume=7; citation_issue=3; citation_publication_date=2010; citation_pages=331-356; citation_doi=10.1142/S0219843610002180; citation_id=CR96"/>

    <meta name="citation_reference" content="Pisharady PK, Vadakkepat P, Loh AP (2010) Graph matching based hand pose recognition using neuro-biologically inspired features. In: Proceedings of international conference on control, automation, robotics and vision, ICARCV, Singapore"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Attention based detection and recognition of hand posture against complex backgrounds; citation_author=PK Pisharady, P Vadakkepat, AP Loh; citation_volume=101; citation_issue=3; citation_publication_date=2013; citation_pages=403-419; citation_doi=10.1007/s11263-012-0560-5; citation_id=CR98"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Vision-based human motion analysis: an overview; citation_author=R Poppe; citation_volume=108; citation_issue=1; citation_publication_date=2007; citation_pages=4-18; citation_doi=10.1016/j.cviu.2006.10.016; citation_id=CR99"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A survey on vision-based human action recogntion; citation_author=R Poppe; citation_volume=28; citation_issue=6; citation_publication_date=2010; citation_pages=976-990; citation_doi=10.1016/j.imavis.2009.11.014; citation_id=CR100"/>

    <meta name="citation_reference" content="citation_journal_title=J Signal Process Syst; citation_title=Real-time hand gesture recognition from depth images using convex shape decomposition method; citation_author=S Qin, X Zhu, Y Yang, Y Jiang; citation_volume=74; citation_publication_date=2014; citation_pages=47-58; citation_doi=10.1007/s11265-013-0778-7; citation_id=CR101"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Multimodal human discourse: gesture and speech; citation_author=F Quck, D MeNeill, R Bryll, S Duncan, X-F Ma, C Kirbas, KE McCullough, R Ansari; citation_volume=9; citation_issue=3; citation_publication_date=2002; citation_pages=171-193; citation_doi=10.1145/568513.568514; citation_id=CR102"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimed; citation_title=Unencumbered gestural interaction; citation_author=FKH Quek; citation_volume=3; citation_issue=4; citation_publication_date=1996; citation_pages=36-47; citation_doi=10.1109/93.556459; citation_id=CR103"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=A tutorial on hidden Markov models and selected applications in speech reognition; citation_author=LR Rabiner; citation_volume=77; citation_issue=2; citation_publication_date=1989; citation_pages=257-285; citation_doi=10.1109/5.18626; citation_id=CR104"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Recognition of dynamic hand gestures; citation_author=A Ramamoorthy, N Vaswani, S Chaudhury, S Banerjee; citation_volume=36; citation_publication_date=2003; citation_pages=2069-2081; citation_doi=10.1016/S0031-3203(03)00042-6; citation_id=CR105"/>

    <meta name="citation_reference" content="Ren Y, Gu C (2010) Real-time hand gesture recognition based on vision. In: Proceedings of the 5th international conference on E-learning and games, Edutainment, Changchun, China"/>

    <meta name="citation_reference" content="Ren Z, Yuan J, Zhang Z (2011) Robust hand gesture recognition based on finger-earth mover&#8217;s distance with a commodity depth camera. In: ACM international conference on multimedia, Scottsdlae, pp 1093&#8211;1096"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Non linear dimensionality reduction by locally linear embedding; citation_author=ST Roweis, LK Saul; citation_volume=290; citation_issue=5500; citation_publication_date=2000; citation_pages=2323-2326; citation_doi=10.1126/science.290.5500.2323; citation_id=CR108"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Biometric identification through hand geometry measurements; citation_author=R Sanches-Reillo, C Sanchez-Avila, A Gonzalez-Macros; citation_volume=22; citation_issue=10; citation_publication_date=2000; citation_pages=1168-1171; citation_doi=10.1109/34.879796; citation_id=CR109"/>

    <meta name="citation_reference" content="Segen J, Kumar S (1998) Gesture VR: vision-based 3D hand interface for spatial interaction. In: 6th ACM international conference on multimedia. ACM Press, New York, pp 455&#8211;464"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Gesture recognition using Bezier curves for visualization navigation from registered 3D data; citation_author=MC Shin, LV Tsap, DB Goldgof; citation_volume=37; citation_issue=5; citation_publication_date=2004; citation_pages=1011-1024; citation_doi=10.1016/j.patcog.2003.11.007; citation_id=CR111"/>

    <meta name="citation_reference" content="Starner T, Pentland A (1995) Visual recognition of American sign language using hidden Markov models. In: Proceeding of international workshop on automatic face and gesture recognition, Zurich, Switzerland"/>

    <meta name="citation_reference" content="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. AAAI technical report FS-96-05, The Media Laboratory Massachusetts Institute of Technology"/>

    <meta name="citation_reference" content="Stenger B, Thayananthan A, Torr P, Cipolla R (2004) Hand pose estimation using hierarchical detection. In: 8th European conference on computer vision workshop on human computer interaction, vol 3058, Springer, Prague, pp 102&#8211;112"/>

    <meta name="citation_reference" content="citation_journal_title=J Eng Appl Artif Intell; citation_title=Hand gesture recognition using a neural shape fitting technique; citation_author=E Stergiopoulou, N Papmarkos; citation_volume=22; citation_publication_date=2009; citation_pages=1141-1158; citation_doi=10.1016/j.engappai.2009.03.008; citation_id=CR115"/>

    <meta name="citation_reference" content="Sturman DJ (1992) Whole hand input. Ph.D. thesis, MIT"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=A survey of glove-based input; citation_author=DJ Sturman, D Zeltzer; citation_volume=14; citation_issue=1; citation_publication_date=1994; citation_pages=30-39; citation_doi=10.1109/38.250916; citation_id=CR117"/>

    <meta name="citation_reference" content="citation_journal_title=J Appl Soft Comput; citation_title=Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic; citation_author=C-J Su, C-Y Chiang, J-Y Huang; citation_volume=22; citation_publication_date=2014; citation_pages=652-666; citation_doi=10.1016/j.asoc.2014.04.020; citation_id=CR118"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Hand gesture recognition based on dynamic Bayesian network framework; citation_author=HI Suk, BK Sin, SW Lee; citation_volume=43; citation_issue=9; citation_publication_date=2010; citation_pages=3059-3072; citation_doi=10.1016/j.patcog.2010.03.016; citation_id=CR119"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Lang Comput; citation_title=A hand gesture recognition system based on locally linear embedding; citation_author=X Teng, B Wu, W Yu, C Liu; citation_volume=16; citation_publication_date=2005; citation_pages=442-454; citation_doi=10.1016/j.jvlc.2005.04.003; citation_id=CR120"/>

    <meta name="citation_reference" content="citation_journal_title=J Inf Sci; citation_title=Hand shape identification on multirange images; citation_author=CM Travieso, JR Ticay-Rivas, JC Briceno, M Pozo-Banos; citation_volume=275; citation_publication_date=2014; citation_pages=45-56; citation_doi=10.1016/j.ins.2014.02.031; citation_id=CR121"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A system for person-independent hand posture recognition against complex backgrounds; citation_author=J Triesch, C Malsburg; citation_volume=23; citation_issue=12; citation_publication_date=2001; citation_pages=1449-1453; citation_doi=10.1109/34.977568; citation_id=CR122"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Circuits Syst Video Technol; citation_title=Machine recognition of human activities: a survey; citation_author=P Turaga, R Chellappa, VS Subrahmanian, O Udrea; citation_volume=18; citation_issue=11; citation_publication_date=2008; citation_pages=1473-1488; citation_doi=10.1109/TCSVT.2008.2005594; citation_id=CR123"/>

    <meta name="citation_reference" content="citation_title=Gesture recognition; citation_inbook_title=Handbook of virtual environments: design, implementation, and applications; citation_publication_date=2002; citation_pages=223-238; citation_id=CR124; citation_author=M Turk; citation_publisher=Lawerence Erlbaum Associates"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Ind Electron; citation_title=A hand-pose estimation for vision-based human interfaces; citation_author=E Ueda, Y Matsumoto, M Imai, T Ogasawara; citation_volume=50; citation_issue=4; citation_publication_date=2003; citation_pages=676-684; citation_doi=10.1109/TIE.2003.814758; citation_id=CR125"/>

    <meta name="citation_reference" content="Virtual Glove Box (VGX) (2016). 
                    http://biovis.arc.nasa.gov/vislab/vgx.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human&#8211;computer interaction framework using skin color tracking and gesture recognition. In: Proceedings of IEEE international conference on computing and Communication Technologies, Research, Innovation, and Vision for the Future, pp 978&#8211;981. doi:
                    10.1109/RIVF.2010.5633368
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Recent development of human motion analysis; citation_author=L Wang, W Hu, T Tan; citation_volume=36; citation_issue=3; citation_publication_date=2003; citation_pages=585-601; citation_doi=10.1016/S0031-3203(02)00100-0; citation_id=CR128"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=2D Gabor face representation method for face recognition with ensemble and multichannel model; citation_author=L Wang; citation_volume=26; citation_publication_date=2008; citation_pages=9; citation_id=CR129"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=An approach to natural gesture in virtual environments; citation_author=A Wexelblat; citation_volume=2; citation_issue=3; citation_publication_date=1995; citation_pages=179-200; citation_doi=10.1145/210079.210080; citation_id=CR130"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=A survey of vision-based methods for action representation, segmentation and recognition; citation_author=D Wienland, R Ronfard, E Boyer; citation_volume=115; citation_issue=2; citation_publication_date=2011; citation_pages=224-241; citation_doi=10.1016/j.cviu.2010.10.002; citation_id=CR131"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Face recognition by elastic bunch graph matching; citation_author=L Wiskott, JM Fellous, N Kruger, C Malsburg; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=775-779; citation_doi=10.1109/34.598235; citation_id=CR132"/>

    <meta name="citation_reference" content="Wysoski SG (2003) A rotation invariant static hand gesture recognition system using boundary information and neural networks. ME thesis, Nagoya Institute of Technology, Japan"/>

    <meta name="citation_reference" content="Wysoski SG, Lamar MV, Kuroyanagi S, Iwata A (2002) A rotation invariant approach on static-gesture recognition using boundary histograms and neural networks. In: IEEE proceedings of the 9th international conference on neural information processing, Singapura"/>

    <meta name="citation_reference" content="Xu W et al (2009) A scale and rotation invariant interest points detector based on Gabor filters. In: Slezak D, Pal S, Kang BH, Gu J, Kuroda H, Kim TH (eds) Signal processing image processing and pattern recognition. Communications in computer and information science, vol 61. Springer, Berlin, p 8"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Extraction of 2D motion trajectories and its application to hand gesture recognition; citation_author=MH Yang, N Ahuja, M Tabb; citation_volume=24; citation_issue=8; citation_publication_date=2002; citation_pages=1061-1074; citation_doi=10.1109/TPAMI.2002.1023803; citation_id=CR136"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Visual understanding of dynamic hand gestures; citation_author=M Yeasin, S Chaudhuri; citation_volume=33; citation_issue=11; citation_publication_date=2000; citation_pages=1805-1817; citation_doi=10.1016/S0031-3203(99)00175-2; citation_id=CR137"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Eng Sci Technol IJEST; citation_title=Artificial neural network approach for hand gesture recognition; citation_author=SK Yewale; citation_volume=34; citation_publication_date=2011; citation_pages=2603-2608; citation_id=CR138"/>

    <meta name="citation_reference" content="Yikai F, Kongqiao W, Jian C, Hanquing L (2007) A real-time hand gesture recognition method. In: Proceeding of the IEEE international conference on mutlimedia and expo (ICME&#8217;07), Beijing, China, pp 995&#8211;998"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Estimation of the fundamental matrix from un-calibrated stereo hand images for 3D hand gesture recognition; citation_author=X Yin, M Xie; citation_volume=36; citation_publication_date=2003; citation_pages=567-584; citation_doi=10.1016/S0031-3203(02)00072-9; citation_id=CR140"/>

    <meta name="citation_reference" content="citation_journal_title=J Pattern Recognit; citation_title=Hand gesture recognition using combined features of location, angle and velocity; citation_author=HS Yoon, J Soh, YJ Bae, HS Yang; citation_volume=34; citation_publication_date=2001; citation_pages=1491-1501; citation_doi=10.1016/S0031-3203(00)00096-0; citation_id=CR141"/>

    <meta name="citation_reference" content="Yun L, Peng Z (2009) An automatic hand gesture recognition system based on Viola&#8211;Jones method and SVM&#8217;s. In: Proceedings of the 2nd international workshop on computer science and engineering (WCSE&#8217;09), pp 72&#8211;76"/>

    <meta name="citation_reference" content="citation_journal_title=Eng Appl Artif Intell; citation_title=Image skin segmentation based on multi-agent learning Bayesian and neural network; citation_author=AA Zaiden, NN Ahmad, H Abdul Karim, M Larbani, BB Zaidan, A Sali; citation_volume=32; citation_publication_date=2014; citation_pages=136-150; citation_doi=10.1016/j.engappai.2014.03.002; citation_id=CR143"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=RIEVL: recursive induction learning in hand gesture recognition; citation_author=M Zhao, FKH Quek, X Wu; citation_volume=20; citation_issue=11; citation_publication_date=1998; citation_pages=1174-1185; citation_doi=10.1109/34.730553; citation_id=CR144"/>

    <meta name="citation_reference" content="Zhou H, Lin DJ, Haung TS (2004) Static hand gesture recognition based on local orientation histogram feature distribution model. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition workshops"/>

    <meta name="citation_reference" content="Zhu C, Sheng W (2009) Online hand gesture recognition using neural network based segmentation. In: International conference on intelligent robots and systems. IEEE Publisher, pp 2415&#8211;2420"/>

    <meta name="citation_reference" content="Zunkel RL (1999) Hand geometry based verification. In: Proceedings of biometrics. Kluwer Academic Publishers, pp 87&#8211;101"/>

    <meta name="citation_author" content="K. Martin Sagayam"/>

    <meta name="citation_author_email" content="martinsagayam.k@gmail.com"/>

    <meta name="citation_author_institution" content="Department of ECE, Karunya University, Coimbatore, India"/>

    <meta name="citation_author" content="D. Jude Hemanth"/>

    <meta name="citation_author_email" content="jude_hemanth@rediffmail.com"/>

    <meta name="citation_author_institution" content="Department of ECE, Karunya University, Coimbatore, India"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-016-0301-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2017/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-016-0301-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Hand posture and gesture recognition techniques for virtual reality applications: a survey"/>
        <meta property="og:description" content="Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Hand posture and gesture recognition techniques for virtual reality applications: a survey | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-016-0301-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Human computer interaction (HCI), Gesture, Posture, Graphical user interface (GUI), HMM","kwrd":["Human_computer_interaction_(HCI)","Gesture","Posture","Graphical_user_interface_(GUI)","HMM"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-016-0301-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-016-0301-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=301;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-016-0301-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Hand posture and gesture recognition techniques for virtual reality applications: a survey
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0301-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0301-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-11-17" itemprop="datePublished">17 November 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Hand posture and gesture recognition techniques for virtual reality applications: a survey</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-K__Martin-Sagayam" data-author-popup="auth-K__Martin-Sagayam">K. Martin Sagayam</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Karunya University" /><meta itemprop="address" content="0000 0000 9896 4772, grid.412056.4, Department of ECE, Karunya University, Coimbatore, 641114, India" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-D__Jude-Hemanth" data-author-popup="auth-D__Jude-Hemanth" data-corresp-id="c1">D. Jude Hemanth<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Karunya University" /><meta itemprop="address" content="0000 0000 9896 4772, grid.412056.4, Department of ECE, Karunya University, Coimbatore, 641114, India" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 21</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">91</span>–<span itemprop="pageEnd">107</span>(<span data-test="article-publication-year">2017</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2042 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">28 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-016-0301-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In the present situation, artificial intelligence is very much efficient in human computer interaction (HCI). Gesture recognition can be termed as a methodology for which the gestures made by the users are recognized by the receiver. Gestures are expressive with body movements including physical action of the fingers, hands, arms, head, face or body with the purpose of: (a) passing on significant data (b) interfacing with nature. They constitute small but interesting subspace of conceivable gesture. A motion might likewise be perceived by the environment as a pressure strategy for the data to be transmitted elsewhere and subsequently reconstructed by the receiver. Gesture recognition has wide-ranging applications such as:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>To develop the hearing capacity of deaf people.</p>
                  </li>
                  <li>
                    <p>To enable young children to interact with PCs.</p>
                  </li>
                  <li>
                    <p>To design systems for legal recognizable forensic identification.</p>
                  </li>
                  <li>
                    <p>To recognize communication via gestures.</p>
                  </li>
                  <li>
                    <p>To navigate or controls in virtual environment.</p>
                  </li>
                  <li>
                    <p>To communicate in video conferencing.</p>
                  </li>
                </ul>
                     <p>A few motions have both static and dynamic component in gesture-based communication (Mitra and Acharya <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:&#xA;                    10.1109/TSMCC&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR80" id="ref-link-section-d79366e349">2007</a>). A dynamic motion is planned to change over a time frame, whereas a static motion is seen at the spurt of time. A waving hand implies goodbye is an example of static motion. To comprehend a full message, it is important to translate all the static and dynamic motion over a timeframe. This unpredictable procedure is called Gesture recognition. Gesture recognition is the procedure of perceiving and translating a stream of successive signal from the given input data (Meena <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Meena S (2011) A study on hand gesture recognition technique. Master thesis, Department of Electronics and Communication Engineering, National Institute of Technology, India" href="/article/10.1007/s10055-016-0301-0#ref-CR78" id="ref-link-section-d79366e352">2011</a>).</p><p>Generally, the actions can be interfaced by data-glove-based system with one to many or many to one mapping. Sometimes, errors occurred due to overlapping of hand gestures, complex background and environment (Pavlovic et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human computer interaction. IEEE Trans Pattern Anal Mach Intell 19(7):677–695" href="/article/10.1007/s10055-016-0301-0#ref-CR93" id="ref-link-section-d79366e358">1997</a>), extending to the scientific models based on hidden Markov model (Rabiner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech reognition. Proc IEEE 77(2):257–285" href="/article/10.1007/s10055-016-0301-0#ref-CR104" id="ref-link-section-d79366e361">1989</a>), to apparatuses or approach based on soft computing (Mitra and Acharya <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Mitra S, Acharya T (2003) Data mining: multimedia, soft computing, and bioinformatics. Wiley, New York" href="/article/10.1007/s10055-016-0301-0#ref-CR79" id="ref-link-section-d79366e364">2003</a>). In addition to the hypothetical aspects, any practical implementation of gesture acknowledgment normally requires the utilization of distinctive imaging and tracking devices. These incorporate instrumented gloves, body suits and marker-based optical tracking. Customary 2-D console, pen- and mouse-arranged graphical user interfaces (GUIs) are often not suitable for working in virtual environment.</p><p>Gestures are extensively characterized into static and dynamic as in natural way of communication via gestures. The characteristic of automatic recognition is persistent over the transient action. Often one needs to indicate the start and end points of a gesture in terms of the frame of movement, both in time and space. Ordinarily, the significance of a motion can be reliant on the following:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>spatial data: where it happens</p>
                  </li>
                  <li>
                    <p>pathic data: the way it takes</p>
                  </li>
                  <li>
                    <p>symbolic data: the sign it makes</p>
                  </li>
                  <li>
                    <p>affective data: its passionate.</p>
                  </li>
                </ul>
                     <p>To determine hand position, angles and pivots, the movements of hand gestures with different illumination conditions should be detected. This is possible either by utilizing the gadgets accomplished to the users. The gadgets can be hand gloves, body suits or cameras. Numerous techniques are used to interact with human and computer through the gestures for conveying nonverbal communication more naturally to express an idea in real-time applications. This will be used to develop the algorithm for virtual reality framework. The following step for human and computer interface (HCI) utilizes image processing methods.</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Sufficient datasets are required.</p>
                  </li>
                  <li>
                    <p>Remove noise factor by using proper preprocessing techniques.</p>
                  </li>
                  <li>
                    <p>Identify suitable feature extraction technique.</p>
                  </li>
                  <li>
                    <p>Segments the desired portion for analysis.</p>
                  </li>
                  <li>
                    <p>Recognition can be done by suitable classifier.</p>
                  </li>
                  <li>
                    <p>Optimize the system by performance metrics.</p>
                  </li>
                  <li>
                    <p>Determine the performance measures in analytic method.</p>
                  </li>
                </ul>
                     <p>To enhance the hand gesture recognition for virtual reality game by machine learning algorithms.</p><h3 class="c-article__sub-heading" id="Sec2">Outline</h3><p>Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec3">2</a> focuses on the motivations and problem statements of hand gesture and posture for human computer interface (HCI). Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec5">3</a> deals with the survey of the data collection and preprocessing. The list of data collected has been tabulated with the website, and the test outcomes are furthermore discussed in this part. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec10">4</a> discusses in detail the literature about hand detection and feature extraction. The idea of 1D Savitzky–Golay channels can be stretched out into 2D polynomial fitting (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Li C, Zhani P, Zheng S, Prabhakaran B (2004) Segmentation and recognition of multi-attribute motion sequences. In: Proceedings of the ACM multimedia conference, pp 836–843" href="/article/10.1007/s10055-016-0301-0#ref-CR65" id="ref-link-section-d79366e466">2004</a>). Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec13">5</a> describes in detail about hand segmentation. The feature descriptors used for image segmentations are height, width, area, centroid, angle, velocity, etc. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec17">6</a> expands the literature about the classification of the hand gestures and postures. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0301-0#Sec21">7</a> concludes the survey by highlighting the paramount components of each technique used for hand gesture recognition applications. Lim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lim CH, Vats E, Chan CS (2015) Fuzzy human motion analysis: a review. J Pattern Recognit 48:1773–1796" href="/article/10.1007/s10055-016-0301-0#ref-CR69" id="ref-link-section-d79366e479">2015</a>) have described about the detailed survey on hand gesture recognition systems (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Detailed literature survey on hand gesture recognition systems</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Motivations and problem statement</h2><div class="c-article-section__content" id="Sec3-content"><p>The new ideas and innovation changes with a few measurements, including precision, determination, inactivity, scope of movement, user comfort and cost.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab2">2</a> listed the issues in gesture recognition for human–computer interaction. Glove-based gestural interface normally requires the user to wear a glove data and carry a load of cable link with the computer. While interacting with the system, it sometimes requires to fold the fingers for few actions to communicate with the system. This obstructs the shocking effect due to short circuit and normally the robustness of the systems gets reduced. Vision-based procedures conquer the issues identified with impediment parts of the user’s hand. While tracking devices can recognize quick and inconspicuous developments of the fingers when the user’s hand is moving, a dream-based framework will get a general feeling of the sort of finger movement. It can likewise differ among them in: (a) the quality of camera utilized, (b) their pace and dormancy, (c) the structure of environment, (d) any user’s surrounding, (e) the low-level features utilized and (f) whether 2D or 3D representation is utilized.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Problems list out for hand gesture recognition for HCI</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The following motivation of hand motion acknowledgment framework is invariant in pivot for hand motions,</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Increase the susceptibility.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Decrease computational complexity.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Increase classification rate.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Reduce error rate.</p>
                    
                  </li>
                </ol>
                     <h3 class="c-article__sub-heading" id="Sec4">Hand gesture recognition system</h3><p>There has been a great emphasis in human computer interface (HCI) research to create easy to utilize interfaces by specially employing natural communication by straight forward utilize of natural communication and manipulation skills of humans. Embracing direct sensing in HCI will permit the deployment of a wide range of applications in more sophisticated computing environments such as virtual environments (VEs) or augmented reality (AR) systems. The advancement of these frameworks involves addressing challenging research problems including viable input/output techniques, interaction styles and evaluation methods. In the input domain, the direct sensing approach requires capturing and interpreting the motion of head, eye, gaze, face, hands, arms or even the entire body. Among distinctive body parts, the hand is the most effective, general-purpose interaction tool due to its dexterous functionality in correspondence and control. Different interaction styles tend to import both modalities to permit instinctive and natural interaction. Sign languages made up of hand postures or motion patterns are employed to implement command and control interfaces (Quek <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Quek FKH (1996) Unencumbered gestural interaction. IEEE Multimed 3(4):36–47" href="/article/10.1007/s10055-016-0301-0#ref-CR103" id="ref-link-section-d79366e1381">1996</a>; Turk <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Turk M (2002) Gesture recognition. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawerence Erlbaum Associates, Hillsdale, pp 223–238" href="/article/10.1007/s10055-016-0301-0#ref-CR124" id="ref-link-section-d79366e1384">2002</a>; Lenman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lenman S, Bretzner L, Thuresson B (2002) Using marking menus to develop command sets for computer vision based hand gesture interfaces. In: NordiCHI’02: second nordic conference on human computer interaction. ACM Press, New York, pp 239–242" href="/article/10.1007/s10055-016-0301-0#ref-CR61" id="ref-link-section-d79366e1387">2002</a>; Nielsen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nielsen M, Storring M, Moeslund TB, Granum E (2003) A procedure for developing intuitive and ergonomic gesture interfaces for HCI. In: 5th international gesture workshop, pp 409–420" href="/article/10.1007/s10055-016-0301-0#ref-CR88" id="ref-link-section-d79366e1390">2003</a>). Gesticulations, which are unconstrained developments of the hand and arms that go with discourse, have appeared to be exceptionally viable instruments in multimodal user interfaces (MUI) (Wexelblat <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):179–200" href="/article/10.1007/s10055-016-0301-0#ref-CR130" id="ref-link-section-d79366e1393">1995</a>; Quck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Quck F, MeNeill D, Bryll R, Duncan S, Ma X-F, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discourse: gesture and speech. ACM Trans Comput Hum Interact 9(3):171–193" href="/article/10.1007/s10055-016-0301-0#ref-CR102" id="ref-link-section-d79366e1397">2002</a>; Bolt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Bolt RA (1980) Put that-there: voice and gestures at the graphics interface. In: SIGGRAPH 80: 7th annual conference on computer graphics and interactive techniques. ACM Press, New York, pp 262–270" href="/article/10.1007/s10055-016-0301-0#ref-CR8" id="ref-link-section-d79366e1400">1980</a>; Koons and Sparrell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: CHI’94: conference companion on human factors in computing systems. ACM Press, New York, pp 453–454" href="/article/10.1007/s10055-016-0301-0#ref-CR55" id="ref-link-section-d79366e1403">1994</a>; Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput Graph 32(4):60–63" href="/article/10.1007/s10055-016-0301-0#ref-CR7" id="ref-link-section-d79366e1406">1998</a>). Object manipulation interfaces (Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bowman D (2002) Principles for the design of performance-oriented interaction techniques. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawerence Erlabum Associates, Hillsdale, pp 201–207" href="/article/10.1007/s10055-016-0301-0#ref-CR9" id="ref-link-section-d79366e1409">2002</a>; Gabbard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Gabbard J (1997) A taxonomy of usability characteristics in virtual environments. Master’s thesis, Department of Computer Science, University of Western Australia" href="/article/10.1007/s10055-016-0301-0#ref-CR30" id="ref-link-section-d79366e1412">1997</a>; Buchmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingAR-tips: gesture based direct manipulation in augmented reality. In: GRAPHITE’04, 2nd international conference on graphics and interactive techniques in Australis and South East Asia. ACM Press, New York, pp 212–221" href="/article/10.1007/s10055-016-0301-0#ref-CR10" id="ref-link-section-d79366e1416">2004</a>) utilize the hand for navigation, selection and manipulation tasks in VEs. In many applications such as complex machinery or manipulator control, computer-based puppetry or musical performance (Sturman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Sturman DJ (1992) Whole hand input. Ph.D. thesis, MIT" href="/article/10.1007/s10055-016-0301-0#ref-CR116" id="ref-link-section-d79366e1419">1992</a>), the hand serves as an efficient, high degree of freedom (DOF) control device. Finally, some immersive VE applications, such as surgical simulations (Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Liu A, Tendick F, Clearly K, Kaufmann C (2003) A survey of surgical simulation: applications, technology and education. Presence Teleoper Virtual Environ 12(6):599–614" href="/article/10.1007/s10055-016-0301-0#ref-CR72" id="ref-link-section-d79366e1422">2003</a>) and training systems (VGX <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Virtual Glove Box (VGX) (2016). &#xA;                    http://biovis.arc.nasa.gov/vislab/vgx.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR126" id="ref-link-section-d79366e1425">2016</a>), have complicated object manipulation in their definitions. Broad deployment of hand gesture-based HCI requires the development of general-purpose hand motion capture and interpretation systems. Currently, the most effective tools for capturing hand motion are electro-mechanical or magnetic sensing device (Sturman and Zeltzer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39" href="/article/10.1007/s10055-016-0301-0#ref-CR117" id="ref-link-section-d79366e1428">1994</a>; Foxlin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Foxlin E (2002) Motion tracking requirements and technologies. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawrence Erlbaum Associates, Hillsdale, pp 163–210" href="/article/10.1007/s10055-016-0301-0#ref-CR28" id="ref-link-section-d79366e1431">2002</a>). These devices are worn on the hand to measure the location of the hand and the finger joint angle. They deliver the most complete, application autonomous set of real-time measurements that permit importing all the functionality of the hand image in HCI. However, they have several drawbacks in terms of casual use as they are very expensive, frustrate the instinctive nature of hand movement and require complex calibration and setup procedures to get exact estimations.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig1">1</a> demonstrates the framework of the hand recognition system. The different modules are image datasets, preprocessing, feature extraction, image segmentation and classification. Image classification and segmentation are performed with either supervised or unsupervised learning strategy. The various literature works available for these individual modules are given in the subsequent sections.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Framework of the hand recognition system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Image dataset and preprocessing</h2><div class="c-article-section__content" id="Sec5-content"><p>The commonly used datasets for hand gestures and postures are: (a) the dataset available in Nanyang Technological University, Singapore at the website <a href="http://eeeweba.ntu.edu.sg/computervision/people/home/renzhou">http://eeeweba.ntu.edu.sg/computervision/people/home/renzhou</a>. This dataset consists of 200 images with dimensions of 352 × 288. These images consist of five gestures with ten variations for each gesture. (b) Another dataset is available in National University of Singapore, Singapore which consists of 450 data (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Black and <i>white</i> and <i>colored</i> hand image dataset (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Pisharady and Saerbeck (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Pisharady PK, Saerbeck M (2015) Recent methods and databases in vision-based hand gesture recognition: a review. J Comput Vis Image Underst 141:152–165" href="/article/10.1007/s10055-016-0301-0#ref-CR95" id="ref-link-section-d79366e1505">2015</a>) depict these with points of interest, for example, number of classes, subjects and tests available. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab3">3</a> records hand poses and gesture databases with the web-links for to download.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Lists hand posture and gesture databases with their web-links</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec6">Image preprocessing</h3><p>Image preprocessing is one of the preliminary steps which are particularly required to guarantee the high precision of the subsequent step. A few researchers are accounted in this literature to minimize the impacts of artifacts in the hand image by using preprocessing technique. Dominio et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recognition. Pattern Recognit Lett 101–111" href="/article/10.1007/s10055-016-0301-0#ref-CR23" id="ref-link-section-d79366e2131">2014</a>) has demonstrated an experiment with a palm region of <i>B</i>(<i>u</i>, <i>v</i>) in three-dimensional plane. First, to find the starting point of the circle <i>C</i> that has to adopt with the algorithm, since the area of palm is larger than the wrist and the finger. Then filter <i>B</i>(<i>u</i>, <i>v</i>) with Gaussian kernel function with a high standard deviation. Zaiden et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi-agent learning Bayesian and neural network. Eng Appl Artif Intell 32:136–150" href="/article/10.1007/s10055-016-0301-0#ref-CR143" id="ref-link-section-d79366e2156">2014</a>) have introduced two novel techniques for preprocessing of the color image. One is used for the Bayesian method, and other one is back propagation neural network. This hybrid method is used to improve the sensitivity and quality of the image under different lighting conditions.</p><p>Feng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extraction from hand images based on new detection operators, Pattern Recognit, pp 1089–1105" href="/article/10.1007/s10055-016-0301-0#ref-CR27" id="ref-link-section-d79366e2162">2011</a>) have used Kalman filter to all the feature points tracked by the camera in order to improve speed and robustness. Su et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic. J Appl Soft Comput 22:652–666" href="/article/10.1007/s10055-016-0301-0#ref-CR118" id="ref-link-section-d79366e2165">2014</a>) have described about the preprocessing by initializing the kinect-based gesture recognition system to improve the quality and consistency value. This can be achieved by transforming kinect coordinates to a local coordinates at “Hip-center” of user’s skeleton observed by kinect sensor. Kiliboz and Gudukbay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Kiliboz NC, Gudukbay U (2015) A hand gesture recognition technique for human–computer interaction. J Vis Commun Image Recognit 28:97–104" href="/article/10.1007/s10055-016-0301-0#ref-CR50" id="ref-link-section-d79366e2168">2015</a>) have proposed threshold-based filtering is used to reduce the noise due to error in hardware and not deliberate. Despite the presence of applied threshold-based filter some minor errors occur so it is applied to smoothing technique. Gupta et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd international conference on communication, computing and security, pp 98–107" href="/article/10.1007/s10055-016-0301-0#ref-CR34" id="ref-link-section-d79366e2171">2012</a>) have proposed two methods of preprocessing techniques in order to get high quality image from the dataset. First method is illumination compensation, regarding change of light source with respect to an environment. Second method is morphological filtering, which removes a noisy pixel from the image. Travieso et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Travieso CM, Ticay-Rivas JR, Briceno JC, del Pozo-Banos M (2014) Hand shape identification on multirange images. J Inf Sci 275:45–56" href="/article/10.1007/s10055-016-0301-0#ref-CR121" id="ref-link-section-d79366e2174">2014</a>) have applied preprocessing at hand contour to isolate the feature point from the contour position with a complex background. This provides useful information with a good quality and consistent value of hand image. Stergiopoulou and Papmarkos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng Appl Artif Intell 22:1141–1158" href="/article/10.1007/s10055-016-0301-0#ref-CR115" id="ref-link-section-d79366e2178">2009</a>) have adopted two techniques for preprocessing such as skin color filtering and palm morphological filtering. Skin color filter is used to reduce noise at any condition including illumination in the YCbCr color space. Palm morphological filter is used to isolate palm portion from hand which is properly positioned in the contour region in 3D plane.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Filtering</h4><p>Wavelet transform can extract spatial and frequency information from a given signal. As compared with other wavelet transforms, the Gabor wavelet has noteworthy properties as far as arithmetic and science. Among different wavelet, Gabor gives an ideal determination in time as well as in frequency domain. Gabor wavelet models are very well known in simulations of the receptive profile of visual cortical cells (Chung <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Chung KYC (2010) Facial expression recognition by using class mean gabor responses with kernel principal component analysis M.Sc Thesis, Russ College of Engineering and Technology, Ohio University, USA, pp 1–69" href="/article/10.1007/s10055-016-0301-0#ref-CR18" id="ref-link-section-d79366e2188">2010</a>). The simple cells of the visual cortex of mammalian brains can be best model as self-similar 2D Gabor wavelets. Gabor filters can derive multi-orientation information from a hand gesture image at distinctive scales. Essential methodology, while developing a Gabor filter for hand gesture or in some other application such as face recognition or emotion detection, is to build a filter bank with different scales and orientations. Subsequently, Gabor filters are utilized for feature detection at various angles and scales (Bashyal and Venayagamoorthy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bashyal S, Venayagamoorthy GK (2008) Recognition of facial expressions using Gabor wavelets and learning vector quantization. Eng Appl Artif Intell 21:10" href="/article/10.1007/s10055-016-0301-0#ref-CR5" id="ref-link-section-d79366e2191">2008</a>; Liu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Liu C (2004) Gabor-Based Kernel PCA with fractional power polynomial models for face recognition. IEEE Trans Pattern Anal Mach Intell 26:10" href="/article/10.1007/s10055-016-0301-0#ref-CR71" id="ref-link-section-d79366e2194">2004</a>; Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wang L et al (2008) 2D Gabor face representation method for face recognition with ensemble and multichannel model. Image Vis Comput 26:9" href="/article/10.1007/s10055-016-0301-0#ref-CR129" id="ref-link-section-d79366e2197">2008</a>; Xu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Xu W et al (2009) A scale and rotation invariant interest points detector based on Gabor filters. In: Slezak D, Pal S, Kang BH, Gu J, Kuroda H, Kim TH (eds) Signal processing image processing and pattern recognition. Communications in computer and information science, vol 61. Springer, Berlin, p 8" href="/article/10.1007/s10055-016-0301-0#ref-CR135" id="ref-link-section-d79366e2200">2009</a>; Huang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Huang Z et al (2010) Study of sign language recognition based on Gabor wavelet transforms. In: International conference on computer design and applications" href="/article/10.1007/s10055-016-0301-0#ref-CR38" id="ref-link-section-d79366e2204">2010</a>). The main principle of Gabor filter is that they can capture visual properties, such as spatial locality, orientation selectivity and spatial frequency characteristics. Because of these attributes numerous applications choose Gabor filter for feature representation. In 2D Gabor filter is represented as multiplication of a 2D Gaussian and a complex sinusoidal function sometimes also called as a complex exponential function. The general articulation of Gabor filter is given by Jemaa and Khanfir (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Jemaa YB, Khanfir S (2009) Automatic local Gabor features extraction for face recognition. Int J Comput Sci Inf Secur 3:1–7" href="/article/10.1007/s10055-016-0301-0#ref-CR43" id="ref-link-section-d79366e2207">2009</a>)</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$g\left( {x,y,\lambda ,\theta ,\psi ,\sigma ,\gamma } \right) = \exp \left( {\frac{{ - x^{2} + \gamma^{2} y^{2} }}{{2\sigma^{2} }}} \right)\cos \left( {\frac{{2\pi x^{2} }}{\lambda } + \psi } \right)$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>x</i>′ = <i>x</i> cos <i>θ</i> + <i>y</i> sin <i>θ</i> and <i>y</i>′ = −<i>x</i> sin <i>θ</i> + <i>y</i> cos <i>θ</i> where <i>λ</i> represents the wavelength of the filter. The standard deviation <i>σ</i> of the Gaussian factor determines the numbers of visible parallel stripes zone. In the proposed method, the ratio has been fixed to <i>σ</i>/<i>λ</i> = 0.50<i>ψ</i>—phase offset to get the symmetry of the kernel in terms of origin. <i>γ</i>—Aspect ratio gives the ellipticity of the receptive field. Here in this proposed strategy, local Gabor filter bank with three distinctive scale and five different orientations <i>σ</i> = {1, 2, 3} and <i>θ</i> randomly used. After the calculation of Gabor function, it will be displayed on the output window.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Thresholding</h4><p>Mutha and Kinage (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Mutha SS, Kinage K (2015) Hand Gesture recognition using LAB thresholding technique. In: 4th post graduate conference (iPGCON-2015), pp 1–5" href="/article/10.1007/s10055-016-0301-0#ref-CR85" id="ref-link-section-d79366e2425">2015</a>) portray the thresholding on LAB color image. In binary thresholding, individual pixels in an image are marked as “object” pixels if their value is greater than some threshold value and as “background” pixels otherwise. Once the thresholding is applied the image is acquired which is in only 2 hues—black or white. Hand gesture image is in black color, and background image is in white color. In thresholding, three sorts of calculations are utilized (a) thresholding, (b) thresholding using HSV, (c) thresholding using LAB.</p><p>In first calculation, the RGB image converts into binary image. RGB image does not give robust result due to an additive combination of red, green and blue color. Result in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig3">3</a> is not as clear contrast with Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig4">4</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Thresholding</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Thresholding using HSV</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0301-0/MediaObjects/10055_2016_301_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Thresholding using LAB</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0301-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Second calculation, change over the RGB image into HSV image. Then it is converted into thresholding image. HSV is the combination of hue, saturation and value. Hue is the color, saturation is shade of color, and value is the darkness of color. HSV gives the separation of all these color and gives robust result than RGB. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig4">4</a> demonstrates that thresholding utilizing HSV gives more robust result than thresholding utilizing RGB.</p><p>Third calculation, change over RGB image into LAB color image. Then it is converted into binary image. Basically, it is pronounced L–A–B not LAB. <i>L</i> stands for lightness, color in the image is split into channels named “<i>A</i>” and “<i>B</i>”, these three segments involve LAB color model. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0301-0#Fig5">5</a> shows the results after LAB thresholding gives more robust result than other calculation.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Tracking</h4><p>A real-time hand tracking system is the most powerful and dependable in complex background. To track the moving hand and then extracting the hand shape quickly and precisely, the trade-off between the computation complexity and robustness need to be considered (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov models. Image Vis Comput 21:745–758" href="/article/10.1007/s10055-016-0301-0#ref-CR14" id="ref-link-section-d79366e2525">2003</a>). Heap and Hogg (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Heap T, Hogg D (1996) Towards 3D hand tracking using a deformable model. In: Proceeding IEEE 2nd international conference on automatic face and gesture recognition" href="/article/10.1007/s10055-016-0301-0#ref-CR35" id="ref-link-section-d79366e2528">1996</a>) present a strategy for tracking of a hand using a deformable model, which also works in the vicinity of complex backgrounds. The deformable model depicts one hand pose and certain variations of it and is not aimed for recognizing distinctive postures. Starner and Pentland (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Starner T, Pentland A (1995) Visual recognition of American sign language using hidden Markov models. In: Proceeding of international workshop on automatic face and gesture recognition, Zurich, Switzerland" href="/article/10.1007/s10055-016-0301-0#ref-CR112" id="ref-link-section-d79366e2531">1995</a>) describe an extensible framework which uses one color camera to track hands in real-time and interprets American sign language (ASL). They use hidden Markov models (HMMs) to recognize a full sentence and show the feasibility of recognizing a series of complicated gesture. Instead of using instrumented glove, they use vision-based approach to capture the hand shape, orientation and direction.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Hand detection and feature extraction</h2><div class="c-article-section__content" id="Sec10-content"><p>The following step in the hand gesture recognition after image preprocessing is detection and feature extraction. Feng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extraction from hand images based on new detection operators, Pattern Recognit, pp 1089–1105" href="/article/10.1007/s10055-016-0301-0#ref-CR27" id="ref-link-section-d79366e2544">2011</a>) have inspected that the glove-based gadgets can estimate hand postures and areas with high precision and rate. This gadget is not suitable for a few sorts of uses, such as human–computer interfaces (HCI) on the environments that it might restrict a user’s movement because of the physical relationship with their controllers. Su et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic. J Appl Soft Comput 22:652–666" href="/article/10.1007/s10055-016-0301-0#ref-CR118" id="ref-link-section-d79366e2547">2014</a>) took skeleton data balanced by the usage of homogenous directions, user’s still need to confront the kinect sensor when performing their activities. In addition, kinect cannot recognize human skeleton features when the user is lying inclined, subsequently making it difficult to evaluate certain kinds of action. Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Li S-Z, Yu B, Wu W, Su S-Z, Ji R-R (2015) Feature learning based on SAE–PCA network for human gesture recognition in RGBD images. J Neuro Comput 151:565–573" href="/article/10.1007/s10055-016-0301-0#ref-CR66" id="ref-link-section-d79366e2550">2015</a>) have proposed depth feature extraction methods, such as histogram of significant distinction will degrade when connected to visual images. This calls for designing individual element representations in both channels, which should then be consolidated by a few ways. Suk et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Suk HI, Sin BK, Lee SW (2010) Hand gesture recognition based on dynamic Bayesian network framework. Pattern Recognit 43(9):3059–3072" href="/article/10.1007/s10055-016-0301-0#ref-CR119" id="ref-link-section-d79366e2553">2010</a>) have taken the arrangement of possible segments, the most essential data around a motion which will be the movement of hands. The movement can be depicted by the direction of a hand in space over time which in turn can be represented by a succession of positions of the hand. Gupta et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd international conference on communication, computing and security, pp 98–107" href="/article/10.1007/s10055-016-0301-0#ref-CR34" id="ref-link-section-d79366e2556">2012</a>) have proposed hand motion acknowledgment frame with four diverse shape-based features that is area, edge, thumb, radial profile and angular position are computed instead of depending on a solitary features. Stergiopoulou and Papmarkos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng Appl Artif Intell 22:1141–1158" href="/article/10.1007/s10055-016-0301-0#ref-CR115" id="ref-link-section-d79366e2560">2009</a>) have discussed the morphology of the hand influence and change the estimations of the finger elements. Subsequently, it is vital to show the focal characteristics of the hand’s shape before proceeding to the feature vectors. Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov models. Image Vis Comput 21:745–758" href="/article/10.1007/s10055-016-0301-0#ref-CR14" id="ref-link-section-d79366e2563">2003</a>) have developed a real-time hand tracking strategy which is intense and reliable in complex background. To track the moving hand and then extracting the hand shape quickly and precisely, the trade-off between the computational complexity and robustness needs to be considered. Vo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human–computer interaction framework using skin color tracking and gesture recognition. In: Proceedings of IEEE international conference on computing and Communication Technologies, Research, Innovation, and Vision for the Future, pp 978–981. doi:&#xA;                    10.1109/RIVF.2010.5633368&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR127" id="ref-link-section-d79366e2566">2010</a>) have discussed the challenging problem of skin detection in computer graphics because human skin color differs from individual to individual, race to race, and is strongly sensitive to lighting condition.</p><h3 class="c-article__sub-heading" id="Sec11">Feature extraction</h3><p>Feature extraction is the strategy of extracting specific features from the pre-processed data’s diverse classifications in a manner that within-class closeness is increased and between-class similarities is minimized. Dominio et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recognition. Pattern Recognit Lett 101–111" href="/article/10.1007/s10055-016-0301-0#ref-CR23" id="ref-link-section-d79366e2576">2014</a>) have proposed that after image acquisition, hand tests are partitioned from the background using both significance and shading information. Here they have four capabilities that are distance features, elevation features, bend components and palm area features. Karami et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Karami A, Zanj B, Sarkaleh AK (2011) Persian sign language (PSL) recognition using wavelet transform and neural networks. Expert Syst Appl 38:2661–2667" href="/article/10.1007/s10055-016-0301-0#ref-CR47" id="ref-link-section-d79366e2579">2011</a>) have used the trimmed and resized images in the gray scale format. Then the DWT is applied on the obtained images and a few elements are isolated by utilizing the wavelet coefficients. Zaiden et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi-agent learning Bayesian and neural network. Eng Appl Artif Intell 32:136–150" href="/article/10.1007/s10055-016-0301-0#ref-CR143" id="ref-link-section-d79366e2582">2014</a>) have discussed the new datasets to setup the proposed skin detector that uses a hybrid system including methodology of a gathering histogram for the Bayesian technique and a back propagation neural network with a neighboring portion settled procedure. These procedures depend on two color spaces, particularly, YCbCr and RGB. respectively. Feng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extraction from hand images based on new detection operators, Pattern Recognit, pp 1089–1105" href="/article/10.1007/s10055-016-0301-0#ref-CR27" id="ref-link-section-d79366e2585">2011</a>) have used texture feature for hand image recognition systems. The highlights from free-hand images can be portrayed into geometric-based approach and vision-based methodology.</p><h3 class="c-article__sub-heading" id="Sec12">Hand detection</h3><p>In computer vision, hand detection plays a major role for determining the feature point from the preprocessed data. Mo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Mo Z, Lewis JP, Neumann U (2005) Smartcanvas: a gesture-driven intelligent drawing desk system. In: IUI’05: 10th international conference on intelligent user interfaces. ACM Press, New York, pp 239–243" href="/article/10.1007/s10055-016-0301-0#ref-CR81" id="ref-link-section-d79366e2596">2005</a>) have discussed about the vision system that has adopted two cameras. First, camera is used to identify a finger touch on the surface. Second, camera is used to track a finger position frequently; light intensity changes with respect to its environment. Martin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Martin J, Devin V, Crowley JL (1998) Active hand tracking. In: FG’98: 3rd international conference on face &amp; gesture recognition. IEEE Computer Society, Washington, p 573" href="/article/10.1007/s10055-016-0301-0#ref-CR76" id="ref-link-section-d79366e2599">1998</a>) have proposed a recursive estimator called as Kalman filter, which is used to isolate the hand image with a good quality. This is also to find the area, size and position of the hand gesture. Vo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human–computer interaction framework using skin color tracking and gesture recognition. In: Proceedings of IEEE international conference on computing and Communication Technologies, Research, Innovation, and Vision for the Future, pp 978–981. doi:&#xA;                    10.1109/RIVF.2010.5633368&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR127" id="ref-link-section-d79366e2602">2010</a>) have discussed many challenges in computer graphics especially for skin color detection. This detection is used to identify the skin portion in different illumination condition with high accuracy. Here boosted Haar wavelet transform is used to improve the efficiency and robustness. Shahzad Malik and Joe Laszlo have described about identifying the finger tip by using model-based approach to analyze its temporal pattern of each gesture even in complex background or mixing of gesture patterns. Hand blob detection is used to extract the required portion from the image by flood-fill technique. This improves the robustness and reliability of the system. Letessier and Berard (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Letessier J, Berard F (2004) Visual tracking of bare fingers for interactive surfaces. In: UIST’04: 17th annual ACM symposium on user interface software and technology. ACM Press, New York, pp 119–122" href="/article/10.1007/s10055-016-0301-0#ref-CR62" id="ref-link-section-d79366e2605">2004</a>) have proposed two novel techniques for extracting features from the hand image. To generate similarity map from the feature vector points available in dataset by using image differencing segmentation (IDS). Also to find the fingertip orient with different angles can be determined by fast rejection filter (FRF) technique. Oka et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: FGR’02: 5th IEEE international conference on automatic face and gesture recognition. IEEE Computer Society, Washington, p 429" href="/article/10.1007/s10055-016-0301-0#ref-CR90" id="ref-link-section-d79366e2608">2002</a>) have described about the prediction techniques which are used to track the multiple finger tips in complex background with different illumination conditions. The author developed an algorithm not only to identify finger tip but also centeriod, palm portion its area, position in three-dimensional plane and perimeter. Koike et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Koike H, Sato Y, Kobayashi Y (2001) Integrating paper and digital information on enhanced desk: a method for real time finger tracking on an augmented desk system. ACM Trans Hum Comput Interact 8(4):307–322" href="/article/10.1007/s10055-016-0301-0#ref-CR53" id="ref-link-section-d79366e2612">2001</a>) have invented a new algorithm for interactive applications by using color segmentation and background subtraction technique. The author has given additional criticize about LCD projector and CCD camera. The CCD camera has better performance than the LCD projector for interactive hand gesture recognition. Kolsch and Turk (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kolsch M, Turk M (2004) Robust hand detection. In: 6th IEEE international conference on automatic face and gesture recognition, vol 614" href="/article/10.1007/s10055-016-0301-0#ref-CR54" id="ref-link-section-d79366e2615">2004</a>) have discussed the performance measure of both cascaded and non-cascaded detector which provides with low false positive rate even if complex orientations and background have. Ong and Bowden (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ong EJ, Bowden R (2004) A boosted classifier tree for hand shape detection. In: 6th IEEE international conference on automatic face and gesture recognition, pp 889–894" href="/article/10.1007/s10055-016-0301-0#ref-CR91" id="ref-link-section-d79366e2618">2004</a>) have worked with unsupervised learning for classification of hand in different position, shape, size, angular position, etc. A boosted cascaded tree structure is used for extracting features from the hand image in the dataset. Stenger et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Stenger B, Thayananthan A, Torr P, Cipolla R (2004) Hand pose estimation using hierarchical detection. In: 8th European conference on computer vision workshop on human computer interaction, vol 3058, Springer, Prague, pp 102–112" href="/article/10.1007/s10055-016-0301-0#ref-CR114" id="ref-link-section-d79366e2621">2004</a>) have proposed the hierarchical filter for detecting the hand feature points from the database. To decompose the edge detection by using Hausdorff matching technique, which separate into number of channels with respect to orientation of the data.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Image segmentation</h2><div class="c-article-section__content" id="Sec13-content"><p>There are many challenges in image segmentation for pattern recognition applications. This typically represents the location of an objects and boundaries in image. Dominio et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recognition. Pattern Recognit Lett 101–111" href="/article/10.1007/s10055-016-0301-0#ref-CR23" id="ref-link-section-d79366e2633">2014</a>) have presented hand segmentation done by separating every sample color, and the reference skin color is surveyed and the specimen whose color contrast is underneath a pre-characterized edge is discarding. The detailed survey for skin color segmentation, rapid hand motion segmentation and shape-based descriptors.</p><h3 class="c-article__sub-heading" id="Sec14">Skin color segmentation</h3><p>Zaiden et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi-agent learning Bayesian and neural network. Eng Appl Artif Intell 32:136–150" href="/article/10.1007/s10055-016-0301-0#ref-CR143" id="ref-link-section-d79366e2643">2014</a>) have discussed about more number of the literature on skin color segmentation based on Naïve Bayesian approach and neural network. The skin portion extracted from the complex background with different lightning conditions. Feng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extraction from hand images based on new detection operators, Pattern Recognit, pp 1089–1105" href="/article/10.1007/s10055-016-0301-0#ref-CR27" id="ref-link-section-d79366e2646">2011</a>) have proposed a new detection operator that has two approaches such as coarse location phase (CLP) and refined location phase (RLP). CLP approach is used to locate the hand contour with its feature points in the three-dimensional plane. RLP approach is used to refine the feature points provided by the CLP method. Su et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic. J Appl Soft Comput 22:652–666" href="/article/10.1007/s10055-016-0301-0#ref-CR118" id="ref-link-section-d79366e2649">2014</a>) have invented home-based rehabilitation system by using kinect sensor. This system was adopted by dynamic time warping (DTW) and fuzzy mean clustering approach. The kinect SDK detects the hand gestures by silhouette descriptor and also tracks the skeletal features in different lightning conditions with less expensive hardware setup. Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Li S-Z, Yu B, Wu W, Su S-Z, Ji R-R (2015) Feature learning based on SAE–PCA network for human gesture recognition in RGBD images. J Neuro Comput 151:565–573" href="/article/10.1007/s10055-016-0301-0#ref-CR66" id="ref-link-section-d79366e2652">2015</a>) have proposed a feature learning strategy which is joined sparse auto-encoder (SAE) with CNN and various layers of principle component analysis (PCA) structure, a different leveled model for American sign language (ASL) finger-spelling acknowledgment. This system was well equipped for training and testing the segmented object with a complex background with different instant of time. Kiliboz and Gudukbay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Kiliboz NC, Gudukbay U (2015) A hand gesture recognition technique for human–computer interaction. J Vis Commun Image Recognit 28:97–104" href="/article/10.1007/s10055-016-0301-0#ref-CR50" id="ref-link-section-d79366e2655">2015</a>) have presented the fastest training method that does not require more data to learn from the available dataset. An experiment with six degree of freedom (DOF) is used for locating the gesture movement in spatial domain. Zhu and Sheng (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zhu C, Sheng W (2009) Online hand gesture recognition using neural network based segmentation. In: International conference on intelligent robots and systems. IEEE Publisher, pp 2415–2420" href="/article/10.1007/s10055-016-0301-0#ref-CR146" id="ref-link-section-d79366e2659">2009</a>) have proposed online-based hand gesture recognition for the development of an artificial intelligence system to assist the human being. In the segmentation module, the neural network is to determine the required gesture from the usual hand gesture in daily life. Joshi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Joshi A, Monnier C, Betke M, Sclaroff S (2016) Comparing random forest approaches to segmenting and classifying gestures. J Image Vis Comput 1–10. doi:&#xA;                    10.1016/j.imavis.2016.06.001&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR45" id="ref-link-section-d79366e2662">2016</a>) have presented the new algorithm for segmentation called random forest approach. This method is used to identify the gesture from a given dataset in three-dimensional position as well as out-of-dataset. Chong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chong Y, Huang J, Pan S (2016) Hand Gesture recognition using appearance features based on 3D point cloud. J Softw Eng Appl 9:103–111" href="/article/10.1007/s10055-016-0301-0#ref-CR17" id="ref-link-section-d79366e2665">2016</a>) have proposed threshold segmentation from 3D point cloud hand gesture data collected from SwissRange 4000 depth camera and then it transforms into binary image. From this binary image, three different feature vector points are extracted by using decision rule approach.</p><h3 class="c-article__sub-heading" id="Sec15">Rapid hand motion segmentation</h3><p>Suk et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Suk HI, Sin BK, Lee SW (2010) Hand gesture recognition based on dynamic Bayesian network framework. Pattern Recognit 43(9):3059–3072" href="/article/10.1007/s10055-016-0301-0#ref-CR119" id="ref-link-section-d79366e2676">2010</a>) have portrayed rapid hand motion recognition by using dynamic Bayesian network model. Here two hands are used to interact with the system, which produce useful information through the accurate segmented potion of an object toward the system. Gupta et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd international conference on communication, computing and security, pp 98–107" href="/article/10.1007/s10055-016-0301-0#ref-CR34" id="ref-link-section-d79366e2679">2012</a>) have enhanced the hand gesture recognition system by using FPGA device. The system has partitioned the multiple skin color pixel from the hand image with a complex environment. Stergiopoulou and Papmarkos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng Appl Artif Intell 22:1141–1158" href="/article/10.1007/s10055-016-0301-0#ref-CR115" id="ref-link-section-d79366e2682">2009</a>) have introduced hand gesture recognition by using shape-based fitting function through self-growing and self-organized neural gas (SGONG). Skin color segmentation is used to identify the different portion of hand features in YCbCr color space. Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov models. Image Vis Comput 21:745–758" href="/article/10.1007/s10055-016-0301-0#ref-CR14" id="ref-link-section-d79366e2685">2003</a>) have demonstrated a real-time tracking system to segment the continuous hand gesture in stationary background by using Fourier descriptor (FD) approach, which characterizes the input sequence by spatial and temporal feature vectors. Bansal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Bansal M, Saxena S, Desale D, Jadhav D (2011) Dynamic gesture recognition using hidden Markov models in static background. Int J Comput Sci 8(6), no. 1, 391–398" href="/article/10.1007/s10055-016-0301-0#ref-CR4" id="ref-link-section-d79366e2688">2011</a>) have discussed about the hand gestures in vulnerable conditions with respect to the illumination, background and environment. First, it converts all the pixels values from RGB to HSV model then segments the skin and non-skin portion using morphological filter. Segen and Kumar (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Segen J, Kumar S (1998) Gesture VR: vision-based 3D hand interface for spatial interaction. In: 6th ACM international conference on multimedia. ACM Press, New York, pp 455–464" href="/article/10.1007/s10055-016-0301-0#ref-CR110" id="ref-link-section-d79366e2692">1998</a>) have described about the tracking the hand gesture in spatial domain at the speed of 60 Hz. The system is designed with five modeling parameters to control the system from the remote place.</p><h3 class="c-article__sub-heading" id="Sec16">Shape-based descriptors</h3><p>Vo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human–computer interaction framework using skin color tracking and gesture recognition. In: Proceedings of IEEE international conference on computing and Communication Technologies, Research, Innovation, and Vision for the Future, pp 978–981. doi:&#xA;                    10.1109/RIVF.2010.5633368&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR127" id="ref-link-section-d79366e2703">2010</a>) have proposed multiple-object tracking for interaction purpose, only the hand and palm are engaged to recognize within circle of the article as the biggest circle inside the article is the deciding objective to evaluate the hand palm position. The hand palm is bound from the wrist–arm part which normally lies away from the circle. Sanches-Reillo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sanches-Reillo R, Sanchez-Avila C, Gonzalez-Macros A (2000) Biometric identification through hand geometry measurements. IEEE Trans Pattern Anal Mach Intell 22(10):1168–1171" href="/article/10.1007/s10055-016-0301-0#ref-CR109" id="ref-link-section-d79366e2706">2000</a>) have discussed the measure of finger widths at distinctive scopes, finger and palm heights, finger deviations and the edge of finger valleys with the level. The twenty-five selected features are displayed with Gaussian mixture models particular to every person. Oden et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Oden C, Ercil A, Buke B (2003) Combining implicit polynomials and geometric features for hand recognition. Pattern Recognit Lett 24:2145–2152" href="/article/10.1007/s10055-016-0301-0#ref-CR89" id="ref-link-section-d79366e2709">2003</a>) have used fourth degree implicit polynomial representation of the extracted finger shapes in addition to such geometric features for finger widths at different positions and the palm size. The subsequent sixteen features are compared using the Mahalanobis distance method. Jain et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Jain AK, Ross A, Pankanti S (1999) A prototype hand geometry based verification system. In: Proceedings of 2nd international conference on audio and video based biometric person authentication, pp 166–171" href="/article/10.1007/s10055-016-0301-0#ref-CR42" id="ref-link-section-d79366e2712">1999</a>) have used a peg-based imaging plot and acquired sixteen features, which incorporate length and width of the fingers, aspect ratio of the palm to fingers, and thickness of the hand. The created framework was tried for a trial for web access over for a group of ten individuals. Jain and Duta (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Jain AK, Duta N (1999) Deformable matching of hand shapes for verification. In: Proceedings of international conference on image processing" href="/article/10.1007/s10055-016-0301-0#ref-CR41" id="ref-link-section-d79366e2715">1999</a>) analyzed the target movement with the accessible dataset, to find the mean square error and its finger alignment. Ibarguren et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ibarguren A, Maurtua I, Sierra B (2010) Layered architecture for real time sign recognition: hand gesture and movement. J Eng Appl Artif Intell 1216–1228" href="/article/10.1007/s10055-016-0301-0#ref-CR40" id="ref-link-section-d79366e2719">2010</a>) have depicted the consistent signs given by the data glove at first by method for the segmentation layer. The descriptor identifies the segmented portion of geometrical features from the hand image.</p><p>Lee and Nakamura (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Lee D, Nakamura Y (2014) Motion recognition and recovery from occluded monocular observations. J Robot Auton Syst 62:818–832" href="/article/10.1007/s10055-016-0301-0#ref-CR60" id="ref-link-section-d79366e2725">2014</a>) have proposed the occluded monocular camera for capturing the gestures in the three-dimensional plane and transform into coordinate plane. This should be estimated by geometrical feature points of segmented portion of palm, finger and hand in different orientations and positions. Lay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lay YL (2000) Hand shape recognition. Opt Laser Technol 32(1):1–5" href="/article/10.1007/s10055-016-0301-0#ref-CR58" id="ref-link-section-d79366e2728">2000</a>) has presented a method where the hand is enlightened with a parallel grating that serves both to segment the background and empowers the user to enroll his hand with the stored contours. The geometric features of the hand shape are captured by the quad tree code. At last, it notices that there exist a number of patents on hand information-based personnel identification, taking into account either geometrical features or on hand profile explained by Zunkel (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Zunkel RL (1999) Hand geometry based verification. In: Proceedings of biometrics. Kluwer Academic Publishers, pp 87–101" href="/article/10.1007/s10055-016-0301-0#ref-CR147" id="ref-link-section-d79366e2731">1999</a>).</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Motion classification</h2><div class="c-article-section__content" id="Sec17-content"><p>Zaiden et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi-agent learning Bayesian and neural network. Eng Appl Artif Intell 32:136–150" href="/article/10.1007/s10055-016-0301-0#ref-CR143" id="ref-link-section-d79366e2744">2014</a>) have analyzed about the classifier in perspective of Cb and Cr, which will confine the request and speed up the calculation in distinguishing region of human skin. Su et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic. J Appl Soft Comput 22:652–666" href="/article/10.1007/s10055-016-0301-0#ref-CR118" id="ref-link-section-d79366e2747">2014</a>) have focused on real-time gesture classification and movement acknowledgment frameworks used to train individual with subjective impairment to engage on professional tasks and later form a kinect-based recovery framework for youthful grown-ups with motors disabilities. Kiliboz and Gudukbay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Kiliboz NC, Gudukbay U (2015) A hand gesture recognition technique for human–computer interaction. J Vis Commun Image Recognit 28:97–104" href="/article/10.1007/s10055-016-0301-0#ref-CR50" id="ref-link-section-d79366e2750">2015</a>) have proposed frameworks with high accuracy and online recognition mechanism making it flexible to any application for gesture-based HCI so that such applications will turn out to be more natural by the way they interact with their users. Yewale (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Huang Z et al (2010) Study of sign language recognition based on Gabor wavelet transforms. In: International conference on computer design and applications" href="/article/10.1007/s10055-016-0301-0#ref-CR38" id="ref-link-section-d79366e2753">2011</a>), Fels and Hinton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Fels SS, Hinton GE (1993) Glove-talk: a neural network interface between a data-glove and a speech synthesizer. IEEE Trans Neural Netw 4(1):2–8. doi:&#xA;                    10.1109/72.182690&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR25" id="ref-link-section-d79366e2756">1993</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Fels SS, Hinton GE (1998) Glove-talk: a neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Trans Neural Netw 9(1):205–212. doi:&#xA;                    10.1109/72.655042&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR26" id="ref-link-section-d79366e2760">1998</a>), Bezdek et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="Bezdek JC, Ehrlich R, Full W (1984) FCM: the fuzzy C-means clustering algorithm. Comput Geosci 10(2–3):191–203" href="/article/10.1007/s10055-016-0301-0#ref-CR6" id="ref-link-section-d79366e2763">1984</a>), Li (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Li X (2003) Gesture recognition based on fuzzy C-means clustering algorithm. Department of Computer Science, The University of Tennessee, Knoxville" href="/article/10.1007/s10055-016-0301-0#ref-CR63" id="ref-link-section-d79366e2766">2003</a>), Malik and Laszlo (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Malik S, Laszlo J (2004) Visual touchpad: a two-handed gestural input device. In: ICMI’04: 6th international conference on multimodal interfaces. ACM Press, New York, pp 289–296" href="/article/10.1007/s10055-016-0301-0#ref-CR73" id="ref-link-section-d79366e2769">2004</a>) have developed several automated motion classification systems.</p><h3 class="c-article__sub-heading" id="Sec18">Hand motion analysis</h3><p>Nguyen-Duc-Thanh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Nguyen-Duc-Thanh N, Lee S, Kim D (2012) Two-stage hidden Markov model in gesture recognition for human robot interaction. Int J Adv Robot Syst 9:1–10" href="/article/10.1007/s10055-016-0301-0#ref-CR87" id="ref-link-section-d79366e2779">2012</a>) have proposed the initial step to recognize the human skeleton from the arrangement of image frames. 3D kinect camera, released by Microsoft Corp, has been used to get the skeletal features that to classify by 2-stage HMM. The first stage HMM is used to provide the command of whole gesture. The second stage HMM is used to provide the role played by that gesture. Lim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lim CH, Vats E, Chan CS (2015) Fuzzy human motion analysis: a review. J Pattern Recognit 48:1773–1796" href="/article/10.1007/s10055-016-0301-0#ref-CR69" id="ref-link-section-d79366e2782">2015</a>) have given the detailed review on hand motion analysis (HMA) which stressed on the survey paper from 2010 to 2014. From this survey, human detection, tracking, feature extraction, datasets and applications are explained in detail. The fundamental factors that invested such capability to these methodologies incorporate the capacity to perform soft labeling and adaptability to adjust to diverse uncertainty. On the other hand, most of the reported works thus did not use the standard HMA datasets as their benchmark. Anyway, the current databases are mostly too ideal to reflect the real-world scenarios that are full of uncertainties. Kohler and Schroter (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Kohler M, Schroter S (1998) A survey of video-based gesture recognition: stereo and mono systems. Technical report no. 693/1998, Informatik VII, University of Dortmund" href="/article/10.1007/s10055-016-0301-0#ref-CR52" id="ref-link-section-d79366e2785">1998</a>) have discussed about binary classifier tree for finger-spelling motions. The author showed the comparison between the conventional classifier with binary classifier tree. This is reliable to hand gestures for counting and determining the spelling.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab4">4</a> demonstrates the basis on which the previous review papers on hand motion analysis (HMA). Note that those models with a “tick” mean the topic is examined exhaustively in comparison with survey paper. The researcher works on the fuzzy set in true issues are flourished. The major objective and contribution is to review the early years of the fuzzy set situated methodologies for HMA and individuating how the fuzzy set may enhance the HMA. Zhu and Sheng (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zhu C, Sheng W (2009) Online hand gesture recognition using neural network based segmentation. In: International conference on intelligent robots and systems. IEEE Publisher, pp 2415–2420" href="/article/10.1007/s10055-016-0301-0#ref-CR146" id="ref-link-section-d79366e2794">2009</a>) have proposed online-based hand gesture recognition system to control the living place through robotic device. Hybridize the neural network gesture spotting method with the hierarchical hidden Markov model (HHMM) to classify the hand motion. Joshi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Joshi A, Monnier C, Betke M, Sclaroff S (2016) Comparing random forest approaches to segmenting and classifying gestures. J Image Vis Comput 1–10. doi:&#xA;                    10.1016/j.imavis.2016.06.001&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR45" id="ref-link-section-d79366e2797">2016</a>) have discussed about the classifier combines with a binary random forest model and a multi-class random forest model to identify gestures from the complex background. Chong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chong Y, Huang J, Pan S (2016) Hand Gesture recognition using appearance features based on 3D point cloud. J Softw Eng Appl 9:103–111" href="/article/10.1007/s10055-016-0301-0#ref-CR17" id="ref-link-section-d79366e2800">2016</a>) have employed decision rule-based hand gesture recognition. This method is used to design a good decision tree to choose the proper logical attributes and judgment. Experimental results show that both accuracy rate and running time achieved in good effect.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Detailed review on HMA with criterion used</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec19">Mathematical modeling-based hand gesture recognition</h3><p>Many researches were applied in the field of gesture recognition using mathematical modeling approaches. HMM is stochastic procedure (Mitra and Acharya <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:&#xA;                    10.1109/TSMCC&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR80" id="ref-link-section-d79366e3361">2007</a>), with a limited number of conditions of Markov chain, and various irregular capacities so that every state has an arbitrary capacity. The system topology is represented by one state for the initial state, a set of output symbols (Mitra and Acharya <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:&#xA;                    10.1109/TSMCC&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR80" id="ref-link-section-d79366e3364">2007</a>; LaViola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola JJ Jr (1999) A survey of hand posture and gesture recognition and technology. Master thesis, NSF Science and Technology Center for Computer Graphics and Scientific Visualization, USA" href="/article/10.1007/s10055-016-0301-0#ref-CR57" id="ref-link-section-d79366e3367">1999</a>), and a set of transition state (Starner and Pentland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. AAAI technical report FS-96-05, The Media Laboratory Massachusetts Institute of Technology" href="/article/10.1007/s10055-016-0301-0#ref-CR113" id="ref-link-section-d79366e3370">1996</a>; LaViola JJ Jr <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola JJ Jr (1999) A survey of hand posture and gesture recognition and technology. Master thesis, NSF Science and Technology Center for Computer Graphics and Scientific Visualization, USA" href="/article/10.1007/s10055-016-0301-0#ref-CR57" id="ref-link-section-d79366e3373">1999</a>). It has demonstrated its proficiency for mathematical modeling spatio-temporal data (Mitra and Acharya <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:&#xA;                    10.1109/TSMCC&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR80" id="ref-link-section-d79366e3377">2007</a>). Gesture-based recognition is one among the most important applications of HMM (Starner and Pentland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. AAAI technical report FS-96-05, The Media Laboratory Massachusetts Institute of Technology" href="/article/10.1007/s10055-016-0301-0#ref-CR113" id="ref-link-section-d79366e3380">1996</a>), and speech recognition (Rabiner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech reognition. Proc IEEE 77(2):257–285" href="/article/10.1007/s10055-016-0301-0#ref-CR104" id="ref-link-section-d79366e3383">1989</a>). Keskin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Keskin C, Erkan A, Akarun L (2003) Real time hand tracking and 3D gesture recognition for interactive interface using HMM. In: Proceedings of international conference on artificial neural networks" href="/article/10.1007/s10055-016-0301-0#ref-CR48" id="ref-link-section-d79366e3386">2003</a>) have presented HCI interface based on real-time hand tracking and 3D motion recognition using HMM. Two hued cameras for 3D construction are used. To conquer the issue of utilizing skin color for hand detection due to hand overlapping with other body parts, markers are used to reduce the complexity in hand recognition process. The bounding box sometimes needs to be stretched to decide the mode of the hand, and the points used to predict the fingertip location in distinct modes of the hand. Kalman filter was used for filtering the trajectory of the hand motion.</p><p>Stergiopoulou and Papmarkos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng Appl Artif Intell 22:1141–1158" href="/article/10.1007/s10055-016-0301-0#ref-CR115" id="ref-link-section-d79366e3392">2009</a>) have analyzed the use of neural systems for motion acknowledgment. Most of the researches used artificial neural network (ANN) as a classifier, while some others used it to extract the shape of the hand. Maung (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Maung TH (2009) Real-time hand tracking and gesture recognition system using neural networks. World Acad Sci Eng Technol 50:466–470" href="/article/10.1007/s10055-016-0301-0#ref-CR77" id="ref-link-section-d79366e3395">2009</a>) presents a framework for hand tracking and motion acknowledgment using neural network to recognize Myanmar alphabet language (MAL). Adobe Photoshop filter is applied to find the edges of the input image and histogram of local orientation employed to extract image feature vector which would be the input to the supervised neural network system. Maraqa and Abu-Zaiter (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Maraqa M, Abu-Zaiter R (2008) Recognition of Arabic Sign Language (ArSL) using recurrent neural networks. In: IEEE 1st international conference on the applications of digital information and web technologies, pp 478–484. doi:&#xA;                    10.1109/ICADIWT.2008.4664396&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR74" id="ref-link-section-d79366e3398">2008</a>) utilized two repetitive neural network architectures to perceive Arabic sign language (ArSL). A colored glove used for input image, and for segmentation process, HSI color model is applied. Murakami and Taguchi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Murakami K, Taguchi H (1999) Gesture recognition using recurrent neural networks. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp 237–242. doi:&#xA;                    10.1145/108844.108900&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR84" id="ref-link-section-d79366e3401">1999</a>) introduced Japanese gesture-based recognition using two distinctive neural network frameworks. First, back propagation algorithm was used for learning postures of Japanese alphabet. The system comprises of three layers, the input layer with 13 nodes, the hidden layer with 100 nodes and the output layer with 42 nodes which relates 42 recognized characters. The recognition rate for learning stage was 71% and for unregistered individuals 48%, while the enhancement rate when additional patterns added to the system, it became 98% for registration and 77% for unregistered individuals.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab5">5</a> discussed about the different techniques for motion recognition, these strategies incorporate neural network (Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Yang MH, Ahuja N, Tabb M (2002) Extraction of 2D motion trajectories and its application to hand gesture recognition. IEEE Trans Pattern Anal Mach Intell 24(8):1061–1074" href="/article/10.1007/s10055-016-0301-0#ref-CR136" id="ref-link-section-d79366e3410">2002</a>), HMM (Lee and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lee KH, Kim JH (1999) An HMM based threshold model approach for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(10):961–973" href="/article/10.1007/s10055-016-0301-0#ref-CR59" id="ref-link-section-d79366e3413">1999</a>; Yoon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yoon HS, Soh J, Bae YJ, Yang HS (2001) Hand gesture recognition using combined features of location, angle and velocity. J Pattern Recognit 34:1491–1501" href="/article/10.1007/s10055-016-0301-0#ref-CR141" id="ref-link-section-d79366e3416">2001</a>; Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov models. Image Vis Comput 21:745–758" href="/article/10.1007/s10055-016-0301-0#ref-CR14" id="ref-link-section-d79366e3419">2003</a>; Ramamoorthy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ramamoorthy A, Vaswani N, Chaudhury S, Banerjee S (2003) Recognition of dynamic hand gestures. Pattern Recognit 36:2069–2081" href="/article/10.1007/s10055-016-0301-0#ref-CR105" id="ref-link-section-d79366e3423">2003</a>; Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kim D, Song J, Kim D (2007) Simultaneous gesture segmentation and recognition based on forward spotting accumulative HMMs. Pattern Recognit 40(11):3012–3026" href="/article/10.1007/s10055-016-0301-0#ref-CR51" id="ref-link-section-d79366e3426">2007</a>; Just and Marcel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Just A, Marcel S (2009) A comparative study of two-state-of-the art sequence processing techniques for hand gesture recognition. Comput Vis Image Underst 113(4):532–543" href="/article/10.1007/s10055-016-0301-0#ref-CR46" id="ref-link-section-d79366e3429">2009</a>; Ng and Ranganath <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ng CW, Ranganath S (2002) Real-time gesture recognition system and application. Image Vis Comput 20:993–1007" href="/article/10.1007/s10055-016-0301-0#ref-CR86" id="ref-link-section-d79366e3432">2002</a>), DBN (Suk et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Suk HI, Sin BK, Lee SW (2010) Hand gesture recognition based on dynamic Bayesian network framework. Pattern Recognit 43(9):3059–3072" href="/article/10.1007/s10055-016-0301-0#ref-CR119" id="ref-link-section-d79366e3435">2010</a>), curve fitting (Shin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using Bezier curves for visualization navigation from registered 3D data. Pattern Recognit 37(5):1011–1024" href="/article/10.1007/s10055-016-0301-0#ref-CR111" id="ref-link-section-d79366e3438">2004</a>), predictive Eigen tracker (Patwardhan and Roy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Patwardhan KS, Roy SD (2007) Hand gesture modeling and recognition involving changing shapes and trajectories using a predictive eigen tracker. Pattern Recognit 28:329–334" href="/article/10.1007/s10055-016-0301-0#ref-CR92" id="ref-link-section-d79366e3442">2007</a>), FSM (Yeasin and Chaudhuri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Yeasin M, Chaudhuri S (2000) Visual understanding of dynamic hand gestures. Pattern Recognit 33(11):1805–1817" href="/article/10.1007/s10055-016-0301-0#ref-CR137" id="ref-link-section-d79366e3445">2000</a>), CDFD and Q-DFFM (Litchtenauer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Litchtenauer JF, Hendriks EA, Reinders MJT (2008) Sign language recognition by combining statistical IDTW and independent classification. IEEE Trans Pattern Anal Mach Intell 30(11):2040–2046" href="/article/10.1007/s10055-016-0301-0#ref-CR70" id="ref-link-section-d79366e3448">2008</a>) for classifying the hand motion based on the features used. Also, application areas for the gestures system are presented.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Detailed survey on various hand gesture recognition methods</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec20">Recognition of hand posture</h3><p>This section briefly discusses the taxonomy of hand pose recognition, and its tool is classified as (a) unsupervised learning, (b) supervised learning, (c) graph matching, (d) 3D model-based method.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab6">6</a> shows the taxonomy of hand posture recognition tool used in supervised and unsupervised approach. Roweis and Saul (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Roweis ST, Saul LK (2000) Non linear dimensionality reduction by locally linear embedding. Science 290(5500):2323–2326" href="/article/10.1007/s10055-016-0301-0#ref-CR108" id="ref-link-section-d79366e3899">2000</a>) examined around an unsupervised learning calculation, which is locally linearly embedding (LLE) that endeavors to guide high-dimensional information to low-dimensional space while preserving the neighborhood relationship. Licsar and Sziranyi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Licsar A, Sziranyi T (2004) Dynamic training of hand gesture recognition system. In: Kittler J, Petrou M, Nixon M (eds) Proceedings of international conference on pattern recognition. ICPR, Cambridge, pp 971–974" href="/article/10.1007/s10055-016-0301-0#ref-CR67" id="ref-link-section-d79366e3902">2004</a>) exhibited a user-adaptive hand posture recognition system with an interactive online training. The system is retrained online for flaw recognized postures if the recognition accuracy reduces, realizing fast adaptation to new users. A supervised training strategy amends for the unrecognized posture classes, and an unsupervised posture classes, and an unsupervised technique persistently rushes to take after slight changes in posture styles. Conte et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Conte D, Foggia P, Sansone C, Vento M (2004) Thirty years of graph matching in pattern recognition. Int J Pattern Recognit Artif Intell 18(3):265–298" href="/article/10.1007/s10055-016-0301-0#ref-CR20" id="ref-link-section-d79366e3905">2004</a>) presented a graph-based technique which is used as a powerful tool for pattern classification and representation. The use of graph matching in computer vision and pattern recognition obtained a growing attention from the research community recently, as the computational cost of the graph-based algorithms is now becoming compatible with the computational power of new generation computers. Wiskott et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Wiskott L, Fellous JM, Kruger N, Malsburg C (1997) Face recognition by elastic bunch graph matching. IEEE Trans Pattern Anal Mach Intell 19(7):775–779" href="/article/10.1007/s10055-016-0301-0#ref-CR132" id="ref-link-section-d79366e3908">1997</a>) portray the Bunch graphs procedure used to model the variability in object appearance. The natural variability in the qualities of relating focus in several images is captured by marking each node with a group of attribute values, extracted from the corresponding points. Yin and Xie (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Yin X, Xie M (2003) Estimation of the fundamental matrix from un-calibrated stereo hand images for 3D hand gesture recognition. Pattern Recognit 36:567–584" href="/article/10.1007/s10055-016-0301-0#ref-CR140" id="ref-link-section-d79366e3912">2003</a>) proposed a computer vision model rather than kinematic model. The algorithm avoids the complexity in estimation of the angular and linear parameters of the kinematic model. They utilized topological features of the hand for 3D hand posture recognition.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Taxonomy of hand pose recognition tool</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab7">7</a> shows the detailed survey on hand posture recognition systems for various applications. Mitra and Acharya (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:&#xA;                    10.1109/TSMCC&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0301-0#ref-CR80" id="ref-link-section-d79366e3983">2007</a>) have presented the applications of hand posture recognition as an alternative level of interaction in different specialization, including virtual environments (VEs), smart surveillance, sign language translation, medical system, etc. Freeman and Roth (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Freeman WT, Roth M (1995) Orientation histograms for hand gesture recognition. In: IEEE international workshop on automatic face and gesture recognition, Zurich" href="/article/10.1007/s10055-016-0301-0#ref-CR29" id="ref-link-section-d79366e3986">1995</a>) discussed a technique for gesture recognition based on histogram-based feature vector. Zhou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou H, Lin DJ, Haung TS (2004) Static hand gesture recognition based on local orientation histogram feature distribution model. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition workshops" href="/article/10.1007/s10055-016-0301-0#ref-CR145" id="ref-link-section-d79366e3989">2004</a>) have presented hand motion recognition based on histogram feature distribution model. Skin color-based segmentation algorithm was used to find a mask for the hand region, where the input RGB image converted into HSI color space, and then map the HSI image, 128 elements in the local orientation histogram feature were used. Wysoski et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Wysoski SG, Lamar MV, Kuroyanagi S, Iwata A (2002) A rotation invariant approach on static-gesture recognition using boundary histograms and neural networks. In: IEEE proceedings of the 9th international conference on neural information processing, Singapura" href="/article/10.1007/s10055-016-0301-0#ref-CR134" id="ref-link-section-d79366e3992">2002</a>) discussed a rotation invariant static gesture acknowledgment method using boundary histograms. Skin color detection was used to identify the clusters of particle in the image. For each clusters the boundary was extracted using an ordinary contour tracking approach.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Detailed survey on various hand posture recognition methods</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The performance measures of the classifiers for hand gesture and posture recognition are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0301-0#Tab8">8</a>. The detailed comparative and analysis made between two performance metrics in this literature such as recognition rate and convergence time.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 Detailed survey on performance measures of hand gesture/posture recognition system</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/tables/8"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusions</h2><div class="c-article-section__content" id="Sec21-content"><p>In this work, the advantages and disadvantages of various classification techniques of hand posture and gesture recognition are highlighted. The suitability of the techniques for various applications is also explained in this survey. Several hybrid approaches can be developed to increase the accuracy of hand gesture recognition system. This survey also aids in highlighting the significant contributions of human gesture recognition for human computer interaction (HCI).</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aggarwal JK, Cai Q (1997) Human motion analysis: a review. In: Proceedings of the IEEE non-rigid and articulat" /><p class="c-article-references__text" id="ref-CR1">Aggarwal JK, Cai Q (1997) Human motion analysis: a review. In: Proceedings of the IEEE non-rigid and articulated motion workshop, pp 99–102</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Aggarwal, MS. Ryoo, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Aggarwal J, Ryoo MS (2011) Human activity analysis: a review. ACM Comput Surv 43(3):16" /><p class="c-article-references__text" id="ref-CR2">Aggarwal J, Ryoo MS (2011) Human activity analysis: a review. ACM Comput Surv 43(3):16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1922649.1922653" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20activity%20analysis%3A%20a%20review&amp;journal=ACM%20Comput%20Surv&amp;volume=43&amp;issue=3&amp;publication_year=2011&amp;author=Aggarwal%2CJ&amp;author=Ryoo%2CMS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aggarwal JK, Cai Q, Liao W, Sabata B (1994) Articulated and elastic non-rigid motion: a review. In: Proceeding" /><p class="c-article-references__text" id="ref-CR3">Aggarwal JK, Cai Q, Liao W, Sabata B (1994) Articulated and elastic non-rigid motion: a review. In: Proceedings of the IEEE workshop on motion of non-rigid and articulated objects, pp 2–14</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bansal M, Saxena S, Desale D, Jadhav D (2011) Dynamic gesture recognition using hidden Markov models in static" /><p class="c-article-references__text" id="ref-CR4">Bansal M, Saxena S, Desale D, Jadhav D (2011) Dynamic gesture recognition using hidden Markov models in static background. Int J Comput Sci 8(6), no. 1, 391–398</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Bashyal, GK. Venayagamoorthy, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bashyal S, Venayagamoorthy GK (2008) Recognition of facial expressions using Gabor wavelets and learning vecto" /><p class="c-article-references__text" id="ref-CR5">Bashyal S, Venayagamoorthy GK (2008) Recognition of facial expressions using Gabor wavelets and learning vector quantization. Eng Appl Artif Intell 21:10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.engappai.2007.11.010" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20facial%20expressions%20using%20Gabor%20wavelets%20and%20learning%20vector%20quantization&amp;journal=Eng%20Appl%20Artif%20Intell&amp;volume=21&amp;publication_year=2008&amp;author=Bashyal%2CS&amp;author=Venayagamoorthy%2CGK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Bezdek, R. Ehrlich, W. Full, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="Bezdek JC, Ehrlich R, Full W (1984) FCM: the fuzzy C-means clustering algorithm. Comput Geosci 10(2–3):191–203" /><p class="c-article-references__text" id="ref-CR6">Bezdek JC, Ehrlich R, Full W (1984) FCM: the fuzzy C-means clustering algorithm. Comput Geosci 10(2–3):191–203</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0098-3004%2884%2990020-7" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=FCM%3A%20the%20fuzzy%20C-means%20clustering%20algorithm&amp;journal=Comput%20Geosci&amp;volume=10&amp;issue=2%E2%80%933&amp;pages=191-203&amp;publication_year=1984&amp;author=Bezdek%2CJC&amp;author=Ehrlich%2CR&amp;author=Full%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput Graph 32(4)" /><p class="c-article-references__text" id="ref-CR7">Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput Graph 32(4):60–63</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F307710.307730" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Put%20that%20where%3F%20Voice%20and%20gesture%20at%20the%20graphics%20interface&amp;journal=SIGGRAPH%20Comput%20Graph&amp;volume=32&amp;issue=4&amp;pages=60-63&amp;publication_year=1998&amp;author=Billinghurst%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolt RA (1980) Put that-there: voice and gestures at the graphics interface. In: SIGGRAPH 80: 7th annual confe" /><p class="c-article-references__text" id="ref-CR8">Bolt RA (1980) Put that-there: voice and gestures at the graphics interface. In: SIGGRAPH 80: 7th annual conference on computer graphics and interactive techniques. ACM Press, New York, pp 262–270</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Bowman, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Bowman D (2002) Principles for the design of performance-oriented interaction techniques. In: Stanney KM (ed) " /><p class="c-article-references__text" id="ref-CR9">Bowman D (2002) Principles for the design of performance-oriented interaction techniques. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawerence Erlabum Associates, Hillsdale, pp 201–207</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20virtual%20environments%3A%20design%2C%20implementation%2C%20and%20applications&amp;pages=201-207&amp;publication_year=2002&amp;author=Bowman%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingAR-tips: gesture based direct manipulation in aug" /><p class="c-article-references__text" id="ref-CR10">Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingAR-tips: gesture based direct manipulation in augmented reality. In: GRAPHITE’04, 2nd international conference on graphics and interactive techniques in Australis and South East Asia. ACM Press, New York, pp 212–221</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Candamo, M. Shreve, DB. Goldgof, DB. Sapper, R. Kasturi, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Candamo J, Shreve M, Goldgof DB, Sapper DB, Kasturi R (2010) Understanding transit scenes: a survey on human b" /><p class="c-article-references__text" id="ref-CR11">Candamo J, Shreve M, Goldgof DB, Sapper DB, Kasturi R (2010) Understanding transit scenes: a survey on human behavior recognition algorithms. IEEE Trans Intell Transp Syst 11(1):206–224</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTITS.2009.2030963" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20transit%20scenes%3A%20a%20survey%20on%20human%20behavior%20recognition%20algorithms&amp;journal=IEEE%20Trans%20Intell%20Transp%20Syst&amp;volume=11&amp;issue=1&amp;pages=206-224&amp;publication_year=2010&amp;author=Candamo%2CJ&amp;author=Shreve%2CM&amp;author=Goldgof%2CDB&amp;author=Sapper%2CDB&amp;author=Kasturi%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cedras, M. Shah, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Cedras C, Shah M (1995) Motion based recognition: a survey. Image Vis Comput 13(2):129–155" /><p class="c-article-references__text" id="ref-CR12">Cedras C, Shah M (1995) Motion based recognition: a survey. Image Vis Comput 13(2):129–155</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0262-8856%2895%2993154-K" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Motion%20based%20recognition%3A%20a%20survey&amp;journal=Image%20Vis%20Comput&amp;volume=13&amp;issue=2&amp;pages=129-155&amp;publication_year=1995&amp;author=Cedras%2CC&amp;author=Shah%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Chaquet, EJ. Carmona, A. Fernandez-Caballero, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chaquet JM, Carmona EJ, Fernandez-Caballero A (2013) A survey of video datasets for human action and activity " /><p class="c-article-references__text" id="ref-CR13">Chaquet JM, Carmona EJ, Fernandez-Caballero A (2013) A survey of video datasets for human action and activity recognition. Comput Vis Image Underst 117(6):633–659</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2013.01.013" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20video%20datasets%20for%20human%20action%20and%20activity%20recognition&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=117&amp;issue=6&amp;pages=633-659&amp;publication_year=2013&amp;author=Chaquet%2CJM&amp;author=Carmona%2CEJ&amp;author=Fernandez-Caballero%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FS. Chen, CM. Fu, CL. Huang, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov m" /><p class="c-article-references__text" id="ref-CR14">Chen FS, Fu CM, Huang CL (2003) Hand gesture recognition using a real-time tracking method and hidden Markov models. Image Vis Comput 21:745–758</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0262-8856%2803%2900070-2" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20using%20a%20real-time%20tracking%20method%20and%20hidden%20Markov%20models&amp;journal=Image%20Vis%20Comput&amp;volume=21&amp;pages=745-758&amp;publication_year=2003&amp;author=Chen%2CFS&amp;author=Fu%2CCM&amp;author=Huang%2CCL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Chen, ND. Georganas, EM. Petriu, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chen Q, Georganas ND, Petriu EM (2008) Hand gesture recognition using Haar-like features and a stochastic cont" /><p class="c-article-references__text" id="ref-CR15">Chen Q, Georganas ND, Petriu EM (2008) Hand gesture recognition using Haar-like features and a stochastic context-free grammar. IEEE Trans Instrum Meas 57(8):1562–1571</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIM.2008.922070" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20using%20Haar-like%20features%20and%20a%20stochastic%20context-free%20grammar&amp;journal=IEEE%20Trans%20Instrum%20Meas&amp;volume=57&amp;issue=8&amp;pages=1562-1571&amp;publication_year=2008&amp;author=Chen%2CQ&amp;author=Georganas%2CND&amp;author=Petriu%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Chen, H. Wei, J. Ferryman, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chen L, Wei H, Ferryman J (2013) A survey of human motion analysis using depth imagery. Pattern Recognit Lett " /><p class="c-article-references__text" id="ref-CR16">Chen L, Wei H, Ferryman J (2013) A survey of human motion analysis using depth imagery. Pattern Recognit Lett 34(15):1995–2006</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patrec.2013.02.006" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20human%20motion%20analysis%20using%20depth%20imagery&amp;journal=Pattern%20Recognit%20Lett&amp;volume=34&amp;issue=15&amp;pages=1995-2006&amp;publication_year=2013&amp;author=Chen%2CL&amp;author=Wei%2CH&amp;author=Ferryman%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Chong, J. Huang, S. Pan, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Chong Y, Huang J, Pan S (2016) Hand Gesture recognition using appearance features based on 3D point cloud. J S" /><p class="c-article-references__text" id="ref-CR17">Chong Y, Huang J, Pan S (2016) Hand Gesture recognition using appearance features based on 3D point cloud. J Softw Eng Appl 9:103–111</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4236%2Fjsea.2016.94009" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20Gesture%20recognition%20using%20appearance%20features%20based%20on%203D%20point%20cloud&amp;journal=J%20Softw%20Eng%20Appl&amp;volume=9&amp;pages=103-111&amp;publication_year=2016&amp;author=Chong%2CY&amp;author=Huang%2CJ&amp;author=Pan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chung KYC (2010) Facial expression recognition by using class mean gabor responses with kernel principal compo" /><p class="c-article-references__text" id="ref-CR18">Chung KYC (2010) Facial expression recognition by using class mean gabor responses with kernel principal component analysis M.Sc Thesis, Russ College of Engineering and Technology, Ohio University, USA, pp 1–69</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chung WK, Wu X, Xu Y (2009) A real-time hand gesture recognition based on haar wavelet representation. In: Pro" /><p class="c-article-references__text" id="ref-CR19">Chung WK, Wu X, Xu Y (2009) A real-time hand gesture recognition based on haar wavelet representation. In: Proceedings of the IEEE international conference on robotics and biometrics (ROBIO’08), Bangkok, Thailand, pp 336–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Conte, P. Foggia, C. Sansone, M. Vento, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Conte D, Foggia P, Sansone C, Vento M (2004) Thirty years of graph matching in pattern recognition. Int J Patt" /><p class="c-article-references__text" id="ref-CR20">Conte D, Foggia P, Sansone C, Vento M (2004) Thirty years of graph matching in pattern recognition. Int J Pattern Recognit Artif Intell 18(3):265–298</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1142%2FS0218001404003228" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Thirty%20years%20of%20graph%20matching%20in%20pattern%20recognition&amp;journal=Int%20J%20Pattern%20Recognit%20Artif%20Intell&amp;volume=18&amp;issue=3&amp;pages=265-298&amp;publication_year=2004&amp;author=Conte%2CD&amp;author=Foggia%2CP&amp;author=Sansone%2CC&amp;author=Vento%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Cristani, R. Raghavendra, A. Bue, V. Murino, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Cristani M, Raghavendra R, Del Bue A, Murino V (2013) Human behavior analysis in video surveillance: a social " /><p class="c-article-references__text" id="ref-CR21">Cristani M, Raghavendra R, Del Bue A, Murino V (2013) Human behavior analysis in video surveillance: a social signal processing perspective. Neuro Comput 100:86–97</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20behavior%20analysis%20in%20video%20surveillance%3A%20a%20social%20signal%20processing%20perspective&amp;journal=Neuro%20Comput&amp;volume=100&amp;pages=86-97&amp;publication_year=2013&amp;author=Cristani%2CM&amp;author=Raghavendra%2CR&amp;author=Bue%2CA&amp;author=Murino%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NH. Dardas, ND. Georganas, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Dardas NH, Georganas ND (2011) Real-time hand gesture detection and recognition using bag-of-features and supp" /><p class="c-article-references__text" id="ref-CR22">Dardas NH, Georganas ND (2011) Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques. IEEE Trans Instrum Meas 60(11):3592–3607</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIM.2011.2161140" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20hand%20gesture%20detection%20and%20recognition%20using%20bag-of-features%20and%20support%20vector%20machine%20techniques&amp;journal=IEEE%20Trans%20Instrum%20Meas&amp;volume=60&amp;issue=11&amp;pages=3592-3607&amp;publication_year=2011&amp;author=Dardas%2CNH&amp;author=Georganas%2CND">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recogniti" /><p class="c-article-references__text" id="ref-CR23">Dominio F, Donadeo M, Zanuttigh P (2014) Combining multiple depth-based descriptors for hand gesture recognition. Pattern Recognit Lett 101–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Elmezai M, Al-Hamadi A, Krell G, El-Etriby S, Michaelis B (2007) Gesture recognition for alphabets from hand m" /><p class="c-article-references__text" id="ref-CR24">Elmezai M, Al-Hamadi A, Krell G, El-Etriby S, Michaelis B (2007) Gesture recognition for alphabets from hand motion trajectory using hidden markov models. In: Proceeding of IEEE international symposium on signal processing and information technologies</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Fels, GE. Hinton, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Fels SS, Hinton GE (1993) Glove-talk: a neural network interface between a data-glove and a speech synthesizer" /><p class="c-article-references__text" id="ref-CR25">Fels SS, Hinton GE (1993) Glove-talk: a neural network interface between a data-glove and a speech synthesizer. IEEE Trans Neural Netw 4(1):2–8. doi:<a href="https://doi.org/10.1109/72.182690">10.1109/72.182690</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F72.182690" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Glove-talk%3A%20a%20neural%20network%20interface%20between%20a%20data-glove%20and%20a%20speech%20synthesizer&amp;journal=IEEE%20Trans%20Neural%20Netw&amp;doi=10.1109%2F72.182690&amp;volume=4&amp;issue=1&amp;pages=2-8&amp;publication_year=1993&amp;author=Fels%2CSS&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Fels, GE. Hinton, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Fels SS, Hinton GE (1998) Glove-talk: a neural network interface which maps gestures to parallel formant speec" /><p class="c-article-references__text" id="ref-CR26">Fels SS, Hinton GE (1998) Glove-talk: a neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Trans Neural Netw 9(1):205–212. doi:<a href="https://doi.org/10.1109/72.655042">10.1109/72.655042</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F72.655042" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Glove-talk%3A%20a%20neural%20network%20interface%20which%20maps%20gestures%20to%20parallel%20formant%20speech%20synthesizer%20controls&amp;journal=IEEE%20Trans%20Neural%20Netw&amp;doi=10.1109%2F72.655042&amp;volume=9&amp;issue=1&amp;pages=205-212&amp;publication_year=1998&amp;author=Fels%2CSS&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Zhiquan. Feng, Bo. Yang, Yuehui. Chen, Yanwei. Zheng, Xu. Tao, Yi. Li, Xu. Ting, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extracti" /><p class="c-article-references__text" id="ref-CR27">Feng Zhiquan, Yang Bo, Chen Yuehui, Zheng Yanwei, Tao Xu, Li Yi, Ting Xu (2011) Deliang Zhu. Features extraction from hand images based on new detection operators, Pattern Recognit, pp 1089–1105</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deliang%20Zhu&amp;pages=1089-1105&amp;publication_year=2011&amp;author=Feng%2CZhiquan&amp;author=Yang%2CBo&amp;author=Chen%2CYuehui&amp;author=Zheng%2CYanwei&amp;author=Tao%2CXu&amp;author=Li%2CYi&amp;author=Ting%2CXu">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. Foxlin, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Foxlin E (2002) Motion tracking requirements and technologies. In: Stanney KM (ed) Handbook of virtual environ" /><p class="c-article-references__text" id="ref-CR28">Foxlin E (2002) Motion tracking requirements and technologies. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawrence Erlbaum Associates, Hillsdale, pp 163–210</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20virtual%20environments%3A%20design%2C%20implementation%2C%20and%20applications&amp;pages=163-210&amp;publication_year=2002&amp;author=Foxlin%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Freeman WT, Roth M (1995) Orientation histograms for hand gesture recognition. In: IEEE international workshop" /><p class="c-article-references__text" id="ref-CR29">Freeman WT, Roth M (1995) Orientation histograms for hand gesture recognition. In: IEEE international workshop on automatic face and gesture recognition, Zurich</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gabbard J (1997) A taxonomy of usability characteristics in virtual environments. Master’s thesis, Department " /><p class="c-article-references__text" id="ref-CR30">Gabbard J (1997) A taxonomy of usability characteristics in virtual environments. Master’s thesis, Department of Computer Science, University of Western Australia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DM. Gavrila, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Gavrila DM (1999) The visual analysis of human movement: a survey. Comput Vis Image Underst 73(1):82–98" /><p class="c-article-references__text" id="ref-CR31">Gavrila DM (1999) The visual analysis of human movement: a survey. Comput Vis Image Underst 73(1):82–98</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcviu.1998.0716" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0924.68174" aria-label="View reference 31 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20analysis%20of%20human%20movement%3A%20a%20survey&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=73&amp;issue=1&amp;pages=82-98&amp;publication_year=1999&amp;author=Gavrila%2CDM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Ge, Y. Yang, TH. Lee, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Ge SS, Yang Y, Lee TH (2008) Hand gesture recognition and tracking based on distributed locally linear embeddi" /><p class="c-article-references__text" id="ref-CR32">Ge SS, Yang Y, Lee TH (2008) Hand gesture recognition and tracking based on distributed locally linear embedding. Image Vis Comput 26:1607–1620</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2008.03.004" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20and%20tracking%20based%20on%20distributed%20locally%20linear%20embedding&amp;journal=Image%20Vis%20Comput&amp;volume=26&amp;pages=1607-1620&amp;publication_year=2008&amp;author=Ge%2CSS&amp;author=Yang%2CY&amp;author=Lee%2CTH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Guo, A. Lai, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Guo G, Lai A (2014) A survey on still image based human action recognition. Pattern Recognit 47:3343–3361" /><p class="c-article-references__text" id="ref-CR33">Guo G, Lai A (2014) A survey on still image based human action recognition. Pattern Recognit 47:3343–3361</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2014.04.018" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20still%20image%20based%20human%20action%20recognition&amp;journal=Pattern%20Recognit&amp;volume=47&amp;pages=3343-3361&amp;publication_year=2014&amp;author=Guo%2CG&amp;author=Lai%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd inte" /><p class="c-article-references__text" id="ref-CR34">Gupta A, Sehrawat VK, Khosla M (2012) FPGA based real time human hand gesture recognition system. In: 2nd international conference on communication, computing and security, pp 98–107</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heap T, Hogg D (1996) Towards 3D hand tracking using a deformable model. In: Proceeding IEEE 2nd international" /><p class="c-article-references__text" id="ref-CR35">Heap T, Hogg D (1996) Towards 3D hand tracking using a deformable model. In: Proceeding IEEE 2nd international conference on automatic face and gesture recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Holte MB, Tran C, Trivedi MM, Moeslund TB (2011) Human action recognition using multiple view: a comparative p" /><p class="c-article-references__text" id="ref-CR36">Holte MB, Tran C, Trivedi MM, Moeslund TB (2011) Human action recognition using multiple view: a comparative perspective on recent developments. In: Proceedings of the joint ACM workshop on human gesture and behavior understanding, pp 47–52</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Hu, T. Tan, L. Wangs, S. Maybank, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Hu W, Tan T, Wangs L, Maybank S (2004) A survey on visual surveillance of object motion and behaviors. IEEE Tr" /><p class="c-article-references__text" id="ref-CR37">Hu W, Tan T, Wangs L, Maybank S (2004) A survey on visual surveillance of object motion and behaviors. IEEE Trans Syst Man Cybern C Appl Rev 34(3):334–352</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCC.2004.829274" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20visual%20surveillance%20of%20object%20motion%20and%20behaviors&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20C%20Appl%20Rev&amp;volume=34&amp;issue=3&amp;pages=334-352&amp;publication_year=2004&amp;author=Hu%2CW&amp;author=Tan%2CT&amp;author=Wangs%2CL&amp;author=Maybank%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Huang Z et al (2010) Study of sign language recognition based on Gabor wavelet transforms. In: International c" /><p class="c-article-references__text" id="ref-CR38">Huang Z et al (2010) Study of sign language recognition based on Gabor wavelet transforms. In: International conference on computer design and applications</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DY. Huang, WC. Hu, SH. Chang, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Huang DY, Hu WC, Chang SH (2011) Gabor filter-based hand pose angle estimation for hand gesture recognition un" /><p class="c-article-references__text" id="ref-CR39">Huang DY, Hu WC, Chang SH (2011) Gabor filter-based hand pose angle estimation for hand gesture recognition under varying illumination. Expert Syst Appl 38(5):6031–6042</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.eswa.2010.11.016" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gabor%20filter-based%20hand%20pose%20angle%20estimation%20for%20hand%20gesture%20recognition%20under%20varying%20illumination&amp;journal=Expert%20Syst%20Appl&amp;volume=38&amp;issue=5&amp;pages=6031-6042&amp;publication_year=2011&amp;author=Huang%2CDY&amp;author=Hu%2CWC&amp;author=Chang%2CSH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ibarguren A, Maurtua I, Sierra B (2010) Layered architecture for real time sign recognition: hand gesture and " /><p class="c-article-references__text" id="ref-CR40">Ibarguren A, Maurtua I, Sierra B (2010) Layered architecture for real time sign recognition: hand gesture and movement. J Eng Appl Artif Intell 1216–1228</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jain AK, Duta N (1999) Deformable matching of hand shapes for verification. In: Proceedings of international c" /><p class="c-article-references__text" id="ref-CR41">Jain AK, Duta N (1999) Deformable matching of hand shapes for verification. In: Proceedings of international conference on image processing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jain AK, Ross A, Pankanti S (1999) A prototype hand geometry based verification system. In: Proceedings of 2nd" /><p class="c-article-references__text" id="ref-CR42">Jain AK, Ross A, Pankanti S (1999) A prototype hand geometry based verification system. In: Proceedings of 2nd international conference on audio and video based biometric person authentication, pp 166–171</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YB. Jemaa, S. Khanfir, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Jemaa YB, Khanfir S (2009) Automatic local Gabor features extraction for face recognition. Int J Comput Sci In" /><p class="c-article-references__text" id="ref-CR43">Jemaa YB, Khanfir S (2009) Automatic local Gabor features extraction for face recognition. Int J Comput Sci Inf Secur 3:1–7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20local%20Gabor%20features%20extraction%20for%20face%20recognition&amp;journal=Int%20J%20Comput%20Sci%20Inf%20Secur&amp;volume=3&amp;pages=1-7&amp;publication_year=2009&amp;author=Jemaa%2CYB&amp;author=Khanfir%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Ji, H. Liu, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ji X, Liu H (2010) Advances in view-invariant human motion analysis: a review. IEEE Trans Syst Man Cybern C Ap" /><p class="c-article-references__text" id="ref-CR44">Ji X, Liu H (2010) Advances in view-invariant human motion analysis: a review. IEEE Trans Syst Man Cybern C Appl Rev 40(1):13–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20view-invariant%20human%20motion%20analysis%3A%20a%20review&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20C%20Appl%20Rev&amp;volume=40&amp;issue=1&amp;pages=13-24&amp;publication_year=2010&amp;author=Ji%2CX&amp;author=Liu%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Joshi A, Monnier C, Betke M, Sclaroff S (2016) Comparing random forest approaches to segmenting and classifyin" /><p class="c-article-references__text" id="ref-CR45">Joshi A, Monnier C, Betke M, Sclaroff S (2016) Comparing random forest approaches to segmenting and classifying gestures. J Image Vis Comput 1–10. doi:<a href="https://doi.org/10.1016/j.imavis.2016.06.001">10.1016/j.imavis.2016.06.001</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Just, S. Marcel, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Just A, Marcel S (2009) A comparative study of two-state-of-the art sequence processing techniques for hand ge" /><p class="c-article-references__text" id="ref-CR46">Just A, Marcel S (2009) A comparative study of two-state-of-the art sequence processing techniques for hand gesture recognition. Comput Vis Image Underst 113(4):532–543</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2008.12.001" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20two-state-of-the%20art%20sequence%20processing%20techniques%20for%20hand%20gesture%20recognition&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=113&amp;issue=4&amp;pages=532-543&amp;publication_year=2009&amp;author=Just%2CA&amp;author=Marcel%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Karami, B. Zanj, AK. Sarkaleh, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Karami A, Zanj B, Sarkaleh AK (2011) Persian sign language (PSL) recognition using wavelet transform and neura" /><p class="c-article-references__text" id="ref-CR47">Karami A, Zanj B, Sarkaleh AK (2011) Persian sign language (PSL) recognition using wavelet transform and neural networks. Expert Syst Appl 38:2661–2667</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.eswa.2010.08.056" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Persian%20sign%20language%20%28PSL%29%20recognition%20using%20wavelet%20transform%20and%20neural%20networks&amp;journal=Expert%20Syst%20Appl&amp;volume=38&amp;pages=2661-2667&amp;publication_year=2011&amp;author=Karami%2CA&amp;author=Zanj%2CB&amp;author=Sarkaleh%2CAK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Keskin C, Erkan A, Akarun L (2003) Real time hand tracking and 3D gesture recognition for interactive interfac" /><p class="c-article-references__text" id="ref-CR48">Keskin C, Erkan A, Akarun L (2003) Real time hand tracking and 3D gesture recognition for interactive interface using HMM. In: Proceedings of international conference on artificial neural networks</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Khaled, SG. Sayed, ESM. Saad, H. Ali, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Khaled H, Sayed SG, Saad ESM, Ali H (2015) Hand gesture recognition using modified 1$ and background subtracti" /><p class="c-article-references__text" id="ref-CR49">Khaled H, Sayed SG, Saad ESM, Ali H (2015) Hand gesture recognition using modified 1$ and background subtraction algorithms. J Math Probl Eng 2015:1–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20using%20modified%201%24%20and%20background%20subtraction%20algorithms&amp;journal=J%20Math%20Probl%20Eng&amp;volume=2015&amp;pages=1-8&amp;publication_year=2015&amp;author=Khaled%2CH&amp;author=Sayed%2CSG&amp;author=Saad%2CESM&amp;author=Ali%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NC. Kiliboz, U. Gudukbay, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Kiliboz NC, Gudukbay U (2015) A hand gesture recognition technique for human–computer interaction. J Vis Commu" /><p class="c-article-references__text" id="ref-CR50">Kiliboz NC, Gudukbay U (2015) A hand gesture recognition technique for human–computer interaction. J Vis Commun Image Recognit 28:97–104</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jvcir.2015.01.015" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20hand%20gesture%20recognition%20technique%20for%20human%E2%80%93computer%20interaction&amp;journal=J%20Vis%20Commun%20Image%20Recognit&amp;volume=28&amp;pages=97-104&amp;publication_year=2015&amp;author=Kiliboz%2CNC&amp;author=Gudukbay%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Kim, J. Song, D. Kim, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Kim D, Song J, Kim D (2007) Simultaneous gesture segmentation and recognition based on forward spotting accumu" /><p class="c-article-references__text" id="ref-CR51">Kim D, Song J, Kim D (2007) Simultaneous gesture segmentation and recognition based on forward spotting accumulative HMMs. Pattern Recognit 40(11):3012–3026</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2007.02.010" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1118.68625" aria-label="View reference 51 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20gesture%20segmentation%20and%20recognition%20based%20on%20forward%20spotting%20accumulative%20HMMs&amp;journal=Pattern%20Recognit&amp;volume=40&amp;issue=11&amp;pages=3012-3026&amp;publication_year=2007&amp;author=Kim%2CD&amp;author=Song%2CJ&amp;author=Kim%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kohler M, Schroter S (1998) A survey of video-based gesture recognition: stereo and mono systems. Technical re" /><p class="c-article-references__text" id="ref-CR52">Kohler M, Schroter S (1998) A survey of video-based gesture recognition: stereo and mono systems. Technical report no. 693/1998, Informatik VII, University of Dortmund</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Koike, Y. Sato, Y. Kobayashi, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Koike H, Sato Y, Kobayashi Y (2001) Integrating paper and digital information on enhanced desk: a method for r" /><p class="c-article-references__text" id="ref-CR53">Koike H, Sato Y, Kobayashi Y (2001) Integrating paper and digital information on enhanced desk: a method for real time finger tracking on an augmented desk system. ACM Trans Hum Comput Interact 8(4):307–322</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F504704.504706" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrating%20paper%20and%20digital%20information%20on%20enhanced%20desk%3A%20a%20method%20for%20real%20time%20finger%20tracking%20on%20an%20augmented%20desk%20system&amp;journal=ACM%20Trans%20Hum%20Comput%20Interact&amp;volume=8&amp;issue=4&amp;pages=307-322&amp;publication_year=2001&amp;author=Koike%2CH&amp;author=Sato%2CY&amp;author=Kobayashi%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kolsch M, Turk M (2004) Robust hand detection. In: 6th IEEE international conference on automatic face and ges" /><p class="c-article-references__text" id="ref-CR54">Kolsch M, Turk M (2004) Robust hand detection. In: 6th IEEE international conference on automatic face and gesture recognition, vol 614</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: CHI’94:" /><p class="c-article-references__text" id="ref-CR55">Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: CHI’94: conference companion on human factors in computing systems. ACM Press, New York, pp 453–454</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="OD. Lara, MA. Labrador, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Lara OD, Labrador MA (2013) A survey on human activity recognition using wearable sensors. IEEE Commun Surv Tu" /><p class="c-article-references__text" id="ref-CR56">Lara OD, Labrador MA (2013) A survey on human activity recognition using wearable sensors. IEEE Commun Surv Tutor 15(3):1192–1209</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FSURV.2012.110112.00192" aria-label="View reference 56">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20human%20activity%20recognition%20using%20wearable%20sensors&amp;journal=IEEE%20Commun%20Surv%20Tutor&amp;volume=15&amp;issue=3&amp;pages=1192-1209&amp;publication_year=2013&amp;author=Lara%2COD&amp;author=Labrador%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LaViola JJ Jr (1999) A survey of hand posture and gesture recognition and technology. Master thesis, NSF Scien" /><p class="c-article-references__text" id="ref-CR57">LaViola JJ Jr (1999) A survey of hand posture and gesture recognition and technology. Master thesis, NSF Science and Technology Center for Computer Graphics and Scientific Visualization, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YL. Lay, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Lay YL (2000) Hand shape recognition. Opt Laser Technol 32(1):1–5" /><p class="c-article-references__text" id="ref-CR58">Lay YL (2000) Hand shape recognition. Opt Laser Technol 32(1):1–5</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=797039" aria-label="View reference 58 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0030-3992%2899%2900105-X" aria-label="View reference 58">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20shape%20recognition&amp;journal=Opt%20Laser%20Technol&amp;volume=32&amp;issue=1&amp;pages=1-5&amp;publication_year=2000&amp;author=Lay%2CYL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KH. Lee, JH. Kim, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Lee KH, Kim JH (1999) An HMM based threshold model approach for gesture recognition. IEEE Trans Pattern Anal M" /><p class="c-article-references__text" id="ref-CR59">Lee KH, Kim JH (1999) An HMM based threshold model approach for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(10):961–973</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.799904" aria-label="View reference 59">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20HMM%20based%20threshold%20model%20approach%20for%20gesture%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=21&amp;issue=10&amp;pages=961-973&amp;publication_year=1999&amp;author=Lee%2CKH&amp;author=Kim%2CJH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Lee, Y. Nakamura, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Lee D, Nakamura Y (2014) Motion recognition and recovery from occluded monocular observations. J Robot Auton S" /><p class="c-article-references__text" id="ref-CR60">Lee D, Nakamura Y (2014) Motion recognition and recovery from occluded monocular observations. J Robot Auton Syst 62:818–832</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.robot.2014.02.002" aria-label="View reference 60">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Motion%20recognition%20and%20recovery%20from%20occluded%20monocular%20observations&amp;journal=J%20Robot%20Auton%20Syst&amp;volume=62&amp;pages=818-832&amp;publication_year=2014&amp;author=Lee%2CD&amp;author=Nakamura%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lenman S, Bretzner L, Thuresson B (2002) Using marking menus to develop command sets for computer vision based" /><p class="c-article-references__text" id="ref-CR61">Lenman S, Bretzner L, Thuresson B (2002) Using marking menus to develop command sets for computer vision based hand gesture interfaces. In: NordiCHI’02: second nordic conference on human computer interaction. ACM Press, New York, pp 239–242</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Letessier J, Berard F (2004) Visual tracking of bare fingers for interactive surfaces. In: UIST’04: 17th annua" /><p class="c-article-references__text" id="ref-CR62">Letessier J, Berard F (2004) Visual tracking of bare fingers for interactive surfaces. In: UIST’04: 17th annual ACM symposium on user interface software and technology. ACM Press, New York, pp 119–122</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li X (2003) Gesture recognition based on fuzzy C-means clustering algorithm. Department of Computer Science, T" /><p class="c-article-references__text" id="ref-CR63">Li X (2003) Gesture recognition based on fuzzy C-means clustering algorithm. Department of Computer Science, The University of Tennessee, Knoxville</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YT. Li, JP. Wachs, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Li YT, Wachs JP (2014) HEGM: A hierarchical elastic graph matching for hand gesture recognition. Pattern Recog" /><p class="c-article-references__text" id="ref-CR64">Li YT, Wachs JP (2014) HEGM: A hierarchical elastic graph matching for hand gesture recognition. Pattern Recognit 47(1):80–88</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2013.05.028" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=HEGM%3A%20A%20hierarchical%20elastic%20graph%20matching%20for%20hand%20gesture%20recognition&amp;journal=Pattern%20Recognit&amp;volume=47&amp;issue=1&amp;pages=80-88&amp;publication_year=2014&amp;author=Li%2CYT&amp;author=Wachs%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li C, Zhani P, Zheng S, Prabhakaran B (2004) Segmentation and recognition of multi-attribute motion sequences." /><p class="c-article-references__text" id="ref-CR65">Li C, Zhani P, Zheng S, Prabhakaran B (2004) Segmentation and recognition of multi-attribute motion sequences. In: Proceedings of the ACM multimedia conference, pp 836–843</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S-Z. Li, B. Yu, W. Wu, S-Z. Su, R-R. Ji, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Li S-Z, Yu B, Wu W, Su S-Z, Ji R-R (2015) Feature learning based on SAE–PCA network for human gesture recognit" /><p class="c-article-references__text" id="ref-CR66">Li S-Z, Yu B, Wu W, Su S-Z, Ji R-R (2015) Feature learning based on SAE–PCA network for human gesture recognition in RGBD images. J Neuro Comput 151:565–573</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Feature%20learning%20based%20on%20SAE%E2%80%93PCA%20network%20for%20human%20gesture%20recognition%20in%20RGBD%20images&amp;journal=J%20Neuro%20Comput&amp;volume=151&amp;pages=565-573&amp;publication_year=2015&amp;author=Li%2CS-Z&amp;author=Yu%2CB&amp;author=Wu%2CW&amp;author=Su%2CS-Z&amp;author=Ji%2CR-R">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Licsar, T. Sziranyi, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Licsar A, Sziranyi T (2004) Dynamic training of hand gesture recognition system. In: Kittler J, Petrou M, Nixo" /><p class="c-article-references__text" id="ref-CR67">Licsar A, Sziranyi T (2004) Dynamic training of hand gesture recognition system. In: Kittler J, Petrou M, Nixon M (eds) Proceedings of international conference on pattern recognition. ICPR, Cambridge, pp 971–974</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 67 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20international%20conference%20on%20pattern%20recognition&amp;pages=971-974&amp;publication_year=2004&amp;author=Licsar%2CA&amp;author=Sziranyi%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Licsar, T. Sziranyi, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Licsar A, Sziranyi T (2005) User-adaptive hand gesture recognition system with interactive training. Image Vis" /><p class="c-article-references__text" id="ref-CR68">Licsar A, Sziranyi T (2005) User-adaptive hand gesture recognition system with interactive training. Image Vis Comput 23:1102–1114</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2005.07.016" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=User-adaptive%20hand%20gesture%20recognition%20system%20with%20interactive%20training&amp;journal=Image%20Vis%20Comput&amp;volume=23&amp;pages=1102-1114&amp;publication_year=2005&amp;author=Licsar%2CA&amp;author=Sziranyi%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CH. Lim, E. Vats, CS. Chan, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Lim CH, Vats E, Chan CS (2015) Fuzzy human motion analysis: a review. J Pattern Recognit 48:1773–1796" /><p class="c-article-references__text" id="ref-CR69">Lim CH, Vats E, Chan CS (2015) Fuzzy human motion analysis: a review. J Pattern Recognit 48:1773–1796</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2014.11.016" aria-label="View reference 69">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fuzzy%20human%20motion%20analysis%3A%20a%20review&amp;journal=J%20Pattern%20Recognit&amp;volume=48&amp;pages=1773-1796&amp;publication_year=2015&amp;author=Lim%2CCH&amp;author=Vats%2CE&amp;author=Chan%2CCS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JF. Litchtenauer, EA. Hendriks, MJT. Reinders, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Litchtenauer JF, Hendriks EA, Reinders MJT (2008) Sign language recognition by combining statistical IDTW and " /><p class="c-article-references__text" id="ref-CR70">Litchtenauer JF, Hendriks EA, Reinders MJT (2008) Sign language recognition by combining statistical IDTW and independent classification. IEEE Trans Pattern Anal Mach Intell 30(11):2040–2046</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2008.123" aria-label="View reference 70">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sign%20language%20recognition%20by%20combining%20statistical%20IDTW%20and%20independent%20classification&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=30&amp;issue=11&amp;pages=2040-2046&amp;publication_year=2008&amp;author=Litchtenauer%2CJF&amp;author=Hendriks%2CEA&amp;author=Reinders%2CMJT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Liu, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Liu C (2004) Gabor-Based Kernel PCA with fractional power polynomial models for face recognition. IEEE Trans P" /><p class="c-article-references__text" id="ref-CR71">Liu C (2004) Gabor-Based Kernel PCA with fractional power polynomial models for face recognition. IEEE Trans Pattern Anal Mach Intell 26:10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 71 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gabor-Based%20Kernel%20PCA%20with%20fractional%20power%20polynomial%20models%20for%20face%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=26&amp;publication_year=2004&amp;author=Liu%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Liu, F. Tendick, K. Clearly, C. Kaufmann, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Liu A, Tendick F, Clearly K, Kaufmann C (2003) A survey of surgical simulation: applications, technology and e" /><p class="c-article-references__text" id="ref-CR72">Liu A, Tendick F, Clearly K, Kaufmann C (2003) A survey of surgical simulation: applications, technology and education. Presence Teleoper Virtual Environ 12(6):599–614</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474603322955905" aria-label="View reference 72">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20surgical%20simulation%3A%20applications%2C%20technology%20and%20education&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=12&amp;issue=6&amp;pages=599-614&amp;publication_year=2003&amp;author=Liu%2CA&amp;author=Tendick%2CF&amp;author=Clearly%2CK&amp;author=Kaufmann%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Malik S, Laszlo J (2004) Visual touchpad: a two-handed gestural input device. In: ICMI’04: 6th international c" /><p class="c-article-references__text" id="ref-CR73">Malik S, Laszlo J (2004) Visual touchpad: a two-handed gestural input device. In: ICMI’04: 6th international conference on multimodal interfaces. ACM Press, New York, pp 289–296</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maraqa M, Abu-Zaiter R (2008) Recognition of Arabic Sign Language (ArSL) using recurrent neural networks. In: " /><p class="c-article-references__text" id="ref-CR74">Maraqa M, Abu-Zaiter R (2008) Recognition of Arabic Sign Language (ArSL) using recurrent neural networks. In: IEEE 1st international conference on the applications of digital information and web technologies, pp 478–484. doi:<a href="https://doi.org/10.1109/ICADIWT.2008.4664396">10.1109/ICADIWT.2008.4664396</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marcel S, Bernier O (1999) Hand posture recognition in bady faced centered space. In: Proceeding of the intern" /><p class="c-article-references__text" id="ref-CR75">Marcel S, Bernier O (1999) Hand posture recognition in bady faced centered space. In: Proceeding of the international gesture workshop, Gif-sur-Yvette, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martin J, Devin V, Crowley JL (1998) Active hand tracking. In: FG’98: 3rd international conference on face &amp; g" /><p class="c-article-references__text" id="ref-CR76">Martin J, Devin V, Crowley JL (1998) Active hand tracking. In: FG’98: 3rd international conference on face &amp; gesture recognition. IEEE Computer Society, Washington, p 573</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TH. Maung, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Maung TH (2009) Real-time hand tracking and gesture recognition system using neural networks. World Acad Sci E" /><p class="c-article-references__text" id="ref-CR77">Maung TH (2009) Real-time hand tracking and gesture recognition system using neural networks. World Acad Sci Eng Technol 50:466–470</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 77 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20hand%20tracking%20and%20gesture%20recognition%20system%20using%20neural%20networks&amp;journal=World%20Acad%20Sci%20Eng%20Technol&amp;volume=50&amp;pages=466-470&amp;publication_year=2009&amp;author=Maung%2CTH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Meena S (2011) A study on hand gesture recognition technique. Master thesis, Department of Electronics and Com" /><p class="c-article-references__text" id="ref-CR78">Meena S (2011) A study on hand gesture recognition technique. Master thesis, Department of Electronics and Communication Engineering, National Institute of Technology, India</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Mitra, T. Acharya, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Mitra S, Acharya T (2003) Data mining: multimedia, soft computing, and bioinformatics. Wiley, New York" /><p class="c-article-references__text" id="ref-CR79">Mitra S, Acharya T (2003) Data mining: multimedia, soft computing, and bioinformatics. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 79 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20mining%3A%20multimedia%2C%20soft%20computing%2C%20and%20bioinformatics&amp;publication_year=2003&amp;author=Mitra%2CS&amp;author=Acharya%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Mitra, T. Acharya, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. " /><p class="c-article-references__text" id="ref-CR80">Mitra S, Acharya T (2007) Gesture recognition: a survey. IEEE Trans Syst Man Cybern C Appl Rev 37(3):311–324. doi:<a href="https://doi.org/10.1109/TSMCC">10.1109/TSMCC</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCC.2007.893280" aria-label="View reference 80">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 80 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gesture%20recognition%3A%20a%20survey&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20C%20Appl%20Rev&amp;doi=10.1109%2FTSMCC&amp;volume=37&amp;issue=3&amp;pages=311-324&amp;publication_year=2007&amp;author=Mitra%2CS&amp;author=Acharya%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mo Z, Lewis JP, Neumann U (2005) Smartcanvas: a gesture-driven intelligent drawing desk system. In: IUI’05: 10" /><p class="c-article-references__text" id="ref-CR81">Mo Z, Lewis JP, Neumann U (2005) Smartcanvas: a gesture-driven intelligent drawing desk system. In: IUI’05: 10th international conference on intelligent user interfaces. ACM Press, New York, pp 239–243</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TB. Moeslund, A. Hilton, V. Kruger, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Moeslund TB, Hilton A, Kruger V (2001) A survey of computer vision-based human motion capture. Comput Vis Imag" /><p class="c-article-references__text" id="ref-CR82">Moeslund TB, Hilton A, Kruger V (2001) A survey of computer vision-based human motion capture. Comput Vis Image Underst 81(3):231–268</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcviu.2000.0897" aria-label="View reference 82">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1011.68548" aria-label="View reference 82 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 82 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20computer%20vision-based%20human%20motion%20capture&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=81&amp;issue=3&amp;pages=231-268&amp;publication_year=2001&amp;author=Moeslund%2CTB&amp;author=Hilton%2CA&amp;author=Kruger%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TB. Moeslund, A. Hilton, V. Kruger, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Moeslund TB, Hilton A, Kruger V (2006) A survey of advances in vision-based human motion capture and analysis." /><p class="c-article-references__text" id="ref-CR83">Moeslund TB, Hilton A, Kruger V (2006) A survey of advances in vision-based human motion capture and analysis. Comput Vis Image Underst 104(2):90–126</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2006.08.002" aria-label="View reference 83">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 83 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20advances%20in%20vision-based%20human%20motion%20capture%20and%20analysis&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=104&amp;issue=2&amp;pages=90-126&amp;publication_year=2006&amp;author=Moeslund%2CTB&amp;author=Hilton%2CA&amp;author=Kruger%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Murakami K, Taguchi H (1999) Gesture recognition using recurrent neural networks. In: Proceedings of the SIGCH" /><p class="c-article-references__text" id="ref-CR84">Murakami K, Taguchi H (1999) Gesture recognition using recurrent neural networks. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp 237–242. doi:<a href="https://doi.org/10.1145/108844.108900">10.1145/108844.108900</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mutha SS, Kinage K (2015) Hand Gesture recognition using LAB thresholding technique. In: 4th post graduate con" /><p class="c-article-references__text" id="ref-CR85">Mutha SS, Kinage K (2015) Hand Gesture recognition using LAB thresholding technique. In: 4th post graduate conference (iPGCON-2015), pp 1–5</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CW. Ng, S. Ranganath, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Ng CW, Ranganath S (2002) Real-time gesture recognition system and application. Image Vis Comput 20:993–1007" /><p class="c-article-references__text" id="ref-CR86">Ng CW, Ranganath S (2002) Real-time gesture recognition system and application. Image Vis Comput 20:993–1007</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0262-8856%2802%2900113-0" aria-label="View reference 86">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 86 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20gesture%20recognition%20system%20and%20application&amp;journal=Image%20Vis%20Comput&amp;volume=20&amp;pages=993-1007&amp;publication_year=2002&amp;author=Ng%2CCW&amp;author=Ranganath%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Nguyen-Duc-Thanh, S. Lee, D. Kim, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Nguyen-Duc-Thanh N, Lee S, Kim D (2012) Two-stage hidden Markov model in gesture recognition for human robot i" /><p class="c-article-references__text" id="ref-CR87">Nguyen-Duc-Thanh N, Lee S, Kim D (2012) Two-stage hidden Markov model in gesture recognition for human robot interaction. Int J Adv Robot Syst 9:1–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.5772%2F50204" aria-label="View reference 87">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two-stage%20hidden%20Markov%20model%20in%20gesture%20recognition%20for%20human%20robot%20interaction&amp;journal=Int%20J%20Adv%20Robot%20Syst&amp;volume=9&amp;pages=1-10&amp;publication_year=2012&amp;author=Nguyen-Duc-Thanh%2CN&amp;author=Lee%2CS&amp;author=Kim%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nielsen M, Storring M, Moeslund TB, Granum E (2003) A procedure for developing intuitive and ergonomic gesture" /><p class="c-article-references__text" id="ref-CR88">Nielsen M, Storring M, Moeslund TB, Granum E (2003) A procedure for developing intuitive and ergonomic gesture interfaces for HCI. In: 5th international gesture workshop, pp 409–420</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Oden, A. Ercil, B. Buke, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Oden C, Ercil A, Buke B (2003) Combining implicit polynomials and geometric features for hand recognition. Pat" /><p class="c-article-references__text" id="ref-CR89">Oden C, Ercil A, Buke B (2003) Combining implicit polynomials and geometric features for hand recognition. Pattern Recognit Lett 24:2145–2152</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0167-8655%2803%2900087-4" aria-label="View reference 89">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 89 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20implicit%20polynomials%20and%20geometric%20features%20for%20hand%20recognition&amp;journal=Pattern%20Recognit%20Lett&amp;volume=24&amp;pages=2145-2152&amp;publication_year=2003&amp;author=Oden%2CC&amp;author=Ercil%2CA&amp;author=Buke%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented " /><p class="c-article-references__text" id="ref-CR90">Oka K, Sato Y, Koike H (2002) Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems. In: FGR’02: 5th IEEE international conference on automatic face and gesture recognition. IEEE Computer Society, Washington, p 429</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ong EJ, Bowden R (2004) A boosted classifier tree for hand shape detection. In: 6th IEEE international confere" /><p class="c-article-references__text" id="ref-CR91">Ong EJ, Bowden R (2004) A boosted classifier tree for hand shape detection. In: 6th IEEE international conference on automatic face and gesture recognition, pp 889–894</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KS. Patwardhan, SD. Roy, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Patwardhan KS, Roy SD (2007) Hand gesture modeling and recognition involving changing shapes and trajectories " /><p class="c-article-references__text" id="ref-CR92">Patwardhan KS, Roy SD (2007) Hand gesture modeling and recognition involving changing shapes and trajectories using a predictive eigen tracker. Pattern Recognit 28:329–334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patrec.2006.04.002" aria-label="View reference 92">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 92 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20modeling%20and%20recognition%20involving%20changing%20shapes%20and%20trajectories%20using%20a%20predictive%20eigen%20tracker&amp;journal=Pattern%20Recognit&amp;volume=28&amp;pages=329-334&amp;publication_year=2007&amp;author=Patwardhan%2CKS&amp;author=Roy%2CSD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VI. Pavlovic, R. Sharma, TS. Huang, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human computer interaction. " /><p class="c-article-references__text" id="ref-CR93">Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human computer interaction. IEEE Trans Pattern Anal Mach Intell 19(7):677–695</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598226" aria-label="View reference 93">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 93 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20interpretation%20of%20hand%20gestures%20for%20human%20computer%20interaction&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=677-695&amp;publication_year=1997&amp;author=Pavlovic%2CVI&amp;author=Sharma%2CR&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Pentland, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Pentland A (2000) Looking at people: sensing for ubiquitous and wearable computing. IEEE Trans Pattern Anal Ma" /><p class="c-article-references__text" id="ref-CR94">Pentland A (2000) Looking at people: sensing for ubiquitous and wearable computing. IEEE Trans Pattern Anal Mach Intell 22(1):107–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.824823" aria-label="View reference 94">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 94 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Looking%20at%20people%3A%20sensing%20for%20ubiquitous%20and%20wearable%20computing&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=1&amp;pages=107-119&amp;publication_year=2000&amp;author=Pentland%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PK. Pisharady, M. Saerbeck, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Pisharady PK, Saerbeck M (2015) Recent methods and databases in vision-based hand gesture recognition: a revie" /><p class="c-article-references__text" id="ref-CR95">Pisharady PK, Saerbeck M (2015) Recent methods and databases in vision-based hand gesture recognition: a review. J Comput Vis Image Underst 141:152–165</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2015.08.004" aria-label="View reference 95">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 95 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20methods%20and%20databases%20in%20vision-based%20hand%20gesture%20recognition%3A%20a%20review&amp;journal=J%20Comput%20Vis%20Image%20Underst&amp;volume=141&amp;pages=152-165&amp;publication_year=2015&amp;author=Pisharady%2CPK&amp;author=Saerbeck%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PK. Pisharady, P. Vadakkepat, AP. Loh, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Pisharady PK, Vadakkepat P, Loh AP (2010a) Hand posture and face recognition using a fuzzy-rough approach. Int" /><p class="c-article-references__text" id="ref-CR96">Pisharady PK, Vadakkepat P, Loh AP (2010a) Hand posture and face recognition using a fuzzy-rough approach. Int J Humanoid Robot 7(3):331–356</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1142%2FS0219843610002180" aria-label="View reference 96">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 96 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20posture%20and%20face%20recognition%20using%20a%20fuzzy-rough%20approach&amp;journal=Int%20J%20Humanoid%20Robot&amp;volume=7&amp;issue=3&amp;pages=331-356&amp;publication_year=2010&amp;author=Pisharady%2CPK&amp;author=Vadakkepat%2CP&amp;author=Loh%2CAP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pisharady PK, Vadakkepat P, Loh AP (2010) Graph matching based hand pose recognition using neuro-biologically " /><p class="c-article-references__text" id="ref-CR97">Pisharady PK, Vadakkepat P, Loh AP (2010) Graph matching based hand pose recognition using neuro-biologically inspired features. In: Proceedings of international conference on control, automation, robotics and vision, ICARCV, Singapore</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PK. Pisharady, P. Vadakkepat, AP. Loh, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Pisharady PK, Vadakkepat P, Loh AP (2013) Attention based detection and recognition of hand posture against co" /><p class="c-article-references__text" id="ref-CR98">Pisharady PK, Vadakkepat P, Loh AP (2013) Attention based detection and recognition of hand posture against complex backgrounds. Int J Comput Vis 101(3):403–419</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-012-0560-5" aria-label="View reference 98">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 98 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20based%20detection%20and%20recognition%20of%20hand%20posture%20against%20complex%20backgrounds&amp;journal=Int%20J%20Comput%20Vis&amp;volume=101&amp;issue=3&amp;pages=403-419&amp;publication_year=2013&amp;author=Pisharady%2CPK&amp;author=Vadakkepat%2CP&amp;author=Loh%2CAP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Poppe, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Poppe R (2007) Vision-based human motion analysis: an overview. Comput Vis Image Underst 108(1):4–18" /><p class="c-article-references__text" id="ref-CR99">Poppe R (2007) Vision-based human motion analysis: an overview. Comput Vis Image Underst 108(1):4–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2006.10.016" aria-label="View reference 99">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 99 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision-based%20human%20motion%20analysis%3A%20an%20overview&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=108&amp;issue=1&amp;pages=4-18&amp;publication_year=2007&amp;author=Poppe%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Poppe, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Poppe R (2010) A survey on vision-based human action recogntion. Comput Vis Image Underst 28(6):976–990" /><p class="c-article-references__text" id="ref-CR100">Poppe R (2010) A survey on vision-based human action recogntion. Comput Vis Image Underst 28(6):976–990</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2009.11.014" aria-label="View reference 100">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 100 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20vision-based%20human%20action%20recogntion&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=28&amp;issue=6&amp;pages=976-990&amp;publication_year=2010&amp;author=Poppe%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Qin, X. Zhu, Y. Yang, Y. Jiang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Qin S, Zhu X, Yang Y, Jiang Y (2014) Real-time hand gesture recognition from depth images using convex shape d" /><p class="c-article-references__text" id="ref-CR101">Qin S, Zhu X, Yang Y, Jiang Y (2014) Real-time hand gesture recognition from depth images using convex shape decomposition method. J Signal Process Syst 74:47–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11265-013-0778-7" aria-label="View reference 101">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 101 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20hand%20gesture%20recognition%20from%20depth%20images%20using%20convex%20shape%20decomposition%20method&amp;journal=J%20Signal%20Process%20Syst&amp;volume=74&amp;pages=47-58&amp;publication_year=2014&amp;author=Qin%2CS&amp;author=Zhu%2CX&amp;author=Yang%2CY&amp;author=Jiang%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Quck, D. MeNeill, R. Bryll, S. Duncan, X-F. Ma, C. Kirbas, KE. McCullough, R. Ansari, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Quck F, MeNeill D, Bryll R, Duncan S, Ma X-F, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discou" /><p class="c-article-references__text" id="ref-CR102">Quck F, MeNeill D, Bryll R, Duncan S, Ma X-F, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discourse: gesture and speech. ACM Trans Comput Hum Interact 9(3):171–193</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F568513.568514" aria-label="View reference 102">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 102 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20human%20discourse%3A%20gesture%20and%20speech&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=9&amp;issue=3&amp;pages=171-193&amp;publication_year=2002&amp;author=Quck%2CF&amp;author=MeNeill%2CD&amp;author=Bryll%2CR&amp;author=Duncan%2CS&amp;author=Ma%2CX-F&amp;author=Kirbas%2CC&amp;author=McCullough%2CKE&amp;author=Ansari%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FKH. Quek, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Quek FKH (1996) Unencumbered gestural interaction. IEEE Multimed 3(4):36–47" /><p class="c-article-references__text" id="ref-CR103">Quek FKH (1996) Unencumbered gestural interaction. IEEE Multimed 3(4):36–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F93.556459" aria-label="View reference 103">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 103 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unencumbered%20gestural%20interaction&amp;journal=IEEE%20Multimed&amp;volume=3&amp;issue=4&amp;pages=36-47&amp;publication_year=1996&amp;author=Quek%2CFKH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LR. Rabiner, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech reognition. Proc IEEE" /><p class="c-article-references__text" id="ref-CR104">Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech reognition. Proc IEEE 77(2):257–285</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5.18626" aria-label="View reference 104">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 104 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial%20on%20hidden%20Markov%20models%20and%20selected%20applications%20in%20speech%20reognition&amp;journal=Proc%20IEEE&amp;volume=77&amp;issue=2&amp;pages=257-285&amp;publication_year=1989&amp;author=Rabiner%2CLR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Ramamoorthy, N. Vaswani, S. Chaudhury, S. Banerjee, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ramamoorthy A, Vaswani N, Chaudhury S, Banerjee S (2003) Recognition of dynamic hand gestures. Pattern Recogni" /><p class="c-article-references__text" id="ref-CR105">Ramamoorthy A, Vaswani N, Chaudhury S, Banerjee S (2003) Recognition of dynamic hand gestures. Pattern Recognit 36:2069–2081</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2803%2900042-6" aria-label="View reference 105">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1035.68106" aria-label="View reference 105 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 105 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20dynamic%20hand%20gestures&amp;journal=Pattern%20Recognit&amp;volume=36&amp;pages=2069-2081&amp;publication_year=2003&amp;author=Ramamoorthy%2CA&amp;author=Vaswani%2CN&amp;author=Chaudhury%2CS&amp;author=Banerjee%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren Y, Gu C (2010) Real-time hand gesture recognition based on vision. In: Proceedings of the 5th internationa" /><p class="c-article-references__text" id="ref-CR106">Ren Y, Gu C (2010) Real-time hand gesture recognition based on vision. In: Proceedings of the 5th international conference on E-learning and games, Edutainment, Changchun, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren Z, Yuan J, Zhang Z (2011) Robust hand gesture recognition based on finger-earth mover’s distance with a co" /><p class="c-article-references__text" id="ref-CR107">Ren Z, Yuan J, Zhang Z (2011) Robust hand gesture recognition based on finger-earth mover’s distance with a commodity depth camera. In: ACM international conference on multimedia, Scottsdlae, pp 1093–1096</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ST. Roweis, LK. Saul, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Roweis ST, Saul LK (2000) Non linear dimensionality reduction by locally linear embedding. Science 290(5500):2" /><p class="c-article-references__text" id="ref-CR108">Roweis ST, Saul LK (2000) Non linear dimensionality reduction by locally linear embedding. Science 290(5500):2323–2326</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.290.5500.2323" aria-label="View reference 108">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 108 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Non%20linear%20dimensionality%20reduction%20by%20locally%20linear%20embedding&amp;journal=Science&amp;volume=290&amp;issue=5500&amp;pages=2323-2326&amp;publication_year=2000&amp;author=Roweis%2CST&amp;author=Saul%2CLK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Sanches-Reillo, C. Sanchez-Avila, A. Gonzalez-Macros, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sanches-Reillo R, Sanchez-Avila C, Gonzalez-Macros A (2000) Biometric identification through hand geometry mea" /><p class="c-article-references__text" id="ref-CR109">Sanches-Reillo R, Sanchez-Avila C, Gonzalez-Macros A (2000) Biometric identification through hand geometry measurements. IEEE Trans Pattern Anal Mach Intell 22(10):1168–1171</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.879796" aria-label="View reference 109">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 109 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Biometric%20identification%20through%20hand%20geometry%20measurements&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=10&amp;pages=1168-1171&amp;publication_year=2000&amp;author=Sanches-Reillo%2CR&amp;author=Sanchez-Avila%2CC&amp;author=Gonzalez-Macros%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Segen J, Kumar S (1998) Gesture VR: vision-based 3D hand interface for spatial interaction. In: 6th ACM intern" /><p class="c-article-references__text" id="ref-CR110">Segen J, Kumar S (1998) Gesture VR: vision-based 3D hand interface for spatial interaction. In: 6th ACM international conference on multimedia. ACM Press, New York, pp 455–464</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MC. Shin, LV. Tsap, DB. Goldgof, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using Bezier curves for visualization navigation from " /><p class="c-article-references__text" id="ref-CR111">Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using Bezier curves for visualization navigation from registered 3D data. Pattern Recognit 37(5):1011–1024</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2003.11.007" aria-label="View reference 111">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 111 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gesture%20recognition%20using%20Bezier%20curves%20for%20visualization%20navigation%20from%20registered%203D%20data&amp;journal=Pattern%20Recognit&amp;volume=37&amp;issue=5&amp;pages=1011-1024&amp;publication_year=2004&amp;author=Shin%2CMC&amp;author=Tsap%2CLV&amp;author=Goldgof%2CDB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Starner T, Pentland A (1995) Visual recognition of American sign language using hidden Markov models. In: Proc" /><p class="c-article-references__text" id="ref-CR112">Starner T, Pentland A (1995) Visual recognition of American sign language using hidden Markov models. In: Proceeding of international workshop on automatic face and gesture recognition, Zurich, Switzerland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov model" /><p class="c-article-references__text" id="ref-CR113">Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. AAAI technical report FS-96-05, The Media Laboratory Massachusetts Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stenger B, Thayananthan A, Torr P, Cipolla R (2004) Hand pose estimation using hierarchical detection. In: 8th" /><p class="c-article-references__text" id="ref-CR114">Stenger B, Thayananthan A, Torr P, Cipolla R (2004) Hand pose estimation using hierarchical detection. In: 8th European conference on computer vision workshop on human computer interaction, vol 3058, Springer, Prague, pp 102–112</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Stergiopoulou, N. Papmarkos, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng App" /><p class="c-article-references__text" id="ref-CR115">Stergiopoulou E, Papmarkos N (2009) Hand gesture recognition using a neural shape fitting technique. J Eng Appl Artif Intell 22:1141–1158</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.engappai.2009.03.008" aria-label="View reference 115">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 115 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20using%20a%20neural%20shape%20fitting%20technique&amp;journal=J%20Eng%20Appl%20Artif%20Intell&amp;volume=22&amp;pages=1141-1158&amp;publication_year=2009&amp;author=Stergiopoulou%2CE&amp;author=Papmarkos%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sturman DJ (1992) Whole hand input. Ph.D. thesis, MIT" /><p class="c-article-references__text" id="ref-CR116">Sturman DJ (1992) Whole hand input. Ph.D. thesis, MIT</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Sturman, D. Zeltzer, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39" /><p class="c-article-references__text" id="ref-CR117">Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.250916" aria-label="View reference 117">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 117 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20glove-based%20input&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=14&amp;issue=1&amp;pages=30-39&amp;publication_year=1994&amp;author=Sturman%2CDJ&amp;author=Zeltzer%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C-J. Su, C-Y. Chiang, J-Y. Huang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warpin" /><p class="c-article-references__text" id="ref-CR118">Su C-J, Chiang C-Y, Huang J-Y (2014) Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic. J Appl Soft Comput 22:652–666</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.asoc.2014.04.020" aria-label="View reference 118">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 118 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Kinect-enabled%20home-based%20rehabilitation%20system%20using%20Dynamic%20Time%20Warping%20and%20fuzzy%20logic&amp;journal=J%20Appl%20Soft%20Comput&amp;volume=22&amp;pages=652-666&amp;publication_year=2014&amp;author=Su%2CC-J&amp;author=Chiang%2CC-Y&amp;author=Huang%2CJ-Y">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HI. Suk, BK. Sin, SW. Lee, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Suk HI, Sin BK, Lee SW (2010) Hand gesture recognition based on dynamic Bayesian network framework. Pattern Re" /><p class="c-article-references__text" id="ref-CR119">Suk HI, Sin BK, Lee SW (2010) Hand gesture recognition based on dynamic Bayesian network framework. Pattern Recognit 43(9):3059–3072</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2010.03.016" aria-label="View reference 119">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1204.68172" aria-label="View reference 119 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 119 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20based%20on%20dynamic%20Bayesian%20network%20framework&amp;journal=Pattern%20Recognit&amp;volume=43&amp;issue=9&amp;pages=3059-3072&amp;publication_year=2010&amp;author=Suk%2CHI&amp;author=Sin%2CBK&amp;author=Lee%2CSW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Teng, B. Wu, W. Yu, C. Liu, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Teng X, Wu B, Yu W, Liu C (2005) A hand gesture recognition system based on locally linear embedding. J Vis La" /><p class="c-article-references__text" id="ref-CR120">Teng X, Wu B, Yu W, Liu C (2005) A hand gesture recognition system based on locally linear embedding. J Vis Lang Comput 16:442–454</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jvlc.2005.04.003" aria-label="View reference 120">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 120 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20hand%20gesture%20recognition%20system%20based%20on%20locally%20linear%20embedding&amp;journal=J%20Vis%20Lang%20Comput&amp;volume=16&amp;pages=442-454&amp;publication_year=2005&amp;author=Teng%2CX&amp;author=Wu%2CB&amp;author=Yu%2CW&amp;author=Liu%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CM. Travieso, JR. Ticay-Rivas, JC. Briceno, M. Pozo-Banos, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Travieso CM, Ticay-Rivas JR, Briceno JC, del Pozo-Banos M (2014) Hand shape identification on multirange image" /><p class="c-article-references__text" id="ref-CR121">Travieso CM, Ticay-Rivas JR, Briceno JC, del Pozo-Banos M (2014) Hand shape identification on multirange images. J Inf Sci 275:45–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ins.2014.02.031" aria-label="View reference 121">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 121 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20shape%20identification%20on%20multirange%20images&amp;journal=J%20Inf%20Sci&amp;volume=275&amp;pages=45-56&amp;publication_year=2014&amp;author=Travieso%2CCM&amp;author=Ticay-Rivas%2CJR&amp;author=Briceno%2CJC&amp;author=Pozo-Banos%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Triesch, C. Malsburg, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Triesch J, Malsburg C (2001) A system for person-independent hand posture recognition against complex backgrou" /><p class="c-article-references__text" id="ref-CR122">Triesch J, Malsburg C (2001) A system for person-independent hand posture recognition against complex backgrounds. IEEE Trans Pattern Anal Mach Intell 23(12):1449–1453</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.977568" aria-label="View reference 122">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 122 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20system%20for%20person-independent%20hand%20posture%20recognition%20against%20complex%20backgrounds&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=23&amp;issue=12&amp;pages=1449-1453&amp;publication_year=2001&amp;author=Triesch%2CJ&amp;author=Malsburg%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Turaga, R. Chellappa, VS. Subrahmanian, O. Udrea, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Turaga P, Chellappa R, Subrahmanian VS, Udrea O (2008) Machine recognition of human activities: a survey. IEEE" /><p class="c-article-references__text" id="ref-CR123">Turaga P, Chellappa R, Subrahmanian VS, Udrea O (2008) Machine recognition of human activities: a survey. IEEE Trans Circuits Syst Video Technol 18(11):1473–1488</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTCSVT.2008.2005594" aria-label="View reference 123">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 123 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20recognition%20of%20human%20activities%3A%20a%20survey&amp;journal=IEEE%20Trans%20Circuits%20Syst%20Video%20Technol&amp;volume=18&amp;issue=11&amp;pages=1473-1488&amp;publication_year=2008&amp;author=Turaga%2CP&amp;author=Chellappa%2CR&amp;author=Subrahmanian%2CVS&amp;author=Udrea%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Turk, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Turk M (2002) Gesture recognition. In: Stanney KM (ed) Handbook of virtual environments: design, implementatio" /><p class="c-article-references__text" id="ref-CR124">Turk M (2002) Gesture recognition. In: Stanney KM (ed) Handbook of virtual environments: design, implementation, and applications. Lawerence Erlbaum Associates, Hillsdale, pp 223–238</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 124 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20virtual%20environments%3A%20design%2C%20implementation%2C%20and%20applications&amp;pages=223-238&amp;publication_year=2002&amp;author=Turk%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Ueda, Y. Matsumoto, M. Imai, T. Ogasawara, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ueda E, Matsumoto Y, Imai M, Ogasawara T (2003) A hand-pose estimation for vision-based human interfaces. IEEE" /><p class="c-article-references__text" id="ref-CR125">Ueda E, Matsumoto Y, Imai M, Ogasawara T (2003) A hand-pose estimation for vision-based human interfaces. IEEE Trans Ind Electron 50(4):676–684</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIE.2003.814758" aria-label="View reference 125">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 125 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20hand-pose%20estimation%20for%20vision-based%20human%20interfaces&amp;journal=IEEE%20Trans%20Ind%20Electron&amp;volume=50&amp;issue=4&amp;pages=676-684&amp;publication_year=2003&amp;author=Ueda%2CE&amp;author=Matsumoto%2CY&amp;author=Imai%2CM&amp;author=Ogasawara%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Virtual Glove Box (VGX) (2016). http://biovis.arc.nasa.gov/vislab/vgx.htm&#xA;                        " /><p class="c-article-references__text" id="ref-CR126">Virtual Glove Box (VGX) (2016). <a href="http://biovis.arc.nasa.gov/vislab/vgx.htm">http://biovis.arc.nasa.gov/vislab/vgx.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human–computer interaction framework using skin " /><p class="c-article-references__text" id="ref-CR127">Vo N, Tran Q, Dinh TB, Dinh TB, Nguyen QM (2010) An efficient human–computer interaction framework using skin color tracking and gesture recognition. In: Proceedings of IEEE international conference on computing and Communication Technologies, Research, Innovation, and Vision for the Future, pp 978–981. doi:<a href="https://doi.org/10.1109/RIVF.2010.5633368">10.1109/RIVF.2010.5633368</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wang, W. Hu, T. Tan, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Wang L, Hu W, Tan T (2003) Recent development of human motion analysis. Pattern Recognit 36(3):585–601" /><p class="c-article-references__text" id="ref-CR128">Wang L, Hu W, Tan T (2003) Recent development of human motion analysis. Pattern Recognit 36(3):585–601</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2802%2900100-0" aria-label="View reference 128">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 128 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20development%20of%20human%20motion%20analysis&amp;journal=Pattern%20Recognit&amp;volume=36&amp;issue=3&amp;pages=585-601&amp;publication_year=2003&amp;author=Wang%2CL&amp;author=Hu%2CW&amp;author=Tan%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wang, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Wang L et al (2008) 2D Gabor face representation method for face recognition with ensemble and multichannel mo" /><p class="c-article-references__text" id="ref-CR129">Wang L et al (2008) 2D Gabor face representation method for face recognition with ensemble and multichannel model. Image Vis Comput 26:9</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 129 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=2D%20Gabor%20face%20representation%20method%20for%20face%20recognition%20with%20ensemble%20and%20multichannel%20model&amp;journal=Image%20Vis%20Comput&amp;volume=26&amp;publication_year=2008&amp;author=Wang%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Wexelblat, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):" /><p class="c-article-references__text" id="ref-CR130">Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):179–200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F210079.210080" aria-label="View reference 130">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 130 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20approach%20to%20natural%20gesture%20in%20virtual%20environments&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=2&amp;issue=3&amp;pages=179-200&amp;publication_year=1995&amp;author=Wexelblat%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Wienland, R. Ronfard, E. Boyer, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Wienland D, Ronfard R, Boyer E (2011) A survey of vision-based methods for action representation, segmentation" /><p class="c-article-references__text" id="ref-CR131">Wienland D, Ronfard R, Boyer E (2011) A survey of vision-based methods for action representation, segmentation and recognition. Comput Vis Image Underst 115(2):224–241</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2010.10.002" aria-label="View reference 131">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 131 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20vision-based%20methods%20for%20action%20representation%2C%20segmentation%20and%20recognition&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=115&amp;issue=2&amp;pages=224-241&amp;publication_year=2011&amp;author=Wienland%2CD&amp;author=Ronfard%2CR&amp;author=Boyer%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Wiskott, JM. Fellous, N. Kruger, C. Malsburg, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Wiskott L, Fellous JM, Kruger N, Malsburg C (1997) Face recognition by elastic bunch graph matching. IEEE Tran" /><p class="c-article-references__text" id="ref-CR132">Wiskott L, Fellous JM, Kruger N, Malsburg C (1997) Face recognition by elastic bunch graph matching. IEEE Trans Pattern Anal Mach Intell 19(7):775–779</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598235" aria-label="View reference 132">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 132 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Face%20recognition%20by%20elastic%20bunch%20graph%20matching&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=775-779&amp;publication_year=1997&amp;author=Wiskott%2CL&amp;author=Fellous%2CJM&amp;author=Kruger%2CN&amp;author=Malsburg%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wysoski SG (2003) A rotation invariant static hand gesture recognition system using boundary information and n" /><p class="c-article-references__text" id="ref-CR133">Wysoski SG (2003) A rotation invariant static hand gesture recognition system using boundary information and neural networks. ME thesis, Nagoya Institute of Technology, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wysoski SG, Lamar MV, Kuroyanagi S, Iwata A (2002) A rotation invariant approach on static-gesture recognition" /><p class="c-article-references__text" id="ref-CR134">Wysoski SG, Lamar MV, Kuroyanagi S, Iwata A (2002) A rotation invariant approach on static-gesture recognition using boundary histograms and neural networks. In: IEEE proceedings of the 9th international conference on neural information processing, Singapura</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xu W et al (2009) A scale and rotation invariant interest points detector based on Gabor filters. In: Slezak D" /><p class="c-article-references__text" id="ref-CR135">Xu W et al (2009) A scale and rotation invariant interest points detector based on Gabor filters. In: Slezak D, Pal S, Kang BH, Gu J, Kuroda H, Kim TH (eds) Signal processing image processing and pattern recognition. Communications in computer and information science, vol 61. Springer, Berlin, p 8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MH. Yang, N. Ahuja, M. Tabb, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Yang MH, Ahuja N, Tabb M (2002) Extraction of 2D motion trajectories and its application to hand gesture recog" /><p class="c-article-references__text" id="ref-CR136">Yang MH, Ahuja N, Tabb M (2002) Extraction of 2D motion trajectories and its application to hand gesture recognition. IEEE Trans Pattern Anal Mach Intell 24(8):1061–1074</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2002.1023803" aria-label="View reference 136">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 136 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Extraction%20of%202D%20motion%20trajectories%20and%20its%20application%20to%20hand%20gesture%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=24&amp;issue=8&amp;pages=1061-1074&amp;publication_year=2002&amp;author=Yang%2CMH&amp;author=Ahuja%2CN&amp;author=Tabb%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Yeasin, S. Chaudhuri, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Yeasin M, Chaudhuri S (2000) Visual understanding of dynamic hand gestures. Pattern Recognit 33(11):1805–1817" /><p class="c-article-references__text" id="ref-CR137">Yeasin M, Chaudhuri S (2000) Visual understanding of dynamic hand gestures. Pattern Recognit 33(11):1805–1817</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2899%2900175-2" aria-label="View reference 137">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 137 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20understanding%20of%20dynamic%20hand%20gestures&amp;journal=Pattern%20Recognit&amp;volume=33&amp;issue=11&amp;pages=1805-1817&amp;publication_year=2000&amp;author=Yeasin%2CM&amp;author=Chaudhuri%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SK. Yewale, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Yewale SK (2011) Artificial neural network approach for hand gesture recognition. Int J Eng Sci Technol IJEST " /><p class="c-article-references__text" id="ref-CR138">Yewale SK (2011) Artificial neural network approach for hand gesture recognition. Int J Eng Sci Technol IJEST 34:2603–2608</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 138 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20neural%20network%20approach%20for%20hand%20gesture%20recognition&amp;journal=Int%20J%20Eng%20Sci%20Technol%20IJEST&amp;volume=34&amp;pages=2603-2608&amp;publication_year=2011&amp;author=Yewale%2CSK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yikai F, Kongqiao W, Jian C, Hanquing L (2007) A real-time hand gesture recognition method. In: Proceeding of " /><p class="c-article-references__text" id="ref-CR139">Yikai F, Kongqiao W, Jian C, Hanquing L (2007) A real-time hand gesture recognition method. In: Proceeding of the IEEE international conference on mutlimedia and expo (ICME’07), Beijing, China, pp 995–998</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Yin, M. Xie, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Yin X, Xie M (2003) Estimation of the fundamental matrix from un-calibrated stereo hand images for 3D hand ges" /><p class="c-article-references__text" id="ref-CR140">Yin X, Xie M (2003) Estimation of the fundamental matrix from un-calibrated stereo hand images for 3D hand gesture recognition. Pattern Recognit 36:567–584</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2802%2900072-9" aria-label="View reference 140">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 140 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20of%20the%20fundamental%20matrix%20from%20un-calibrated%20stereo%20hand%20images%20for%203D%20hand%20gesture%20recognition&amp;journal=Pattern%20Recognit&amp;volume=36&amp;pages=567-584&amp;publication_year=2003&amp;author=Yin%2CX&amp;author=Xie%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HS. Yoon, J. Soh, YJ. Bae, HS. Yang, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Yoon HS, Soh J, Bae YJ, Yang HS (2001) Hand gesture recognition using combined features of location, angle and" /><p class="c-article-references__text" id="ref-CR141">Yoon HS, Soh J, Bae YJ, Yang HS (2001) Hand gesture recognition using combined features of location, angle and velocity. J Pattern Recognit 34:1491–1501</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2800%2900096-0" aria-label="View reference 141">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0985.68741" aria-label="View reference 141 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 141 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20using%20combined%20features%20of%20location%2C%20angle%20and%20velocity&amp;journal=J%20Pattern%20Recognit&amp;volume=34&amp;pages=1491-1501&amp;publication_year=2001&amp;author=Yoon%2CHS&amp;author=Soh%2CJ&amp;author=Bae%2CYJ&amp;author=Yang%2CHS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yun L, Peng Z (2009) An automatic hand gesture recognition system based on Viola–Jones method and SVM’s. In: P" /><p class="c-article-references__text" id="ref-CR142">Yun L, Peng Z (2009) An automatic hand gesture recognition system based on Viola–Jones method and SVM’s. In: Proceedings of the 2nd international workshop on computer science and engineering (WCSE’09), pp 72–76</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AA. Zaiden, NN. Ahmad, H. Abdul Karim, M. Larbani, BB. Zaidan, A. Sali, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi" /><p class="c-article-references__text" id="ref-CR143">Zaiden AA, Ahmad NN, Abdul Karim H, Larbani M, Zaidan BB, Sali A (2014) Image skin segmentation based on multi-agent learning Bayesian and neural network. Eng Appl Artif Intell 32:136–150</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.engappai.2014.03.002" aria-label="View reference 143">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 143 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20skin%20segmentation%20based%20on%20multi-agent%20learning%20Bayesian%20and%20neural%20network&amp;journal=Eng%20Appl%20Artif%20Intell&amp;volume=32&amp;pages=136-150&amp;publication_year=2014&amp;author=Zaiden%2CAA&amp;author=Ahmad%2CNN&amp;author=Abdul%20Karim%2CH&amp;author=Larbani%2CM&amp;author=Zaidan%2CBB&amp;author=Sali%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Zhao, FKH. Quek, X. Wu, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Zhao M, Quek FKH, Wu X (1998) RIEVL: recursive induction learning in hand gesture recognition. IEEE Trans Patt" /><p class="c-article-references__text" id="ref-CR144">Zhao M, Quek FKH, Wu X (1998) RIEVL: recursive induction learning in hand gesture recognition. IEEE Trans Pattern Anal Mach Intell 20(11):1174–1185</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.730553" aria-label="View reference 144">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 144 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=RIEVL%3A%20recursive%20induction%20learning%20in%20hand%20gesture%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=20&amp;issue=11&amp;pages=1174-1185&amp;publication_year=1998&amp;author=Zhao%2CM&amp;author=Quek%2CFKH&amp;author=Wu%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhou H, Lin DJ, Haung TS (2004) Static hand gesture recognition based on local orientation histogram feature d" /><p class="c-article-references__text" id="ref-CR145">Zhou H, Lin DJ, Haung TS (2004) Static hand gesture recognition based on local orientation histogram feature distribution model. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition workshops</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhu C, Sheng W (2009) Online hand gesture recognition using neural network based segmentation. In: Internation" /><p class="c-article-references__text" id="ref-CR146">Zhu C, Sheng W (2009) Online hand gesture recognition using neural network based segmentation. In: International conference on intelligent robots and systems. IEEE Publisher, pp 2415–2420</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zunkel RL (1999) Hand geometry based verification. In: Proceedings of biometrics. Kluwer Academic Publishers, " /><p class="c-article-references__text" id="ref-CR147">Zunkel RL (1999) Hand geometry based verification. In: Proceedings of biometrics. Kluwer Academic Publishers, pp 87–101</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-016-0301-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of ECE, Karunya University, Coimbatore, 641114, India</p><p class="c-article-author-affiliation__authors-list">K. Martin Sagayam &amp; D. Jude Hemanth</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-K__Martin-Sagayam"><span class="c-article-authors-search__title u-h3 js-search-name">K. Martin Sagayam</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;K. Martin+Sagayam&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=K. Martin+Sagayam" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22K. Martin+Sagayam%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-D__Jude-Hemanth"><span class="c-article-authors-search__title u-h3 js-search-name">D. Jude Hemanth</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;D. Jude+Hemanth&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=D. Jude+Hemanth" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22D. Jude+Hemanth%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-016-0301-0/email/correspondent/c1/new">D. Jude Hemanth</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Hand%20posture%20and%20gesture%20recognition%20techniques%20for%20virtual%20reality%20applications%3A%20a%20survey&amp;author=K.%20Martin%20Sagayam%20et%20al&amp;contentID=10.1007%2Fs10055-016-0301-0&amp;publication=1359-4338&amp;publicationDate=2016-11-17&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-016-0301-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-016-0301-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Sagayam, K.M., Hemanth, D.J. Hand posture and gesture recognition techniques for virtual reality applications: a survey.
                    <i>Virtual Reality</i> <b>21, </b>91–107 (2017). https://doi.org/10.1007/s10055-016-0301-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-016-0301-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-01-06">06 January 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-11-10">10 November 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-11-17">17 November 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-06">June 2017</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-016-0301-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-016-0301-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Human computer interaction (HCI)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gesture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Posture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Graphical user interface (GUI)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">HMM</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0301-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=301;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

