<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Influence of multisensory feedback on haptic accessibility tasks"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Environments of a certain nature, such as those related to maintenance tasks can benefited from haptic stimuli by performing accessibility simulation in a realistic manner. Accessibility is defined..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Influence of multisensory feedback on haptic accessibility tasks"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-04-27"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Environments of a certain nature, such as those related to maintenance tasks can benefited from haptic stimuli by performing accessibility simulation in a realistic manner. Accessibility is defined as the physical feasibility of accessing an element of a 3D model avoiding undesirable collisions. This paper studies the benefits that multisensory systems can provide in performing this kind of tasks. The research is specially focused on the improvements provided by auditory feedback to the user&#8217;s performance. We have carried out a user study where participants had to perform an accessibility task with the aid of different combinations of sensorial stimuli. A large haptic interface for aeronautic maintainability has been extended with real-time sound generation capabilities to study this issue. The results of these experiments show that auditory stimuli provide with useful cues to the users helping them to correct trajectories and hence improving their performance."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-04-27"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="31"/>

    <meta name="prism.endingPage" content="40"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0028-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0028-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0028-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0028-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Influence of multisensory feedback on haptic accessibility tasks"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2006/05"/>

    <meta name="citation_online_date" content="2006/04/27"/>

    <meta name="citation_firstpage" content="31"/>

    <meta name="citation_lastpage" content="40"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0028-4"/>

    <meta name="DOI" content="10.1007/s10055-006-0028-4"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0028-4"/>

    <meta name="description" content="Environments of a certain nature, such as those related to maintenance tasks can benefited from haptic stimuli by performing accessibility simulation in a "/>

    <meta name="dc.creator" content="I&#241;aki D&#237;az"/>

    <meta name="dc.creator" content="Josune Hernantes"/>

    <meta name="dc.creator" content="Ignacio Mansa"/>

    <meta name="dc.creator" content="Alberto Lozano"/>

    <meta name="dc.creator" content="Diego Borro"/>

    <meta name="dc.creator" content="Jorge Juan Gil"/>

    <meta name="dc.creator" content="Emilio S&#225;nchez"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: Proceedings of the IEEE virtual reality annual international symposium, pp 203&#8211;210"/>

    <meta name="citation_reference" content="Adelstein BD, Begault DR, Anderson MR, Wenzel EM (2003) Sensitivity to haptic-audio asynchrony. In: Proceedings of the 5th international conference on multimodal interfaces, Vancouver, Canada, pp 73&#8211;76 DOI 10.1145/958432.958448"/>

    <meta name="citation_reference" content="Boff KR, Lincoln JE (1988) Engineering data compendium: human perception and performance. Wright-Patterson Air Force Base, Harry G. Armstrong Aerospace Medical Research Laboratory, Ohio"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=A large haptic device for aircraft engine maintainability; citation_author=D Borro, J Savall, A Amundarain, JJ Gil, A Garc&#237;a-Alonso, L Matey; citation_volume=24; citation_issue=6; citation_publication_date=2004a; citation_pages=70-74; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Approximation of optimal voxel size for collision detection in maintainability simulations within massive virtual environments; citation_author=D Borro, A Garc&#237;a-Alonso, L Matey; citation_volume=23; citation_issue=1; citation_publication_date=2004b; citation_pages=13-23; citation_id=CR5"/>

    <meta name="citation_reference" content="Colgate JE, Schenkel G (1994) Passivity of a class of sampled-data systems: application to haptic interfaces. In: Proceedings of the American control conference, Baltimore, MD, pp 3236&#8211;3240"/>

    <meta name="citation_reference" content="DiFilippo D (2000) The AHI: an audio and haptic interface for simulating contact interactions. Master&#8217;s thesis, University of British Columbia"/>

    <meta name="citation_reference" content="DiFilippo D, Pai DK (2000) The AHI: an audio and haptic interface for contact interactions. In: Proceedings of the 13th annual ACM symposium on user interface software and technology, San Diego, California, USA, pp 149&#8211;158 DOI 10.1145/354401.354437"/>

    <meta name="citation_reference" content="DiFranco DE, Beauregard GL, Srinivasan MA (1997) The effect of auditory cues on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1997 ASME international mechanical engineering congress and exposition, Dallas, TX, USA, pp 17&#8211;22"/>

    <meta name="citation_reference" content="van den Doel K (1998) Sound synthesis for virtual reality and computer games. Ph. D. thesis, University of British Columbia"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators Virtual Environ; citation_title=The sounds of physical shapes; citation_author=K Doel, DK Pai; citation_volume=7; citation_issue=4; citation_publication_date=1998; citation_pages=382-395; citation_doi=10.1162/105474698565794; citation_id=CR11"/>

    <meta name="citation_reference" content="van den Doel K, Pai DK (2001) JASS: a java audio synthesis system for programmers. In: Proceedings of the 2001 international conference on auditory display, Espoo, Finland, pp 150&#8211;154"/>

    <meta name="citation_reference" content="van den Doel K, Kry PG, Pai DK (2001) FoleyAutomatic: Physically-based sound effects for interactive simulation and animation. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 537&#8211;544 DOI 10.1145/383259.383322"/>

    <meta name="citation_reference" content="Garc&#237;a-Alonso A, Gil JJ, Borro D (2005) Interfaces for VR applications development in design. In: Proceedings of the virtual concept 2005, Biarritz, France, p 109"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Control Syst Technol; citation_title=Stability analysis of a 1 DOF haptic interface using the Routh&#8211;Hurwitz criterion; citation_author=JJ Gil, A Avello, &#193; Rubio, J Fl&#243;rez; citation_volume=12; citation_issue=4; citation_publication_date=2004; citation_pages=583-588; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res; citation_title=Audiotactile interactions in roughness perception; citation_author=S Guest, C Catmur, D Lloyd, C Spence; citation_volume=146; citation_publication_date=2002; citation_pages=161-171; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Visual and tactual texture perception: intersensory cooperation; citation_author=MA Heller; citation_volume=31; citation_issue=4; citation_publication_date=1982; citation_pages=339-344; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Tutorial: time-multiplexed stereoscopic computer graphics; citation_author=LF Hodges; citation_volume=12; citation_issue=2; citation_publication_date=1992; citation_pages=20-30; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators Virtual Environ; citation_title=Geometric considerations for stereoscopic virtual environments; citation_author=LF Hodges, ET Davis; citation_volume=2; citation_issue=1; citation_publication_date=1993; citation_pages=34-43; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=Auditory texture perception; citation_author=SJ Lederman; citation_volume=8; citation_publication_date=1979; citation_pages=93-103; citation_doi=10.1068/p080093; citation_id=CR20"/>

    <meta name="citation_reference" content="Levitin DJ, MacLean K, Mathews M, Chu LY, Jensen ER (2000) The perception of cross-modal simultaneity. In: Proceedings of International Journal of Computing Anticipatory Systems, vol 517 (1), pp 323&#8211;329 DOI 10.1063/1.1291270"/>

    <meta name="citation_reference" content="Mansa I, Amundarain A, Hernantes J, Garc&#237;a-Alonso A, Borro D (2006) Occlusion culling for dense geometric scenarios. Proceedings of the Laval Virtual 2006, Laval, France (in press)"/>

    <meta name="citation_reference" content="Mc Gee MR, Gray PD, Brewster SA (2000) The effective combination of haptic and auditory textural information. In: Proceedings of the first international workshop on haptic human&#8211;computer interaction, Lecture Notes in Computer Science, Springer, 2058:118&#8211;126"/>

    <meta name="citation_reference" content="Mc Gee MR, Gray PD, Brewster SA (2001) Feeling rough: multimodal perception of virtual roughness. In: Proceedings of the 1st Eurohaptics conference, Birmingham, UK, pp 29&#8211;33"/>

    <meta name="citation_reference" content="O&#8217;Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 529&#8211;536 DOI 10.1145/383259.383321"/>

    <meta name="citation_reference" content="Pai DK (2005) Multisensory interaction: real and virtual, In: P Dario P, Chatila R (eds) Robotics research: the eleventh international symposium, vol 15. Springer tracts in advanced robotics. Springer, Berlin Heidelberg New York, pp 489&#8211;500 DOI 10.1007/11008941_52"/>

    <meta name="citation_reference" content="Poling GL, Weisenberger JM, Kerwin K (2003) The role of multisensory feedback in haptic surface perception. In: Proceedings of the 11th annual symposium on haptic interfaces for virtual environments and teleoperator systems, Los Angeles, CA, pp 187&#8211;194 DOI 10.1109/HAPTIC.2003.1191271"/>

    <meta name="citation_reference" content="Richmond JL, Pai DK (2000) Active measurement of contact sounds. In: Proceedings of the 2000 IEEE international conference on robotics and automation, San Francisco, CA, USA, pp 2146&#8211;2152 DOI 10.1109/ROBOT.2000.846346"/>

    <meta name="citation_reference" content="Salisbury JK, Brock DL, Massie TH, Swarup N, Zilles C (1995) Haptic rendering: programming touch interaction with virtual objects. In: Proceedings of the 1995 symposium on interactive 3D graphics, Monterey, CA, USA, pp 123&#8211;130 DOI 10.1145/199404.199426"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Inst Radio Eng; citation_title=Communication in the presence of noise; citation_author=CE Shannon; citation_volume=37; citation_issue=1; citation_publication_date=1949; citation_pages=10-21; citation_id=CR30"/>

    <meta name="citation_reference" content="Srinivasan MA, Beauregard GL, Brock DL (1996) The impact of visual information on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1996 ASME international mechanical engineering congress and exposition, Atlanta, Georgia, USA, pp 555&#8211;559"/>

    <meta name="citation_reference" content="Stein BE, Meredith MA (1993) The merging of the senses. MIT Press, Cambridge"/>

    <meta name="citation_reference" content="Veron H, Southard DA, Leger JR, Conway JL (1990) Stereoscopic displays for terrain database visualization. Proc Stereosc Displays Appl 124&#8211;135"/>

    <meta name="citation_reference" content="citation_journal_title=ASME Dynam Syst Control Div; citation_title=Visual, haptic, and bimodal perception of size and stiffness in virtual environments; citation_author=W Wu, C Basdogan, MA Srinivasan; citation_volume=67; citation_publication_date=1999; citation_pages=19-26; citation_id=CR34"/>

    <meta name="citation_reference" content="Yano H, Iwata H (2001) Software architecture for audio and haptic rendering based on a physical model. In: Proceedings of the 8th IFIP TC13 conference on human&#8211;computer interaction, Tokyo, Japan, pp 19&#8211;26"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Limits of fusion and depth judgment in stereoscopic color displays; citation_author=YY Yeh, LD Silverstein; citation_volume=32; citation_issue=1; citation_publication_date=1990; citation_pages=45-60; citation_id=CR36"/>

    <meta name="citation_author" content="I&#241;aki D&#237;az"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Josune Hernantes"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Ignacio Mansa"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Alberto Lozano"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Diego Borro"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Jorge Juan Gil"/>

    <meta name="citation_author_email" content="jjgil@ceit.es"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_author" content="Emilio S&#225;nchez"/>

    <meta name="citation_author_institution" content="CEIT,  San Sebasti&#225;n, Spain"/>

    <meta name="citation_author_institution" content="TECNUN, University of Navarra, San Sebasti&#225;n, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0028-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/05/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0028-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Influence of multisensory feedback on haptic accessibility tasks"/>
        <meta property="og:description" content="Environments of a certain nature, such as those related to maintenance tasks can benefited from haptic stimuli by performing accessibility simulation in a realistic manner. Accessibility is defined as the physical feasibility of accessing an element of a 3D model avoiding undesirable collisions. This paper studies the benefits that multisensory systems can provide in performing this kind of tasks. The research is specially focused on the improvements provided by auditory feedback to the user’s performance. We have carried out a user study where participants had to perform an accessibility task with the aid of different combinations of sensorial stimuli. A large haptic interface for aeronautic maintainability has been extended with real-time sound generation capabilities to study this issue. The results of these experiments show that auditory stimuli provide with useful cues to the users helping them to correct trajectories and hence improving their performance."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Influence of multisensory feedback on haptic accessibility tasks | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0028-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Haptics, Accessibility, Force feedback, Auditory, Virtual environments, Synchronization, Multisensory interaction, Multimodal","kwrd":["Haptics","Accessibility","Force_feedback","Auditory","Virtual_environments","Synchronization","Multisensory_interaction","Multimodal"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0028-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0028-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=28;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0028-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Influence of multisensory feedback on haptic accessibility tasks
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0028-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0028-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-04-27" itemprop="datePublished">27 April 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Influence of multisensory feedback on haptic accessibility tasks</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-I_aki-D_az" data-author-popup="auth-I_aki-D_az">Iñaki Díaz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Josune-Hernantes" data-author-popup="auth-Josune-Hernantes">Josune Hernantes</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ignacio-Mansa" data-author-popup="auth-Ignacio-Mansa">Ignacio Mansa</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Alberto-Lozano" data-author-popup="auth-Alberto-Lozano">Alberto Lozano</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Diego-Borro" data-author-popup="auth-Diego-Borro">Diego Borro</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jorge_Juan-Gil" data-author-popup="auth-Jorge_Juan-Gil" data-corresp-id="c1">Jorge Juan Gil<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Emilio-S_nchez" data-author-popup="auth-Emilio-S_nchez">Emilio Sánchez</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CEIT" /><meta itemprop="address" content="grid.13822.3a, 0000000106601972, CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="TECNUN, University of Navarra" /><meta itemprop="address" content="grid.5924.a, 0000000419370271, TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">31</span>–<span itemprop="pageEnd">40</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">180 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0028-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Environments of a certain nature, such as those related to maintenance tasks can benefited from haptic stimuli by performing accessibility simulation in a realistic manner. Accessibility is defined as the physical feasibility of accessing an element of a 3D model avoiding undesirable collisions. This paper studies the benefits that multisensory systems can provide in performing this kind of tasks. The research is specially focused on the improvements provided by auditory feedback to the user’s performance. We have carried out a user study where participants had to perform an accessibility task with the aid of different combinations of sensorial stimuli. A large haptic interface for aeronautic maintainability has been extended with real-time sound generation capabilities to study this issue. The results of these experiments show that auditory stimuli provide with useful cues to the users helping them to correct trajectories and hence improving their performance.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Humans are able to perceive the environment using all their senses. Usually sight is the predominant sense, although some of the other senses are also needed to perform most tasks. Sometimes, it is necessary to perceive the environment in more detail and all our senses are unconsciously used to obtain the information we need. For instance, maintainability studies need of accessibility testing to verify whether each part of the model is accessible or not. Obviously, a visual test is not enough to detect possible inaccessible parts. In addition to this, the worker has to explore and manipulate the model or different parts of it to complete the assessment, thus more than one sense is necessary.</p><p>Providing users with the natural ability to use all their senses in a simulation environment is an important goal in the Virtual Reality research area. Within this context, haptic devices are used to provide us with force feedback in domains where it is needed, such is the case of the accessibility studies. Using these devices, interactivity is enhanced. Furthermore, the simulation efficiency can be improved by making users aware of other physical characteristics of the objects, such as the weight or surface smoothness. However, haptic technology is quite recent and as a result the tactile realism obtained is not as good as it would be desirable. In addition, rigid bodies cannot be haptically simulated as rigid as they appear in reality due to mechanical and control constraints (Gil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Gil JJ, Avello A, Rubio Á, Flórez J (2004) Stability analysis of a 1 DOF haptic interface using the Routh–Hurwitz criterion. IEEE Trans Control Syst Technol 12(4):583–588 DOI 10.1109/TCST.2004.825134" href="/article/10.1007/s10055-006-0028-4#ref-CR15" id="ref-link-section-d81609e378">2004</a>; Colgate and Schenkel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Colgate JE, Schenkel G (1994) Passivity of a class of sampled-data systems: application to haptic interfaces. In: Proceedings of the American control conference, Baltimore, MD, pp 3236–3240" href="/article/10.1007/s10055-006-0028-4#ref-CR6" id="ref-link-section-d81609e381">1994</a>) and the sensation of real surface textures in virtual environments still remains very complex. In an attempt to improve the overall perception, we can help users by adding sound to haptic systems. In reality, if we hear a person hitting an object with a hammer, we can deduce how much strength was used without really having to experience the impact force. Thus, adding sound to haptic applications seems like a valid hypothesis in improving the haptic perception.</p><p>The research in this paper is intended to validate this hypothesis for a specific problem with haptic interaction: accessibility. This problem relates to the accuracy to guide the virtual haptic representation through a three-dimensional (3D) model. Users must avoid colliding with the virtual model. However, when the collision is unavoidable, users will receive adequate feedback (visual, auditory and haptic) to be able to correct the trajectory. This paper presents the experiments made with a haptic system in order to prove the enhancements in performing accessibility tasks thanks to multisensory feedback. Even more precisely, it sets focus on the advantages that auditory feedback can offer to help users recover from collisions.</p><p>The remainder of this article is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec2">2</a> presents other related work on the influence of multisensory interaction in haptic systems. Later on, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec3">3</a>, we address the problems to do with accessibility using haptic interfaces. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec4">4</a> describes the multisensory (visual, haptic and auditory) system implemented for our experiments. The different parts in our haptic system are explained and some points on haptic, visual and audio rendering are made and reviewed. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec11">5</a>, we present the experiments performed and the methodology used, while results are discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec14">6</a>. Finally, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0028-4#Sec15">7</a> some conclusions are drawn.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The first Virtual Reality systems were mainly based on visual stimuli. In order to immerse the user, stereoscopic techniques were developed. Several studies prove the benefits of using these techniques to enhance the visual perception. Stereoscopic visualization contributes to understand spatial relationships and discriminate objects from ground (Yeh and Silverstein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Yeh YY, Silverstein LD (1990) Limits of fusion and depth judgment in stereoscopic color displays. Hum Factors 32(1):45–60" href="/article/10.1007/s10055-006-0028-4#ref-CR36" id="ref-link-section-d81609e416">1990</a>). For instance, in the design phase of sheet metal parts, stereo visualization helped engineers to correct 75% of their mistakes (Veron et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Veron H, Southard DA, Leger JR, Conway JL (1990) Stereoscopic displays for terrain database visualization. Proc Stereosc Displays Appl 124–135" href="/article/10.1007/s10055-006-0028-4#ref-CR33" id="ref-link-section-d81609e419">1990</a>). In many cases, the interaction with the virtual environment helps users to accomplish more accurate movements. The use of stereo improves both accuracy and time reaction in tasks such as navigation, manipulation or even educational training.</p><p>Through the years, new stimuli have been introduced in virtual environments in an attempt to improve the overall perception. There are several studies that have dealt with multisensory interaction in haptic systems. In Pai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Pai DK (2005) Multisensory interaction: real and virtual, In: P Dario P, Chatila R (eds) Robotics research: the eleventh international symposium, vol 15. Springer tracts in advanced robotics. Springer, Berlin Heidelberg New York, pp 489–500 DOI 10.1007/11008941_52" href="/article/10.1007/s10055-006-0028-4#ref-CR26" id="ref-link-section-d81609e425">2005</a>), a good introduction to multisensory processing is given and in Stein and Meredith (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Stein BE, Meredith MA (1993) The merging of the senses. MIT Press, Cambridge" href="/article/10.1007/s10055-006-0028-4#ref-CR32" id="ref-link-section-d81609e428">1993</a>), multisensory perception is studied more widely. A full example of a haptic system with audio interaction is presented in DiFilippo and Pai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="DiFilippo D, Pai DK (2000) The AHI: an audio and haptic interface for contact interactions. In: Proceedings of the 13th annual ACM symposium on user interface software and technology, San Diego, California, USA, pp 149–158 DOI 10.1145/354401.354437" href="/article/10.1007/s10055-006-0028-4#ref-CR8" id="ref-link-section-d81609e431">2000</a>).</p><p>Some studies have analysed in particular, the benefits of adding auditory feedback to haptic systems. DiFranco et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="DiFranco DE, Beauregard GL, Srinivasan MA (1997) The effect of auditory cues on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1997 ASME international mechanical engineering congress and exposition, Dallas, TX, USA, pp 17–22" href="/article/10.1007/s10055-006-0028-4#ref-CR9" id="ref-link-section-d81609e437">1997</a>) showed how auditory stimuli of pre-recorded tapping sounds influenced on the stiffness perception of the virtual objects. In Mc Gee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Mc Gee MR, Gray PD, Brewster SA (2001) Feeling rough: multimodal perception of virtual roughness. In: Proceedings of the 1st Eurohaptics conference, Birmingham, UK, pp 29–33" href="/article/10.1007/s10055-006-0028-4#ref-CR24" id="ref-link-section-d81609e440">2001</a>), a combination of haptic and auditory feedback was proposed as a solution to increase the quantity and quality of available textural information. In the same field, Guest et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Guest S, Catmur C, Lloyd D, Spence C (2002) Audiotactile interactions in roughness perception. Exp Brain Res 146:161–171 DOI 10.1007/s00221-002-1164–z" href="/article/10.1007/s10055-006-0028-4#ref-CR16" id="ref-link-section-d81609e443">2002</a>) suggested that users could be influenced by auditory input on surface textural perception. Apart from auditory stimuli, visual cues had also been used to alter haptic perception. For instance, Srinivasan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Srinivasan MA, Beauregard GL, Brock DL (1996) The impact of visual information on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1996 ASME international mechanical engineering congress and exposition, Atlanta, Georgia, USA, pp 555–559" href="/article/10.1007/s10055-006-0028-4#ref-CR31" id="ref-link-section-d81609e446">1996</a>) showed how visual stimuli could modify the stiffness perception of a virtual spring.</p><p>However, coupling three sensory modalities in a haptic application does not guarantee a better performance on its own. On the one hand, synchronization and latency problems are commonly faced, since each modality is processed individually and then coupled together. In this area, several studies have tried to establish a valid threshold for the asynchrony between audio and haptic stimuli. Aldestein et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Adelstein BD, Begault DR, Anderson MR, Wenzel EM (2003) Sensitivity to haptic-audio asynchrony. In: Proceedings of the 5th international conference on multimodal interfaces, Vancouver, Canada, pp 73–76 DOI 10.1145/958432.958448" href="/article/10.1007/s10055-006-0028-4#ref-CR2" id="ref-link-section-d81609e452">2003</a>) established a threshold that lies between 18 and 25 ms as the JND (Just Noticeable Difference) for the asynchrony between haptic and audio feedback, whereas for Levitin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Levitin DJ, MacLean K, Mathews M, Chu LY, Jensen ER (2000) The perception of cross-modal simultaneity. In: Proceedings of International Journal of Computing Anticipatory Systems, vol 517 (1), pp 323–329 DOI 10.1063/1.1291270" href="/article/10.1007/s10055-006-0028-4#ref-CR21" id="ref-link-section-d81609e455">2000</a>), it was about 42 ms. In DiFilippo and Pai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="DiFilippo D, Pai DK (2000) The AHI: an audio and haptic interface for contact interactions. In: Proceedings of the 13th annual ACM symposium on user interface software and technology, San Diego, California, USA, pp 149–158 DOI 10.1145/354401.354437" href="/article/10.1007/s10055-006-0028-4#ref-CR8" id="ref-link-section-d81609e458">2000</a>), 2 ms was established as a valid lower latency threshold for the synchronization of haptic and audio rendering in collision events.</p><p>On the other hand, cross-modal effects can also appear and reduce performance when several sensory modalities are coupled together. Numerous studies have analysed whether multimodal presentation of stimuli improves or decrements performance. McGee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Mc Gee MR, Gray PD, Brewster SA (2000) The effective combination of haptic and auditory textural information. In: Proceedings of the first international workshop on haptic human–computer interaction, Lecture Notes in Computer Science, Springer, 2058:118–126" href="/article/10.1007/s10055-006-0028-4#ref-CR23" id="ref-link-section-d81609e465">2000</a>) described the different ways (conflicting, redundant, and complementary) in which sound and haptic information may be integrated to influence the user’s perception. While some authors (Lederman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Lederman SJ (1979) Auditory texture perception. Perception 8:93–103" href="/article/10.1007/s10055-006-0028-4#ref-CR20" id="ref-link-section-d81609e468">1979</a>; Wu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Wu W, Basdogan C, Srinivasan MA (1999) Visual, haptic, and bimodal perception of size and stiffness in virtual environments. ASME Dynam Syst Control Div 67:19–26" href="/article/10.1007/s10055-006-0028-4#ref-CR34" id="ref-link-section-d81609e471">1999</a>) suggest that one stimulus becomes dominant to the others depending on the tasks; others (Heller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Heller MA (1982) Visual and tactual texture perception: intersensory cooperation. Percept Psychophys 31(4):339–344" href="/article/10.1007/s10055-006-0028-4#ref-CR17" id="ref-link-section-d81609e474">1982</a>; Poling et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Poling GL, Weisenberger JM, Kerwin K (2003) The role of multisensory feedback in haptic surface perception. In: Proceedings of the 11th annual symposium on haptic interfaces for virtual environments and teleoperator systems, Los Angeles, CA, pp 187–194 DOI 10.1109/HAPTIC.2003.1191271" href="/article/10.1007/s10055-006-0028-4#ref-CR27" id="ref-link-section-d81609e477">2003</a>) suggest that multimodal inputs improve perception. As it can be noticed, there is still much research to be done in this field. Therefore, it is very important to take all these considerations into account in the development of multisensory systems in order to guarantee the desired performance.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Accessibility problems in haptic systems</h2><div class="c-article-section__content" id="Sec3-content"><p>In this paper, a study about the influence of multisensory processing is carried out for the specific case of accessibility tasks. These tasks can be found in different fields such as surgery training or maintainability. For instance, in a simulation of a surgical procedure on a kidney, it is essential to find the most suitable path to reach the damaged area without harming other organs. Another well-known field where accessibility must be analysed in detail is in maintainability tasks over mechanical systems. In this context, the models used are complex engines with a limited workspace. Hence, it is usually difficult to reach a specific component due to the high number of elements (wires, pipes and harnesses) which can be found in the way (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig1">1</a>). Therefore, it is necessary to find a suitable path in order to access the part to assemble or disassemble. All this brings in what it is known as accessibility.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Sequence of a virtual disassembly operation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>As it can be deduced from the previous examples, there are many problems in performing these accessibility tasks accurately. Using merely visual feedback, users do not receive enough information for a correct fulfilment of these tasks, since they can penetrate inside rigid objects of the environment. As a result, the trajectories and guidance are not realistic enough. The need to increase realism and interaction motivates the integration of an additional sense. Therefore, the touch sense using haptic devices is added. Thanks to force feedback, the user can correct the trajectory in the access to some parts of the virtual environment. However, the haptic performance obtained with current haptic devices is not as realistic as it would be desirable yet. Hence, in present work, the aim is to analyse whether the auditory feedback would enhance accuracy in accessibility tasks and help to correct the trajectory, as it happens when force feedback is used. In the real world, when a collision between two bodies happens, the sound heard by humans has implicit information such as the localization of the impact, the material of the object or its size. Sometimes, it is also possible to describe the snapshot of the action relying only on sound. Therefore, the addition of the auditory sense could be well motivated to help users to get more information about the virtual haptic environment and it should be used in applications where the user’s reaction time becomes critical. Furthermore, it gives instant awareness of collision events and helps users to correct the trajectory and hence improving the overall performance.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">System overview</h2><div class="c-article-section__content" id="Sec4-content"><p>Our experiments have been carried out using a large haptic device called LHIfAM (Borro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Borro D, Savall J, Amundarain A, Gil JJ, García-Alonso A, Matey L (2004a) A large haptic device for aircraft engine maintainability. IEEE Comput Graph Appl 24(6):70–74 DOI 10.1109/MCG.2004.45" href="/article/10.1007/s10055-006-0028-4#ref-CR4" id="ref-link-section-d81609e519">2004a</a>). It allows users to perform training and aircraft maintainability tasks in a virtual environment. The user interacts and manipulates mechanical assemblies with a virtual tool handled by the haptic device (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig2">2</a>). In this way, users can validate assembly–disassembly sequences in the product design phase. This evaluation is achieved studying the accessibility of the objects, i.e. one object is not accessible if it cannot be reached by means of the haptic device.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>User interaction with our multisensory system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Recently, auditory feedback has been added to the system in order to study multisensory interaction in accessibility simulations. In the following subsections, the different system components are briefly described, focusing on those modules involved in the generation of sensorial feedback. The method used to create the sound feedback and other approaches are also presented. Finally, the importance of a suitable synchronization of audio, visual and tactile stimuli is discussed, as well as the architecture developed to accomplish it.</p><h3 class="c-article__sub-heading" id="Sec5">Haptic device</h3><p>The LHIfAM is a floor-grounded haptic interface with large workspace especially designed for aeronautic maintainability applications (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig3">3</a>). It provides force feedback in three translational degrees of freedom, while three additional orientations are measured, but not actuated, by a compact wrist.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Workspace of the LHIfAM</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>An interesting design feature of this haptic system is that its workspace can be relocated. Therefore, it is possible to reproduce different maintainability operations and check different ergonomic situations. Thanks to its workspace relocation and mechanical design, it can be used not only into the aeronautical field but also over any kind of mechanical system.</p><h3 class="c-article__sub-heading" id="Sec6">Graphic rendering module</h3><p>A key issue to perform accessibility tasks is to avoid unnecessary collisions with the 3D model when the user is accessing to an element of the model. Therefore, a proper visualization of both the 3D environment and the virtual representation of worker’s tools are essential. As the initial system was designed for aircraft maintainability tasks, the visual module was conceived to render large and compact models. These CAD models can easily rise up to several millions of polygons. Deficient or not real-time visualization of such large models would influence negatively on the user’s accessing tasks. Hence, several optimization techniques were integrated. In addition to well-known optimizations such as culling and level of details methods, a specific occlusion culling algorithm was developed (Mansa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Mansa I, Amundarain A, Hernantes J, García-Alonso A, Borro D (2006) Occlusion culling for dense geometric scenarios. Proceedings of the Laval Virtual 2006, Laval, France (in press)" href="/article/10.1007/s10055-006-0028-4#ref-CR22" id="ref-link-section-d81609e583">2006</a>).</p><p>Performing accessibility tasks requires an accurate and natural representation of spatial information, thus only real-time visualization is not enough for this purpose. A stereoscopic visualization is available to provide users a more realistic experience and a better understanding of the 3D world. Several factors contribute to assess the quality of the stereo visualization. One of them is the binocular disparity (Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Hodges LF (1992) Tutorial: time-multiplexed stereoscopic computer graphics. IEEE Comput Graph Appl 12(2):20–30 DOI 10.1109/38.124285" href="/article/10.1007/s10055-006-0028-4#ref-CR18" id="ref-link-section-d81609e589">1992</a>). The generation of stereo is achieved by creating two views of the 3D scene. These views are quite similar; they only differ in their horizontal position. Another important factor is related to the screen parallax (Hodges and Davis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Hodges LF, Davis ET (1993) Geometric considerations for stereoscopic virtual environments. Presence: Teleoperators Virtual Environ 2(1):34–43" href="/article/10.1007/s10055-006-0028-4#ref-CR19" id="ref-link-section-d81609e592">1993</a>). Objects in stereo can appear in front of the display screen (negative parallax) or beyond it (positive parallax). Objects visualized with positive parallax are seen as if users were looking through a window. On the other hand, using negative parallax objects emerge from the screen. In our case, the visualization of compact models is more suitable using a salient perception (negative parallax) that immerses users and abstracts them from the real world.</p><h3 class="c-article__sub-heading" id="Sec7">Collision detection module</h3><p>The collision detection module is another main module responsible for providing appropriate feedback to the user. It detects all the collisions among the objects of the virtual scene using an uniform spatial grid decomposition based on voxels. The collision method developed (Borro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004b" title="Borro D, García-Alonso A, Matey L (2004b) Approximation of optimal voxel size for collision detection in maintainability simulations within massive virtual environments. Comput Graph Forum 23(1):13–23 DOI 10.1111/j.1467-8659.2004.00002.x" href="/article/10.1007/s10055-006-0028-4#ref-CR5" id="ref-link-section-d81609e604">2004b</a>), is essentially focused on huge and compact models which contain millions of polygons in a few cubic meters. And after computing collisions among objects in the virtual environment, this module computes the contact response.</p><p>The force response consists of computing the interaction force between the avatar and the virtual objects when a collision is detected. This force must approximate as closely as possible the contact forces that users would normally feel during contact among real objects. In order to compute this force, the collision module collects the information necessary to determine the correct interaction, i.e, penetrations and normal vectors, taking into account the user’s movement. This information is sent to the haptic rendering module, which computes the final force and applies it on the user.</p><h3 class="c-article__sub-heading" id="Sec8">Haptic rendering module</h3><p>The second type of sensory feedback provided to the users is the haptic feedback. The haptic loop has two main tasks. Firstly, it acquires the position and orientation of the tracking device and secondly, it calculates the force which is restored to the user taking into account the information sent by the collision module. As the collision module and the haptic rendering module have different frequencies, interpolation and extrapolation techniques are used to couple both modules. More technical details can be found in García-Alonso et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="García-Alonso A, Gil JJ, Borro D (2005) Interfaces for VR applications development in design. In: Proceedings of the virtual concept 2005, Biarritz, France, p 109" href="/article/10.1007/s10055-006-0028-4#ref-CR14" id="ref-link-section-d81609e617">2005</a>).</p><p>The sampling rate of the haptic module is an important factor to achieve a realistic contact sensation. Since the mechanoreceptors in our fingertips detect signals up to 1 kHz, with the highest sensitivity around 300 Hz (Boff and Lincoln <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Boff KR, Lincoln JE (1988) Engineering data compendium: human perception and performance. Wright-Patterson Air Force Base, Harry G. Armstrong Aerospace Medical Research Laboratory, Ohio" href="/article/10.1007/s10055-006-0028-4#ref-CR3" id="ref-link-section-d81609e623">1988</a>), the sampling rate is set to 2 kHz in the LHIfAM.</p><p>Usability has been a main issue in the design of the haptic device. As explained before, LHIfAM is the device handled by the user. Its dimensions can make uncomfortable its manipulation what would be a serious problem to perform accesibility tasks. In order to avoid this disturbing problem and to achieve a higher transparency and therefore a better usability, an impedance force law was implemented in the LHIfAM. A force feedforward loop was added in order to decrease the inertia of this device along its translational degree-of-freedom.</p><h3 class="c-article__sub-heading" id="Sec9">Audio rendering module</h3><p>This module has been developed to analyse the benefits that auditory feedback provides to the haptic interaction. Two approaches were initially considered to generate this feedback: pre-recorded sounds and real-time generation of sounds.</p><p>The creation of a sound effect is the first step in providing audio to an application. Sound effects can be recorded or created by a programmer. Recorded sounds are real sounds that will be played back later in an application when a similar event takes place. To achieve realistic contact sounds, the sound emitted should provide the user with different information, such as the localization of the contact, the material of the colliding objects or the impact force applied. This fact implies that for the combination of different objects, contact points and applied forces, there should be a pre-recorded sound to be played back at the right time. Obviously, it is not worth building such a sound library for a simple tap between two objects. Thus, it becomes necessary the second approach to generate auditory feedback in real-time: a sound model of high flexibility in order to generate or manipulate the sound in real-time, taking into account the actions performed.</p><p>Recent studies have addressed this problem and have focused on formulating a physical model of contact sounds (van den Doel and Pai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="van den Doel K, Pai DK (2001) JASS: a java audio synthesis system for programmers. In: Proceedings of the 2001 international conference on auditory display, Espoo, Finland, pp 150–154" href="/article/10.1007/s10055-006-0028-4#ref-CR12" id="ref-link-section-d81609e640">2001</a>; van den Doel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="van den Doel K, Kry PG, Pai DK (2001) FoleyAutomatic: Physically-based sound effects for interactive simulation and animation. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 537–544 DOI 10.1145/383259.383322" href="/article/10.1007/s10055-006-0028-4#ref-CR13" id="ref-link-section-d81609e643">2001</a>). The aim has been to model them by a mathematical parameterized expression that could generate, as well as manipulate, the sound emitted. To solve this question, the natural production of sound has been taken into account. The collision between two objects leads to the vibration of their outer surfaces. These vibrations create pressure waves in the air, which are sensed and perceived by the human ear. The main idea is to simulate these waves through a physical sound model.</p><p>In the last years, there have been two main approaches to build that physical model of sounds. The first one is based on FEM simulation (Yano and Iwata <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yano H, Iwata H (2001) Software architecture for audio and haptic rendering based on a physical model. In: Proceedings of the 8th IFIP TC13 conference on human–computer interaction, Tokyo, Japan, pp 19–26" href="/article/10.1007/s10055-006-0028-4#ref-CR35" id="ref-link-section-d81609e649">2001</a>; O’Brien et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 529–536 DOI 10.1145/383259.383321" href="/article/10.1007/s10055-006-0028-4#ref-CR25" id="ref-link-section-d81609e652">2001</a>) whilst the other one on modal synthesis (van den Doel and Pai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="van den Doel K, Pai DK (1998) The sounds of physical shapes. Presence: Teleoperators Virtual Environ 7(4):382–395" href="/article/10.1007/s10055-006-0028-4#ref-CR11" id="ref-link-section-d81609e655">1998</a>). However, it is quite difficult to run a simulation using FEM at audio rates (44.1 kHz) with current hardware due to the high computational requirements needed. Although using FEM vibration and sound propagation could be modelled in detail, this model is computationally too slow for real-time haptic simulations.</p><p>The approach implemented in our system is the modal synthesis technique, since it seems to be the most appropriate method up to now to generate collision sounds in real-time. The modal audio synthesis algorithm used in our system is widely described in van den Doel and Pai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="van den Doel K, Pai DK (1998) The sounds of physical shapes. Presence: Teleoperators Virtual Environ 7(4):382–395" href="/article/10.1007/s10055-006-0028-4#ref-CR11" id="ref-link-section-d81609e662">1998</a>). It is based on vibration dynamics and it takes into account properties of the sounding object such as the material, the size and the contact location. The impulse response <i>y</i>(<i>t</i>) of an arbitrary solid object to an external force at a particular point, can be described as a sum of damped sinusoids: </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ y(t) = {\sum\limits_{n = 1}^N {a_{{nk}} {\text{e}}^{{ - d_{n} t}} \sin (2\pi f_{n} t)} }. $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> In this expression, <i>f</i>
                           <sub>
                    <i>n</i>
                  </sub> are the modal frequencies, <i>d</i>
                           <sub>
                    <i>n</i>
                  </sub> are the decay rates, <i>N</i> is the number of modal nodes, <i>a</i>
                           <sub>
                    <i>nk</i>
                  </sub> are the gains for each mode at different locations and <i>y</i>(<i>t</i>) denotes the audio signal as a function of time. The frequencies and dampings are determined by material properties, while the coupling gains of the modes depend on the contact localization. There are many methods to find for each type of sound, the optimal values for these parameters (van den Doel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="van den Doel K (1998) Sound synthesis for virtual reality and computer games. Ph. D. thesis, University of British Columbia" href="/article/10.1007/s10055-006-0028-4#ref-CR10" id="ref-link-section-d81609e717">1998</a>; Richmond and Pai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Richmond JL, Pai DK (2000) Active measurement of contact sounds. In: Proceedings of the 2000 IEEE international conference on robotics and automation, San Francisco, CA, USA, pp 2146–2152 DOI 10.1109/ROBOT.2000.846346" href="/article/10.1007/s10055-006-0028-4#ref-CR28" id="ref-link-section-d81609e720">2000</a>) or they can also be estimated empirically. One advantage of this synthesis algorithm is that it is linear; there is a relationship between the output signal and the input excitation. Therefore, the haptic contact forces calculated could also become the audio force inputs.</p><p>Computation time of the modal synthesis technique depends on the total amount of modal modes processed, <i>N</i>, and on the audio sampling rate used. To represent sounds of frequencies up to <i>f</i> Hz, the Shannon sampling theorem (Shannon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1949" title="Shannon CE (1949) Communication in the presence of noise. Proc Inst Radio Eng 37(1):10–21" href="/article/10.1007/s10055-006-0028-4#ref-CR30" id="ref-link-section-d81609e732">1949</a>) states that the sampling rate must remain higher that twice the higher <i>f</i> frequency; otherwise, frequencies above <i>f</i> would produce distortions. Humans can perceive frequencies between 20 and 20,000 Hz and a typical audio sampling rate is 44,100 Hz (CD quality). However, it is often difficult to achieve this sampling rate with current hardware. The number of modes necessary to generate a sound of good quality depends on the object collided. For example, if the object is a simple metallic rigid bar, between five and ten modes are usually enough for an accurate sound. In addition, if the bar is made out of wood, the audio sampling rate does not need to be as high as for metallic objects. In general, the programmer has to decide the relation between the audio sampling rate and the number of modes computed, because the quality of the sound depends on both factors. The higher the sampling rate, the lower the amount of modes the computer will be able to process.</p><p>The modal synthesis technique fulfills in general the requirements to model sounds for tapping, sliding, rolling or scraping events and for objects made out of metal, plastic or wood. Our system uses this technique to render audio and provide realistic auditory feedback to the haptic application. Its parameters are determined depending on the application to provide virtual objects with adequate sounds.</p><h3 class="c-article__sub-heading" id="Sec10">Synchronization of the feedback modules</h3><p>The synchronization of multisensory data is a key issue to achieve a proper behaviour of the whole system. If data are not perfectly synchronized, the perception of certain events can be very poor and it can also reduce user concentration, what results in errors in performing accessibility tasks. For instance, if the manipulation of the haptic device is not correctly updated in the display screen, the overall action would appear unrealistic. Similar phenomenon happens when haptic forces are applied and the sound corresponding to the event is emitted some time later. It could be compared to the natural effect of watching fireworks far away the source.</p><p>The main problem in the synchronization of different sensory modalities is the different sampling rates needed. As it is mentioned before, haptic feedback needs at least a sampling rate of 1 kHz for a good tactile sensation. The collision detection loop, however, has not a fixed sampling rate. Its frequency depends on the geometrical complexity of virtual hand tool instead of the whole model complexity (Borro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Borro D, Savall J, Amundarain A, Gil JJ, García-Alonso A, Matey L (2004a) A large haptic device for aircraft engine maintainability. IEEE Comput Graph Appl 24(6):70–74 DOI 10.1109/MCG.2004.45" href="/article/10.1007/s10055-006-0028-4#ref-CR4" id="ref-link-section-d81609e753">2004a</a>). In each haptic loop, intermediate values have to be estimated until new information from the collision detection loop arrives. Otherwise, the haptic sampling rate would be imposed by the collision detection module which is much slower, and the resultant haptic feeling would be very poor. There are many techniques (Boff and Lincoln <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Boff KR, Lincoln JE (1988) Engineering data compendium: human perception and performance. Wright-Patterson Air Force Base, Harry G. Armstrong Aerospace Medical Research Laboratory, Ohio" href="/article/10.1007/s10055-006-0028-4#ref-CR3" id="ref-link-section-d81609e756">1988</a>; Adachi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: Proceedings of the IEEE virtual reality annual international symposium, pp 203–210" href="/article/10.1007/s10055-006-0028-4#ref-CR1" id="ref-link-section-d81609e759">1995</a>) to couple the haptic fast loop with the collision slow loop. On the other hand, the visual refresh rate cannot be fixed to a certain frecuency without limiting the quality of the final image since it depends on the complexity of the model. Indeed, for human eyes it is essential a visualization frecuency of at least 25 Hz to consider the simulation as a real-time response. Finally, for audio synthesis 20 kHz is considered a good sampling rate.</p><p>Apart from the sampling rates, latencies must also be taken into account. The latencies produced working with complex industrial models and with stereoscopic visualization would cause an asynchronized response between haptic and visual stimuli. So far, with much more affordable models of two millions of polygons (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig1">1</a>), the synchonization is totally guaranteed. Other system latencies can appear in coupling audio and haptic feedback. Audio latencies must be considered when sound is rendered in a computer and then is sent to the loudspeakers through a sound card. These latencies depend on the OS, and were measured in DiFilippo (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="DiFilippo D (2000) The AHI: an audio and haptic interface for simulating contact interactions. Master’s thesis, University of British Columbia" href="/article/10.1007/s10055-006-0028-4#ref-CR7" id="ref-link-section-d81609e768">2000</a>). In this case, it seems that the best option would be to utilize OpenAL and EAX<sup>™</sup> on Creative Sound Blaster Audigy or X-Fi for hardware acceleration. On the other hand, latencies are usually unnoticeable if sound is rendered in a DSP, and the output signal is directly conducted to the loudspeakers from an analog output.</p><p>In our system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig4">4</a>), the LHIfAM haptic interface is controlled by a dSPACE DS1104 board that reads position and joint information, processes the control loop and outputs the force commands to the motors. Graphic rendering and collision detection are performed in a separated PC—Pentium IV 3 GHz running Windows XP—and using Multithread programming. Information between graphics and control is sent via Ethernet. Audio synthesis and rendering is also performed in the same dSPACE board in order to guarantee a real-time synchronization between the output force consign to the motors and the output audio signal. This audio signal is directly sent from an analog output channel of the board to the loudspeakers.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>System modules and loops</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>This two PC-based architecture has several advantages. One of the benefits is the independence of control and graphic PCs which allows the connection of different haptic devices such as a PHANToM. This property could be useful to carry out a benchmark to test the behavior of our system using different haptic devices. Another important advantage is that processes are better managed using different CPUs for control and simulation, i.e. the system overload is avoided and applications can be run in real time.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Multisensory experiments</h2><div class="c-article-section__content" id="Sec11-content"><p>A set of experiments has been carried out with the collaboration of several participants. The aim of these tests is to check the influence of multisensory feedback in performing accessibility tasks, such as those related to maintenance. We show that user performance is improved using a combination of auditory, visual and haptic stimuli.</p><p>The scenario used and the methodology followed for the experiments are described in detail in the following subsections.</p><h3 class="c-article__sub-heading" id="Sec12">Participants</h3><p>Twelve subjects took part in the experiments, nine men and three women. They were from 25 to 32 years old. All of them had normal or corrected to normal vision, reported normal tactile function and they were free of auditory impairments. Most of the subjects had no prior experience with haptic interfaces. All participants were naive to the details and hypothesis of the experiments.</p><h3 class="c-article__sub-heading" id="Sec13">Procedure</h3><p>Firstly, a suitable scenario had to be chosen for the experiments. This was not an easy task since we found many problems. Accessibility tasks, in general, imply accurate displacements and a good perception of the environment. However, almost none of the participants in the experiments were familiarized with these types of virtual environments and accessibility tasks. Another problem we found was that the realistic models usually manipulated in these training tasks allow multiple accessing paths, due to their inherent complexity and their high number of components. Therefore, it is difficult to find a common and single movement pattern for all users, in order to analyse and make a reliable comparison among all the results obtained. Taking into account all these constraints and that the quality of the sound does not depend on the complexity of the model, we concluded that it was necessary to create an intuitive and straightforward scenario where users could accomplish the experiments without having any knowledge about this type of environments. In addition, it was easier to fix a path which all users could repeat through the different trials, something really difficult if we work with large and complex models. All these factors led us to model a maze (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig5">5</a>), where participants were forced to move through all the haptic workspace and in very different ergonomic positions as it happens in accessibility operations.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Virtual environment for the experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The procedure of the experiments was the following. The labyrinth was presented graphically to the subjects on a screen from a top view. The image was projected with two projectors mounted on the ceiling that displayed the application in stereoscopic. Users were provided with special glasses to perceive the 3D visual perception.</p><p>A solid sphere was located at the entrance of the labyrinth and its position was handled with the end effector of the LHIfAM. The size of the outer rectangle of the labyrinth was set to 1 m × 1 m in order to cover all the workspace of the haptic device. In such way, participants were forced to interact with it in different ergonomic conditions. The size of the sphere (80 mm diameter) was nearly the same as the distance from wall to wall of the maze (82 mm) which made accessibility more difficult. The height of the walls was large enough in order to guarantee that subjects could not escape from the top of the model whereas at the bottom, a rigid floor was modelled.</p><p>The contact model used in the walls of the labyrinth was based on a spring-damper system, and the contact force was computed by means of this model. The stiffness and damping coefficients of the labyrinth walls in the normal direction were set to 600 N/m and 1 N s/m respectively, in order to achieve a proper behaviour of the LHIfAM. In the tangential movement along the virtual walls, a friction force model was implemented following the one proposed by Salisbury (Salisbury et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Salisbury JK, Brock DL, Massie TH, Swarup N, Zilles C (1995) Haptic rendering: programming touch interaction with virtual objects. In: Proceedings of the 1995 symposium on interactive 3D graphics, Monterey, CA, USA, pp 123–130 DOI 10.1145/199404.199426" href="/article/10.1007/s10055-006-0028-4#ref-CR29" id="ref-link-section-d81609e852">1995</a>). The static coefficient of friction was set to 0.4, while the dynamic coefficient of friction was 0.2 and the stiffness in the tangential movement was 100 N/m.</p><p>The sound generated when the ball collided with the walls of the maze was a metallic sound rendered at a sampling rate of 20 kHz and with ten modal modes computed. It was rendered as a tapping sound proportional to the haptic force sent to the user. And if the contact remained, a “stick-slip” sound was rendered proportional to the haptic friction force.</p><p>Initially, participants were allowed to interact with other different haptic applications for a short period of time, so that they could familiarize with haptic devices and get used to visual accommodation. Then, they were requested to cover the whole path from the entrance of the labyrinth to the centre and back. In each experiment, collision information of the sphere with the maze was provided from different sensory channels. Visual feedback (stereoscopic visualization) was always displayed, while sound and force feedback were alternatively restored in four possible combinations: only visual display (V), visual with sound feedback (VS), visual with force feedback (VF) and visual, force and sound feedback together (VSF). Each participant did the experiment four times, each time with one of the four possible combinations. However, the order of the combinations changed from one subject to another and there was also an interval of time of more than 1 h between each experiment performed by each subject. The aim was to avoid that participants could acquire some kind of training and get used to the path they had to follow.</p><p>Subjects were told to complete the task at the speed they felt more comfortable, and that the time they spent to complete the task would not be measured. The only instructions given to them were that the main target was to avoid collisions between the ball and the walls of maze, and that they could not go backwards. They were also informed that sound and force feedback would be provided depending on the experiment, whenever they collided with any of the walls.</p><p>Visual, haptic and auditory stimuli should prevent the user from penetrating further inside the labyrinth walls. Therefore, the accessibility was analysed by evaluating the collisions between the sphere and the labyrinth walls. To make an analysis of the results, we considered lower penetration values in the maze as an indicator of a better perception of the environment and a higher accuracy in accessibility tasks. In each experiment and for each subject, the penetration of the sphere in the walls was measured.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Results and discussion</h2><div class="c-article-section__content" id="Sec14-content"><p>The results obtained in the experimental phase are analysed in this section. The aim is to determine whether additional information obtained from another sensory channel—audio—could improve or decrease the user’s performance in accessibility tasks. As mentioned in the previous section, the criterion adopted is that smaller penetrations represent that subjects achieved an overall better performance in the task. The average penetration values measured for each experiment and each subject are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0028-4#Tab1">1</a>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Experimental results</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0028-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Analysing each subject individually, all participants had better results with force feedback than without it. This could be expected since force feedback prevents users physically from entering the walls. If we divide the experiments into two groups, the ones with and without force feedback, the main interest of the tests remains in comparing the results of these two groups with and without sound. In the group of experiments without force feedback, all the subjects except one (subject 7) obtained better results when sound feedback was added. In the experiments with force feedback, all participants also obtained better results when sound feedback was provided.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig6">6</a> shows the means and the variances of all penetration values for each type of experiment. It can be noticed that force feedback prevented participants from penetrating further inside the walls more than sound feedback did. This fact suggests that for this particular type of applications, force feedback has a greater influence over the participants than sound.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>
                                    <i>Box</i> and <i>whisker diagram</i> indicating penetration values measured for each trial. <i>Line inside the box</i> represents median values and <i>star point</i> represents mean values. <i>Cross points</i> represent outliers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Taking as reference the experiments performed only with the aid of visual stimuli (V), the mean values obtained in VS experiments showed that there was an improvement of 20% compared to V test whilst in VF and VSF, the average enhancement was of 80 and 85%, respectively.</p><p>It is also interesting to analyse how the different combinations of force, vision and sound restitutions affected the general values of penetration when considering all the experiments together. Variance analysis of the mean values between V and VS trials confirmed that sound improved results (<i>P</i> = 0.019). Mean penetration value was reduced in 20% with sound. The same analysis between VF and VFS showed the same conclusion (<i>P</i> = 0.018). In this case, mean penetration was reduced in 23%.</p><p>One-way analysis of the variance (ANOVA) between all trials showed no evidence of interaction between stimuli, except for the interaction between VF and VSF. The explanation for these results comes from the fact that some participants penetrate more with both sound and force feedback than others with only force feedback, depending on their ability to handle the haptic device. However, it is interesting to notice that all participants individually improved their results when sound feedback was provided. The analysis of the results shows that the addition of sound stimuli reduces penetration values.</p><p>Therefore, it can be concluded that results confirmed the hypothesis of participants improving general performance in these experiments when sound feedback was added.</p><p>Apart from the measurement of penetration values, the comments made by the participants and their behaviour while performing the experiments were also taken into account for further analysis. Although subjects were told that performance time would not be measured, it was however measured. Time results were very different from one subject to another since they performed the experiments at very different speed rates. However, it was quite common that overall subjects needed more time to perform the experiments with sound feedback than without it (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0028-4#Tab2">2</a>).
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Mean values and standard deviations for time (s) measured for each trial</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0028-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Comparing the mean values obtained in V and VS trials, participants spent 13.4 s more with sound feedback, whilst in VSF trial participants needed 11.5 s more on average than in VF to complete the task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0028-4#Fig7">7</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0028-4/MediaObjects/10055_2006_28_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mean time (s) and standard deviation for each trial</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0028-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>A paired samples <i>t</i> test has been performed to compare the means of the trials. The results of this analysis show that the addition of audio significantly increases the time required to complete the experiment: V–VS (<i>P</i> = 0.024) and VF–VSF (<i>P</i> = 0.005). However, since the number of subjects is rather small, a further analysis on a larger sample is needed to validate this conclusion.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Conclusions and current line of research</h2><div class="c-article-section__content" id="Sec15-content"><p>Improving the interaction and perception between humans and computers is a continuous challenge. There is a need to increase the user’s immersion and interaction in Virtual Reality systems and this motivates to bring in additional senses into the process.</p><p>This paper presents a user study to show the influence of multisensory processing at reinforcing the sensation of the user in virtual environments with haptic interfaces. Results demonstrate that the synergies of visual, haptic and auditory stimuli in haptic systems led to improvements in accessibility tasks more than in systems with fewer stimuli. The maze used in the experiments reproduces the general accessibility problems in applications like training for maintenance tasks or other training applications. Therefore, the results suggest that multisensory feedback can improve general accessibility applications. Perception of the environment, as well as collision events, become more realistic and users can perform tasks with higher accuracy.</p><p>The whole multisensory system presented has been improved to perform accessibility tasks by adding auditory feedback. The key to build systems that provide successful multisensory experiences for the users is to achieve a proper synchronization of all sensory modalities involved.</p><p>Currently, we are considering several ways to improve the quality of the multisensory system. One of them is the enhancement of the sound generated by the auditory feedback module. The aim is to develop more complex sounds that will represent complex textures and interactions. Another area of improvement would have to do with researching the benefits that an accurate directionality of sound can provide users with. Accurate generation of sound direction could provide with important cues to the perception of collision locations.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR1">Adachi Y, Kumano T, Ogino K (1995) Intermediate representation for stiff virtual objects. In: Proceedings of the IEEE virtual reality annual international symposium, pp 203–210</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adelstein BD, Begault DR, Anderson MR, Wenzel EM (2003) Sensitivity to haptic-audio asynchrony. In: Proceeding" /><p class="c-article-references__text" id="ref-CR2">Adelstein BD, Begault DR, Anderson MR, Wenzel EM (2003) Sensitivity to haptic-audio asynchrony. In: Proceedings of the 5th international conference on multimodal interfaces, Vancouver, Canada, pp 73–76 DOI 10.1145/958432.958448</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Boff KR, Lincoln JE (1988) Engineering data compendium: human perception and performance. Wright-Patterson Air" /><p class="c-article-references__text" id="ref-CR3">Boff KR, Lincoln JE (1988) Engineering data compendium: human perception and performance. Wright-Patterson Air Force Base, Harry G. Armstrong Aerospace Medical Research Laboratory, Ohio</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Borro, J. Savall, A. Amundarain, JJ. Gil, A. García-Alonso, L. Matey, " /><meta itemprop="datePublished" content="2004a" /><meta itemprop="headline" content="Borro D, Savall J, Amundarain A, Gil JJ, García-Alonso A, Matey L (2004a) A large haptic device for aircraft e" /><p class="c-article-references__text" id="ref-CR4">Borro D, Savall J, Amundarain A, Gil JJ, García-Alonso A, Matey L (2004a) A large haptic device for aircraft engine maintainability. IEEE Comput Graph Appl 24(6):70–74 DOI 10.1109/MCG.2004.45</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20large%20haptic%20device%20for%20aircraft%20engine%20maintainability&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=24&amp;issue=6&amp;pages=70-74&amp;publication_year=2004a&amp;author=Borro%2CD&amp;author=Savall%2CJ&amp;author=Amundarain%2CA&amp;author=Gil%2CJJ&amp;author=Garc%C3%ADa-Alonso%2CA&amp;author=Matey%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Borro, A. García-Alonso, L. Matey, " /><meta itemprop="datePublished" content="2004b" /><meta itemprop="headline" content="Borro D, García-Alonso A, Matey L (2004b) Approximation of optimal voxel size for collision detection in maint" /><p class="c-article-references__text" id="ref-CR5">Borro D, García-Alonso A, Matey L (2004b) Approximation of optimal voxel size for collision detection in maintainability simulations within massive virtual environments. Comput Graph Forum 23(1):13–23 DOI 10.1111/j.1467-8659.2004.00002.x</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Approximation%20of%20optimal%20voxel%20size%20for%20collision%20detection%20in%20maintainability%20simulations%20within%20massive%20virtual%20environments&amp;journal=Comput%20Graph%20Forum&amp;volume=23&amp;issue=1&amp;pages=13-23&amp;publication_year=2004b&amp;author=Borro%2CD&amp;author=Garc%C3%ADa-Alonso%2CA&amp;author=Matey%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Colgate JE, Schenkel G (1994) Passivity of a class of sampled-data systems: application to haptic interfaces. " /><p class="c-article-references__text" id="ref-CR6">Colgate JE, Schenkel G (1994) Passivity of a class of sampled-data systems: application to haptic interfaces. In: Proceedings of the American control conference, Baltimore, MD, pp 3236–3240</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DiFilippo D (2000) The AHI: an audio and haptic interface for simulating contact interactions. Master’s thesis" /><p class="c-article-references__text" id="ref-CR7">DiFilippo D (2000) The AHI: an audio and haptic interface for simulating contact interactions. Master’s thesis, University of British Columbia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DiFilippo D, Pai DK (2000) The AHI: an audio and haptic interface for contact interactions. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR8">DiFilippo D, Pai DK (2000) The AHI: an audio and haptic interface for contact interactions. In: Proceedings of the 13th annual ACM symposium on user interface software and technology, San Diego, California, USA, pp 149–158 DOI 10.1145/354401.354437</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DiFranco DE, Beauregard GL, Srinivasan MA (1997) The effect of auditory cues on the haptic perception of stiff" /><p class="c-article-references__text" id="ref-CR9">DiFranco DE, Beauregard GL, Srinivasan MA (1997) The effect of auditory cues on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1997 ASME international mechanical engineering congress and exposition, Dallas, TX, USA, pp 17–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Doel K (1998) Sound synthesis for virtual reality and computer games. Ph. D. thesis, University of Bri" /><p class="c-article-references__text" id="ref-CR10">van den Doel K (1998) Sound synthesis for virtual reality and computer games. Ph. D. thesis, University of British Columbia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Doel, DK. Pai, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="van den Doel K, Pai DK (1998) The sounds of physical shapes. Presence: Teleoperators Virtual Environ 7(4):382–" /><p class="c-article-references__text" id="ref-CR11">van den Doel K, Pai DK (1998) The sounds of physical shapes. Presence: Teleoperators Virtual Environ 7(4):382–395</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565794" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20sounds%20of%20physical%20shapes&amp;journal=Presence%3A%20Teleoperators%20Virtual%20Environ&amp;volume=7&amp;issue=4&amp;pages=382-395&amp;publication_year=1998&amp;author=Doel%2CK&amp;author=Pai%2CDK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Doel K, Pai DK (2001) JASS: a java audio synthesis system for programmers. In: Proceedings of the 2001" /><p class="c-article-references__text" id="ref-CR12">van den Doel K, Pai DK (2001) JASS: a java audio synthesis system for programmers. In: Proceedings of the 2001 international conference on auditory display, Espoo, Finland, pp 150–154</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Doel K, Kry PG, Pai DK (2001) FoleyAutomatic: Physically-based sound effects for interactive simulatio" /><p class="c-article-references__text" id="ref-CR13">van den Doel K, Kry PG, Pai DK (2001) FoleyAutomatic: Physically-based sound effects for interactive simulation and animation. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 537–544 DOI 10.1145/383259.383322</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="García-Alonso A, Gil JJ, Borro D (2005) Interfaces for VR applications development in design. In: Proceedings " /><p class="c-article-references__text" id="ref-CR14">García-Alonso A, Gil JJ, Borro D (2005) Interfaces for VR applications development in design. In: Proceedings of the virtual concept 2005, Biarritz, France, p 109</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Gil, A. Avello, Á. Rubio, J. Flórez, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Gil JJ, Avello A, Rubio Á, Flórez J (2004) Stability analysis of a 1 DOF haptic interface using the Routh–Hurw" /><p class="c-article-references__text" id="ref-CR15">Gil JJ, Avello A, Rubio Á, Flórez J (2004) Stability analysis of a 1 DOF haptic interface using the Routh–Hurwitz criterion. IEEE Trans Control Syst Technol 12(4):583–588 DOI 10.1109/TCST.2004.825134</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stability%20analysis%20of%20a%201%20DOF%20haptic%20interface%20using%20the%20Routh%E2%80%93Hurwitz%20criterion&amp;journal=IEEE%20Trans%20Control%20Syst%20Technol&amp;volume=12&amp;issue=4&amp;pages=583-588&amp;publication_year=2004&amp;author=Gil%2CJJ&amp;author=Avello%2CA&amp;author=Rubio%2C%C3%81&amp;author=Fl%C3%B3rez%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Guest, C. Catmur, D. Lloyd, C. Spence, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Guest S, Catmur C, Lloyd D, Spence C (2002) Audiotactile interactions in roughness perception. Exp Brain Res 1" /><p class="c-article-references__text" id="ref-CR16">Guest S, Catmur C, Lloyd D, Spence C (2002) Audiotactile interactions in roughness perception. Exp Brain Res 146:161–171 DOI 10.1007/s00221-002-1164–z</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Audiotactile%20interactions%20in%20roughness%20perception&amp;journal=Exp%20Brain%20Res&amp;volume=146&amp;pages=161-171&amp;publication_year=2002&amp;author=Guest%2CS&amp;author=Catmur%2CC&amp;author=Lloyd%2CD&amp;author=Spence%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Heller, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Heller MA (1982) Visual and tactual texture perception: intersensory cooperation. Percept Psychophys 31(4):339" /><p class="c-article-references__text" id="ref-CR17">Heller MA (1982) Visual and tactual texture perception: intersensory cooperation. Percept Psychophys 31(4):339–344</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7110887" aria-label="View reference 17 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20and%20tactual%20texture%20perception%3A%20intersensory%20cooperation&amp;journal=Percept%20Psychophys&amp;volume=31&amp;issue=4&amp;pages=339-344&amp;publication_year=1982&amp;author=Heller%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LF. Hodges, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Hodges LF (1992) Tutorial: time-multiplexed stereoscopic computer graphics. IEEE Comput Graph Appl 12(2):20–30" /><p class="c-article-references__text" id="ref-CR18">Hodges LF (1992) Tutorial: time-multiplexed stereoscopic computer graphics. IEEE Comput Graph Appl 12(2):20–30 DOI 10.1109/38.124285</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tutorial%3A%20time-multiplexed%20stereoscopic%20computer%20graphics&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=12&amp;issue=2&amp;pages=20-30&amp;publication_year=1992&amp;author=Hodges%2CLF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LF. Hodges, ET. Davis, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Hodges LF, Davis ET (1993) Geometric considerations for stereoscopic virtual environments. Presence: Teleopera" /><p class="c-article-references__text" id="ref-CR19">Hodges LF, Davis ET (1993) Geometric considerations for stereoscopic virtual environments. Presence: Teleoperators Virtual Environ 2(1):34–43</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometric%20considerations%20for%20stereoscopic%20virtual%20environments&amp;journal=Presence%3A%20Teleoperators%20Virtual%20Environ&amp;volume=2&amp;issue=1&amp;pages=34-43&amp;publication_year=1993&amp;author=Hodges%2CLF&amp;author=Davis%2CET">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Lederman, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Lederman SJ (1979) Auditory texture perception. Perception 8:93–103" /><p class="c-article-references__text" id="ref-CR20">Lederman SJ (1979) Auditory texture perception. Perception 8:93–103</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=432084" aria-label="View reference 20 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp080093" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20texture%20perception&amp;journal=Perception&amp;volume=8&amp;pages=93-103&amp;publication_year=1979&amp;author=Lederman%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levitin DJ, MacLean K, Mathews M, Chu LY, Jensen ER (2000) The perception of cross-modal simultaneity. In: Pro" /><p class="c-article-references__text" id="ref-CR21">Levitin DJ, MacLean K, Mathews M, Chu LY, Jensen ER (2000) The perception of cross-modal simultaneity. In: Proceedings of International Journal of Computing Anticipatory Systems, vol 517 (1), pp 323–329 DOI 10.1063/1.1291270</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mansa I, Amundarain A, Hernantes J, García-Alonso A, Borro D (2006) Occlusion culling for dense geometric scen" /><p class="c-article-references__text" id="ref-CR22">Mansa I, Amundarain A, Hernantes J, García-Alonso A, Borro D (2006) Occlusion culling for dense geometric scenarios. Proceedings of the Laval Virtual 2006, Laval, France (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mc Gee MR, Gray PD, Brewster SA (2000) The effective combination of haptic and auditory textural information. " /><p class="c-article-references__text" id="ref-CR23">Mc Gee MR, Gray PD, Brewster SA (2000) The effective combination of haptic and auditory textural information. In: Proceedings of the first international workshop on haptic human–computer interaction, Lecture Notes in Computer Science, Springer, 2058:118–126</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mc Gee MR, Gray PD, Brewster SA (2001) Feeling rough: multimodal perception of virtual roughness. In: Proceedi" /><p class="c-article-references__text" id="ref-CR24">Mc Gee MR, Gray PD, Brewster SA (2001) Feeling rough: multimodal perception of virtual roughness. In: Proceedings of the 1st Eurohaptics conference, Birmingham, UK, pp 29–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. In: Proceedings of the 28" /><p class="c-article-references__text" id="ref-CR25">O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. In: Proceedings of the 28th annual conference on computer graphics and interactive techniques, Los Angeles, USA, pp 529–536 DOI 10.1145/383259.383321</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pai DK (2005) Multisensory interaction: real and virtual, In: P Dario P, Chatila R (eds) Robotics research: th" /><p class="c-article-references__text" id="ref-CR26">Pai DK (2005) Multisensory interaction: real and virtual, In: P Dario P, Chatila R (eds) Robotics research: the eleventh international symposium, vol 15. Springer tracts in advanced robotics. Springer, Berlin Heidelberg New York, pp 489–500 DOI 10.1007/11008941_52</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poling GL, Weisenberger JM, Kerwin K (2003) The role of multisensory feedback in haptic surface perception. In" /><p class="c-article-references__text" id="ref-CR27">Poling GL, Weisenberger JM, Kerwin K (2003) The role of multisensory feedback in haptic surface perception. In: Proceedings of the 11th annual symposium on haptic interfaces for virtual environments and teleoperator systems, Los Angeles, CA, pp 187–194 DOI 10.1109/HAPTIC.2003.1191271</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richmond JL, Pai DK (2000) Active measurement of contact sounds. In: Proceedings of the 2000 IEEE internationa" /><p class="c-article-references__text" id="ref-CR28">Richmond JL, Pai DK (2000) Active measurement of contact sounds. In: Proceedings of the 2000 IEEE international conference on robotics and automation, San Francisco, CA, USA, pp 2146–2152 DOI 10.1109/ROBOT.2000.846346</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salisbury JK, Brock DL, Massie TH, Swarup N, Zilles C (1995) Haptic rendering: programming touch interaction w" /><p class="c-article-references__text" id="ref-CR29">Salisbury JK, Brock DL, Massie TH, Swarup N, Zilles C (1995) Haptic rendering: programming touch interaction with virtual objects. In: Proceedings of the 1995 symposium on interactive 3D graphics, Monterey, CA, USA, pp 123–130 DOI 10.1145/199404.199426</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CE. Shannon, " /><meta itemprop="datePublished" content="1949" /><meta itemprop="headline" content="Shannon CE (1949) Communication in the presence of noise. Proc Inst Radio Eng 37(1):10–21" /><p class="c-article-references__text" id="ref-CR30">Shannon CE (1949) Communication in the presence of noise. Proc Inst Radio Eng 37(1):10–21</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=28549" aria-label="View reference 30 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Communication%20in%20the%20presence%20of%20noise&amp;journal=Proc%20Inst%20Radio%20Eng&amp;volume=37&amp;issue=1&amp;pages=10-21&amp;publication_year=1949&amp;author=Shannon%2CCE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Srinivasan MA, Beauregard GL, Brock DL (1996) The impact of visual information on the haptic perception of sti" /><p class="c-article-references__text" id="ref-CR31">Srinivasan MA, Beauregard GL, Brock DL (1996) The impact of visual information on the haptic perception of stiffness in virtual environments. In: Proceedings of the 1996 ASME international mechanical engineering congress and exposition, Atlanta, Georgia, USA, pp 555–559</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stein BE, Meredith MA (1993) The merging of the senses. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR32">Stein BE, Meredith MA (1993) The merging of the senses. MIT Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Veron H, Southard DA, Leger JR, Conway JL (1990) Stereoscopic displays for terrain database visualization. Pro" /><p class="c-article-references__text" id="ref-CR33">Veron H, Southard DA, Leger JR, Conway JL (1990) Stereoscopic displays for terrain database visualization. Proc Stereosc Displays Appl 124–135</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Wu, C. Basdogan, MA. Srinivasan, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Wu W, Basdogan C, Srinivasan MA (1999) Visual, haptic, and bimodal perception of size and stiffness in virtual" /><p class="c-article-references__text" id="ref-CR34">Wu W, Basdogan C, Srinivasan MA (1999) Visual, haptic, and bimodal perception of size and stiffness in virtual environments. ASME Dynam Syst Control Div 67:19–26</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%2C%20haptic%2C%20and%20bimodal%20perception%20of%20size%20and%20stiffness%20in%20virtual%20environments&amp;journal=ASME%20Dynam%20Syst%20Control%20Div&amp;volume=67&amp;pages=19-26&amp;publication_year=1999&amp;author=Wu%2CW&amp;author=Basdogan%2CC&amp;author=Srinivasan%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yano H, Iwata H (2001) Software architecture for audio and haptic rendering based on a physical model. In: Pro" /><p class="c-article-references__text" id="ref-CR35">Yano H, Iwata H (2001) Software architecture for audio and haptic rendering based on a physical model. In: Proceedings of the 8th IFIP TC13 conference on human–computer interaction, Tokyo, Japan, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YY. Yeh, LD. Silverstein, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Yeh YY, Silverstein LD (1990) Limits of fusion and depth judgment in stereoscopic color displays. Hum Factors " /><p class="c-article-references__text" id="ref-CR36">Yeh YY, Silverstein LD (1990) Limits of fusion and depth judgment in stereoscopic color displays. Hum Factors 32(1):45–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=2376407" aria-label="View reference 36 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Limits%20of%20fusion%20and%20depth%20judgment%20in%20stereoscopic%20color%20displays&amp;journal=Hum%20Factors&amp;volume=32&amp;issue=1&amp;pages=45-60&amp;publication_year=1990&amp;author=Yeh%2CYY&amp;author=Silverstein%2CLD">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0028-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The research work presented in this paper is supported by the European Commission, under the FP6 IST-2002-002114 Enactive Network of Excellence (<a href="http://www.enactivenetwork.org/">http://www.enactivenetwork.org/</a>).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CEIT, Paseo Manuel Lardizábal, 15, 20018,  San Sebastián, Spain</p><p class="c-article-author-affiliation__authors-list">Iñaki Díaz, Josune Hernantes, Ignacio Mansa, Alberto Lozano, Diego Borro, Jorge Juan Gil &amp; Emilio Sánchez</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">TECNUN, University of Navarra, Paseo Manuel Lardizábal, 13, 20018, San Sebastián, Spain</p><p class="c-article-author-affiliation__authors-list">Iñaki Díaz, Josune Hernantes, Ignacio Mansa, Alberto Lozano, Diego Borro, Jorge Juan Gil &amp; Emilio Sánchez</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-I_aki-D_az"><span class="c-article-authors-search__title u-h3 js-search-name">Iñaki Díaz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;I%C3%B1aki+D%C3%ADaz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=I%C3%B1aki+D%C3%ADaz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22I%C3%B1aki+D%C3%ADaz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Josune-Hernantes"><span class="c-article-authors-search__title u-h3 js-search-name">Josune Hernantes</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Josune+Hernantes&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Josune+Hernantes" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Josune+Hernantes%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ignacio-Mansa"><span class="c-article-authors-search__title u-h3 js-search-name">Ignacio Mansa</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ignacio+Mansa&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ignacio+Mansa" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ignacio+Mansa%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Alberto-Lozano"><span class="c-article-authors-search__title u-h3 js-search-name">Alberto Lozano</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Alberto+Lozano&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alberto+Lozano" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alberto+Lozano%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Diego-Borro"><span class="c-article-authors-search__title u-h3 js-search-name">Diego Borro</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Diego+Borro&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Diego+Borro" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Diego+Borro%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jorge_Juan-Gil"><span class="c-article-authors-search__title u-h3 js-search-name">Jorge Juan Gil</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jorge Juan+Gil&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jorge Juan+Gil" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jorge Juan+Gil%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Emilio-S_nchez"><span class="c-article-authors-search__title u-h3 js-search-name">Emilio Sánchez</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Emilio+S%C3%A1nchez&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Emilio+S%C3%A1nchez" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Emilio+S%C3%A1nchez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0028-4/email/correspondent/c1/new">Jorge Juan Gil</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Influence%20of%20multisensory%20feedback%20on%20haptic%20accessibility%20tasks&amp;author=I%C3%B1aki%20D%C3%ADaz%20et%20al&amp;contentID=10.1007%2Fs10055-006-0028-4&amp;publication=1359-4338&amp;publicationDate=2006-04-27&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Díaz, I., Hernantes, J., Mansa, I. <i>et al.</i> Influence of multisensory feedback on haptic accessibility tasks.
                    <i>Virtual Reality</i> <b>10, </b>31–40 (2006). https://doi.org/10.1007/s10055-006-0028-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0028-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12-23">23 December 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04-03">03 April 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04-27">27 April 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05">May 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0028-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0028-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Accessibility</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Force feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Auditory</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Synchronization</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multisensory interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0028-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=28;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

