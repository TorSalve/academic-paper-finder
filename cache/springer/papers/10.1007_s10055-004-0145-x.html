<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Virtual environment cultural training for operational readiness (VECTO"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content=""/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Virtual environment cultural training for operational readiness (VECTOR)"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-05-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content=""/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-05-13"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="156"/>

    <meta name="prism.endingPage" content="167"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0145-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0145-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0145-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0145-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Virtual environment cultural training for operational readiness (VECTOR)"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2005/06"/>

    <meta name="citation_online_date" content="2005/05/13"/>

    <meta name="citation_firstpage" content="156"/>

    <meta name="citation_lastpage" content="167"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0145-x"/>

    <meta name="DOI" content="10.1007/s10055-004-0145-x"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0145-x"/>

    <meta name="description" content=""/>

    <meta name="dc.creator" content="John E. Deaton"/>

    <meta name="dc.creator" content="Charles Barba"/>

    <meta name="dc.creator" content="Tom Santarelli"/>

    <meta name="dc.creator" content="Larry Rosenzweig"/>

    <meta name="dc.creator" content="Vance Souders"/>

    <meta name="dc.creator" content="Chris McCollum"/>

    <meta name="dc.creator" content="Jason Seip"/>

    <meta name="dc.creator" content="Bruce W. Knerr"/>

    <meta name="dc.creator" content="Michael J. Singer"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_title=Cognitive apprenticeship: teaching the craft of reading, writing, and mathematics; citation_inbook_title=Cognition and instruction: issues and agendas; citation_publication_date=1987; citation_id=CR1; citation_author=A Collins; citation_author=JS Brown; citation_author=S Newman; citation_publisher=Erlbaum"/>

    <meta name="citation_reference" content="Gott SP (1989) Apprenticeship instruction for real-world tasks: the coordination of procedures, mental models, and strategies. In: Rothkopf EZ (ed) Review of research in education, vol. 15. American Educational Research Association, Washington, pp 97&#8211;169"/>

    <meta name="citation_reference" content="citation_journal_title=J Cogn Syst Res; citation_title=AutoTutor: a simulation of a human tutor; citation_author=AC Graesser, K Wiemer-Hastings, P Wiemer-Hastings, R Kreuz; citation_volume=1; citation_publication_date=1999; citation_pages=35-51; citation_doi=10.1016/S1389-0417(99)00005-4; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Training US army officers for peace; citation_author=null Olsen; citation_volume=operations; citation_publication_date=1999; citation_pages=lessons; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Case-based teaching and constructivism: carpenters and tools; citation_inbook_title=Constructivist learning environments: case studies in instructional design; citation_publication_date=1996; citation_id=CR5; citation_author=CK Riesbeck; citation_publisher=Educational Technology Publishing"/>

    <meta name="citation_reference" content="citation_title=Automatic text processing: the transformation, analysis, and retrieval of information by computer; citation_publication_date=1989; citation_id=CR6; citation_author=G Salton; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="Velasquez JD (1997). Modeling emotions and other motivations in synthetic agents. In: Proceedings of AAAI-97. AAAI Press and the MIT Press, pp 10&#8211;15"/>

    <meta name="citation_reference" content="Wise C, Hannaman D, Kozumplik P, Ellen F, Leaver B (1998) ARI contract report 98&#8211;06: methods to improve cultural communication skills in special operations forces. United States Army Research Institute for the Behavioral and Social Sciences"/>

    <meta name="citation_reference" content="Zachary W, Le Mentec JC (1999) A Framework for developing intelligent agents based on human information processing architecture. In: Proceedings of the IASTED international conference on artificial intelligence and soft computing. IASTED/Acta Press, Anaheim, pp 427&#8211;431"/>

    <meta name="citation_reference" content="Zachary W, Ryder J, Santarelli T, Weiland M (2000) Applications for executable cognitive models: a case study approach. In: Proceedings of the human factors and ergonomics society 44th annual meeting, Santa Monica"/>

    <meta name="citation_author" content="John E. Deaton"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Charles Barba"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Tom Santarelli"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Larry Rosenzweig"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Vance Souders"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Chris McCollum"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Jason Seip"/>

    <meta name="citation_author_institution" content="CHI Systems, Inc, USA"/>

    <meta name="citation_author" content="Bruce W. Knerr"/>

    <meta name="citation_author_institution" content="US Army Research Institute for the Behavioral and Social Sciences, USA"/>

    <meta name="citation_author" content="Michael J. Singer"/>

    <meta name="citation_author_institution" content="US Army Research Institute for the Behavioral and Social Sciences, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0145-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0145-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Virtual environment cultural training for operational readiness (VECTOR)"/>
        
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Virtual environment cultural training for operational readiness (VECTOR) | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0145-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual Environment, Cognitive Model, Synthetic Actor, Game Engine, Cultural Behavior","kwrd":["Virtual_Environment","Cognitive_Model","Synthetic_Actor","Game_Engine","Cultural_Behavior"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0145-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0145-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=145;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0145-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Virtual environment cultural training for operational readiness (VECTOR)
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0145-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0145-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-05-13" itemprop="datePublished">13 May 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Virtual environment cultural training for operational readiness (VECTOR)</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John_E_-Deaton" data-author-popup="auth-John_E_-Deaton" data-corresp-id="c1">John E. Deaton<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Charles-Barba" data-author-popup="auth-Charles-Barba">Charles Barba</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tom-Santarelli" data-author-popup="auth-Tom-Santarelli">Tom Santarelli</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Larry-Rosenzweig" data-author-popup="auth-Larry-Rosenzweig">Larry Rosenzweig</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Vance-Souders" data-author-popup="auth-Vance-Souders">Vance Souders</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chris-McCollum" data-author-popup="auth-Chris-McCollum">Chris McCollum</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jason-Seip" data-author-popup="auth-Jason-Seip">Jason Seip</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CHI Systems, Inc" /><meta itemprop="address" content="grid.421111.6, CHI Systems, Inc, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bruce_W_-Knerr" data-author-popup="auth-Bruce_W_-Knerr">Bruce W. Knerr</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="US Army Research Institute for the Behavioral and Social Sciences" /><meta itemprop="address" content="grid.483751.c, 0000 0000 9813 7216, US Army Research Institute for the Behavioral and Social Sciences, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michael_J_-Singer" data-author-popup="auth-Michael_J_-Singer">Michael J. Singer</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="US Army Research Institute for the Behavioral and Social Sciences" /><meta itemprop="address" content="grid.483751.c, 0000 0000 9813 7216, US Army Research Institute for the Behavioral and Social Sciences, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">156</span>–<span itemprop="pageEnd">167</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">315 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">22 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0145-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The number of military activities classified as “operations other than war” is on the increase. These activities may include such things as counter-insurgency campaigns, hostage rescue operations, low intensity conflicts, military operations in urban terrain, and peacekeeping operations. Such activities will require a vastly different set of tactics, equipment, training and skills than conventional military engagements of the past. Future conflicts may not involve commitments of massive numbers of troops to fixed battle zones, but will likely involve combating small units of fanatical terrorists wielding weapons of mass destruction (WMD) and other sophisticated tactics and technologies. Moreover, these missions will require leaders and soldiers to possess a different set of skills from what was required for success in traditional combat situations. They will most likely require leaders, at all levels of command, to interact and communicate personally and effectively with people whose cultures, languages, lifestyles, and beliefs are very different from those found in the U.S.</p><p>Recent US military experience in Bosnia resulted in a series of lessons that should inform US military policy in future situations of this kind. One of the more important lessons to come out of the Bosnian experience was the need to refocus training and develop senior military leaders for participation on peace operations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Olsen H, Davis J (1999) Training US army officers for peace operations: lessons from Bosnia. United States Institute of Peace special report" href="/article/10.1007/s10055-004-0145-x#ref-CR4" id="ref-link-section-d161852e336">4</a>]. US military officers were confronted with new problems that included non-traditional challenges such as negotiating with factional leaders and local government officials, managing civil-military relations, and securing a safe environment for implementation of the Dayton Peace Accords. According to Olsen and Davis, the peace operations in Kosovo required skills such as patience, the confidence to delegate authority and take risks, and the ability to communicate with people outside the military, including representatives of nongovernmental and international organizations and the media. The conclusion put forth by these authors emphasized a need to develop a set of general skills that enhances all levels of officer education to include cultural awareness and interpersonal skills. Preparation for leadership roles in future peacekeeping like the Bosnian operation will be a critical need in the upcoming decade. While these conclusions have been drawn on the officer corps, it is a safe assumption that senior enlisted personnel are in need of similar training.</p><p>Soldiers often perform missions requiring them to teach, negotiate, guide, and lead people from different cultures. To be successful in such missions, soldiers must first of all possess an understanding of these cultures, and the cultural communication skills necessary to work effectively with their counterparts within the host nation, and other coalition forces, all of which probably have cultures different from our own. While the US should have tremendous advantages from its advanced information and battlefield management systems, we are unprepared from a personnel readiness standpoint to deal with people from different cultures. This state of affairs makes our soldiers prone to making ill-advised decisions that potentially undermine our ability to successfully complete missions that involve interacting with members of a different culture.</p><p>The following paper will describe in some detail the initial training concept developed to meet the needs addressed above. In particular, the development of the training scenario designed to provide a practice environment in which cultural rules could be demonstrated will be discussed. Cultural behaviors and the cultural training strategy used in the training scenario to make trainees more culturally sensitive will be highlighted. The cultural model framework used to construct the training scenario and interaction scripts will be presented. Finally, the technology and the instructional design approach used in designing the present cultural training simulation will identify the feedback mechanisms used to construct performance metrics in the form of after action reviews (AARs).</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Initial virtual environment cultural training for operational readiness (VECTOR) concept</h2><div class="c-article-section__content" id="Sec2-content"><p>The virtual environment cultural training for operational readiness (VECTOR) program is being developed as a Department of Defense SBIR (small business innovative research) effort to provide a new technology for training in cultural familiarization through the application of highly experiential, scenario-based training in virtual environments that can be used to develop specific skills for interacting with members of a culture of interest. VECTOR initially focuses on the Kurdish culture in Iraq. This training will directly support requirements that all personnel assigned to a particular region/country/operation be better prepared to deal with decisions linked to an unfamiliar social context.</p><p>The VECTOR virtual environment is designed to encourage a high degree of human interaction with the indigenous non-player characters (NPCs), as the trainee encounters prototypical social contexts and military situations. The effects of the trainee’s actions, either positive or negative, are readily observable by the actions and affective state of NPCs within the game. The scenario reacts to these responses, and in the process, provides important information concerning the consequences of particular actions or omissions. The outcome of the simulation is, thus, not predetermined, but instead depends on human participants interpreting and reacting to the evolving scenario. The scenario represents a collection of synthetic actors (i.e., NPCs), in the cultural environment that have predefined roles, tasks, and motivations. Some external events happen at fixed times, while others happen on the basis of proximity or other contingencies. Essentially, however, the scenario plays out solely on the basis of the trainee’s actions in the environment. In this respect, it has much in common with current game technology. At the heart of this immersive environment are executable cognitive models and emotion models, which influence the overall reactions and behaviors of the NPC’s toward the trainee. The ultimate goal is to produce a highly interactive, realistic virtual environment for cultural familiarization training.</p><p>We initially considered a number of key technology areas and types of expertise in developing such a training system. These technologies included: 1) virtual environment/3-D graphics development, 2) speech recognition and synthesis, 3) natural language processing, 4) cognitive modeling, 5) intelligent tutoring/training systems, and 6) advanced interaction technologies (including body/head/eye tracking, haptic interaction). The concept was to employ such technologies in the design and development of synthetic actors to represent the members of the other culture. VECTOR was initially formulated and built to deliver instruction in critical culturally related decision-making skills as well as opportunities to practice these skills with synthetic agents and instructors in a vignette-oriented virtual simulation. The following sections deal more specifically with the VECTOR developmental efforts.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">High-level simulation interaction and behavior</h2><div class="c-article-section__content" id="Sec3-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig1">1</a> provides a high-level description of the overall interactivity concept within the VECTOR simulation. In order for interaction to take place three basic elements are necessary—a location, an indigenous entity, and the trainee entity. The trainee entity is the visual representation of the VECTOR user’s presence and position within the virtual environment. The trainee entity is under the direct control of the user in terms of locomotion, selection of dialog, selection of actions, and the choice to interact with all other simulation entities. Interaction with indigenous entities can only occur in specific simulation locations (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0145-x#Sec4">Scenario locations</a> below). This is necessary to provide predefined interaction opportunities, which will contain variable outcomes and interaction evolutions based on user choices and the degree to which he or she can build trust in or otherwise negatively affect the indigenous entities present in the location. Interaction opportunities will be based on the proximity of the trainee entity to locations and indigenous entities.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xflb1.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xflb1.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The high-level VECTOR interaction scheme</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>All entities have a set of predefined possible actions and dialogs. An entity response (either an action or a dialog) is the direct result of some entity process which evaluates the trainees actions and assesses its own internal and external psychological state in order to generate a response from a set of possible responses. Only responses which are consistent with the entities predisposition and allowable under the location’s present sub-plot position are selected.</p><p>The user (trainee) can interact with other entities, e.g., squad members and the translator, for assistance in conducting the military mission. These interactions can occur anywhere in the virtual world and do not require a designated location. In order to directly interact with other simulation entities the translator must be instructed to do so by the user. Autonomous behavior of the translator would reduce the trial and error value of the simulation. The translator can, however, offer preemptive advice to the user in the case where a blatantly inappropriate cultural action is about to be taken by the user. The translator entity can also be queried by the user and can serve as an instructor agent for cultural information.</p><p>Squad member entities, and to a lesser extent the translator entity, will function in a semi-autonomous mode with respect to gross movements and actions. Squad member entities will, in effect, follow the trainee entity throughout the simulation and provide automatic context specific behaviors (e.g., forming a perimeter when the trainee entity approaches a door). Squad members’ actions will be based on US army squad level movement doctrine.</p><p>The major entities depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig1">1</a> are the trainee, the translator, the squad member, and indigenous entities. Indigenous entities will form the bulk of the synthetic actors present in the VECTOR simulation and will include four distinct types of entities (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0145-x#Sec6">Scenario entities</a> below). Major processes include: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Response generation—what an NPC will decide to do or say</p>
                  </li>
                  <li>
                    <p>Advice generation—what the translator may say in response to a query from the trainee</p>
                  </li>
                  <li>
                    <p>Interaction evaluation—how will the trainee’s actions be interpreted</p>
                  </li>
                  <li>
                    <p>Emotion appraisal—how interaction may effect the whole village and</p>
                  </li>
                  <li>
                    <p>Training evaluation—how feedback is provided to the trainee.</p>
                  </li>
                </ul><p>Data stores will consist mainly of action and dialog alternatives, which represent all possible entity behaviors. Additional data stores will include scenario-related parameters and cultural rules. Transient data stores will be used to represent behavioral state information for both individual entities and the overall village (i.e., the village mood).</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Scenario locations</h2><div class="c-article-section__content" id="Sec4-content"><p>The basic elements of the visual simulation within VECTOR are the simulated village and the avatars. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig2">2</a> presents our initial map for the simulated village. Individual locations on the map are numerically labeled using the “L” as a prefix. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig2">2</a> shows seven residences, a mosque, a market place, a local police station, two crossing roads, and several non-interactive structures meant to enhance the visual scene.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Initial visual map of the VECTOR scenario village</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As stated above, both the trainee entity and at least one indigenous entity must be present at the same location for an interaction to take place. This means that the user must move the trainee entity (using the mouse, keyboard, or an advanced locomotion sensor) into a required proximity of a location before an interaction can occur. For example, the trainee must approach a door and knock before an indigenous entity will take the action of opening the door. The proximity of the trainee to the door allows the possibility for interaction; the trainee’s action of knocking at the door in turn triggers an indigenous entity to perform the action of opening the door. Indigenous entities can also be programmed to directly respond to the proximity of the trainee in the absence of an overt action. For example, as the trainee approaches within a certain proximity of a loiter point, indigenous entities within the loiter point can turn to face the trainee.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Game engine</h2><div class="c-article-section__content" id="Sec5-content"><p>The visual simulation within VECTOR was accomplished by leveraging on a commercially available game engine, the Lithtech Jupiter engine. The appearance of man-made objects, geographical and natural features, and avatars (i.e., synthetic entities) was governed by the tools and rendering ability of the game engine. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig3">3</a> shows the VECTOR display interface and the major GUI components. </p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>General status display: This is used to display Mission Objective information, including MO changes, additions, and completion status.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>NPC placard: Each NPC’s name, and their overall emotional state, are displayed above their avatar and is always parallel to the player’s field of view. By default, the NPC emotional state is not displayed until initial dialog interactions commence.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Mission objectives summary: This display toggle is used to view the current list of MOs and their status. Those MOs with a leading ‘–’ character (hyphen) are not yet complete, those with a leading ‘X’ are completed. The mission objectives summary can be viewed at any time by holding down the ‘Tab’ key.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Dialog interactions: This display is used to present a text version of the utterances that an NPC generates (this is in parallel to the ‘wav’ file that is played as a companion modality to present the auditory version of the NPC utterance). Additionally, the user can ‘speak’ to an NPC by moving the dialog selection cursor, indicated by the ‘&gt;’ character and hitting return to send the utterance to the NPC that is being spoken to.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">5.</span>
                    
                      <p>Scenario map: This display a static 2-D overhead map of the current scenario.</p>
                    
                  </li>
                </ol> <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>VECTOR display components</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig4">4</a> presents an example of a typical first-person perspective view of a VECTOR scenario location.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Example first person perspective view of a VECTOR vignette location</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Scenario entities</h2><div class="c-article-section__content" id="Sec6-content"><p>The VECTOR training simulation contains eight high-level entity types. Entity types are defined as individual visual elements, which are interactive and can have an effect on the evolution of the simulation. The eight high-level entities are as follows: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Locations—buildings or loiter points</p>
                  </li>
                  <li>
                    <p>Civilians—indigenous village inhabitants</p>
                  </li>
                  <li>
                    <p>Squad members—US soldiers at the command of the trainee</p>
                  </li>
                  <li>
                    <p>Translator—a trusted indigenous counterpart</p>
                  </li>
                  <li>
                    <p>The trainee—the person interacting with the VECTOR system and who provides the first person perspective in the VECTOR visual simulation</p>
                  </li>
                  <li>
                    <p>Local authorities—local police trained by US forces</p>
                  </li>
                  <li>
                    <p>Persons of interest—indigenous personnel who the US military wishes to take into custody and</p>
                  </li>
                  <li>
                    <p>Imam—the spiritual leader of the mosque.</p>
                  </li>
                </ul></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Cultural behaviors</h2><div class="c-article-section__content" id="Sec7-content"><p>The goal of the VECTOR training simulation is to provide the opportunity to explore the breadth of cultural behaviors related to a specific cultural group and to observe the effect of cultural behaviors (appropriate or inappropriate) within the context of a realistic military mission. In order to maximize our opportunity to deliver said training, it was necessary to engineer the scenario to target certain categories of cultural specific behaviors. The highest-level categories of cultural behaviors that were targeted by the initial VECTOR scenario included: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Behaviors related to religious buildings (i.e., a mosque) and to interacting with local religious leaders</p>
                  </li>
                  <li>
                    <p>Behaviors related to interacting with local civilian authorities</p>
                  </li>
                  <li>
                    <p>Behaviors related to gaining access to residences and in the development of rapport with civilian occupants</p>
                  </li>
                  <li>
                    <p>Behaviors related to dealing with indigenous children and</p>
                  </li>
                  <li>
                    <p>Behaviors related to public interactions in places of congregation such as a market place.</p>
                  </li>
                </ul><p>In general, the goal of the VECTOR simulation is to use appropriate cultural behavior to gain information in order to identify and apprehend persons of interest within the parameters of the mission. The apprehension of the persons of interest can be considered as ultimately “winning the game” while the cumulative summary of behaviors exhibited by the trainee will determine how well he or she performed. Interactions with persons of interest will require less emphasis on cultural behaviors and more on military behaviors necessary to insure the completion of the mission.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Cultural training strategy</h2><div class="c-article-section__content" id="Sec8-content"><p>The main goal of VECTOR training scenarios is to make trainees more culturally sensitive, in general, rather than to instill in them specific rules of behavior in particular contexts. In other words, if trainees come away from the training with a list of specific rules (i.e., do’s and don’ts) for this cultural situation, that is useful, but even more important is that trainees begin to develop (or become aware of their need for) higher level cognitive/interpersonal strategies for dealing with difficult/ambiguous cultural contexts more generally. The essential lesson for trainees to learn, then, is that foreign cultural situations (whether in Kurdistan, China, the Philippines, etc.) will inevitably call for behaviors and interpretations that are at odds with, or diverge from, their normal, taken-for-granted ways of acting or processing social cues.</p><p>To this end, we believe we should strive to create scenarios that require trainees to recognize that a particular situation calls for cultural sensitivity and to use their general knowledge of the culture to assess the most appropriate course of action. Remembering or getting the cultural rule correct is important, but only to the extent that it helps trainees develop higher level reasoning skills, which will serve them well regardless of the specific cultural context in which they will be entering. If we only give trainees a large set of rules at the beginning of the scenario, and then measure how effectively they apply those rules, the training will be of limited value. Of more value will be the creation of situations in which trainees will have to recognize that some cultural rule, different from their own, probably applies in a particular instance (but they do not know what the specific rule is). “Success” will be measured by how well trainees go about determining the best way to proceed, which may involve any and all of the following: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Asking the translator how best to proceed</p>
                  </li>
                  <li>
                    <p>“Reading” the emotional/interpersonal responses of indigenous entities and</p>
                  </li>
                  <li>
                    <p>Learning from past mistakes (which upset indigenous entities) and building on past successes (which engendered cooperation from indigenous entities).</p>
                  </li>
                </ul><p>We believe that in order to bring out these nuances of cultural sensitivity/interpretation, the best scenario(s) will require the trainee to elicit/encourage cooperation or some degree of help from the indigenous entities, along the lines of providing information or guidance. Perhaps the best way to do this is not to merely have the trainee go house to house looking for someone (or people), but also trying to obtain information about where this person (i.e., a person of interest) may be—this makes matters less aggressive and more cooperative. We can instruct the trainee that although the person sought may be hiding in one of the houses (so be careful), the more likely situation is that the inhabitants of one or more of the houses have crucial information about where the individual is or may be, but are not necessarily harboring him or even supporting him in any way. The better the trainee interprets/responds to culturally sensitive factors, the more trust/cooperation he elicits from these individuals and, ultimately, the more information he/she will be able to collect from informants. Conversely, the worse he/she performs in the scenario(s), the more uncooperative will be these potential informants, and, consequently, the more difficult will be the trainee’s task (e.g., getting leads/cooperation).</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Major cultural dimensions</h2><div class="c-article-section__content" id="Sec9-content"><p>Some of the major cultural dimensions which will be required by US forces to understand in Kurdistan, as well as many other cultures, include: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Gender</p>
                  </li>
                  <li>
                    <p>Religion</p>
                  </li>
                  <li>
                    <p>Status</p>
                  </li>
                  <li>
                    <p>Perceptions of/attitudes toward American culture/individuals</p>
                  </li>
                  <li>
                    <p>Interpersonal space (proxemics) and interaction and</p>
                  </li>
                  <li>
                    <p>Emotional/personality tendencies or predispositions.</p>
                  </li>
                </ul><p>The overarching cultural principle for all of these dimensions is: <i>identify/respond to divergences from American norms</i>.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Cultural modeling in VECTOR</h2><div class="c-article-section__content" id="Sec10-content"><p>How can such cultural differences be represented, and how can they be dealt with computationally in a training system? The focus of the VECTOR effort is to develop training approaches for culture, rather than purely modeling culture, so we look at this from the perspective of engineering a solution to the cultural training problem. One of the potential problems with attempting to model culture at a deep level and having that induce culture-specific behaviors in the training context is that it can be difficult to validate these models, and difficult to predict how and when these behaviors will emerge during training. Another thing to consider is that dialog with individuals is the primary mechanism by which cultural factors come into play. This leads us to consider the option of bringing a large part of the cultural model into the dialog itself. Effectively, an interaction script is constructed based on anticipated dialog that could occur in the training scenario. Each utterance received from the trainee is associated with a set of utterances that could be generated in response by a synthetic actor in the virtual environment (VE). The synthetic actor’s cognitive and emotional state, based on prior interactions and events in the environment, affect the choice of responses from the script. Subsequent sections will provide greater detail on the processing of the interaction script, particularly in light of its limited ability to represent the broad range of possible naturalistic interactions.</p><h3 class="c-article__sub-heading" id="Sec11">Cultural model framework</h3><p>Wise’s cultural template [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Wise C, Hannaman D, Kozumplik P, Ellen F, Leaver B (1998) ARI contract report 98–06: methods to improve cultural communication skills in special operations forces. United States Army Research Institute for the Behavioral and Social Sciences" href="/article/10.1007/s10055-004-0145-x#ref-CR8" id="ref-link-section-d161852e746">8</a>] provided the basis for development of a cultural model in VECTOR. Effectively, it posited the existence of a broad set of common cultural variables that could be identified for a range of cultures. The set of cultural variables was validated with respect to their practical impact on military missions. Thus, the result of this study provides a useful underpinning to the VECTOR work. In addition to this basic framework, the study identified useful cultural behavioral rules that can be utilized in constructing training scenarios and interaction scripts. In this section, we describe the cultural model that we initially developed. We are currently refining this model and implementing it in software.</p><p>One of the principal determinants of successful application of VECTOR as a training technology is the tractability of quickly generating new cultural models, when required, and generating training scenarios and interaction scripts to carry out training. An approach we identified to facilitate such rapid generation of new training material is to provide for a layered modeling approach, that provides for extensive model re-use. The first exemplar of a VECTOR cultural model has been implemented, and should permit ready adaptation to other cultures. The layered model comprises four levels: </p><ul class="u-list-style-dash">
                    <li>
                      <p>Cultural template, that specifies generic, parameterizable cultural attributes that define broad, largely universal characteristics of a culture</p>
                    </li>
                    <li>
                      <p>Cultural specialization, that provides for definition of idiosyncratic cultural norms and behaviors, as well as recognition of gesture and dialog cues</p>
                    </li>
                    <li>
                      <p>Individual specialization, that further provides for definition of individuals within a training scenario and provides such information as gender, name, specific goals and intentions, and that can establish offsets from the parameters provided at the higher levels of the model and</p>
                    </li>
                    <li>
                      <p>Individual memory, that provides recall of specific interactions that took place within the context of a scenario.</p>
                    </li>
                  </ul><p>As mentioned above, the dialog itself represents a significant portion of the encoded cultural knowledge. Language is one of the main cultural differentiators, but in addition to the obvious differences in language, the dialog (interaction script) can encode culturally determined linguistic responses to gestures and other extra-linguistic cues. We intend that the responses be conditional on the basis of variables such as animosity and level of trust, that can both reflect cultural norms (e.g., characteristic distrust of strangers) and be the result of disregard of correct protocols (e.g., dislike based on offensive behaviors). A final characteristic of these dialogs is that they be threaded, to permit a back-and-forth exchange to be encoded.</p><p>As mentioned previously, the intent of the use of the dialog is that a simple, readily adapted encoding of both cultural information and the training scenario be available. Effectively, the dialog that is constructed by the training developer represents a sample interaction, with some possible branching built in. The ability of the combined dialog system and synthetic actor/cultural model to perform loose matching against the trainee’s utterances, and to select responses from the dialog based on the history of the interaction means that robust, yet not excessively constrained, training interaction is possible.</p><p>The individual synthetic actors are controlled by a hybrid model comprised of three components: </p><ul class="u-list-style-dash">
                    <li>
                      <p>A perceptual model, consisting of ad hoc software recognizers for gestures and physical actions that are of interest in particular cultural contexts, or within particular implementations of virtual environments</p>
                    </li>
                    <li>
                      <p>An emotional model, based on existing theoretical models that posit “basic” emotions, extended with a set of emotional parameters that are chosen for pragmatic reasons within the VECTOR context</p>
                    </li>
                    <li>
                      <p>A cognitive model, based on the COGNET framework and implemented within the iGEN toolset (described in greater detail in the next section), that handles decision making and memory for the synthetic actor.</p>
                    </li>
                  </ul><p>The perceptual model is a set of monitors of VE interaction device data (such as position trackers) that detect and signal physical actions. Examples of this might be a special-purpose detector that monitors the position of hands (for handshakes, waves, or gestures such as the improper use of the left hand in some cultures); or a gaze follower that determines what or who is being examined or addressed (assuming at least head tracking). The purpose of this component is to reduce the volume of data being sent to the cognitive/emotional models and to enable treating them at a higher level of abstraction (i.e., detected a specific gesture, rather than hand at coordinate (<i>x</i>, <i>y</i>, <i>z</i>)). This component may require updating for extension to other cultures, because of the wide range of culture-specific gestures and customs.</p><p>The emotional model plays a significant role in the overall reactions and behaviors of the synthetic actors toward the trainee, and forms an important part of cultural training. By ultimately providing the ability to drive facial expressions on the avatars, the emotional model also offers the prospect of more compelling interactions with the synthetic actors. The underlying concept is that we establish a set of variables that represent levels of each of six “basic” emotions. Commonly, this set includes happiness, sadness, anger, fear, disgust, and surprise (see <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Velasquez JD (1997). Modeling emotions and other motivations in synthetic agents. In: Proceedings of AAAI-97. AAAI Press and the MIT Press, pp 10–15" href="/article/10.1007/s10055-004-0145-x#ref-CR7" id="ref-link-section-d161852e814">7</a>]. For our purposes, we can choose to augment this set with other emotional variables (such as trust). Such additional variables can be chosen on the basis of their benefit to the realism of training, regardless of their actual theoretical validity. These emotional variables are controlled by a set of (culture-specific) mappings that take as input events in the environment, particularly dialog and actions on the part of the trainee, and apply corresponding adjustments to one or more of these variables. The mechanism can be elaborated to include excitatory and inhibitory effects between emotional variables, as well as saturation levels and a decay curve.</p><p>Finally, the cognitive model component is constructed in COGNET/ iGEN. This component provides decision-making, memory, and drives verbal and physical actions. The cognitive model is responsive to the outputs of the dialog manager and the perceptual model, as well as the states of the emotional variables. Because of its flexibility, iGEN can be used to construct arbitrarily complex behaviors within a synthetic actor, but typically, the cognitive model would be used to decide which responses from the interaction script to emit given the current state of memory, the environment, and emotional factors. So, for example, a lack of trust might cause the cognitive model to decide to lie, or not answer a question posed. A brief description of the capabilities of the iGEN formalism follows.</p><h3 class="c-article__sub-heading" id="Sec12">COGNET/iGEN</h3><p>COGNET includes a rich language for describing and formalizing knowledge in a form that corresponds to the ways in which it is used by human decision makers (e.g., perception, problem-representation, reasoning, and performance of actions) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig5">5</a>]. COGNET combines several powerful cognitive engineering representations and models, including production rules, pandemonium theory, blackboard representations, and semantic networks (see <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Zachary W, Ryder J, Santarelli T, Weiland M (2000) Applications for executable cognitive models: a case study approach. In: Proceedings of the human factors and ergonomics society 44th annual meeting, Santa Monica" href="/article/10.1007/s10055-004-0145-x#ref-CR10" id="ref-link-section-d161852e830">10</a>]. It offers a framework for representing knowledge in a heterogeneous structure. In addition, the iGEN tool offers an implementation of COGNET that provides the capability to use the same structures to efficiently construct intelligent algorithms for non-cognitive modeling applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Zachary W, Le Mentec JC (1999) A Framework for developing intelligent agents based on human information processing architecture. In: Proceedings of the IASTED international conference on artificial intelligence and soft computing. IASTED/Acta Press, Anaheim, pp 427–431" href="/article/10.1007/s10055-004-0145-x#ref-CR9" id="ref-link-section-d161852e833">9</a>].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xflb5.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xflb5.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>COGNET architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In order to execute a cognitive model, there must be a software simulation of the underlying cognitive architecture and some way to encode the COGNET model in a form accessible to that executable architecture. The software development environment called iGEN performs these functions. iGEN is built around a software implementation of the COGNET information processing architecture. This executable architecture uses domain expertise, encoded in a computational form of the COGNET description language, to process sensory inputs from, and to generate actions toward, some real or simulated problem environment. The inputs/outputs to the external environment are coded in a domain-specific manner using a C++ application programmer interface (API). The API links sensory cues to the COGNET perceptual processes (e.g., perceive speech), and links the COGNET motor processes (i.e., perform action) to system controls that generate environmental effects (e.g., answer question).</p><p>To return to the issue of the layered model of culture being adopted, the basic cultural template consists of a baseline perceptual model, emotional model, and cognitive model with a set of tasks to handle dialog actions and basic actions and interactions (locomotion and greetings, etc.). For a particular culture, the emotional model mapping and activation thresholds in the cognitive model are adjusted. To construct individuals, each individual synthetic actor is specified in a definition file that provides parameter offsets for some of these quantities. In addition, individual characteristics such as name, gender, age, and association with avatar (graphical representation) are entered into the definition and loaded by the associated cognitive model when the actor is instantiated. Instantiation is controlled by a scenario script that defines which actors are “cast” and where they should appear, as well as any ongoing tasks that might be specified for them. Finally, each individual is provided with an interaction script, analogous to the script that an actor would be given as part of a movie.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Current software architecture</h2><div class="c-article-section__content" id="Sec13-content"><p>This section describes the software architecture that we are currently implementing. This discussion provides a high-level view of the software components that are to be built, as well as commercially available components with which we will integrate. After presenting the structural view, we will discuss construction details, including platform, integration techniques, and tools to be used.</p><h3 class="c-article__sub-heading" id="Sec14">Components</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0145-x#Fig6">6</a> shows the primary components of the VECTOR architecture. These components include: </p><ul class="u-list-style-dash">
                    <li>
                      <p>The user interface, which is implemented using the COTS Lithtech Jupitor game engine, and provides the basic virtual environment and support avatar display, dialog GUI display etc</p>
                    </li>
                    <li>
                      <p>The game engine, including the dialog manager component, which handles details of “face-to-face” verbal interaction with synthetic actors in the virtual environment</p>
                    </li>
                    <li>
                      <p>The iGEN-controlled synthetic actor component, which is where the cultural models are instantiated; this component will perform the cognitive and emotive functions of the human entities in the scenario and drive avatar actions, movements, and speech (the latter via the dialog manager).</p>
                    </li>
                  </ul> <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0145-x/MediaObjects/s10055-004-0145-xfhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>High-level VECTOR architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0145-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>There are various input descriptions to these components that define the environment, culture, and specific training scenario. These consist of: </p><ul class="u-list-style-dash">
                    <li>
                      <p>Terrain and object databases to define the VE and avatars</p>
                    </li>
                    <li>
                      <p>The scenario and actor definitions provided to the synthetic actors, which determine the overall cultural attributes of actors instantiated in the scenario as well as providing for time-driven and situational behaviors on the part of the actors and</p>
                    </li>
                    <li>
                      <p>The interaction scripts, which provides the dialog manager a mapping from raw utterances (coming from the speech recognizer) to semantic content and provides the range of acceptable responses.</p>
                    </li>
                  </ul><p>The VE and speech processing components are largely COTS, with some adaptation layer provided to interoperate with the other components. However, the dialog manager and synthetic actor components require more detailed description (in the section which follows) because they are central to the development of VECTOR, and because they have a fairly complex internal structure.</p><h3 class="c-article__sub-heading" id="Sec15">Dialog manager</h3><p>Speech recognition, in its current state, offers highly variable performance, depending on factors ranging from the subject’s voice characteristics, to amount of training of the recognizer, to variations based on room noise level and the subject’s posture. The assumption needs to be made that some non-trivial percentage of subject utterances will be incorrectly recognized. Furthermore, the dialog in the training contexts we have proposed is open-ended and not highly constrained; the subject’s level of familiarity with the target language may be limited, as well. These factors serve to further reduce the likelihood that utterances detected will be well formed.</p><p>The dialog manager serves to provide a mapping between the utterances delivered by the speech recognition system and the set of utterances deemed likely for the given training scenario by the interaction script. This is accomplished by use of “fuzzy” matching techniques that select utterances from the interaction script on the basis of a Cartesian product match. The idea is to treat both the “heard” utterance and each utterance of the interaction script as a term vector. The Cartesian (or “dot”) product of the term vectors yields the cosine of the “angle” between the vectors. The largest cosine value found (i.e., closest to 1) represents the closest match. This vector space approach has a considerable history of use in information retrieval applications (see <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Salton G (1989) Automatic text processing: the transformation, analysis, and retrieval of information by computer. Addison-Wesley, Boston" href="/article/10.1007/s10055-004-0145-x#ref-CR6" id="ref-link-section-d161852e942">6</a>]. In addition, it has seen recent application in dialog systems such as AutoTutor [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Graesser AC, Wiemer-Hastings K, Wiemer-Hastings P, Kreuz R, TRG (1999) AutoTutor: a simulation of a human tutor. J Cogn Syst Res 1:35–51" href="/article/10.1007/s10055-004-0145-x#ref-CR3" id="ref-link-section-d161852e945">3</a>].</p><p>Besides its direct matching of spoken to expected utterances, the dialog manager must incorporate extra-linguistic cues in its processing. Examples of these might be physical gestures (such as pointing, hand shakes, touching objects, etc.) and proximity and gaze/body facing cues to identify the intended listener. We plan to represent these as tokens appearing in the interaction script. The gestures themselves are monitored by a separate software component that receives data from the trackers and devices in the VE. The gesture recognizer will potentially need adaptation/extension as future cultural training scenarios are developed that require monitoring of new gestures.</p><h3 class="c-article__sub-heading" id="Sec16">Synthetic actors and cultural models</h3><p>The synthetic actor component is responsible for representing the behaviors of the simulated human entities. As described above, the synthetic actors represent a hybrid of perceptual, cognitive, and emotive models. The implementation of the perceptual models is through separate perceptual detectors that reside outside of the model, but communicate with the iGEN cognitive model through its perceptual demons. The emotive model we believe should be implemented initially in the iGEN “shell” layer—the software layer that provides an interface to the environment and arbitrary low-level processing. Ultimately, as the emotive model design proves its worth and proper higher-level mechanisms are identified for representing it, the emotive model would be a candidate for inclusion in iGEN proper (as an extension to the modeling language).</p><p>As has been described, a capability is desired to be able to adapt generic models for a specific culture, as well as extending them to support “idiosyncratic” cultural features (ones that do not fit within the cultural template, but are still required for training) and instantiating them for individuals. The initial implementation of this will take the form of cultural profiles and scenario scripts. When the scenario script specifies instantiation of a specific actor (whose characteristics are drawn from a specific cultural profile), the model instance is created and reads the profile. The parameters of this profile are adjusted and extended by reading additional parameters specified with the actor definition within the scenario script. These additional parameters represent offsets from the norm represented by the cultural profile, as well as simple extensions that may be used by the cognitive model itself (such as name and gender of the actor). In addition, specialized aspects of the cognitive model may be composed with the main, generic model either by developing a preprocessing step (to extend the generic model with specialized capabilities) or by developing a capability to load additional model code (such as tasks and methods). This introduces the complexity of having to extend the blackboard definition after loading, which may not be feasible, but is worth some investigation. Initially, we will adopt the preprocessing approach.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Entity behavior and memory</h2><div class="c-article-section__content" id="Sec17-content"><p>Much research has been conducted on the development of more realistic emotional behaviors for intelligent virtual agents. VECTOR will leverage on this experimental research in order to produce entities, which respond to the trainee in a realistic way during an evolving and variable plot sequence. Implementation of behavior and memory within VECTOR will require the adoption of three major constructs—a behavioral taxonomy, a mathematical model for behavioral interaction, and a behavior action generation model. Each of these decisions is non-trivial and has immense consequences on the ultimate psychological and mathematical validity of the VECTOR system.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Training and instructional design</h2><div class="c-article-section__content" id="Sec18-content"><p>The goal of the VECTOR simulation is to train soldiers in cultural understanding as it relates to the performance of their duties in foreign locales. We have developed an approach based on immersive virtual environments, featuring, to the extent possible, naturalistic interaction (natural language speech and gesture), and involving scenario-based instruction. While we have selected Military Police/Military Intelligence missions and personnel as the initial target audience, we anticipate that the ultimate benefits to training conventional forces are compelling, and potentially far greater. While VECTOR offers the possibility of effective, intensive instruction in advance of such real-world experience, it is unlikely that any computer-based training system can offer the depth of understanding obtained from living within a culture. On the other hand, we expect that conventional forces (such as Military Police) will increasingly be required to undertake missions, such as peacekeeping, where cross-cultural interactions play a significant role in determining success or failure. These conventional forces will not have the benefit of intensive real-world experience, which is common in special forces, and may in fact need to be deployed with limited forewarning. A brief, intensive, effective, and rapidly adapted training program in cultural interaction is what VECTOR is intended to deliver, and will benefit conventional forces subject to time constraints and training needs on a large scale.</p><p>Educational research has shown that active student involvement produces better results, particularly when the goal of the instruction is acquiring some practical skill (e.g., as opposed to learning abstract concepts). A common way of providing this active participation is to allow students to practice the skill in a real or simulated setting. The heavy and high-payoff investment that the aviation industry has made in flight simulators is a highly successful example of this principle in action.</p><p>However, practice by itself is also insufficient, even when used to reinforce didactic instruction. The idea that teaching should be tailored to individuals has a long history in education. A great deal of data shows that carefully individualized instruction is superior to conventional group instruction, and this is particularly the case in active processes. In an article dealing with applied skill training, Gott [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Gott SP (1989) Apprenticeship instruction for real-world tasks: the coordination of procedures, mental models, and strategies. In: Rothkopf EZ (ed) Review of research in education, vol. 15. American Educational Research Association, Washington, pp 97–169" href="/article/10.1007/s10055-004-0145-x#ref-CR2" id="ref-link-section-d161852e980">2</a>] discusses the need for training methods that are a combination of modern understanding of skill acquisition and traditional apprenticeship training techniques, such as mentoring and coaching. “Cognitive apprenticeship” is a model of instruction developed by Collins [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Collins A, Brown JS, Newman S (1987) Cognitive apprenticeship: teaching the craft of reading, writing, and mathematics. In: Resnick LB (ed) Cognition and instruction: issues and agendas. Erlbaum, Hillsdale" href="/article/10.1007/s10055-004-0145-x#ref-CR1" id="ref-link-section-d161852e983">1</a>], who adapted the traditional view of apprenticeship in learning physical skills in tangible activities to learning cognitive skills. This form of instruction involves taking trainees through successive approximations of mature practice as they learn how to perform job tasks. The important aspects of cognitive apprenticeship training are: </p><ul class="u-list-style-dash">
                  <li>
                    <p>Realistic practice environments and problems, in which students can execute realistic tasks in a realistic environment and see how knowledge (acquired through didactic instruction) can be practically put to use</p>
                  </li>
                  <li>
                    <p>Support from the tutor, including real-time comments on student performance, and providing hints, reminders, and explanations</p>
                  </li>
                  <li>
                    <p>A gradual fading out of this support as skill builds and</p>
                  </li>
                  <li>
                    <p>Careful sequencing of problems and challenges that are appropriate for the growing skill level of the learner.</p>
                  </li>
                </ul><p>Gott concludes that technical skill development is best achieved by “guided experience in instructional environments that provide progressive, explanation-based, and otherwise generally supported practice in the mechanics of solving problems.” This overall method, which combines didactic instruction and individually mentored active practice, can be simply termed “guided practice”.</p><p>Training cultural knowledge requires more than merely enumerating a set of behavioral rules to a trainee. To some extent, the trainee needs to internalize a mindset; a specific set of cultural encounters in a training scenario will never serve to cover all possible cases that a soldier may encounter in real life, but needs to be designed to generalize into a broader understanding. In this sense, the training approach selected for VECTOR needs to be constructivist. There is significant academic work that establishes the validity of this kind of approach; for example, Schank’s work in case-based reasoning has been extended to case-based learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Riesbeck CK (1996) Case-based teaching and constructivism: carpenters and tools. In: Wilson BG (ed) Constructivist learning environments: case studies in instructional design. Educational Technology Publishing, Englewood Cliffs" href="/article/10.1007/s10055-004-0145-x#ref-CR5" id="ref-link-section-d161852e1014">5</a>], where learning is conceived of as a process of compiling cases (specific examples of actions and results) and constructing an index of cases to apply to novel situations (being able to generalize appropriately from cases). Case-based learning implies that trainees be exposed to a broad base of experiences, in a compelling and memorable environment. In addition, being able to index those experiences requires that motivation (underlying goals and plans) be present, that both failure and success be experienced, and that reasons for failure be understood. In applying the CBL approach to VECTOR, we can see that a virtual environment provides the necessary vivid and realistic experiences. In addition, a concept of mission (a mission briefing or orders by a virtual commanding officer, for example), and strongly branching scenarios that permit failure (bad decisions) to be followed flexibly to their conclusion, provides the basis for effective CBL.</p><p>In terms of instructional design, the overall principle of training we envision in VECTOR combines these notions of constructivist, case-based learning with guided practice. In this approach, a trainee is placed in a scenario within a simulated environment. In the early phases of learning, the trainee is provided with behavioral modeling (i.e., behaviors are demonstrated and explained) and significant ongoing feedback and coaching. As instruction progresses, the amount of continuous feedback is reduced, until the trainee progresses through the entire scenario without intervention, and is provided with an after-action review (AAR) at the end. The degree of intervention is based on the trainee’s evolving skill level and amount of exposure to the material.</p><p>The notion of scenario in VECTOR bears some explanation. Rather than being conceived of as a rigid and potentially brittle sequence of actions that take place in the environment on the basis of fixed times, the scenario represents a collection of players in the environment that have predefined roles, tasks, and motivations. Some external events may happen at fixed times (e.g., arrival of local authorities); others may happen on the basis of proximity or other contingency (e.g., knocking on a particular door or gaining information based on the correct application of cultural behaviors). Essentially, however, the scenario plays out solely on the basis of the trainee’s actions in the environment. In this respect, it has much in common with current game technology.</p><p>Control over the interventions, and the content of those interventions, must be provided by a tutoring agent. The tutor can be embodied as a soldier who accompanies the trainee through a scenario; alternatively, the tutor can “channel” through a number of avatars (such as a commanding officer for initial mission briefing and AAR, or other soldiers or actors during the mission). In a full-fledged implementation of this concept, the tutor would be monitoring interactions and maintaining a set of measures of performance (MOP’s) and measures of effectiveness (MOE’s). MOP’s might include behaviors indicative of a particular skill being trained and the percentage of their correct versus total applications. MOE’s might include time to completion of the scenario, achievement of mission parameters (successful achievement of objectives, number of casualties, etc.).</p><p>After-action reviews (AARs) could take several forms, including tabular display of statistics, an overall rating, and list of good and bad actions (possibly provided by an avatar). In addition, a narrated replay of the mission might be an effective tool for reinforcing learning and indexing.</p><p>In order to strike a reasonable balance between instructional generality and timely implementation, we will implement the instructional system and tutoring component in stages. The first stage will provide a mission briefing and scenario-based execution of the mission. Next, basic metrics will be maintained and used to construct a summary AAR. As scoring of the trainee’s individual behaviors is implemented, the notion of explanation can be implemented (basically, when things went bad, the cognitive state of the actors involved can be analyzed to determine why particular actions were bad, e.g., “when the trainee said X, the actor was offended,” etc.). iGEN has an explanation facility that may be used to implement this type of functionality. In turn, we would use this to provide some types of simple feedback during the mission. At this point, the VECTOR system would have several operating modes, ranging from “model behaviors and provide correction,” to “provide limited feedback,” to “provide only AAR.”</p><p>Future implementations might take this further by developing a full-fledged tutor that could use this kind of information to manage the scenario more carefully, ensuring that trainees do not depart too far from the scenario, or adaptively modifying the level of feedback based on performance and trainee history.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusion</h2><div class="c-article-section__content" id="Sec19-content"><p>We have developed in VECTOR a virtual training environment incorporating cognitive-model-controlled NPCs that facilitates the delivery of cultural-familiarization training. Through the use of a canonical cognitive model of NPC behaviors and the utilization of a generic scripting language, cultural rules are encoded and mapped to scenario-specific NPC dialog and behaviors in order to provide a set of virtual NPC’s with which the VECTOR trainee can interact. Additionally, the NPC model contains an emotional model that maps emotional states to underlying trainee actions and dialog in order to measure cultural rule violations. Future areas of VECTOR development will include use of NPC emotional state to drive real-time facial animation of NPC avatars, deepening the VECTOR scenario to encode additional sets of cultural rule support, and adding additional features to the NPC emotion model.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Collins, JS. Brown, S. Newman, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Collins A, Brown JS, Newman S (1987) Cognitive apprenticeship: teaching the craft of reading, writing, and mat" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Collins A, Brown JS, Newman S (1987) Cognitive apprenticeship: teaching the craft of reading, writing, and mathematics. In: Resnick LB (ed) Cognition and instruction: issues and agendas. Erlbaum, Hillsdale</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognition%20and%20instruction%3A%20issues%20and%20agendas&amp;publication_year=1987&amp;author=Collins%2CA&amp;author=Brown%2CJS&amp;author=Newman%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gott SP (1989) Apprenticeship instruction for real-world tasks: the coordination of procedures, mental models," /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Gott SP (1989) Apprenticeship instruction for real-world tasks: the coordination of procedures, mental models, and strategies. In: Rothkopf EZ (ed) Review of research in education, vol. 15. American Educational Research Association, Washington, pp 97–169</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AC. Graesser, K. Wiemer-Hastings, P. Wiemer-Hastings, R. Kreuz, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Graesser AC, Wiemer-Hastings K, Wiemer-Hastings P, Kreuz R, TRG (1999) AutoTutor: a simulation of a human tuto" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Graesser AC, Wiemer-Hastings K, Wiemer-Hastings P, Kreuz R, TRG (1999) AutoTutor: a simulation of a human tutor. J Cogn Syst Res 1:35–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1389-0417%2899%2900005-4" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=AutoTutor%3A%20a%20simulation%20of%20a%20human%20tutor&amp;journal=J%20Cogn%20Syst%20Res&amp;volume=1&amp;pages=35-51&amp;publication_year=1999&amp;author=Graesser%2CAC&amp;author=Wiemer-Hastings%2CK&amp;author=Wiemer-Hastings%2CP&amp;author=Kreuz%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Olsen, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Olsen H, Davis J (1999) Training US army officers for peace operations: lessons from Bosnia. United States Ins" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Olsen H, Davis J (1999) Training US army officers for peace operations: lessons from Bosnia. United States Institute of Peace special report</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Training%20US%20army%20officers%20for%20peace&amp;volume=operations&amp;publication_year=1999&amp;author=Olsen%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="CK. Riesbeck, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Riesbeck CK (1996) Case-based teaching and constructivism: carpenters and tools. In: Wilson BG (ed) Constructi" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Riesbeck CK (1996) Case-based teaching and constructivism: carpenters and tools. In: Wilson BG (ed) Constructivist learning environments: case studies in instructional design. Educational Technology Publishing, Englewood Cliffs</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Constructivist%20learning%20environments%3A%20case%20studies%20in%20instructional%20design&amp;publication_year=1996&amp;author=Riesbeck%2CCK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Salton, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Salton G (1989) Automatic text processing: the transformation, analysis, and retrieval of information by compu" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Salton G (1989) Automatic text processing: the transformation, analysis, and retrieval of information by computer. Addison-Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20text%20processing%3A%20the%20transformation%2C%20analysis%2C%20and%20retrieval%20of%20information%20by%20computer&amp;publication_year=1989&amp;author=Salton%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Velasquez JD (1997). Modeling emotions and other motivations in synthetic agents. In: Proceedings of AAAI-97. " /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Velasquez JD (1997). Modeling emotions and other motivations in synthetic agents. In: Proceedings of AAAI-97. AAAI Press and the MIT Press, pp 10–15</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wise C, Hannaman D, Kozumplik P, Ellen F, Leaver B (1998) ARI contract report 98–06: methods to improve cultur" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Wise C, Hannaman D, Kozumplik P, Ellen F, Leaver B (1998) ARI contract report 98–06: methods to improve cultural communication skills in special operations forces. United States Army Research Institute for the Behavioral and Social Sciences</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zachary W, Le Mentec JC (1999) A Framework for developing intelligent agents based on human information proces" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Zachary W, Le Mentec JC (1999) A Framework for developing intelligent agents based on human information processing architecture. In: Proceedings of the IASTED international conference on artificial intelligence and soft computing. IASTED/Acta Press, Anaheim, pp 427–431</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zachary W, Ryder J, Santarelli T, Weiland M (2000) Applications for executable cognitive models: a case study " /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Zachary W, Ryder J, Santarelli T, Weiland M (2000) Applications for executable cognitive models: a case study approach. In: Proceedings of the human factors and ergonomics society 44th annual meeting, Santa Monica</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0145-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CHI Systems, Inc, USA</p><p class="c-article-author-affiliation__authors-list">John E. Deaton, Charles Barba, Tom Santarelli, Larry Rosenzweig, Vance Souders, Chris McCollum &amp; Jason Seip</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">US Army Research Institute for the Behavioral and Social Sciences, USA</p><p class="c-article-author-affiliation__authors-list">Bruce W. Knerr &amp; Michael J. Singer</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-John_E_-Deaton"><span class="c-article-authors-search__title u-h3 js-search-name">John E. Deaton</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;John E.+Deaton&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John E.+Deaton" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John E.+Deaton%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Charles-Barba"><span class="c-article-authors-search__title u-h3 js-search-name">Charles Barba</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Charles+Barba&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Charles+Barba" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Charles+Barba%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tom-Santarelli"><span class="c-article-authors-search__title u-h3 js-search-name">Tom Santarelli</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tom+Santarelli&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tom+Santarelli" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tom+Santarelli%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Larry-Rosenzweig"><span class="c-article-authors-search__title u-h3 js-search-name">Larry Rosenzweig</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Larry+Rosenzweig&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Larry+Rosenzweig" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Larry+Rosenzweig%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Vance-Souders"><span class="c-article-authors-search__title u-h3 js-search-name">Vance Souders</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Vance+Souders&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Vance+Souders" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Vance+Souders%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Chris-McCollum"><span class="c-article-authors-search__title u-h3 js-search-name">Chris McCollum</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Chris+McCollum&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chris+McCollum" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chris+McCollum%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jason-Seip"><span class="c-article-authors-search__title u-h3 js-search-name">Jason Seip</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jason+Seip&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jason+Seip" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jason+Seip%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Bruce_W_-Knerr"><span class="c-article-authors-search__title u-h3 js-search-name">Bruce W. Knerr</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Bruce W.+Knerr&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Bruce W.+Knerr" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bruce W.+Knerr%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Michael_J_-Singer"><span class="c-article-authors-search__title u-h3 js-search-name">Michael J. Singer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Michael J.+Singer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michael J.+Singer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael J.+Singer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                John E. Deaton.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Virtual%20environment%20cultural%20training%20for%20operational%20readiness%20%28VECTOR%29&amp;author=John%20E.%20Deaton%20et%20al&amp;contentID=10.1007%2Fs10055-004-0145-x&amp;publication=1359-4338&amp;publicationDate=2005-05-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Deaton, J.E., Barba, C., Santarelli, T. <i>et al.</i> Virtual environment cultural training for operational readiness (VECTOR).
                    <i>Virtual Reality</i> <b>8, </b>156–167 (2005). https://doi.org/10.1007/s10055-004-0145-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0145-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-05-13">13 May 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-06">June 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0145-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0145-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual Environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cognitive Model</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Synthetic Actor</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Game Engine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cultural Behavior</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0145-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=145;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

