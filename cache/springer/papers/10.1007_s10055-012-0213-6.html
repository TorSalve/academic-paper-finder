<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="NAVIG: augmented reality guidance system for the visually impaired"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="NAVIG: augmented reality guidance system for the visually impaired"/>

    <meta name="dc.source" content="Virtual Reality 2012 16:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-06-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-06-12"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="253"/>

    <meta name="prism.endingPage" content="269"/>

    <meta name="prism.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-012-0213-6"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-012-0213-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-012-0213-6.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-012-0213-6"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="NAVIG: augmented reality guidance system for the visually impaired"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2012/11"/>

    <meta name="citation_online_date" content="2012/06/12"/>

    <meta name="citation_firstpage" content="253"/>

    <meta name="citation_lastpage" content="269"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-012-0213-6"/>

    <meta name="DOI" content="10.1007/s10055-012-0213-6"/>

    <meta name="citation_doi" content="10.1007/s10055-012-0213-6"/>

    <meta name="description" content="Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artifi"/>

    <meta name="dc.creator" content="Brian F. G. Katz"/>

    <meta name="dc.creator" content="Slim Kammoun"/>

    <meta name="dc.creator" content="Ga&#235;tan Parseihian"/>

    <meta name="dc.creator" content="Olivier Gutierrez"/>

    <meta name="dc.creator" content="Adrien Brilhault"/>

    <meta name="dc.creator" content="Malika Auvray"/>

    <meta name="dc.creator" content="Philippe Truillet"/>

    <meta name="dc.creator" content="Michel Denis"/>

    <meta name="dc.creator" content="Simon Thorpe"/>

    <meta name="dc.creator" content="Christophe Jouffrais"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Memory Cogn; citation_title=Structural properties of spatial representations in blind people: scanning images constructed from haptic exploration or from locomotion in a 3-D audio virtual environment; citation_author=A Afonso, A Blum, BFG Katz, P Tarroux, G Borst, M Denis; citation_volume=38; citation_publication_date=2010; citation_pages=591-604; citation_doi=10.3758/MC.38.5.591; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Cogn Psychol; citation_title=Principles and practices for communicating route knowledge; citation_author=GL Allen; citation_volume=14; citation_publication_date=2000; citation_pages=333-359; citation_doi=10.1002/1099-0720(200007/08)14:4&lt;333::AID-ACP655&gt;3.0.CO;2-C; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Sci; citation_title=Perception with compensatory devices. From sensory substitution to sensorimotor extension; citation_author=M Auvray, E Myin; citation_volume=33; citation_publication_date=2009; citation_pages=1036-1058; citation_doi=10.1111/j.1551-6709.2009.01040.x; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=Learning to perceive with a visuo-auditory substitution system: localization and object recognition with the voice; citation_author=M Auvray, S Hanneton, JK O&#8217;Regan; citation_volume=36; citation_publication_date=2007; citation_pages=416-430; citation_doi=10.1068/p5631; citation_id=CR5"/>

    <meta name="citation_reference" content="Bar-Shalom Y (1987) Tracking and data association. Academic Press Professional, ISBN: 0-120-79760-7"/>

    <meta name="citation_reference" content="citation_title=3-D sound for virtual reality and multimedia; citation_publication_date=1994; citation_id=CR7; citation_author=DR Begault; citation_publisher=Academic Press"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Impair Blind; citation_title=Audible signage as a wayfinding aid: Comparison of Verbal Landmarks    $$\circledR$$  and Talking Signs    $$\circledR$$; citation_author=B Bentzen, P Mitchell; citation_volume=89; citation_publication_date=1995; citation_pages=494-505; citation_id=CR8"/>

    <meta name="citation_reference" content="Berger JO (1985) Statistical decision theory and bayesian analysis (2nd edn). Springer Series, ISBN: 978-0387960982"/>

    <meta name="citation_reference" content="citation_title=Techniques pratiques pour l&#8217;&#233;tude des activit&#233;s expertes; citation_publication_date=1999; citation_id=CR10; citation_author=A Bisseret; citation_author=S Sebillote; citation_author=P Falzon; citation_publisher=Octar&#232;s Editions"/>

    <meta name="citation_reference" content="citation_journal_title=SIGCHI Bull; citation_title=Earcons and icons: their structure and common design principles; citation_author=MM Blattner, DA Sumikawa, RM Greenberg; citation_volume=21; citation_publication_date=1989; citation_pages=123-124; citation_doi=10.1145/67880.1046599; citation_id=CR11"/>

    <meta name="citation_reference" content="Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to improve blind pedestrian positioning. International conference on new technologies, mobility and security, IEEE, France"/>

    <meta name="citation_reference" content="Brunet L (2010) &#201;tude des besoins et des strat&#233;gies des personnes non-voyantes lors de la navigation pour la conception d&#8217;un dispositif d&#8217;aide performant et accept&#233; (Needs and strategy study of blind people during navigation for the design of a functional and accepted aid device). Master&#8217;s thesis, Department of Ergonomics, Universit&#233; Paris-Sud, Orsay, France"/>

    <meta name="citation_reference" content="Buisson M, Bustico A, Chatty S, Colin F-R, Jestin Y, Maury S, Mertz C, Truillet P (2002) Ivy: un bus logiciel au service du d&#233;veloppement de prototypes de syst&#232;mes interactifs. 14th French-speaking conference on human&#8211; computer interaction (IHM &#8217;02) pp 223-2 26"/>

    <meta name="citation_reference" content="Burrough PA (1986) Principles of geographical information systems for land resources, assessment. Oxford, Clarendon Press, Monographs on soil and resource surveys, No. 12"/>

    <meta name="citation_reference" content="Canadian Institute for the Blind (2005) In&#233;galit&#233; des chances : Rapport sur les besoins des personnes aveugles ou handicap&#233;es visuelles vivant au Canada. Technical Report. 
                    http://www.cnib.ca/fr/apropos/publications/recherche
                    
                  
                "/>

    <meta name="citation_reference" content="Cappelle C, El Najjar ME, Pomorski D, Charpillet F (2010) Multi-sensors data fusion using dynamic bayesian network for robotised vehicle geo-localisation. International conference on information fusion, June 30&#8211;July 3 2008, Cologne, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=Curr Pychol Cogn; citation_title=The description of routes: a cognitive approach to the production of spatial discourse; citation_author=M Denis; citation_volume=16; citation_publication_date=1997; citation_pages=409-458; citation_id=CR18"/>

    <meta name="citation_reference" content="Dingler T, Lindsay J, Walker BN (2008) Learnability of sound cues for environmental features: Auditory icons, earcons, spearcons, and speech, methods, pp 1&#8211;6"/>

    <meta name="citation_reference" content="Dramas F, Oriola B, Katz BFG, Thorpe S, Jouffrais C (2008) Designing an assistive device for the blind based on object localization and augmented auditory reality. ACM conference on computers and accessibility, ASSETS, Halifax, Canada, pp 263&#8211;264"/>

    <meta name="citation_reference" content="citation_journal_title=J Image Graph; citation_title=Artificial vision for the blind: a bio-inspired algorithm for objects and obstacles detection; citation_author=F Dramas, SJ Thorpe, C Jouffrais; citation_volume=10; citation_issue=4; citation_publication_date=2010; citation_pages=531-544; citation_doi=10.1142/S0219467810003871; citation_id=CR21"/>

    <meta name="citation_reference" content="Durrant-Whyte HF (1988) Sensor models and multisensory integration. Special issue on Sensor Data Fusion, ISSN: 0278-3649 7(6):97&#8211;113"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Multisensory VR interaction for protein-docking in the CoRSAIRe project; citation_author=N F&#233;rey, J Nelson, C Martin, L Picinali, G Bouyer, A Tek, P Bourdot, JM Burkhardt, BFG Katz, M Ammi, C Etchebest, L Autin; citation_volume=13; citation_publication_date=2009; citation_pages=273-293; citation_doi=10.1007/s10055-009-0136-z; citation_id=CR23"/>

    <meta name="citation_reference" content="Fletcher JF (1980) Spatial representation in blind children. 1: development compared to sighted children. J Vis Impair Blind 381&#8211;385"/>

    <meta name="citation_reference" content="Gallay M, Denis M, Auvray M (2012) Navigation assistance for blind pedestrians: guidelines for the design of devices and implications for spatial cognition. In: Thora Tenbrink, Jan Wiener, Christophe Claramunt (eds) Representing space in cognition: Interrelations of behaviour, language, and formal models. Oxford University Press, UK (in press)"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comp Interact; citation_title=Exploring the functional specifications of a localized wayfinding verbal aid for blind pedestrians: simple and structured urban areas; citation_author=F Gaunet, X Briffault; citation_volume=20; citation_publication_date=2005; citation_pages=267-314; citation_doi=10.1207/s15327051hci2003_2; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Hum-Comput Interact; citation_title=Auditory icons: using sound in computer interfaces; citation_author=W Gaver; citation_volume=2; citation_publication_date=1986; citation_pages=167-177; citation_doi=10.1207/s15327051hci0202_3; citation_id=CR27"/>

    <meta name="citation_reference" content="Golledge RG, Klatzky RL, Loomis JM, Speigle J, Tietz J (1998) A geographical information system for a GPS-based personal guidance system. Int J Geograph Inform Sci 727&#8211;749"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Impair Blind; citation_title=Stated preferences for components of a Personal Guidance System for nonvisual navigation; citation_author=RG Golledge, JR Marston, JM Loomis, RL Klatzky; citation_volume=98; citation_publication_date=2004; citation_pages=135-147; citation_id=CR29"/>

    <meta name="citation_reference" content="Helal A, Moore SE, Ramachandran B (2001) Drishti: an integrated navigation system for visually impaired and disabled. International symposium on wearable computers ISWC, IEEE Computer Society, Washington DC, pp 149&#8211;156"/>

    <meta name="citation_reference" content="Hub A, Diepstraten J, Ertl T (2004) Design and development of an indoor navigation and object identification system for the blind. ACM SIGACCESS accessibility and computing (77&#8211;78):147&#8211;152"/>

    <meta name="citation_reference" content="citation_journal_title=Trans Geograph Inform Syst; citation_title=GIS and people with visual impairments or blindness: Exploring the potential for education, orientation, and navigation; citation_author=RD Jacobson, RM Kitchin; citation_volume=2; citation_issue=4; citation_publication_date=1997; citation_pages=315-332; citation_id=CR33"/>

    <meta name="citation_reference" content="Kammoun S, Dramas F, Oriola B, Jouffrais C (2010) Route Selection Algorithm for Blind Pedestrian. International conference on control, automation and systems, IEEE, KINTEX, Gyeonggi-do, Korea, pp 2223&#8211;2228"/>

    <meta name="citation_reference" content="citation_journal_title=Ing&#233;nierie et Recherche Biom&#233;dicale; citation_title=Navigation and space perception assistance for the visually impaired: the NAVIG project; citation_author=S Kammoun, G Parseihian, O Gutierrez, A Brilhault, A Serpa, M Raynal, B Oriola, M Mac&#233;, M Auvray, M Denis, S Thorpe, P Truillet, BFG Katz, C Jouffrais; citation_volume=33; citation_publication_date=2012; citation_pages=182-189; citation_id=CR35"/>

    <meta name="citation_reference" content="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: Proceedings of 14th meeting of the international conference on auditory display (ICAD), Paris 24&#8211;27 June, pp 1-7"/>

    <meta name="citation_reference" content="Katz BFG, Truillet P, Thorpe S, Jouffrais C (2010) NAVIG: Navigation assisted by artificial vision and GNSS. Workshop on multimodal location based techniques for extreme navigation (Pervasive 2010), Helsinki"/>

    <meta name="citation_reference" content="Katz BFG, Rio E, Picinali L (2010) LIMSI spatialisation engine, InterDeposit Digital Number IDDN.FR. 001.340014.000.S.P. 2010.000.31235"/>

    <meta name="citation_reference" content="Katz BFG, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M, Truillet P, Denis M, Thorpe S, Jouffrais C (2012) NAVIG: guidance system for the visually impaired using virtual augmented reality. J Technol Disability 24(2) (in press)"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Appl; citation_title=Cognitive load of navigating without vision when guided by virtual sound versus spatial language; citation_author=RL Klatzky, JR Marston, NA Giudice, RG Golledge, JM Loomis; citation_volume=12; citation_publication_date=2006; citation_pages=223-232; citation_doi=10.1037/1076-898X.12.4.223; citation_id=CR40"/>

    <meta name="citation_reference" content="Knapek M, Oropeza RS, Kriegman DJ (2000) Selecting promising landmarks, Proc. ICRA&#8217;00. IEEE international conference on robotics and automation vol 4, pp 3771&#8211;3777"/>

    <meta name="citation_reference" content="Liu X (2008) A camera phone based currency reader for the visually impaired. In: Proceedings of the 10th international ACM SIGACCESS conference on computers and accessibility, Halifax, Nova Scotia, Canada, pp 305&#8211;306"/>

    <meta name="citation_reference" content="Loomis JM, Golledge RG, Klatzlty RL, Speigle J, Tietz J (1994) Personal guidance system for visually impaired. In: Proceedings of first annual ACM conference on assistive technologies (Assets &#8217;94), pp 85&#8211;90"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators Virtual Environ; citation_title=Navigation system for the blind: auditory display modes and guidance; citation_author=JM Loomis, RG Golledge, RL Klatzky; citation_volume=7; citation_publication_date=1998; citation_pages=193-203; citation_doi=10.1162/105474698565677; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Impair Blind; citation_title=Personal guidance system for people with visual impairment: a comparison of spatial displays for route guidance; citation_author=JM Loomis, JR Marston, RG Golledge, RL Klatzky; citation_volume=99; citation_publication_date=2005; citation_pages=219-232; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_title=Assisting wayfinding in visually impaired travelers; citation_inbook_title=Applied spatial cognition: from research to cognitive technology; citation_publication_date=2006; citation_id=CR46; citation_author=JM Loomis; citation_author=RG Golledge; citation_author=RL Klatzky; citation_author=JR Marston; citation_publisher=Lawrence Erlbaum Associates"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=Smith EL Evaluation of spatial displays for navigation without sight; citation_author=JR Marston, JM Loomis, RL Klatzky, RG Golledge; citation_volume=3; citation_publication_date=2006; citation_pages=110-124; citation_doi=10.1145/1141897.1141900; citation_id=CR47"/>

    <meta name="citation_reference" content="M&#233;n&#233;las B, Picinali L, Katz BFG, Bourdot P (2010) Audio haptic feedbacks in a task of targets acquisition. IEEE symposium on 3d user interfaces (3DUI 2010), Waltham, USA, pp 51&#8211;54"/>

    <meta name="citation_reference" content="Mitchell HB (2007) Multi-sensor data fusion: an introduction. Springer, ISBN: 978-3540714637"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=The influence of visual experience on the ability to form spatial mental models based on route and survey descriptions; citation_author=ML Noordzij, S Zuidhoek, A Postma; citation_volume=100; citation_publication_date=2006; citation_pages=321-342; citation_doi=10.1016/j.cognition.2005.05.006; citation_id=CR51"/>

    <meta name="citation_reference" content="Park S-K, Suh YS, Do TN (2009) The pedestrian navigation system using vision and inertial sensors. ICROS-SICE international joint conference on 2009, Fukuoka, Japan 18&#8211;21 Aug., pp 3970&#8211;3974"/>

    <meta name="citation_reference" content="Parlouar RD, Mac&#233; FM, Jouffrais C (2009) Assistive device for the blind based on object recognition: an application to identify currency bills. ACM conference on computers and accessibility (ASSETS 2009), Pittsburgh, PA, USA, pp 227&#8211;228"/>

    <meta name="citation_reference" content="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc (accepted 2012&#8211;01&#8211;19)"/>

    <meta name="citation_reference" content="Parseihian G, Brilhault A, Dramas F (2010) NAVIG: an object localization system for the blind. Workshop on Multimodal Location Based Techniques for Extreme Navigation (Pervasive 2010), Helsinki, Finland"/>

    <meta name="citation_reference" content="Ran L, Helal S, Moore (2004) S Drishti: an integrated indoor/outdoor blind navigation system and service. IEEE international conference on pervasive computing and communications (PerCom&#8217;04), pp 23&#8211;30"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Impair Blind; citation_title=Inventory of electronic mobility aids for persons with visual impairments: a literature review; citation_author=UR Roentgen, GJ Gelderblom, M Soede, LP Witte; citation_volume=102; citation_issue=11; citation_publication_date=2008; citation_pages=702-724; citation_id=CR58"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track. Proc. CVPR&#8217;94. IEEE computer society conference on computer vision and pattern recognition, pp 593&#8211;600"/>

    <meta name="citation_reference" content="Strothotte T, Petrie H, Johnson V Reichert L (1995) Mobic: user needs and preliminary design for a mobility aid for blind and elderly travelers. In: Porrero IP, de la Bellacasa RP (eds) The European context for assistive technology, pp 348&#8211;352"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Speed of processing in the human visual system; citation_author=S Thorpe, D Fize, C Marlot; citation_volume=381; citation_issue=6582; citation_publication_date=1996; citation_pages=520-522; citation_doi=10.1038/381520a0; citation_id=CR62"/>

    <meta name="citation_reference" content="citation_title=Robotic mapping: A survey; citation_inbook_title=Exploring artificial intelligence in the new millennium; citation_publication_date=2002; citation_pages=1-35; citation_id=CR63; citation_author=S Thrun; citation_publisher=Elsevier Science"/>

    <meta name="citation_reference" content="citation_journal_title=Evaluation of acoustic beacon characteristics for navigation tasks. Ergonomics; citation_author=T Tran, T Letowski, K Abouchacra; citation_volume=43; citation_issue=6; citation_publication_date=2000; citation_pages=807-827; citation_id=CR64"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project; citation_author=J-M V&#233;zien, B M&#233;n&#233;las, J Nelson, L Picinali, P Bourdot, M Ammi, BFG Katz, JM Burkhardt, L Pastur, F Lusseyran; citation_volume=13; citation_publication_date=2009; citation_pages=257-271; citation_doi=10.1007/s10055-009-0134-1; citation_id=CR65"/>

    <meta name="citation_reference" content="V&#246;lkel T, K&#252;hn R, Weber G (2008) Mobility impaired pedestrians are not cars: requirements for the annotation of geographical data. Computers Helping People with Special Needs, LNCS 2008 5105:1085&#8211;1092"/>

    <meta name="citation_reference" content="Walker BN, Lindsay J (2005) Navigation performance in a virtual environment with bonephones. In: Proceedings of international conference on auditory display, Limerick, Ireland, pp 260&#8211;263"/>

    <meta name="citation_reference" content="citation_journal_title=Navigation performance with a virtual auditory display: Navigation performance with a virtual auditory display: effects of beacon sound, capture radius, and practice. Hum Factors: J Hum Fact Ergonomics Soci Sum; citation_author=BN Walker, J Lindsay; citation_volume=48; citation_issue=2; citation_publication_date=2006; citation_pages=265-278; citation_id=CR69"/>

    <meta name="citation_reference" content="Walker BN, Nance A, Lindsay J (2006) Spearcons: speech-based earcons improve navigation performance in auditory menus. In: Proceedings of international conference on auditory display (ICAD2006), pp 95&#8211;98"/>

    <meta name="citation_reference" content="Zheng J, Winstanley A, Pan Z, Coveney S (2009) Spatial characteristics of walking areas for pedestrian navigation. Third International conference on multimedia and ubiquitous engineering, IEEE, China, pp 452&#8211;458"/>

    <meta name="citation_author" content="Brian F. G. Katz"/>

    <meta name="citation_author_email" content="brian.katz@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, Universit&#233; Paris Sud, Orsay, France"/>

    <meta name="citation_author" content="Slim Kammoun"/>

    <meta name="citation_author_email" content="kammoun@irit.fr"/>

    <meta name="citation_author_institution" content="IRIT, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author" content="Ga&#235;tan Parseihian"/>

    <meta name="citation_author_email" content="gaetan.parseihian@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, Universit&#233; Paris Sud, Orsay, France"/>

    <meta name="citation_author" content="Olivier Gutierrez"/>

    <meta name="citation_author_email" content="gutierrez@irit.fr"/>

    <meta name="citation_author_institution" content="IRIT, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author" content="Adrien Brilhault"/>

    <meta name="citation_author_email" content="adrien.brilhault@cerco.ups-tls.fr"/>

    <meta name="citation_author_institution" content="IRIT, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author_institution" content="CerCo, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author" content="Malika Auvray"/>

    <meta name="citation_author_email" content="malika.auvray@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, Universit&#233; Paris Sud, Orsay, France"/>

    <meta name="citation_author" content="Philippe Truillet"/>

    <meta name="citation_author_email" content="truillet@irit.fr"/>

    <meta name="citation_author_institution" content="IRIT, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author" content="Michel Denis"/>

    <meta name="citation_author_email" content="michel.denis@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, Universit&#233; Paris Sud, Orsay, France"/>

    <meta name="citation_author" content="Simon Thorpe"/>

    <meta name="citation_author_email" content="simon.thorpe@cerco.ups-tls.fr"/>

    <meta name="citation_author_institution" content="CerCo, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_author" content="Christophe Jouffrais"/>

    <meta name="citation_author_email" content="jouffrais@irit.fr"/>

    <meta name="citation_author_institution" content="IRIT, CNRS &amp; Universit&#233; Paul Sabatier, Toulouse, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-012-0213-6&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-012-0213-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="NAVIG: augmented reality guidance system for the visually impaired"/>
        <meta property="og:description" content="Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>NAVIG: augmented reality guidance system for the visually impaired | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-012-0213-6","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Assisted navigation, Guidance, Spatial audio, Visually impaired assistive device, Need analysis","kwrd":["Assisted_navigation","Guidance","Spatial_audio","Visually_impaired_assistive_device","Need_analysis"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-012-0213-6","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-012-0213-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=213;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-012-0213-6">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            NAVIG: augmented reality guidance system for the visually impaired
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0213-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0213-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-06-12" itemprop="datePublished">12 June 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">NAVIG: augmented reality guidance system for the visually impaired</h1><p lang="en">Combining object localization, GNSS, and spatial audio</p>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brian_F__G_-Katz" data-author-popup="auth-Brian_F__G_-Katz" data-corresp-id="c1">Brian F. G. Katz<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université Paris Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, Université Paris Sud, 91403, Orsay, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Slim-Kammoun" data-author-popup="auth-Slim-Kammoun">Slim Kammoun</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ga_tan-Parseihian" data-author-popup="auth-Ga_tan-Parseihian">Gaëtan Parseihian</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université Paris Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, Université Paris Sud, 91403, Orsay, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Olivier-Gutierrez" data-author-popup="auth-Olivier-Gutierrez">Olivier Gutierrez</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adrien-Brilhault" data-author-popup="auth-Adrien-Brilhault">Adrien Brilhault</a></span><sup class="u-js-hide"><a href="#Aff2">2</a>,<a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, CerCo, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Malika-Auvray" data-author-popup="auth-Malika-Auvray">Malika Auvray</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université Paris Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, Université Paris Sud, 91403, Orsay, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Philippe-Truillet" data-author-popup="auth-Philippe-Truillet">Philippe Truillet</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michel-Denis" data-author-popup="auth-Michel-Denis">Michel Denis</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université Paris Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, Université Paris Sud, 91403, Orsay, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Simon-Thorpe" data-author-popup="auth-Simon-Thorpe">Simon Thorpe</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, CerCo, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christophe-Jouffrais" data-author-popup="auth-Christophe-Jouffrais">Christophe Jouffrais</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS &amp; Université Paul Sabatier" /><meta itemprop="address" content="grid.15781.3a, 000000010723035X, IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">253</span>–<span itemprop="pageEnd">269</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2125 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">59 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-012-0213-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>According to many reports issued by national committees [see e.g. In´egalit´e des Chances (Canadian Institute for the Blind <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Canadian Institute for the Blind (2005) Inégalité des chances : Rapport sur les besoins des personnes aveugles ou handicapées visuelles vivant au Canada. Technical Report. &#xA;                    http://www.cnib.ca/fr/apropos/publications/recherche&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0213-6#ref-CR16" id="ref-link-section-d5961e472">2005</a>)], visually impaired—including blind and low vision—people require assistance in their daily life. One of the most problematic tasks is navigation, which involves two main action components—mobility and orientation. These two processes were defined by Loomis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Loomis JM, Golledge RG, Klatzky RL, Marston JR (2006) Assisting wayfinding in visually impaired travelers. In: Allen G (eds) Applied spatial cognition: from research to cognitive technology, Lawrence Erlbaum Associates, Mahwah" href="/article/10.1007/s10055-012-0213-6#ref-CR46" id="ref-link-section-d5961e475">2006</a>). The first element relates to sensing the immediate (or near-field) environment, including obstacles and potential paths in the vicinity, for the purpose of moving through it. It may rely on visual, auditory, or olfactory stimuli identification and localization. In the literature, this set of processes has been labeled as Micro-Navigation or Mobility. The second component, typically termed Orientation or Macro-Navigation, includes multiple processes such as being oriented, selecting an appropriate path, maintaining a path, and detecting when the destination has been reached. All these tasks are dedicated to processing the remote (or far-field) environment, beyond the immediately perceptible one. In the case of visual impairment, the main cues used for sensing the environment (e.g., detecting obstacles, landmarks, and paths) are lacking. As such, both micro- and macro-navigation processes are degraded, resulting in difficulties related to obstacle avoidance, finding immediate paths, piloting (guidance from place to place using landmarks), correct orientation or heading, maintaining the path, etc.</p><p>A recent literature review of existing electronic aids (2007–2008) for visually impaired (VI) individuals identified more than 140 products, systems, and assistive devices while providing details on 21 commercially available systems (Roentgen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Roentgen UR, Gelderblom GJ, Soede M, de Witte LP (2008) Inventory of electronic mobility aids for persons with visual impairments: a literature review. J Vis Impair Blind 102(11):702–724" href="/article/10.1007/s10055-012-0213-6#ref-CR58" id="ref-link-section-d5961e481">2008</a>). The different systems were divided into two categories: (1) obstacle detection or micro-navigation and (2) macro-navigation. The different cited micro-navigation systems are mainly devoted to improving obstacle avoidance (by extending the range of the white cane) or “shorelining”. They do not provide assistance for object localization or more complex navigation tasks.</p><p>Macro-navigation systems are almost exclusively based on the use of Global Positioning System (GPS) with adaptations for visually impaired users (see Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Loomis JM, Marston JR, Golledge RG, Klatzky RL (2005) Personal guidance system for people with visual impairment: a comparison of spatial displays for route guidance. J Vis Impair Blind 99:219–232" href="/article/10.1007/s10055-012-0213-6#ref-CR45" id="ref-link-section-d5961e487">2005</a>; Marston et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Marston JR, Loomis JM, Klatzky RL, Golledge RG (2006) Smith EL Evaluation of spatial displays for navigation without sight. ACM Trans Appl Percept 3:110–124" href="/article/10.1007/s10055-012-0213-6#ref-CR47" id="ref-link-section-d5961e490">2006</a> for an evaluation overview of GPS devices for VI individuals). The MoBIC (Strothotte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Strothotte T, Petrie H, Johnson V Reichert L (1995) Mobic: user needs and preliminary design for a mobility aid for blind and elderly travelers. In: Porrero IP, de la Bellacasa RP (eds) The European context for assistive technology, pp 348–352" href="/article/10.1007/s10055-012-0213-6#ref-CR61" id="ref-link-section-d5961e493">1995</a>) project (mobility for blind and elderly people interacting with computers), presented in 1995, addressed outdoor navigation problems for the visually impaired. The MoBIC system consisted of two components: the MoPS pre-journey system that enables preparation at home, and the MoODS outdoor system that provides positioning and easy access to all necessary information. Evaluations have shown that such a system is primarily limited by the precision of the positioning system and the details in the geographic database. Another very important project in the area of navigation without sight was conducted over a period of twenty years by Loomis, Golledge, and Klatzky. Their goal was the development of a personal guidance system (PGS) for the blind (Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Loomis JM, Golledge RG, Klatzlty RL, Speigle J, Tietz J (1994) Personal guidance system for visually impaired. In: Proceedings of first annual ACM conference on assistive technologies (Assets ’94), pp 85–90" href="/article/10.1007/s10055-012-0213-6#ref-CR43" id="ref-link-section-d5961e496">1994</a>). The PGS system included three modules: (1) a module for determining the position and orientation of the traveler using Differential GPS (DGPS), (2) a Geographic Information System (GIS) module, and (3) a user interface. Several evaluations were performed, which demonstrated the effectiveness of such a device in helping visually impaired persons navigate.</p><p>Other works include (Ran et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ran L, Helal S, Moore (2004) S Drishti: an integrated indoor/outdoor blind navigation system and service. IEEE international conference on pervasive computing and communications (PerCom’04), pp 23–30" href="/article/10.1007/s10055-012-0213-6#ref-CR57" id="ref-link-section-d5961e502">2004</a>), which extended the outdoor version of Drishti (Helal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Helal A, Moore SE, Ramachandran B (2001) Drishti: an integrated navigation system for visually impaired and disabled. International symposium on wearable computers ISWC, IEEE Computer Society, Washington DC, pp 149–156" href="/article/10.1007/s10055-012-0213-6#ref-CR31" id="ref-link-section-d5961e505">2001</a>), an integrated navigation system for the visually impaired and disabled, to a navigation assistive device integrating indoor positioning. Indoor positioning was accomplished using an ultrasound device. Results showed an indoor accuracy of 22 cm. Concerning outdoor positioning, DGPS was recommended. Differential GPS improves GPS accuracy to approximately ±10 cm in the best conditions. Unfortunately, it relies on a network of fixed, ground-based reference stations that are currently only available in North America (via the Wide Area Augmentation System).</p><p>Outside of research related projects, several adapted GPS-based navigation systems for VI users have been commercialized, with probably the most popular GPS-based personal guidance systems being <b>BrailleNotesGPS</b> (Sendero Inc.) and <b>Trekker</b> (HumanWare Inc.). Although they are very useful in unknown environments, they still suffer usability limitations (especially due to GPS precision and map weakness). While no commercial products that were able to detect and locate specific objects without the necessity of pre-equipping them with dedicated sensors (e.g., RFID tags) were reported, several research projects are considering this issue. A visual-based system, incorporating a handheld stereo camera and WiFi-based tracking for indoor use was presented in Hub et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hub A, Diepstraten J, Ertl T (2004) Design and development of an indoor navigation and object identification system for the blind. ACM SIGACCESS accessibility and computing (77–78):147–152" href="/article/10.1007/s10055-012-0213-6#ref-CR32" id="ref-link-section-d5961e518">2004</a>). This system relies on the use of 3D models of precise objects and predefined spaces in order for them to be identified, greatly limiting its use outside of the designed environment. Direct sensory substitution systems, which directly transform data from images to auditory or tactile devices without any interpretation, can be used for rudimentary obstacle detection and avoidance, or for research on brain plasticity and learning (Auvray and Myin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Auvray M, Myin E (2009) Perception with compensatory devices. From sensory substitution to sensorimotor extension. Cogn Sci 33:1036–1058" href="/article/10.1007/s10055-012-0213-6#ref-CR4" id="ref-link-section-d5961e521">2009</a>; Auvray et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Auvray M, Hanneton S, O’Regan JK (2007) Learning to perceive with a visuo-auditory substitution system: localization and object recognition with the voice. Perception 36:416–430" href="/article/10.1007/s10055-012-0213-6#ref-CR5" id="ref-link-section-d5961e524">2007</a>). However, these systems have a large learning curve and are very hard to use in any practical sense.</p><p>Thus, it appears that micro-navigation devices focus on obstacle detection only and that macro-navigation devices are mainly regular GPS systems with the addition of non-visual interactions. None of the assistive devices mentioned have aimed to improve the sensing of the immediate environment, which is a fundamental process in navigation and spatial cognition in general.</p><p>Based on the needs of VI individuals (Gallay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gallay M, Denis M, Auvray M (2012) Navigation assistance for blind pedestrians: guidelines for the design of devices and implications for spatial cognition. In: Thora Tenbrink, Jan Wiener, Christophe Claramunt (eds) Representing space in cognition: Interrelations of behaviour, language, and formal models. Oxford University Press, UK (in press)" href="/article/10.1007/s10055-012-0213-6#ref-CR25" id="ref-link-section-d5961e532">2012</a>; Golledge et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Golledge RG, Marston JR, Loomis JM, Klatzky RL (2004) Stated preferences for components of a Personal Guidance System for nonvisual navigation. J Vis Impair Blind 98:135–147" href="/article/10.1007/s10055-012-0213-6#ref-CR29" id="ref-link-section-d5961e535">2004</a>), the NAVIG<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> project (2009–2012) aims to design an assistive device that provides aid in two problematic situations: (1) near-field sensing (specific objects identification and localization) and guidance (heading, grasping, and piloting); and (2) far-field navigation relying on an adapted GIS database and route selection process adapted to VI individuals). For both near-field and far-field tasks, guidance will be provided through the generation of an audio augmented reality environment via binaural rendering, providing both spatialized text-to-speech and sonifications, allowing the full exploitation of the human perceptual and cognitive capacity for spatial hearing (Dramas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dramas F, Oriola B, Katz BFG, Thorpe S, Jouffrais C (2008) Designing an assistive device for the blind based on object localization and augmented auditory reality. ACM conference on computers and accessibility, ASSETS, Halifax, Canada, pp 263–264" href="/article/10.1007/s10055-012-0213-6#ref-CR20" id="ref-link-section-d5961e551">2008</a>). The combination of these two functions provides a powerful assistive device. The system should permit VI individuals to move at their own pace toward a desired destination in a sure and precise manner, without interfering with normal behavior or mobility. Through the use of an artificial vision module, this system also assists users in the localization and grasping of objects in the immediate environment without the need to pre-equip them with electronic tags.</p><p>The aim of this paper is to present the NAVIG system architecture and the specifics related to macro and micro-navigational use. NAVIG follows a method of participatory design and some aspects of this process are presented here. A presentation of the first prototype is also shown, with a discussion of developments underway. Portions of this work have been previously presented (see Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz BFG, Truillet P, Thorpe S, Jouffrais C (2010) NAVIG: Navigation assisted by artificial vision and GNSS. Workshop on multimodal location based techniques for extreme navigation (Pervasive 2010), Helsinki" href="/article/10.1007/s10055-012-0213-6#ref-CR37" id="ref-link-section-d5961e557">2010</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Katz BFG, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M, Truillet P, Denis M, Thorpe S, Jouffrais C (2012) NAVIG: guidance system for the visually impaired using virtual augmented reality. J Technol Disability 24(2) (in press)" href="/article/10.1007/s10055-012-0213-6#ref-CR39" id="ref-link-section-d5961e560">2012</a>; Parseihian et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Parseihian G, Brilhault A, Dramas F (2010) NAVIG: an object localization system for the blind. Workshop on Multimodal Location Based Techniques for Extreme Navigation (Pervasive 2010), Helsinki, Finland" href="/article/10.1007/s10055-012-0213-6#ref-CR55" id="ref-link-section-d5961e563">2010</a>; Kammoun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kammoun S, Dramas F, Oriola B, Jouffrais C (2010) Route Selection Algorithm for Blind Pedestrian. International conference on control, automation and systems, IEEE, KINTEX, Gyeonggi-do, Korea, pp 2223–2228" href="/article/10.1007/s10055-012-0213-6#ref-CR34" id="ref-link-section-d5961e566">2010</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kammoun S, Parseihian G, Gutierrez O, Brilhault A, Serpa A, Raynal M, Oriola B, Macé M, Auvray M, Denis M, Thorpe S, Truillet P, Katz BFG, Jouffrais C (2012) Navigation and space perception assistance for the visually impaired: the NAVIG project. Ingénierie et Recherche Biomédicale 33:182–189" href="/article/10.1007/s10055-012-0213-6#ref-CR35" id="ref-link-section-d5961e569">2012</a>; Brilhault et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to improve blind pedestrian positioning. International conference on new technologies, mobility and security, IEEE, France" href="/article/10.1007/s10055-012-0213-6#ref-CR12" id="ref-link-section-d5961e573">2011</a>; Parlouar et al.  <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Parlouar RD, Macé FM, Jouffrais C (2009) Assistive device for the blind based on object recognition: an application to identify currency bills. ACM conference on computers and accessibility (ASSETS 2009), Pittsburgh, PA, USA, pp 227–228" href="/article/10.1007/s10055-012-0213-6#ref-CR53" id="ref-link-section-d5961e576">2009</a>).</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">User needs and participatory design</h2><div class="c-article-section__content" id="Sec2-content"><p>The first point that must be emphasized is that the NAVIG system is not intended to replace the white cane or the guide dog. It should rather be considered as a complement to these devices and general Orientation and Mobility (O&amp;M) training sessions. Therefore, the primary objective for the device is to help VI people (i.e., early and late blind as well as people partially deprived of vision) improve their daily autonomy and ability to mentally represent their environment beyond what is possible with traditional assistive devices. The visually impaired most often employ egocentric spatial representation strategies (Gallay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gallay M, Denis M, Auvray M (2012) Navigation assistance for blind pedestrians: guidelines for the design of devices and implications for spatial cognition. In: Thora Tenbrink, Jan Wiener, Christophe Claramunt (eds) Representing space in cognition: Interrelations of behaviour, language, and formal models. Oxford University Press, UK (in press)" href="/article/10.1007/s10055-012-0213-6#ref-CR25" id="ref-link-section-d5961e587">2012</a>; Noordzij et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Noordzij ML, Zuidhoek S, Postma A (2006) The influence of visual experience on the ability to form spatial mental models based on route and survey descriptions. Cognition 100:321–342" href="/article/10.1007/s10055-012-0213-6#ref-CR51" id="ref-link-section-d5961e590">2006</a>; Afonso et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Afonso A, Blum A, Katz BFG, Tarroux P, Borst G, Denis M (2010) Structural properties of spatial representations in blind people: scanning images constructed from haptic exploration or from locomotion in a 3-D audio virtual environment. Memory Cogn 38:591–604" href="/article/10.1007/s10055-012-0213-6#ref-CR2" id="ref-link-section-d5961e593">2010</a>). However, the integration of several different paths into a general or global representation is necessary for route variations, such as detours, shortcuts, and journey reorganization. Creating these internal representations requires additional effort on the part of the individual. A study of the cognitive load associated with the use of language and virtual sounds for navigation guidance can be found in (Klatzky et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Klatzky RL, Marston JR, Giudice NA, Golledge RG, Loomis JM (2006) Cognitive load of navigating without vision when guided by virtual sound versus spatial language. J Exp Psychol Appl 12:223–232" href="/article/10.1007/s10055-012-0213-6#ref-CR40" id="ref-link-section-d5961e596">2006</a>).</p><p>Designing an assistive device for VI users implies the need to sufficiently describe the problems these individuals typically face and hence the needs that the device should respond to. In order to provide ongoing assistance throughout the project, the NAVIG team included the Institute for the Young Blind, Toulouse, enabling a participatory design strategy with potential users of the system. A panel of four O&amp;M instructors and a number of VI participants were involved in brainstorming, participatory design sessions, as well as psychological and ergonomic experiments and prototype evaluations. The panel comprised 21 VI potential users (6 female/15 male, aged 17–57, mean 37). A questionnaire was given concerning their degree of autonomy and technological knowledge. For daily mobility, five of the panel employed a guide dog, 12 used a white cane, while the remaining 4 preferred to have a person to guide them. A product of the various design sessions with O&amp;M instructors and visually impaired volunteers has been the construction of a number of guidelines for the development of the assistive device, whose main results are provided below (see Brunet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Brunet L (2010) Étude des besoins et des stratégies des personnes non-voyantes lors de la navigation pour la conception d’un dispositif d’aide performant et accepté (Needs and strategy study of blind people during navigation for the design of a functional and accepted aid device). Master’s thesis, Department of Ergonomics, Université Paris-Sud, Orsay, France" href="/article/10.1007/s10055-012-0213-6#ref-CR13" id="ref-link-section-d5961e602">2010</a> for further details).</p><h3 class="c-article__sub-heading" id="Sec3">Route planning study</h3><p>A brainstorming session with six VI panel members was conducted to address the needs for journey planning for an autonomous pedestrian itinerary (Brunet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Brunet L (2010) Étude des besoins et des stratégies des personnes non-voyantes lors de la navigation pour la conception d’un dispositif d’aide performant et accepté (Needs and strategy study of blind people during navigation for the design of a functional and accepted aid device). Master’s thesis, Department of Ergonomics, Université Paris-Sud, Orsay, France" href="/article/10.1007/s10055-012-0213-6#ref-CR13" id="ref-link-section-d5961e612">2010</a>). As the study aimed at generalizing findings to both early and late blind people, a mixture of participants was included. Results indicated that a detailed preliminary planning phase improved cognitive representations of the environment.</p><p>The need for a customizable preliminary planning phase was then confirmed by an empirical study focusing on the interconnection between the preparation and execution phases of navigation and on the internal and external factors underlying this activity. Six visually impaired (4 early blind and 2 late blind, ages 27–57, mean 38.9) Parisian participants were followed by experimenters while preparing and completing a 2-km unfamiliar route in a residential area of Paris, France, (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig1">1</a>) using the technique of information on-demand (adapted from Bisseret et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bisseret A, Sebillote S, Falzon P (1999) Techniques pratiques pour l’étude des activités expertes. Octarès Editions, Toulouse" href="/article/10.1007/s10055-012-0213-6#ref-CR10" id="ref-link-section-d5961e621">1999</a>). This experiment consisted of allowing the participants to ask the experimenter the desired information instead of searching for it by themselves, so that the experimenter simulated an ideal navigation assistive device.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Scenario used for route planning study</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Participants were divided into three groups according to the level of preparatory planning performed (preparation with human assistance, without human assistance, and no preparation at all). A description of the different groups and the number of questions posed by each group to the experimenter is given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0213-6#Tab1">1</a> for both the preparatory and route guidance phases. Analysis of the transcribed questions allowed for the classification of the different questions posed by all participants. Several categories of questions could be defined, allowing them to be classified according to their relationship to the state of progress along the route. The occurrence of the different categories of questions with respect to the different groups highlights the relation between route planning and the type, and the amount of required information for an efficient and pleasant trip, in addition to the category descriptions, is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0213-6#Tab2">2</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Group descriptions and group means of the number of questions posed during the <i>information on-demand</i> experiment during the preparatory (Prep.) and route exploration (Route) phases</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0213-6/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Classification of questions posed during the route guidance phase of the <i>information on-demand</i> experiment according to trajectory segment, <b>T</b>, relative to the current point <b>T0</b>, and the proportion of each question category as a function of experimental group <b><i>G</i></b>
                        </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0213-6/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The amount of information requested by the participants to complete the journey, combined with the analysis of post-experimental interviews, was used to define several user profiles. Each profile consisted of different strategies and as a result had a specific set of needs that should be separately addressed by the NAVIG system.</p><p>This design study thus allows for the creation of guidelines related to the guidance information that is to be provided to the user during the course of navigation, taking into account the presence, or absence, of preparatory route planning. In particular, the advantages observed for users who performed preparatory route planning have led to the definition of an overview function for the guidance system. Users of the NAVIG system should be able to prepare and preview their upcoming journey at home, to access an overview or a more detailed description of the path to follow, to receive knowledge about the different points of interest along the path including appropriate spatial cues to locate their specific path with respect to the general surrounding environment, and to be able to preview the actual route guidance.</p><p>Information relative to a route segment, and when to present this information, is a component of the route guidance system. A certain degree of flexibility in these rules should be designed into the system. The adjustment of these parameters will be the subject of further testing with the actual system.</p><h3 class="c-article__sub-heading" id="Sec4">Route difficulty assessment</h3><p>Instructors and potential users have specifically insisted on the fact that an assistance device must be highly customizable. Concerning the routes offered by the system (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec14">3.6</a>), this implies that the most independent and confident users can request the shortest path, even if it involves various difficulties, whereas the most conservative users can request a longer but easier to follow more prudent path route.</p><p>A means of identifying or classifying route points to favor or avoid when calculating an optimal route was seen as necessary. A 2-h brainstorming session was conducted with six panel participants. A consensus set of preferable route elements was found: simple crossings, roads with lighter traffic, larger walkways to allow for faster walking, and the shortest route if time is a chosen consideration. In addition, a set of elements to avoid was also established: plazas, roundabouts, large open areas, very large walkways (due to the increased presence of obstacles), very narrow walkways, shared pedestrian/automobile areas, and areas with many poles or fences. These findings converge with those by Gaunet and Briffault (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gaunet F, Briffault X (2005) Exploring the functional specifications of a localized wayfinding verbal aid for blind pedestrians: simple and structured urban areas. Hum Comp Interact 20:267–314" href="/article/10.1007/s10055-012-0213-6#ref-CR26" id="ref-link-section-d5961e1126">2005</a>).</p><p>In the interest of refining these results, in order to be able to apply them to automated route selection, a method to associate difficulty scores for the most common urban elements was established. Participants were first asked to cite the three types of events/obstacles they find the most problematic in a pedestrian journey. A list of items was derived from their answers. Participants were then asked to rate the difficulty of each element on a scale from 1 to 5 (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0213-6#Tab2">3</a>). Those scores can be used to create a list of proposed paths which users will be able to select from, based on their own criteria of confidence, time available, and acceptable level of difficulty.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Predominant pedestrian obstacles encountered with associated mean difficulty scores with standard deviation (rated on a scale from 1 to 5)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0213-6/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec5">Ideal guidance information</h3><p>Analysis of discussions concerning the audio guidance provided by an ideal system highlighted the need to control the amount of information presented. The quantity of information should take into account current conditions as well as individual user needs and preferences. In general, the amount of information provided should be minimal, avoiding excess, presenting only what is necessary, and sufficient to aid the user (see also Allen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Allen GL (2000) Principles and practices for communicating route knowledge. Appl Cogn Psychol 14:333–359" href="/article/10.1007/s10055-012-0213-6#ref-CR3" id="ref-link-section-d5961e1288">2000</a>; Denis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Denis M (1997) The description of routes: acognitive approach to the production of spatial discourse. Curr Pychol Cogn 16:409–458" href="/article/10.1007/s10055-012-0213-6#ref-CR18" id="ref-link-section-d5961e1291">1997</a>). The information provided should be highly efficient and minimally intrusive. These guidelines have been taken into consideration in the modification of georeferenced information databases (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec11">3.4.1</a>) and in the design of the audio feedback to the user (see Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec13">3.5</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec14">3.6</a>).</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">System design</h2><div class="c-article-section__content" id="Sec6-content"><p>The different objectives of NAVIG will be attained by combining input data furnished through satellite-based geolocalization and an ultra-rapid image recognition system. Guidance will be provided using spatialized audio rendering with both text-to-speech and specifically designed semantic sonification metaphors.</p><p>The system prototype architecture is divided into several functional elements structured around a multi-agent framework using a communication protocol based on the IVY middleware (Buisson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Buisson M, Bustico A, Chatty S, Colin F-R, Jestin Y, Maury S, Mertz C, Truillet P (2002) Ivy: un bus logiciel au service du développement de prototypes de systèmes interactifs. 14th French-speaking conference on human– computer interaction (IHM ’02) pp 223-2 26" href="/article/10.1007/s10055-012-0213-6#ref-CR14" id="ref-link-section-d5961e1314">2002</a>). With this architecture, agents are able to dynamically connect to, or disconnect from, different data streams on the IVY bus. The general architecture of the system is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig2">2</a>. The main operating elements of the NAVIG system can be divided into three groups: data input, user communication, and internal system control. The data input elements consist of a satellite-based geopositioning system, acceleration and orientation sensors, Geographic Information System (GIS) map databases, an ultra-rapid module processing images from head-mounted stereoscopic cameras, and a data fusion module. User communications are handled predominantly through a voice recognition system for input and an audio rendering engine using text-to-speech and conceptual trajectory sonification for output. In the following sections, the different system components will be presented and discussed individually.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>NAVIG system architecture overview</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec7">The user interface</h3><p>The user interface (UI) acts as the directing component between user’s requirements and system functions. In order to design an adapted UI, two separate brainstorming sessions were organized with VI individuals and system designers to develop a working scenario and interaction techniques. The goal was to define the nature and quantity of information required during a guided travel, as well as appropriate modalities that would not interfere with learned O&amp;M techniques and abilities so as not be obtrusive. At the conclusion of these meetings, all participants cited speech interaction as the most natural and preferred method. An interactive menu was implemented based on a voice recognition engine (Dragon Naturally Speaking) and a text-to-speech generator (Elan Real Speak). The voice menu offers different possibilities for the user to interact with the NAVIG system in indoor and outdoor situations.</p><p>In both indoor and outdoor navigation, the user may ask for a specific object known to the system or for a destination to reach. Indoors, known objects may include stationary localized targets such as signs, elevators, or vending machines, as well as standardized objects such as furniture. If the map of the building is embedded, the destination may be a specific location or room. In outdoor situations, the main function consists in entering an address or place (e.g., postal office), as well as a known object (e.g., the mailbox, door entrance). An explicit strategy for dialog was implemented that allows a novice user to understand the menu hierarchy. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0213-6#Tab4">4</a> illustrates a typical address input, with the departure location provided by the actual geolocation of the user.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 A prototypic address input scenario</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0213-6/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec8">Object Identification and Localization</h3><p>Improvements in computer hardware, the cheap cost of cameras, and increasingly sophisticated computer vision algorithms have contributed to the development of systems where the recognition and localization of visual features can be used in navigation tasks. The typical approach is to track a set of arbitrary visual features (which are both perceptually salient and visually distinctive to be detected robustly, see Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. Proc. CVPR’94. IEEE computer society conference on computer vision and pattern recognition, pp 593–600" href="/article/10.1007/s10055-012-0213-6#ref-CR59" id="ref-link-section-d5961e1473">1994</a>; Knapek et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Knapek M, Oropeza RS, Kriegman DJ (2000) Selecting promising landmarks, Proc. ICRA’00. IEEE international conference on robotics and automation vol 4, pp 3771–3777" href="/article/10.1007/s10055-012-0213-6#ref-CR41" id="ref-link-section-d5961e1476">2000</a>) to estimate the changes in self-position and, from this information, to build a local map of the environment. In robotics, this technique, termed SLAM (Simultaneous localization and mapping Thrun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Thrun S (2002) Robotic mapping: A survey. In: Lakemeyer G, Nebel B (eds) Exploring artificial intelligence in the new millennium, Elsevier Science, USA, pp 1–35" href="/article/10.1007/s10055-012-0213-6#ref-CR63" id="ref-link-section-d5961e1479">2002</a>), usually requires an independent set of sensors to compute the robot’s motion over time. While inertial odometry can be quite accurate for wheeled robots and vehicles where the angles of steering and wheel rotation encoders provide reliable estimates of the motion from a given starting location, it is much more complex in the case of pedestrian displacement. The motion of a walking person exhibits high variation in velocity and trajectory, and the estimate of the number of steps, step length, and heading from pedometers and accelerometers is rarely of sufficient accuracy. In addition, visual odometry as well as inertial odometry inevitably accumulate errors over time, which results in an increasing drift in position with time, if these errors are not regularly corrected with an absolute reference position. For these reasons, the proposed system employs visual landmarks with known geographic positions (annotated in the GIS database, see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec10">3.4</a>) to refine the pedestrian position estimated by a GPS. Few other systems have proposed such a solution (see Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Park S-K, Suh YS, Do TN (2009) The pedestrian navigation system using vision and inertial sensors. ICROS-SICE international joint conference on 2009, Fukuoka, Japan 18–21 Aug., pp 3970–3974" href="/article/10.1007/s10055-012-0213-6#ref-CR52" id="ref-link-section-d5961e1485">2009</a>).</p><p>The vision unit of the NAVIG system is designed to extract relevant visual information from the environment in order to compensate for the visual deficit of the user. To do so, the user is equipped with head-mounted stereoscopic cameras providing images of the surroundings, which are processed in real-time by computer vision algorithms localizing objects of interest. Different categories of targets can be detected depending on the circumstances. These can be either objects requested by the user, or geolocated landmarks requested by the system and used to compute the user’s position. In both cases, the core function of performing pattern-matching remains the same, relying on a biologically inspired image processing library called SpikeNet.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
                </p><p>The SpikeNet recognition algorithms are based on biological vision research, specifically on the mechanisms involved in ultra-rapid recognition within the human visual system (Thorpe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381(6582):520–522" href="/article/10.1007/s10055-012-0213-6#ref-CR62" id="ref-link-section-d5961e1507">1996</a>). Computational models of these mechanisms led to the development of a recognition engine providing very fast processing, high robustness to noise and light conditions, and good invariance properties to scale or rotation. SpikeNet uses a model for each visual pattern that needs to be recognized, which encodes the visual saliencies of the shape in a small structure (30 × 30 px (pixel) patch), thus requiring very little memory. Several models are usually needed to detect a specific object from different points of view, but even in an embedded system millions of models can easily be stored and loaded when needed given their small size. In terms of speed, the processing time strongly depends on the size of the input and target images. As an example, for the current NAVIG prototype based on a notebook equipped with an Intel i7 820QM processor (1.73 GHz) and 4 GB memory, the recognition engine achieved a stable analysis speed of 15 fps (frames per second) with a 320 × 240 px image stream while concurrently searching for 750 visual shapes of size 120 × 120 px. Maintaining a rate of 15 fps, the number of different models that could be tested was reduced to 250 for 60 × 60 px models, and to 65 with 30 × 30 px models.</p><p>In the “object-identification" mode, the user expressly requests an object of current interest. Then the system automatically loads the corresponding models. When the target is detected, the relative location is computed by stereovision methods. The object is then rendered via virtual 3D sounds (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec13">3.5</a>), with its position updated every 60 ms and interpolated between updates using head rotation data provided by an inertial measurement unit attached to the helmet. This function plays a very important role in the NAVIG device as it restores a functional visuomotor loop (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec15">4</a>) allowing a visually impaired user to move his/her body or hand to targets of interest (Dramas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Dramas F, Thorpe SJ, Jouffrais C (2010) Artificial vision for the blind: a bio-inspired algorithm for objects and obstacles detection. J Image Graph 10(4):531–544" href="/article/10.1007/s10055-012-0213-6#ref-CR21" id="ref-link-section-d5961e1519">2010</a>).</p><p>The second function of the vision unit, detailed in (Brilhault et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to improve blind pedestrian positioning. International conference on new technologies, mobility and security, IEEE, France" href="/article/10.1007/s10055-012-0213-6#ref-CR12" id="ref-link-section-d5961e1526">2011</a>), provides user-positioning features. As the user is guided to a chosen destination, the system attempts to detect geolocated visual targets along the itinerary. These detected objects are not displayed to the user but are used as anchors to refine the current GPS position. These visual landmarks (termed visual reference points, <i>VP</i>) can be objects such as signs, statues, mailboxes as well as facades, particular layouts of buildings, etc. (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig3">3</a>), which are stored in the Geographic Information System with their geographic coordinates. When a visual landmark is detected, the user’s position can be estimated using the distance and angle of the target relative to the cameras (provided by the stereoscopic depth map) and data from other sensors (e.g., magnetometers, accelerometers). This positioning method can provide an estimate relying exclusively on vision, in the event of GPS signal loss, or can be integrated into a larger fusion scheme as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec9">3.3</a>. It should be emphasized that it is not the user who determines the particular set of visual targets to be loaded. Instead, the system automatically loads the models corresponding to the rough location of the user given by the GPS.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Examples of geolocated visual landmarks used for user-positioning (shop sign, facade, road sign, mailbox)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The creation of models remains a key issue, which in the current version of the system has been performed manually from recorded videos of the evaluation test site. For generic objects that might be requested, the device could be preconfigured with an initial set of models covering a large number of common objects, built automatically from segmented image databases. A tool providing this automatic generation of models is currently under development. Each individual user should also be able to add new models. Model creation could entail the user rotating the new unknown object in front of the cameras, allowing segmentation of the image of interest within the optical flow, and then dynamic creation of an ensemble of models to ensure adequate coverage of the object.</p><p>For visual landmarks, the problem is more complex as the correct GPS coordinates of the target are required. Exploitation of services similar to Google Street View<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>  could allow for automatic construction of model sets in a new area. To examine this approach, the Topographic Department of the City of Toulouse has contributed 3D visual recordings of all streets of the city to the project, combining eight different views taken from car-mounted cameras at one meter intervals, combined with laser data allowing retrieval of GPS coordinates of any point in these images. With this database, it could be possible to randomly search for patterns throughout the city streets which are distinctive enough so as not to trigger false detections in neighboring streets, and to automatically store them as visual reference points with their associated coordinates.</p><p>Some studies have presented interesting and complementary approaches based on social cooperation (Völkel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Völkel T, Kühn R, Weber G (2008) Mobility impaired pedestrians are not cars: requirements for the annotation of geographical data. Computers Helping People with Special Needs, LNCS 2008 5105:1085–1092" href="/article/10.1007/s10055-012-0213-6#ref-CR67" id="ref-link-section-d5961e1577">2008</a>). It is proposed that the annotation of SIG databases may rely on data collected “on the move” by users themselves, combined with information gathered by the internal sensors of the device (e.g., compass, GPS). To increase data sources and facilitate sharing between users, a client-server architecture was proposed with the database stored on a remote server. The database is constantly updated and anonymously shared among users. An alternate method to add visual reference points could be based on the cooperate effort of “web workers.” One such approach, VizWiz,<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> combines automatic and human-powered services to answer visual questions for visually impaired users.</p><h3 class="c-article__sub-heading" id="Sec9">Data fusion for pedestrian navigation</h3><p>The measurement of physical quantities such as position, orientation, and acceleration relies on sensors that inherently report approximated values. This fact, in addition to occasional sensor dropout or failure, results in a given system receiving somewhat inaccurate or incomplete information. As such, the NAVIG system employs a collection of different sensors to obtain the same information, for example position. These different estimates must then be combined to provide the best estimate through a data fusion model. There are three main issues identified in sensor data fusion:
</p><ul class="u-list-style-dash">
                    <li>
                      <p>Interpretation and representation: Typically handled with probabilistic descriptions (Durrant-Whyte <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Durrant-Whyte HF (1988) Sensor models and multisensory integration. Special issue on Sensor Data Fusion, ISSN: 0278-3649 7(6):97–113" href="/article/10.1007/s10055-012-0213-6#ref-CR22" id="ref-link-section-d5961e1608">1988</a>).</p>
                    </li>
                    <li>
                      <p>Fusion and estimation: Methods such as Bayesian estimation (Berger <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Berger JO (1985) Statistical decision theory and bayesian analysis (2nd edn). Springer Series, ISBN: 978-0387960982" href="/article/10.1007/s10055-012-0213-6#ref-CR9" id="ref-link-section-d5961e1617">1985</a>) and Kalman filtering (Bar-Shalom <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Bar-Shalom Y (1987) Tracking and data association. Academic Press Professional, ISBN: 0-120-79760-7" href="/article/10.1007/s10055-012-0213-6#ref-CR6" id="ref-link-section-d5961e1620">1987</a>) are widely used.</p>
                    </li>
                    <li>
                      <p>Sensor management: Solutions are based either on a centralized or a decentralized sensor architecture (Mitchell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitchell HB (2007) Multi-sensor data fusion: an introduction. Springer, ISBN: 978-3540714637" href="/article/10.1007/s10055-012-0213-6#ref-CR50" id="ref-link-section-d5961e1629">2007</a>).</p>
                    </li>
                  </ul>
                <p>Centralizing the fusion process combines all of the raw data from all the sensors in one main processing module. In principle, this is the best way to realize the data fusion, as all the information is still present. In practice, centralized fusion frequently overloads the processing unit with large amounts of data. Preprocessing the data at each sensor drastically reduces the required data flow, and in practice, the optimal setup is usually a hybrid of these two types. Care must be taken in fusing different types of data, ensuring that transformations are performed to provide a unified coordinate system before the data fusion process.</p><p>It is important to note that different sensor systems operate with different and sometimes variable refresh rates. The sensor fusion strategy takes into account the amount of time from the last received data to automatically adjust the weights (i.e. estimated accuracy) attributed to each sensor. For example, a sudden drop-out in GPS signal for more than a few seconds (in an urban canyon) would gradually reduce the weight attributed to the GPS data. Similar corrections would be applied to the other sensors, depending on the specific time characteristics of each of these sensors.</p><p>Several GPS systems equipped with different sensors have been developed to increase navigation accuracy in vehicles (Cappelle et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Cappelle C, El Najjar ME, Pomorski D, Charpillet F (2010) Multi-sensors data fusion using dynamic bayesian network for robotised vehicle geo-localisation. International conference on information fusion, June 30–July 3 2008, Cologne, pp 1–8" href="/article/10.1007/s10055-012-0213-6#ref-CR17" id="ref-link-section-d5961e1642">2010</a>). In these systems, the inertia of the vehicle is important and dead reckoning strategies are appropriate for predicting the position at the next time step (<i>p</i>
                  <sub>
                    <i>t</i>+1</sub>). Furthermore, velocity and trajectory of a vehicle exhibit smooth and relatively slow variations. Finally, there is a high probability that the vehicle follows the sense of traffic known for the given side of the road. All these elements make accurate position estimation possible for vehicles, but they are not applicable in the case of pedestrian navigation.</p><p>The aim of the current fusion algorithm is to manage these two issues: first, by taking into account pedestrian ways of moving; secondly, by employing user mounted cameras that will recognize natural objects in urban scenes and allow for a precise estimate of the position of the user. This solution avoids the time and expense of equipping the environment with specific instrumentation as mentioned in Park et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Park S-K, Suh YS, Do TN (2009) The pedestrian navigation system using vision and inertial sensors. ICROS-SICE international joint conference on 2009, Fukuoka, Japan 18–21 Aug., pp 3970–3974" href="/article/10.1007/s10055-012-0213-6#ref-CR52" id="ref-link-section-d5961e1658">2009</a>), Bentzen and Mitchell (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bentzen B, Mitchell P (1995) Audible signage as a wayfinding aid: Comparison of Verbal Landmarks&#xA;                    &#xA;                      &#xA;                    &#xA;                    $$\circledR$$&#xA;                   and Talking Signs&#xA;                    &#xA;                      &#xA;                    &#xA;                    $$\circledR$$&#xA;                  . J Vis Impair Blind 89:494–505" href="/article/10.1007/s10055-012-0213-6#ref-CR8" id="ref-link-section-d5961e1661">1995</a>).</p><p>The fusion of positional information from the image recognition and geolocalization systems in real-time is a novel approach that results in an improvement in precision for the estimation of the user’s position. The approach is to combine satellite data from the Global Navigation Satellite System (GNSS) element and position estimations based on the visual reference points with known geographic coordinates (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig4">4</a>). Using a detailed database that contains embedded coordinates of these landmarks, the position of the user can be geometrically estimated to a high degree of precision. The integration of accelerometers provides added stability in separating tracking jitter from actual user motion.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Process of computing the user location when a <i>VP</i> (here the bench with known GPS coordinates) is detected by the embedded vision module. Using the 3D position of the object in the camera reference frame (obtained through stereovision), and the orientation of the head (acquired by an Inertial Measurement Unit), it is possible to estimate the geolocation of the user</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The fusion algorithm uses three different inputs. First, a commercial GPS sensor, assisted by an inertial system, provides accurate coordinates. Second, a GIS is used to verify that positions are coherent with map constraints. Finally, the vision system provides information on any geographically located objects detected. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig5">5</a> presents preliminary results for the estimation of user location by the fusion of information provided by the GPS receiver and the location estimate relying on embedded vision (Brilhault et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to improve blind pedestrian positioning. International conference on new technologies, mobility and security, IEEE, France" href="/article/10.1007/s10055-012-0213-6#ref-CR12" id="ref-link-section-d5961e1698">2011</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>A test path at the Toulouse University campus indicating buildings (<i>gray polygons</i>) and archways (<i>pink polygons</i>). <i>VP</i> with known GPS coordinates detected during the journey are shown with<i> green dot</i>. The different paths shown include the expected itinerary (<i>purple filled square</i>), commercial GPS sensor positioning (<i>yellow diamond</i>), user locations estimated by the vision module (<i>red star</i>), and the more accurate position computed using the data fusion module (<i>blue dot</i>). See the realization of a variety Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec11">3.4.1</a> for label descriptions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec10">Geographical information system</h3><p>A Geographic Information System (GIS) has been defined by Burrough (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Burrough PA (1986) Principles of geographical information systems for land resources, assessment. Oxford, Clarendon Press, Monographs on soil and resource surveys, No. 12" href="/article/10.1007/s10055-012-0213-6#ref-CR15" id="ref-link-section-d5961e1756">1986</a>) as a tool for capturing, manipulating, displaying, querying, and analyzing geographic data. The GIS is an important component in the design of an <i>electronic orientation aid</i> for VI persons (Golledge et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Golledge RG, Klatzky RL, Loomis JM, Speigle J, Tietz J (1998) A geographical information system for a GPS-based personal guidance system. Int J Geograph Inform Sci 727–749" href="/article/10.1007/s10055-012-0213-6#ref-CR28" id="ref-link-section-d5961e1762">1998</a>). Relying on a digitized spatial database and analytical tools, the NAVIG GIS module provides the user with accurate environmental information to ensure the success of the navigation task.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Digitized spatial data base</h4><p>Many studies (see e.g. Fletcher <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Fletcher JF (1980) Spatial representation in blind children. 1: development compared to sighted children. J Vis Impair Blind 381–385" href="/article/10.1007/s10055-012-0213-6#ref-CR24" id="ref-link-section-d5961e1772">1980</a>) have shown that building a cognitive map is useful to resolving spatial tasks. To use GIS databases for VI pedestrian navigation aids, it is necessary to augment them with additional classes of important objects, in order to provide the user with important specific information concerning the itinerary and surroundings (see Jacobson and Kitchin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Jacobson RD, Kitchin RM (1997) GIS and people with visual impairments or blindness: Exploring the potential for education, orientation, and navigation. Trans Geograph Inform Syst 2(4):315–332" href="/article/10.1007/s10055-012-0213-6#ref-CR33" id="ref-link-section-d5961e1775">1997</a>). This information must then be rendered during preparatory planning or actual navigation and may serve to build sparse but useful representations of the environment.</p><p>In the context of wayfinding aids for blind pedestrians, (Gaunet and Briffault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gaunet F, Briffault X (2005) Exploring the functional specifications of a localized wayfinding verbal aid for blind pedestrians: simple and structured urban areas. Hum Comp Interact 20:267–314" href="/article/10.1007/s10055-012-0213-6#ref-CR26" id="ref-link-section-d5961e1781">2005</a>) showed that adapted GIS databases for pedestrian navigation should include streets, sidewalks, crosswalks, and intersections. In addition, they specified that guidance functions consist of a combination of orientation and localization, goal location, intersection, crosswalks, and warning information as well as of progressions, crossings, orientations, and route-ending instructions. Therefore, all of those stated features concerning the path, the surroundings, and adapted guidance should be collected and stored with a high degree of spatial precision in the GIS. They should be incorporated into route selection procedures and displayed to the user during on-site guidance. Their utility during preliminary preparation of a journey should also be examined.</p><p>Currently, commercial GIS systems have been almost exclusively developed for car travel. A series of brainstorming sessions and interviews were conducted with potential VI users and orientation and mobility (O&amp;M) instructors (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec2">2</a>). The results led to five classes of objects that should be included in a GIS adapted to VI pedestrian navigation: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Walking Areas (<i>WA</i>): All possible pedestrian paths as defined in Zheng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zheng J, Winstanley A, Pan Z, Coveney S (2009) Spatial characteristics of walking areas for pedestrian navigation. Third International conference on multimedia and ubiquitous engineering, IEEE, China, pp 452–458" href="/article/10.1007/s10055-012-0213-6#ref-CR71" id="ref-link-section-d5961e1803">2009</a>) (e.g., sidewalks, and pedestrian crossings).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Landmarks (<i>LM</i>): Places or objects that can be detected by the user in order to make a decision or confirm his own position along the itinerary (e.g. changes in texture of the ground, telephone poles, or traffic lights).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Difficult Points (<i>DP</i>): Places that represent potential mobility difficulties for VI pedestrians (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec4">2.2</a>).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Points of Interest (<i>POI</i>): Places that are potential destinations or that contain interesting features. When they are not used as a destination, they are useful or interesting places offering the user a better understanding of the environment while traveling (e.g. public buildings, shops, etc.).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Visual Reference Points (<i>VP</i>): Geolocalized objects used by the vision module.</p>
                        
                      </li>
                    </ol>
                  <p>For each object in the database, multiple tags were possible. For instance, a bus stop was tagged as a <i>LM</i> because it can be detected by the user. It was also tagged as <i>POI</i> as it is a potential destination and as a <i>VP</i> if it could be detected by the artificial vision module. In addition, the user has the possibility to add specific locations (such as home, work, or sidewalks which could be slippery when wet) that will be integrated in a specific user layer of the GIS. The class of these objects is called Favorite Point (<i>FP</i>). Each point will be associated with a specific tag defined by the user.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">GIS software: route selection</h4><p>Once the user position and destination have been determined, route selection is necessary and is usually included in the GIS component. It is defined as the procedure of choosing an optimal pathway between origin and destination. When considering pedestrian navigation, the shortest path might be appropriate but should rely on a GIS database including essential information for pedestrian mobility (e.g., sidewalks and pedestrian crossings). Traditionally, route or path selection is assumed to be the result of minimization procedures such as selecting the shortest or the quickest path. For visually impaired users, a longer route may be more convenient than a shorter route, in order to avoid various obstacles or other difficulties. These route optimization rules can vary between individual users, due to mobility training or experience and other personal factors (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec4">2.2</a>). An adapted routing algorithm for visually impaired pedestrians has been proposed to improve path choice (Kammoun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kammoun S, Dramas F, Oriola B, Jouffrais C (2010) Route Selection Algorithm for Blind Pedestrian. International conference on control, automation and systems, IEEE, KINTEX, Gyeonggi-do, Korea, pp 2223–2228" href="/article/10.1007/s10055-012-0213-6#ref-CR34" id="ref-link-section-d5961e1895">2010</a>). The aim is to find the preferred route that connects the origin and destination points. The selected path is represented as a road map containing a succession of Itinerary Points (<i>IP</i>) and possibly Difficult Points (<i>DP</i>), such as pedestrian crossing and intersections, linked by <i>WA</i> as well as a collection of nearby <i>POI</i>, <i>FP</i>, <i>LM</i>, and <i>VP</i> as defined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec11">3.4.1</a>.</p><h3 class="c-article__sub-heading" id="Sec13">Spatial audio</h3><p>While the locations of the user and obstacles, and the determination of the itinerary to follow to attain the intended goal, are fundamental properties of the system, this information is not useful if it cannot be exploited by the user. The NAVIG system proposes to make use of the human capacity for hearing, and specifically spatial audition, by presenting guidance and navigational information via binaural 3D audio scenes (Begault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, Cambridge" href="/article/10.1007/s10055-012-0213-6#ref-CR7" id="ref-link-section-d5961e1932">1994</a>; Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Loomis JM, Golledge RG, Klatzky RL (1998) Navigation system for the blind: auditory display modes and guidance. Presence: Teleoperators Virtual Environ 7:193–203" href="/article/10.1007/s10055-012-0213-6#ref-CR44" id="ref-link-section-d5961e1935">1998</a>). The 3D sound module provides binaural rendering over headphones using a high performance spatialization engine (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz BFG, Rio E, Picinali L (2010) LIMSI spatialisation engine, InterDeposit Digital Number IDDN.FR. 001.340014.000.S.P. 2010.000.31235" href="/article/10.1007/s10055-012-0213-6#ref-CR38" id="ref-link-section-d5961e1938">2010</a>) developed under the Max/MSP programming environment.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup>
                </p><p>In contrast to traditional devices that rely on turn by turn instructions, the NAVIG consortium is working toward providing spatial information to the user concerning the trajectory, their position in it, and important landmarks. This additional information will help users become more confident when navigating in unknown environments.</p><p>Many visually impaired persons are already exploiting their sense of hearing beyond the capacities of most sighted people. Using the auditory modality channel to provide additional, and potentially important, information requires careful design in order to minimize cognitive load and to maximize understanding. Although the use of stereo headphones is required to produce binaural 3D sound, wearing traditional headphones results in a certain degree of masking of real sounds, which is problematic for VI individuals. Instead, a solution employing bonephones—headphones that work via the transmission of vibrations against the side of head, transmitting the sound via bone conduction, was adopted. Previous studies have demonstrated the efficient use of bonephones within a virtual 3D audio orientation context (Walker and Lindsay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Walker BN, Lindsay J (2005) Navigation performance in a virtual environment with bonephones. In: Proceedings of international conference on auditory display, Limerick, Ireland, pp 260–263" href="/article/10.1007/s10055-012-0213-6#ref-CR68" id="ref-link-section-d5961e1959">2005</a>). These particular headphones, situated just in front of the ears without any obstruction of the ear canal or pinna, permit the use of 3D audio without any masking of the real acoustic environment. Because of the bonephone’s complex frequency response, tailored equalization is necessary in order to properly render all the spectral cues of the Head Related Transfer Function.</p><p>There are many instances where textural verbal communication is optimal, such as indicating street names or landmarks. At the same time, a path or a spatial display are not verbal objects, but spatial objects. As such, the exploitation of auditory trajectory rendering can be more informative and more intuitive than a list of verbal instructions. The ability to have a global representation of the surroundings (survey representation) and a sense of the trajectory (route representation) is also highly desirable.</p><p>In contrast to previous works on sensory substitution, where images captured by a camera are directly transformed into sound, the aim of the 3D sound module is to generate informational auditory content at the spatial position, which directly coincides with that of a specific target. Various methods for semantic or informative spatial sonification have been shown to be effective in spatial manipulations of virtual objects (Férey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt JM, Katz BFG, Ammi M, Etchebest C, Autin L (2009) Multisensory VR interaction for protein-docking in the CoRSAIRe project. Virtual Reality 13:273–293" href="/article/10.1007/s10055-012-0213-6#ref-CR23" id="ref-link-section-d5961e1968">2009</a>) and scientific exploration and navigation tasks within large abstract datasets (Vézien et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Vézien J-M, Ménélas B, Nelson J, Picinali L, Bourdot P, Ammi M, Katz BFG, Burkhardt JM, Pastur L, Lusseyran F (2009) Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project. Virtual Reality 13:257–271" href="/article/10.1007/s10055-012-0213-6#ref-CR65" id="ref-link-section-d5961e1971">2009</a>) in multimodal virtual environments. Spatial sonification for spatial data exploration and guidance (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: Proceedings of 14th meeting of the international conference on auditory display (ICAD), Paris 24–27 June, pp 1-7" href="/article/10.1007/s10055-012-0213-6#ref-CR36" id="ref-link-section-d5961e1974">2008</a>) and target acquisition (Ménélas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ménélas B, Picinali L, Katz BFG, Bourdot P (2010) Audio haptic feedbacks in a task of targets acquisition. IEEE symposium on 3d user interfaces (3DUI 2010), Waltham, USA, pp 51–54" href="/article/10.1007/s10055-012-0213-6#ref-CR49" id="ref-link-section-d5961e1977">2010</a>) in audio and audio-haptic virtual environments without visual renderings have also been shown to be effective in previous studies.</p><p>A previous study has examined the precision of hand reaching movement toward nearby real acoustic sources through a localization accuracy task (Dramas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dramas F, Oriola B, Katz BFG, Thorpe S, Jouffrais C (2008) Designing an assistive device for the blind based on object localization and augmented auditory reality. ACM conference on computers and accessibility, ASSETS, Halifax, Canada, pp 263–264" href="/article/10.1007/s10055-012-0213-6#ref-CR20" id="ref-link-section-d5961e1983">2008</a>). Results showed that the accuracy of localization varies relative to source stimuli, azimuth, and distance. Taking into account these results, preparations are underway for a grasping task experiment with virtual sounds and different stimuli to optimize the accuracy of localization.</p><h3 class="c-article__sub-heading" id="Sec14">User guidance</h3><p>Once the user position has been determined, and the location or object identified, the primary task for the assistive system is to guide the user in a safe and reliable manner. Depending on their preferences and knowledge of the system, users have the possibility to choose different levels of detail of information and the way that this information will be presented to them. A series of brainstorming sessions and interviews with VI panel members identified at least two types of navigation to be considered (Brunet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Brunet L (2010) Étude des besoins et des stratégies des personnes non-voyantes lors de la navigation pour la conception d’un dispositif d’aide performant et accepté (Needs and strategy study of blind people during navigation for the design of a functional and accepted aid device). Master’s thesis, Department of Ergonomics, Université Paris-Sud, Orsay, France" href="/article/10.1007/s10055-012-0213-6#ref-CR13" id="ref-link-section-d5961e1994">2010</a>).</p><p>First, the <i>normal</i> mode is used for point-to-point guidance along a calculated itinerary from point A to a point B. In this mode, the user needs only a minimum amount of information to understand and perform the navigation task, with only the <i>IP</i>, <i>DP</i>, and <i>LM</i> elements being necessary. In contrast, in <i>exploration</i> mode, the user is interested in exploring a neighborhood or a specific itinerary. As such, there is a need to provide additional information, such as the presence and location of bakeries, municipal buildings, or bus stops. This mode requires the presentation of <i>IP</i>, <i>DP</i>, <i>LM</i>, and <i>POI</i>. At each use, the user can personalize the presented information by selecting certain types of <i>POI</i> or <i>LM</i> that are of personal interest. To facilitate this categorical presentation, each object class is divided into several categories that can be used to filter the information (7 categories of <i>POI</i>, 4 <i>LM</i>, and 3 <i>FP</i>).</p><p>Different levels of verbalization are provided in the NAVIG system depending on user needs (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0213-6#Sec2">2</a>). If the <i>IP</i> are always rendered by placing a virtual 3D sound object at the next waypoint along the trajectory, the <i>POI</i>, <i>PF</i>, and <i>LM</i> can be rendered using text-to-speech (TTS) or semantic sounds. All presented information is spatialized so that the user hears the description of each object coming from its corresponding position. Users can choose to use only TTS, a mix of spatialized TTS and semantic sounds, or only semantic sound. To accommodate spatialized TTS, a version of Acapela<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> was incorporated into the 3D real-time rendering system (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz BFG, Rio E, Picinali L (2010) LIMSI spatialisation engine, InterDeposit Digital Number IDDN.FR. 001.340014.000.S.P. 2010.000.31235" href="/article/10.1007/s10055-012-0213-6#ref-CR38" id="ref-link-section-d5961e2076">2010</a>).</p><p>For the semantic sound mode, it is important to study how all the information will be displayed in an ergonomic and intuitive sound display. For a navigation task using virtual audio display with several types of information (<span class="mathjax-tex">\(IP, DP, POI, \ldots\)</span>), two types of auditory cues can be used to create beacon sounds:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Auditory Icons are described in Gaver (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum-Comput Interact 2:167–177" href="/article/10.1007/s10055-012-0213-6#ref-CR27" id="ref-link-section-d5961e2099">1986</a>) as “everyday sounds mapped to computer events by analogy with everyday sound producing events”. They are brief sounds that can be seen as the auditory equivalent of visual icons used in personal computer.</p>
                    </li>
                    <li>
                      <p>Earcons are abstract, synthetic, and mostly musical tones or sound patterns that can be combined in a structured way to produce a sound grammar. They are defined in Blattner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Blattner MM, Sumikawa DA, Greenberg RM (1989) Earcons and icons: their structure and common design principles. SIGCHI Bull 21:123–124" href="/article/10.1007/s10055-012-0213-6#ref-CR11" id="ref-link-section-d5961e2108">1989</a>) as “non-verbal audio messages used in the computer interface to provide information to the user about some computer objects, operation or interaction”. Earcons allow for the construction of a syntactic hierarchical system in order to represent data trees with several levels of information.</p>
                    </li>
                    <li>
                      <p>Spearcons, introduced in Walker et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Walker BN, Nance A, Lindsay J (2006) Spearcons: speech-based earcons improve navigation performance in auditory menus. In: Proceedings of international conference on auditory display (ICAD2006), pp 95–98" href="/article/10.1007/s10055-012-0213-6#ref-CR70" id="ref-link-section-d5961e2117">2006</a>), use spoken phrases sped up until they may no longer be recognized as speech. Built on the basis of the text describing the information they represent, spearcons can easily be created using TTS software and an algorithm to speed up the phrase. Since the mapping between a spearcon and the object it represents is non-arbitrary, only a short training is required.</p>
                    </li>
                  </ul>
                <p>The advantages and disadvantages of these various sonification methods are relatively well known in auditory displays. Several studies have explored the learnability of such displays; for example, (Dingler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dingler T, Lindsay J, Walker BN (2008) Learnability of sound cues for environmental features: Auditory icons, earcons, spearcons, and speech, methods, pp 1–6" href="/article/10.1007/s10055-012-0213-6#ref-CR19" id="ref-link-section-d5961e2127">2008</a>) presents the superiority of spearcons compared to auditory icons and earcons in term of learnability. Other studies explored navigation performance, comparing different types of beacon sounds and the effects of the display rate (see Walker and Lindsay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Walker BN, Lindsay J (2006) Navigation performance with a virtual auditory display: Navigation performance with a virtual auditory display: effects of beacon sound, capture radius, and practice. Hum Factors: J Hum Fact Ergonomics Soci Sum 48(2):265–278" href="/article/10.1007/s10055-012-0213-6#ref-CR69" id="ref-link-section-d5961e2130">2006</a>; Tran et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Tran T, Letowski T, Abouchacra K (2000) Evaluation of acoustic beacon characteristics for navigation tasks. Ergonomics 43(6):807–827" href="/article/10.1007/s10055-012-0213-6#ref-CR64" id="ref-link-section-d5961e2133">2000</a> for an ergonomic evaluation of acoustic beacon characteristics and differences between speech and sound beacon). While the effectiveness and the efficiency of acoustic beacons have been well investigated (see Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Loomis JM, Golledge RG, Klatzlty RL, Speigle J, Tietz J (1994) Personal guidance system for visually impaired. In: Proceedings of first annual ACM conference on assistive technologies (Assets ’94), pp 85–90" href="/article/10.1007/s10055-012-0213-6#ref-CR43" id="ref-link-section-d5961e2136">1994</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Loomis JM, Golledge RG, Klatzky RL (1998) Navigation system for the blind: auditory display modes and guidance. Presence: Teleoperators Virtual Environ 7:193–203" href="/article/10.1007/s10055-012-0213-6#ref-CR44" id="ref-link-section-d5961e2139">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Loomis JM, Marston JR, Golledge RG, Klatzky RL (2005) Personal guidance system for people with visual impairment: a comparison of spatial displays for route guidance. J Vis Impair Blind 99:219–232" href="/article/10.1007/s10055-012-0213-6#ref-CR45" id="ref-link-section-d5961e2143">2005</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Loomis JM, Golledge RG, Klatzky RL, Marston JR (2006) Assisting wayfinding in visually impaired travelers. In: Allen G (eds) Applied spatial cognition: from research to cognitive technology, Lawrence Erlbaum Associates, Mahwah" href="/article/10.1007/s10055-012-0213-6#ref-CR46" id="ref-link-section-d5961e2146">2006</a> for systematic studies of the value of virtual sound for guidance), studies concerning user satisfaction with auditory navigation systems are still severely lacking. For the NAVIG project, the concept of morphological earcons (morphocons) has been introduced in order to improve user satisfaction through the development of a customizable user audio-interface.</p><p>Morphocons (morphological earcons) allow the construction of a hierarchical sound grammar based on temporal variation of several acoustical parameters. With this method, it is then possible to apply these morphological variations to all types of sounds (natural or artificial) and therefore to construct an infinite number of sound palettes while maintaining a certain level of coherence among objects or messages to be displayed. For the NAVIG project, a semantic sound grammar has been developed to allow the user to rapidly identify and differentiate between each class of objects (<i>IP</i>, <i>DP</i>, <i>POI</i>, <i>PF</i>, and <i>LM</i>) and to be informed about the subcategories within each class. This grammar has been established so that each sound can be easily localized (i.e. large spectrum, sharp attack), the possibility of confusion between classes is minimized, and considerations are made concerning the superposition of the virtual soundscape and the real acoustic world. The semantic sound grammar is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig6">6</a> and is described as follows:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Illustration of semantic sound grammar indicating (<i>upper</i>) intensity and (<i>lower</i>) frequency profiles of each element of the palette</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                
                  <ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <i>IP</i> : a brief sound</p>
                    </li>
                    <li>
                      <p>
                        <i>DP</i> : a sequence of two brief sounds</p>
                    </li>
                    <li>
                      <p>
                        <i>LM</i> : a rhythmic pattern of three brief sounds. Rhythmic variations of this pattern allow for the differentiation of <i>LM</i> type.</p>
                    </li>
                    <li>
                      <p>
                        <i>POI</i> : a sound whose frequency increases steadily, followed by a brief sound. The first sound is common to all categories of <i>POI</i>, while the brief sound differentiates between them.</p>
                    </li>
                    <li>
                      <p>
                        <i>FP</i> : a sound whose frequency decreases steadily, followed by a brief sound. The first sound is common to all categories of <i>FP</i>, while the brief sound differentiates between them.</p>
                    </li>
                  </ul>
                <p>Sound durations are between 0.2 and 1.5 s. This common grammar allows for the realization of a variety of sound palettes (e.g., birds, water, musical, videogame) satisfying individual user preferences in terms of sound esthetic while maintaining a common semantic language. As such, switching between palettes should not imply any significant change in cognitive load or learning period. Three different sound palettes (<i>natural</i>, <i>instrumental</i>, and <i>electronic</i>)
<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> were constructed and perceptually evaluated by 60 subjects (31 sighted and 29 blind subjects) with an online classification test.
<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup> Results showed a good recognition rate for discrimination between the categories (78 ± 22 %) with no difference between sighted and blind subjects. Concerning the discrimination between subcategories, the recognition rate was 63 ± 23 % for the <i>POI</i>, 58 ± 29 % for the <i>LM</i> and 87 ± 19 % for the <i>FP</i>. These results showed that the rhythm variations used to differentiate the <i>LM</i> subcategories were too similar and should be improved. They also pointed to specific problems for some sounds within each palette which were problematic. On the basis of these results, three new sound palettes are being created for the next phase of navigation testing. Additional details concerning the developed morphocons can be found in Parseihian and Katz (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc (accepted 2012–01–19)" href="/article/10.1007/s10055-012-0213-6#ref-CR54" id="ref-link-section-d5961e2307">2012</a>).</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Near-field assistive mode</h2><div class="c-article-section__content" id="Sec15-content"><p>In its simplest form, direct point-to-point guidance can be used to attain any requested object. The task of object localization or grasping then consists of a direct loop between the recognition algorithm detecting the target and the sound spatialization engine attributing and rendering a sound object at the physical location of the actual object. As such, the architecture for near-field guidance is dynamically simplified in an attempt to optimize performance and minimize system latencies.</p><p>When the object of interest is detected, the position of the target is directly sent to the sonification module. Rapid image recognition of objects in the camera’s field of view provides head-centered coordinates for detected objects, offering built-in head tracking (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig7">7</a>). For robustness in the case of lost identification or objects drifting out of the field of view, a 3D head orientation tracking device is also included to interpolate object positions, insuring fluidity and maintaining a refresh latency of no more than 10 ms with respect to head movements.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The user interacts with the system via voice recognition; in this case, he would like to grab his phone, the system then activates the research of the corresponding models in the artificial vision module. If the object is detected, the position of the target is directly sent to the sonification module to display the position of the phone</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>To improve the micro-navigation task, route selection should also be addressed. Unlike in the macro-scale pedestrian navigation, trajectory determination in the near-field, or in indoor situations, is more difficult, as the only data source is that from the image recognition platform. Nevertheless, intelligent navigation paths could be developed even in these situations. A contextual example of a typical situation would be to find a knife on a cluttered kitchen counter top. The user requests the knife. While the object can be easily and quickly identified, and its position determined, in this context there can be a number of obstacles which could be in the direct path to the knife, such as seasoning bottles. In addition, a knife has a preferred orientation for grasping, and it would be preferable if the assistive device was aware of the orientation of the object and would direct the user accordingly to the handle, and not the blade.</p><p>Outside the context of micro-navigation, this assistive device may also serve for general object recognition. Indeed, during the user-centered design sessions, participants mentioned the recurrent problem of distinguishing among similar objects (e.g., canned foods, bank notes). The NAVIG prototype has been tested in a study where participants had to classify different euro (€) currency notes (Parlouar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Parlouar RD, Macé FM, Jouffrais C (2009) Assistive device for the blind based on object recognition: an application to identify currency bills. ACM conference on computers and accessibility (ASSETS 2009), Pittsburgh, PA, USA, pp 227–228" href="/article/10.1007/s10055-012-0213-6#ref-CR53" id="ref-link-section-d5961e2347">2009</a>). As there are few mobile systems that are able to satisfactorily recognize different bank notes (see Liu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Liu X (2008) A camera phone based currency reader for the visually impaired. In: Proceedings of the 10th international ACM SIGACCESS conference on computers and accessibility, Halifax, Nova Scotia, Canada, pp 305–306" href="/article/10.1007/s10055-012-0213-6#ref-CR42" id="ref-link-section-d5961e2350">2008</a>), the aim was to evaluate this sub-function of the device’s vision module. Due to the high-speed and robustness of the recognition algorithm, users were able to identify 100 % of the bills that were presented and performed the sorting task flawlessly. Average measured response times (including bill manipulation, recognition, and classification tasks) were slightly above 10 s per bill. Users were in agreement that the usability of the system was good.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">NAVIG guidance prototype</h2><div class="c-article-section__content" id="Sec16-content"><p>The first functional prototype (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0213-6#Fig8">8</a>) operates on a laptop. The artificial vision module currently uses video streams from two head-mounted cameras (320 × 240 px at 48 Hz). The prototype employs a stereo camera pair with an approximately 100° viewing angle, allowing for the computation of distance to the objects based on stereoscopic disparity and the calibration matrix of the lenses. The prototype hardware is based on an ANGEO GPS (NAVOCAP, Inc), a BumbleBee stereoscopic camera system (Point Grey Research, Inc), an XSens orientation tracker, headphones, microphone, and a notebook computer. The NAVIG prototype has been successfully tested on a simple scenario in the Toulouse University campus. Preliminary experiments with this prototype have shown that it is possible to design a wearable device that can provide fully analyzed information to the user.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0213-6/MediaObjects/10055_2012_213_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>NAVIG Prototype V1</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0213-6/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The design of an assistive device for visually impaired users must take into account users’ needs as well as their behavioral and cognitive abilities in spatial and navigational tasks. This first prototype, pretested by blindfolded participants, will be evaluated in the fall of 2012 by a panel of 20 visually impaired participants involved in the project, with the headphones replaced by bonephones.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusion</h2><div class="c-article-section__content" id="Sec17-content"><p>This paper has introduced the NAVIG augmented reality assistance system for the visually impaired whose aim is to increase individual autonomy and mobility in the context of both sensing the immediate environment and pedestrian navigation. Combining satellite, image, and other sensor information, high precision geolocalization is achieved. Exploiting a rapid image recognition platform and spatial audio rendering, detailed trajectories can be determined and presented to the user for attaining macro- or micro-navigational destinations. An advanced dialog controller is being developed to facilitate usage and optimize performance for visually impaired users.</p><p>This kind of assistive device, or <i>electronic orientation aid</i>, does not replace traditional mobility aids such as the cane or the guide dog, but is considered as an additional device providing the VI user with important information for spatial cognition including landmarks (e.g., important points on the itinerary related to decision or confirmation), routes to follows (guidance), and spatial description. Specifically, it restores fundamental visuomotor processes such as grasping, heading, and piloting. In addition, it allows selection of adapted routes for VI pedestrians. Finally, we suggest that a spatial environment description mode, based on 3D synthesis of the relative location of important points in the surrounding, may help visually impaired users generate a sparse but functional mental map of the environment. This function will be evaluated as part of the ongoing ergonomic evaluations of the NAVIG system.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Projet NAVIG, <a href="http://navig.irit.fr">http://navig.irit.fr</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>SpikeNet Technology, <a href="http://www.spikenet-technology.com">http://www.spikenet-technology.com</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>Google Street View, <a href="http://maps.google.com/help/maps/streetview/">http://maps.google.com/help/maps/streetview/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>VizWiz, <a href="http://vizwiz.org">http://vizwiz.org</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>Max visual programming language for music and multimedia (2012) <a href="http://cycling74.com/products/maxmspjitter/">http://cycling74.com/products/maxmspjitter/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>Acapela group, <a href="http://www.acapela-group.fr/">http://www.acapela-group.fr/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>Sounds available online; <a href="http://groupeaa.limsi.fr/projets:navig:start?&amp;#palettes">http://groupeaa.limsi.fr/projets:navig:start?&amp;#palettes</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p>Online questionnaire; <a href="http://groupeaa.limsi.fr/projets:navig:start?&amp;#questionnaire">http://groupeaa.limsi.fr/projets:navig:start?&amp;#questionnaire</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Afonso, A. Blum, BFG. Katz, P. Tarroux, G. Borst, M. Denis, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Afonso A, Blum A, Katz BFG, Tarroux P, Borst G, Denis M (2010) Structural properties of spatial representation" /><p class="c-article-references__text" id="ref-CR2">Afonso A, Blum A, Katz BFG, Tarroux P, Borst G, Denis M (2010) Structural properties of spatial representations in blind people: scanning images constructed from haptic exploration or from locomotion in a 3-D audio virtual environment. Memory Cogn 38:591–604</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FMC.38.5.591" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Structural%20properties%20of%20spatial%20representations%20in%20blind%20people%3A%20scanning%20images%20constructed%20from%20haptic%20exploration%20or%20from%20locomotion%20in%20a%203-D%20audio%20virtual%20environment&amp;journal=Memory%20Cogn&amp;volume=38&amp;pages=591-604&amp;publication_year=2010&amp;author=Afonso%2CA&amp;author=Blum%2CA&amp;author=Katz%2CBFG&amp;author=Tarroux%2CP&amp;author=Borst%2CG&amp;author=Denis%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GL. Allen, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Allen GL (2000) Principles and practices for communicating route knowledge. Appl Cogn Psychol 14:333–359" /><p class="c-article-references__text" id="ref-CR3">Allen GL (2000) Principles and practices for communicating route knowledge. Appl Cogn Psychol 14:333–359</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2F1099-0720%28200007%2F08%2914%3A4%3C333%3A%3AAID-ACP655%3E3.0.CO%3B2-C" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Principles%20and%20practices%20for%20communicating%20route%20knowledge&amp;journal=Appl%20Cogn%20Psychol&amp;volume=14&amp;pages=333-359&amp;publication_year=2000&amp;author=Allen%2CGL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Auvray, E. Myin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Auvray M, Myin E (2009) Perception with compensatory devices. From sensory substitution to sensorimotor extens" /><p class="c-article-references__text" id="ref-CR4">Auvray M, Myin E (2009) Perception with compensatory devices. From sensory substitution to sensorimotor extension. Cogn Sci 33:1036–1058</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1551-6709.2009.01040.x" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20with%20compensatory%20devices.%20From%20sensory%20substitution%20to%20sensorimotor%20extension&amp;journal=Cogn%20Sci&amp;volume=33&amp;pages=1036-1058&amp;publication_year=2009&amp;author=Auvray%2CM&amp;author=Myin%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Auvray, S. Hanneton, JK. O’Regan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Auvray M, Hanneton S, O’Regan JK (2007) Learning to perceive with a visuo-auditory substitution system: locali" /><p class="c-article-references__text" id="ref-CR5">Auvray M, Hanneton S, O’Regan JK (2007) Learning to perceive with a visuo-auditory substitution system: localization and object recognition with the voice. Perception 36:416–430</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp5631" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20perceive%20with%20a%20visuo-auditory%20substitution%20system%3A%20localization%20and%20object%20recognition%20with%20the%20voice&amp;journal=Perception&amp;volume=36&amp;pages=416-430&amp;publication_year=2007&amp;author=Auvray%2CM&amp;author=Hanneton%2CS&amp;author=O%E2%80%99Regan%2CJK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bar-Shalom Y (1987) Tracking and data association. Academic Press Professional, ISBN: 0-120-79760-7" /><p class="c-article-references__text" id="ref-CR6">Bar-Shalom Y (1987) Tracking and data association. Academic Press Professional, ISBN: 0-120-79760-7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DR. Begault, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, Cambridge" /><p class="c-article-references__text" id="ref-CR7">Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3-D%20sound%20for%20virtual%20reality%20and%20multimedia&amp;publication_year=1994&amp;author=Begault%2CDR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Bentzen, P. Mitchell, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Bentzen B, Mitchell P (1995) Audible signage as a wayfinding aid: Comparison of Verbal Landmarks\(\circledR\) " /><p class="c-article-references__text" id="ref-CR8">Bentzen B, Mitchell P (1995) Audible signage as a wayfinding aid: Comparison of Verbal Landmarks<span class="mathjax-tex">\(\circledR\)</span> and Talking Signs<span class="mathjax-tex">\(\circledR\)</span>. J Vis Impair Blind 89:494–505</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Audible%20signage%20as%20a%20wayfinding%20aid%3A%20Comparison%20of%20Verbal%20Landmarks%20%24%24%5CcircledR%24%24%20and%20Talking%20Signs%20%24%24%5CcircledR%24%24&amp;journal=J%20Vis%20Impair%20Blind&amp;volume=89&amp;pages=494-505&amp;publication_year=1995&amp;author=Bentzen%2CB&amp;author=Mitchell%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berger JO (1985) Statistical decision theory and bayesian analysis (2nd edn). Springer Series, ISBN: 978-03879" /><p class="c-article-references__text" id="ref-CR9">Berger JO (1985) Statistical decision theory and bayesian analysis (2nd edn). Springer Series, ISBN: 978-0387960982</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Bisseret, S. Sebillote, P. Falzon, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Bisseret A, Sebillote S, Falzon P (1999) Techniques pratiques pour l’étude des activités expertes. Octarès Edi" /><p class="c-article-references__text" id="ref-CR10">Bisseret A, Sebillote S, Falzon P (1999) Techniques pratiques pour l’étude des activités expertes. Octarès Editions, Toulouse</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Techniques%20pratiques%20pour%20l%E2%80%99%C3%A9tude%20des%20activit%C3%A9s%20expertes&amp;publication_year=1999&amp;author=Bisseret%2CA&amp;author=Sebillote%2CS&amp;author=Falzon%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Blattner, DA. Sumikawa, RM. Greenberg, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Blattner MM, Sumikawa DA, Greenberg RM (1989) Earcons and icons: their structure and common design principles." /><p class="c-article-references__text" id="ref-CR11">Blattner MM, Sumikawa DA, Greenberg RM (1989) Earcons and icons: their structure and common design principles. SIGCHI Bull 21:123–124</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F67880.1046599" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Earcons%20and%20icons%3A%20their%20structure%20and%20common%20design%20principles&amp;journal=SIGCHI%20Bull&amp;volume=21&amp;pages=123-124&amp;publication_year=1989&amp;author=Blattner%2CMM&amp;author=Sumikawa%2CDA&amp;author=Greenberg%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to imp" /><p class="c-article-references__text" id="ref-CR12">Brilhault A, Kammoun S, Gutierrez O, Truillet P, Jouffrais C (2011) Fusion of artificial vision and GPS to improve blind pedestrian positioning. International conference on new technologies, mobility and security, IEEE, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brunet L (2010) Étude des besoins et des stratégies des personnes non-voyantes lors de la navigation pour la c" /><p class="c-article-references__text" id="ref-CR13">Brunet L (2010) Étude des besoins et des stratégies des personnes non-voyantes lors de la navigation pour la conception d’un dispositif d’aide performant et accepté (Needs and strategy study of blind people during navigation for the design of a functional and accepted aid device). Master’s thesis, Department of Ergonomics, Université Paris-Sud, Orsay, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buisson M, Bustico A, Chatty S, Colin F-R, Jestin Y, Maury S, Mertz C, Truillet P (2002) Ivy: un bus logiciel " /><p class="c-article-references__text" id="ref-CR14">Buisson M, Bustico A, Chatty S, Colin F-R, Jestin Y, Maury S, Mertz C, Truillet P (2002) Ivy: un bus logiciel au service du développement de prototypes de systèmes interactifs. 14th French-speaking conference on human– computer interaction (IHM ’02) pp 223-2 26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burrough PA (1986) Principles of geographical information systems for land resources, assessment. Oxford, Clar" /><p class="c-article-references__text" id="ref-CR15">Burrough PA (1986) Principles of geographical information systems for land resources, assessment. Oxford, Clarendon Press, Monographs on soil and resource surveys, No. 12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Canadian Institute for the Blind (2005) Inégalité des chances : Rapport sur les besoins des personnes aveugles" /><p class="c-article-references__text" id="ref-CR16">Canadian Institute for the Blind (2005) Inégalité des chances : Rapport sur les besoins des personnes aveugles ou handicapées visuelles vivant au Canada. Technical Report. <a href="http://www.cnib.ca/fr/apropos/publications/recherche">http://www.cnib.ca/fr/apropos/publications/recherche</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cappelle C, El Najjar ME, Pomorski D, Charpillet F (2010) Multi-sensors data fusion using dynamic bayesian net" /><p class="c-article-references__text" id="ref-CR17">Cappelle C, El Najjar ME, Pomorski D, Charpillet F (2010) Multi-sensors data fusion using dynamic bayesian network for robotised vehicle geo-localisation. International conference on information fusion, June 30–July 3 2008, Cologne, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Denis, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Denis M (1997) The description of routes: acognitive approach to the production of spatial discourse. Curr Pyc" /><p class="c-article-references__text" id="ref-CR18">Denis M (1997) The description of routes: acognitive approach to the production of spatial discourse. Curr Pychol Cogn 16:409–458</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20description%20of%20routes%3A%20a%20cognitive%20approach%20to%20the%20production%20of%20spatial%20discourse&amp;journal=Curr%20Pychol%20Cogn&amp;volume=16&amp;pages=409-458&amp;publication_year=1997&amp;author=Denis%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dingler T, Lindsay J, Walker BN (2008) Learnability of sound cues for environmental features: Auditory icons, " /><p class="c-article-references__text" id="ref-CR19">Dingler T, Lindsay J, Walker BN (2008) Learnability of sound cues for environmental features: Auditory icons, earcons, spearcons, and speech, methods, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dramas F, Oriola B, Katz BFG, Thorpe S, Jouffrais C (2008) Designing an assistive device for the blind based o" /><p class="c-article-references__text" id="ref-CR20">Dramas F, Oriola B, Katz BFG, Thorpe S, Jouffrais C (2008) Designing an assistive device for the blind based on object localization and augmented auditory reality. ACM conference on computers and accessibility, ASSETS, Halifax, Canada, pp 263–264</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Dramas, SJ. Thorpe, C. Jouffrais, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Dramas F, Thorpe SJ, Jouffrais C (2010) Artificial vision for the blind: a bio-inspired algorithm for objects " /><p class="c-article-references__text" id="ref-CR21">Dramas F, Thorpe SJ, Jouffrais C (2010) Artificial vision for the blind: a bio-inspired algorithm for objects and obstacles detection. J Image Graph 10(4):531–544</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1142%2FS0219467810003871" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20vision%20for%20the%20blind%3A%20a%20bio-inspired%20algorithm%20for%20objects%20and%20obstacles%20detection&amp;journal=J%20Image%20Graph&amp;volume=10&amp;issue=4&amp;pages=531-544&amp;publication_year=2010&amp;author=Dramas%2CF&amp;author=Thorpe%2CSJ&amp;author=Jouffrais%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Durrant-Whyte HF (1988) Sensor models and multisensory integration. Special issue on Sensor Data Fusion, ISSN:" /><p class="c-article-references__text" id="ref-CR22">Durrant-Whyte HF (1988) Sensor models and multisensory integration. Special issue on Sensor Data Fusion, ISSN: 0278-3649 7(6):97–113</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Férey, J. Nelson, C. Martin, L. Picinali, G. Bouyer, A. Tek, P. Bourdot, JM. Burkhardt, BFG. Katz, M. Ammi, C. Etchebest, L. Autin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt JM, Katz BFG, Ammi M, Etchebest" /><p class="c-article-references__text" id="ref-CR23">Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt JM, Katz BFG, Ammi M, Etchebest C, Autin L (2009) Multisensory VR interaction for protein-docking in the CoRSAIRe project. Virtual Reality 13:273–293</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0136-z" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multisensory%20VR%20interaction%20for%20protein-docking%20in%20the%20CoRSAIRe%20project&amp;journal=Virtual%20Reality&amp;volume=13&amp;pages=273-293&amp;publication_year=2009&amp;author=F%C3%A9rey%2CN&amp;author=Nelson%2CJ&amp;author=Martin%2CC&amp;author=Picinali%2CL&amp;author=Bouyer%2CG&amp;author=Tek%2CA&amp;author=Bourdot%2CP&amp;author=Burkhardt%2CJM&amp;author=Katz%2CBFG&amp;author=Ammi%2CM&amp;author=Etchebest%2CC&amp;author=Autin%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fletcher JF (1980) Spatial representation in blind children. 1: development compared to sighted children. J Vi" /><p class="c-article-references__text" id="ref-CR24">Fletcher JF (1980) Spatial representation in blind children. 1: development compared to sighted children. J Vis Impair Blind 381–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gallay M, Denis M, Auvray M (2012) Navigation assistance for blind pedestrians: guidelines for the design of d" /><p class="c-article-references__text" id="ref-CR25">Gallay M, Denis M, Auvray M (2012) Navigation assistance for blind pedestrians: guidelines for the design of devices and implications for spatial cognition. In: Thora Tenbrink, Jan Wiener, Christophe Claramunt (eds) Representing space in cognition: Interrelations of behaviour, language, and formal models. Oxford University Press, UK (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Gaunet, X. Briffault, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Gaunet F, Briffault X (2005) Exploring the functional specifications of a localized wayfinding verbal aid for " /><p class="c-article-references__text" id="ref-CR26">Gaunet F, Briffault X (2005) Exploring the functional specifications of a localized wayfinding verbal aid for blind pedestrians: simple and structured urban areas. Hum Comp Interact 20:267–314</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327051hci2003_2" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20the%20functional%20specifications%20of%20a%20localized%20wayfinding%20verbal%20aid%20for%20blind%20pedestrians%3A%20simple%20and%20structured%20urban%20areas&amp;journal=Hum%20Comp%20Interact&amp;volume=20&amp;pages=267-314&amp;publication_year=2005&amp;author=Gaunet%2CF&amp;author=Briffault%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Gaver, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum-Comput Interact 2:167–177" /><p class="c-article-references__text" id="ref-CR27">Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum-Comput Interact 2:167–177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327051hci0202_3" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20icons%3A%20using%20sound%20in%20computer%20interfaces&amp;journal=Hum-Comput%20Interact&amp;volume=2&amp;pages=167-177&amp;publication_year=1986&amp;author=Gaver%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Golledge RG, Klatzky RL, Loomis JM, Speigle J, Tietz J (1998) A geographical information system for a GPS-base" /><p class="c-article-references__text" id="ref-CR28">Golledge RG, Klatzky RL, Loomis JM, Speigle J, Tietz J (1998) A geographical information system for a GPS-based personal guidance system. Int J Geograph Inform Sci 727–749</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RG. Golledge, JR. Marston, JM. Loomis, RL. Klatzky, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Golledge RG, Marston JR, Loomis JM, Klatzky RL (2004) Stated preferences for components of a Personal Guidance" /><p class="c-article-references__text" id="ref-CR29">Golledge RG, Marston JR, Loomis JM, Klatzky RL (2004) Stated preferences for components of a Personal Guidance System for nonvisual navigation. J Vis Impair Blind 98:135–147</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stated%20preferences%20for%20components%20of%20a%20Personal%20Guidance%20System%20for%20nonvisual%20navigation&amp;journal=J%20Vis%20Impair%20Blind&amp;volume=98&amp;pages=135-147&amp;publication_year=2004&amp;author=Golledge%2CRG&amp;author=Marston%2CJR&amp;author=Loomis%2CJM&amp;author=Klatzky%2CRL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Helal A, Moore SE, Ramachandran B (2001) Drishti: an integrated navigation system for visually impaired and di" /><p class="c-article-references__text" id="ref-CR31">Helal A, Moore SE, Ramachandran B (2001) Drishti: an integrated navigation system for visually impaired and disabled. International symposium on wearable computers ISWC, IEEE Computer Society, Washington DC, pp 149–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hub A, Diepstraten J, Ertl T (2004) Design and development of an indoor navigation and object identification s" /><p class="c-article-references__text" id="ref-CR32">Hub A, Diepstraten J, Ertl T (2004) Design and development of an indoor navigation and object identification system for the blind. ACM SIGACCESS accessibility and computing (77–78):147–152</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RD. Jacobson, RM. Kitchin, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Jacobson RD, Kitchin RM (1997) GIS and people with visual impairments or blindness: Exploring the potential fo" /><p class="c-article-references__text" id="ref-CR33">Jacobson RD, Kitchin RM (1997) GIS and people with visual impairments or blindness: Exploring the potential for education, orientation, and navigation. Trans Geograph Inform Syst 2(4):315–332</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=GIS%20and%20people%20with%20visual%20impairments%20or%20blindness%3A%20Exploring%20the%20potential%20for%20education%2C%20orientation%2C%20and%20navigation&amp;journal=Trans%20Geograph%20Inform%20Syst&amp;volume=2&amp;issue=4&amp;pages=315-332&amp;publication_year=1997&amp;author=Jacobson%2CRD&amp;author=Kitchin%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kammoun S, Dramas F, Oriola B, Jouffrais C (2010) Route Selection Algorithm for Blind Pedestrian. Internationa" /><p class="c-article-references__text" id="ref-CR34">Kammoun S, Dramas F, Oriola B, Jouffrais C (2010) Route Selection Algorithm for Blind Pedestrian. International conference on control, automation and systems, IEEE, KINTEX, Gyeonggi-do, Korea, pp 2223–2228</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Kammoun, G. Parseihian, O. Gutierrez, A. Brilhault, A. Serpa, M. Raynal, B. Oriola, M. Macé, M. Auvray, M. Denis, S. Thorpe, P. Truillet, BFG. Katz, C. Jouffrais, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Kammoun S, Parseihian G, Gutierrez O, Brilhault A, Serpa A, Raynal M, Oriola B, Macé M, Auvray M, Denis M, Tho" /><p class="c-article-references__text" id="ref-CR35">Kammoun S, Parseihian G, Gutierrez O, Brilhault A, Serpa A, Raynal M, Oriola B, Macé M, Auvray M, Denis M, Thorpe S, Truillet P, Katz BFG, Jouffrais C (2012) Navigation and space perception assistance for the visually impaired: the NAVIG project. Ingénierie et Recherche Biomédicale 33:182–189</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigation%20and%20space%20perception%20assistance%20for%20the%20visually%20impaired%3A%20the%20NAVIG%20project&amp;journal=Ing%C3%A9nierie%20et%20Recherche%20Biom%C3%A9dicale&amp;volume=33&amp;pages=182-189&amp;publication_year=2012&amp;author=Kammoun%2CS&amp;author=Parseihian%2CG&amp;author=Gutierrez%2CO&amp;author=Brilhault%2CA&amp;author=Serpa%2CA&amp;author=Raynal%2CM&amp;author=Oriola%2CB&amp;author=Mac%C3%A9%2CM&amp;author=Auvray%2CM&amp;author=Denis%2CM&amp;author=Thorpe%2CS&amp;author=Truillet%2CP&amp;author=Katz%2CBFG&amp;author=Jouffrais%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration" /><p class="c-article-references__text" id="ref-CR36">Katz BFG, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: Proceedings of 14th meeting of the international conference on auditory display (ICAD), Paris 24–27 June, pp 1-7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Truillet P, Thorpe S, Jouffrais C (2010) NAVIG: Navigation assisted by artificial vision and GNSS. W" /><p class="c-article-references__text" id="ref-CR37">Katz BFG, Truillet P, Thorpe S, Jouffrais C (2010) NAVIG: Navigation assisted by artificial vision and GNSS. Workshop on multimodal location based techniques for extreme navigation (Pervasive 2010), Helsinki</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Rio E, Picinali L (2010) LIMSI spatialisation engine, InterDeposit Digital Number IDDN.FR. 001.34001" /><p class="c-article-references__text" id="ref-CR38">Katz BFG, Rio E, Picinali L (2010) LIMSI spatialisation engine, InterDeposit Digital Number IDDN.FR. 001.340014.000.S.P. 2010.000.31235</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz BFG, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M," /><p class="c-article-references__text" id="ref-CR39">Katz BFG, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M, Truillet P, Denis M, Thorpe S, Jouffrais C (2012) NAVIG: guidance system for the visually impaired using virtual augmented reality. J Technol Disability 24(2) (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RL. Klatzky, JR. Marston, NA. Giudice, RG. Golledge, JM. Loomis, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Klatzky RL, Marston JR, Giudice NA, Golledge RG, Loomis JM (2006) Cognitive load of navigating without vision " /><p class="c-article-references__text" id="ref-CR40">Klatzky RL, Marston JR, Giudice NA, Golledge RG, Loomis JM (2006) Cognitive load of navigating without vision when guided by virtual sound versus spatial language. J Exp Psychol Appl 12:223–232</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F1076-898X.12.4.223" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20load%20of%20navigating%20without%20vision%20when%20guided%20by%20virtual%20sound%20versus%20spatial%20language&amp;journal=J%20Exp%20Psychol%20Appl&amp;volume=12&amp;pages=223-232&amp;publication_year=2006&amp;author=Klatzky%2CRL&amp;author=Marston%2CJR&amp;author=Giudice%2CNA&amp;author=Golledge%2CRG&amp;author=Loomis%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knapek M, Oropeza RS, Kriegman DJ (2000) Selecting promising landmarks, Proc. ICRA’00. IEEE international conf" /><p class="c-article-references__text" id="ref-CR41">Knapek M, Oropeza RS, Kriegman DJ (2000) Selecting promising landmarks, Proc. ICRA’00. IEEE international conference on robotics and automation vol 4, pp 3771–3777</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liu X (2008) A camera phone based currency reader for the visually impaired. In: Proceedings of the 10th inter" /><p class="c-article-references__text" id="ref-CR42">Liu X (2008) A camera phone based currency reader for the visually impaired. In: Proceedings of the 10th international ACM SIGACCESS conference on computers and accessibility, Halifax, Nova Scotia, Canada, pp 305–306</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Loomis JM, Golledge RG, Klatzlty RL, Speigle J, Tietz J (1994) Personal guidance system for visually impaired." /><p class="c-article-references__text" id="ref-CR43">Loomis JM, Golledge RG, Klatzlty RL, Speigle J, Tietz J (1994) Personal guidance system for visually impaired. In: Proceedings of first annual ACM conference on assistive technologies (Assets ’94), pp 85–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Loomis, RG. Golledge, RL. Klatzky, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Loomis JM, Golledge RG, Klatzky RL (1998) Navigation system for the blind: auditory display modes and guidance" /><p class="c-article-references__text" id="ref-CR44">Loomis JM, Golledge RG, Klatzky RL (1998) Navigation system for the blind: auditory display modes and guidance. Presence: Teleoperators Virtual Environ 7:193–203</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565677" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigation%20system%20for%20the%20blind%3A%20auditory%20display%20modes%20and%20guidance&amp;journal=Presence%3A%20Teleoperators%20Virtual%20Environ&amp;volume=7&amp;pages=193-203&amp;publication_year=1998&amp;author=Loomis%2CJM&amp;author=Golledge%2CRG&amp;author=Klatzky%2CRL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Loomis, JR. Marston, RG. Golledge, RL. Klatzky, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Loomis JM, Marston JR, Golledge RG, Klatzky RL (2005) Personal guidance system for people with visual impairme" /><p class="c-article-references__text" id="ref-CR45">Loomis JM, Marston JR, Golledge RG, Klatzky RL (2005) Personal guidance system for people with visual impairment: a comparison of spatial displays for route guidance. J Vis Impair Blind 99:219–232</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Personal%20guidance%20system%20for%20people%20with%20visual%20impairment%3A%20a%20comparison%20of%20spatial%20displays%20for%20route%20guidance&amp;journal=J%20Vis%20Impair%20Blind&amp;volume=99&amp;pages=219-232&amp;publication_year=2005&amp;author=Loomis%2CJM&amp;author=Marston%2CJR&amp;author=Golledge%2CRG&amp;author=Klatzky%2CRL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JM. Loomis, RG. Golledge, RL. Klatzky, JR. Marston, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Loomis JM, Golledge RG, Klatzky RL, Marston JR (2006) Assisting wayfinding in visually impaired travelers. In:" /><p class="c-article-references__text" id="ref-CR46">Loomis JM, Golledge RG, Klatzky RL, Marston JR (2006) Assisting wayfinding in visually impaired travelers. In: Allen G (eds) Applied spatial cognition: from research to cognitive technology, Lawrence Erlbaum Associates, Mahwah</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Applied%20spatial%20cognition%3A%20from%20research%20to%20cognitive%20technology&amp;publication_year=2006&amp;author=Loomis%2CJM&amp;author=Golledge%2CRG&amp;author=Klatzky%2CRL&amp;author=Marston%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JR. Marston, JM. Loomis, RL. Klatzky, RG. Golledge, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Marston JR, Loomis JM, Klatzky RL, Golledge RG (2006) Smith EL Evaluation of spatial displays for navigation w" /><p class="c-article-references__text" id="ref-CR47">Marston JR, Loomis JM, Klatzky RL, Golledge RG (2006) Smith EL Evaluation of spatial displays for navigation without sight. ACM Trans Appl Percept 3:110–124</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1141897.1141900" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Smith%20EL%20Evaluation%20of%20spatial%20displays%20for%20navigation%20without%20sight&amp;journal=ACM%20Trans%20Appl%20Percept&amp;volume=3&amp;pages=110-124&amp;publication_year=2006&amp;author=Marston%2CJR&amp;author=Loomis%2CJM&amp;author=Klatzky%2CRL&amp;author=Golledge%2CRG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ménélas B, Picinali L, Katz BFG, Bourdot P (2010) Audio haptic feedbacks in a task of targets acquisition. IEE" /><p class="c-article-references__text" id="ref-CR49">Ménélas B, Picinali L, Katz BFG, Bourdot P (2010) Audio haptic feedbacks in a task of targets acquisition. IEEE symposium on 3d user interfaces (3DUI 2010), Waltham, USA, pp 51–54</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mitchell HB (2007) Multi-sensor data fusion: an introduction. Springer, ISBN: 978-3540714637" /><p class="c-article-references__text" id="ref-CR50">Mitchell HB (2007) Multi-sensor data fusion: an introduction. Springer, ISBN: 978-3540714637</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ML. Noordzij, S. Zuidhoek, A. Postma, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Noordzij ML, Zuidhoek S, Postma A (2006) The influence of visual experience on the ability to form spatial men" /><p class="c-article-references__text" id="ref-CR51">Noordzij ML, Zuidhoek S, Postma A (2006) The influence of visual experience on the ability to form spatial mental models based on route and survey descriptions. Cognition 100:321–342</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cognition.2005.05.006" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20influence%20of%20visual%20experience%20on%20the%20ability%20to%20form%20spatial%20mental%20models%20based%20on%20route%20and%20survey%20descriptions&amp;journal=Cognition&amp;volume=100&amp;pages=321-342&amp;publication_year=2006&amp;author=Noordzij%2CML&amp;author=Zuidhoek%2CS&amp;author=Postma%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park S-K, Suh YS, Do TN (2009) The pedestrian navigation system using vision and inertial sensors. ICROS-SICE " /><p class="c-article-references__text" id="ref-CR52">Park S-K, Suh YS, Do TN (2009) The pedestrian navigation system using vision and inertial sensors. ICROS-SICE international joint conference on 2009, Fukuoka, Japan 18–21 Aug., pp 3970–3974</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parlouar RD, Macé FM, Jouffrais C (2009) Assistive device for the blind based on object recognition: an applic" /><p class="c-article-references__text" id="ref-CR53">Parlouar RD, Macé FM, Jouffrais C (2009) Assistive device for the blind based on object recognition: an application to identify currency bills. ACM conference on computers and accessibility (ASSETS 2009), Pittsburgh, PA, USA, pp 227–228</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio E" /><p class="c-article-references__text" id="ref-CR54">Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc (accepted 2012–01–19)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parseihian G, Brilhault A, Dramas F (2010) NAVIG: an object localization system for the blind. Workshop on Mul" /><p class="c-article-references__text" id="ref-CR55">Parseihian G, Brilhault A, Dramas F (2010) NAVIG: an object localization system for the blind. Workshop on Multimodal Location Based Techniques for Extreme Navigation (Pervasive 2010), Helsinki, Finland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ran L, Helal S, Moore (2004) S Drishti: an integrated indoor/outdoor blind navigation system and service. IEEE" /><p class="c-article-references__text" id="ref-CR57">Ran L, Helal S, Moore (2004) S Drishti: an integrated indoor/outdoor blind navigation system and service. IEEE international conference on pervasive computing and communications (PerCom’04), pp 23–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="UR. Roentgen, GJ. Gelderblom, M. Soede, LP. Witte, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Roentgen UR, Gelderblom GJ, Soede M, de Witte LP (2008) Inventory of electronic mobility aids for persons with" /><p class="c-article-references__text" id="ref-CR58">Roentgen UR, Gelderblom GJ, Soede M, de Witte LP (2008) Inventory of electronic mobility aids for persons with visual impairments: a literature review. J Vis Impair Blind 102(11):702–724</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Inventory%20of%20electronic%20mobility%20aids%20for%20persons%20with%20visual%20impairments%3A%20a%20literature%20review&amp;journal=J%20Vis%20Impair%20Blind&amp;volume=102&amp;issue=11&amp;pages=702-724&amp;publication_year=2008&amp;author=Roentgen%2CUR&amp;author=Gelderblom%2CGJ&amp;author=Soede%2CM&amp;author=Witte%2CLP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track. Proc. CVPR’94. IEEE computer society conference on computer vis" /><p class="c-article-references__text" id="ref-CR59">Shi J, Tomasi C (1994) Good features to track. Proc. CVPR’94. IEEE computer society conference on computer vision and pattern recognition, pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Strothotte T, Petrie H, Johnson V Reichert L (1995) Mobic: user needs and preliminary design for a mobility ai" /><p class="c-article-references__text" id="ref-CR61">Strothotte T, Petrie H, Johnson V Reichert L (1995) Mobic: user needs and preliminary design for a mobility aid for blind and elderly travelers. In: Porrero IP, de la Bellacasa RP (eds) The European context for assistive technology, pp 348–352</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Thorpe, D. Fize, C. Marlot, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381(6582):520–522" /><p class="c-article-references__text" id="ref-CR62">Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381(6582):520–522</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F381520a0" aria-label="View reference 57">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Speed%20of%20processing%20in%20the%20human%20visual%20system&amp;journal=Nature&amp;volume=381&amp;issue=6582&amp;pages=520-522&amp;publication_year=1996&amp;author=Thorpe%2CS&amp;author=Fize%2CD&amp;author=Marlot%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Thrun, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Thrun S (2002) Robotic mapping: A survey. In: Lakemeyer G, Nebel B (eds) Exploring artificial intelligence in " /><p class="c-article-references__text" id="ref-CR63">Thrun S (2002) Robotic mapping: A survey. In: Lakemeyer G, Nebel B (eds) Exploring artificial intelligence in the new millennium, Elsevier Science, USA, pp 1–35</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20artificial%20intelligence%20in%20the%20new%20millennium&amp;pages=1-35&amp;publication_year=2002&amp;author=Thrun%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Tran, T. Letowski, K. Abouchacra, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Tran T, Letowski T, Abouchacra K (2000) Evaluation of acoustic beacon characteristics for navigation tasks. Er" /><p class="c-article-references__text" id="ref-CR64">Tran T, Letowski T, Abouchacra K (2000) Evaluation of acoustic beacon characteristics for navigation tasks. Ergonomics 43(6):807–827</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Evaluation%20of%20acoustic%20beacon%20characteristics%20for%20navigation%20tasks.%20Ergonomics&amp;volume=43&amp;issue=6&amp;pages=807-827&amp;publication_year=2000&amp;author=Tran%2CT&amp;author=Letowski%2CT&amp;author=Abouchacra%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J-M. Vézien, B. Ménélas, J. Nelson, L. Picinali, P. Bourdot, M. Ammi, BFG. Katz, JM. Burkhardt, L. Pastur, F. Lusseyran, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Vézien J-M, Ménélas B, Nelson J, Picinali L, Bourdot P, Ammi M, Katz BFG, Burkhardt JM, Pastur L, Lusseyran F " /><p class="c-article-references__text" id="ref-CR65">Vézien J-M, Ménélas B, Nelson J, Picinali L, Bourdot P, Ammi M, Katz BFG, Burkhardt JM, Pastur L, Lusseyran F (2009) Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project. Virtual Reality 13:257–271</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0134-1" aria-label="View reference 60">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multisensory%20VR%20exploration%20for%20computer%20fluid%20dynamics%20in%20the%20CoRSAIRe%20project&amp;journal=Virtual%20Reality&amp;volume=13&amp;pages=257-271&amp;publication_year=2009&amp;author=V%C3%A9zien%2CJ-M&amp;author=M%C3%A9n%C3%A9las%2CB&amp;author=Nelson%2CJ&amp;author=Picinali%2CL&amp;author=Bourdot%2CP&amp;author=Ammi%2CM&amp;author=Katz%2CBFG&amp;author=Burkhardt%2CJM&amp;author=Pastur%2CL&amp;author=Lusseyran%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Völkel T, Kühn R, Weber G (2008) Mobility impaired pedestrians are not cars: requirements for the annotation o" /><p class="c-article-references__text" id="ref-CR67">Völkel T, Kühn R, Weber G (2008) Mobility impaired pedestrians are not cars: requirements for the annotation of geographical data. Computers Helping People with Special Needs, LNCS 2008 5105:1085–1092</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Walker BN, Lindsay J (2005) Navigation performance in a virtual environment with bonephones. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR68">Walker BN, Lindsay J (2005) Navigation performance in a virtual environment with bonephones. In: Proceedings of international conference on auditory display, Limerick, Ireland, pp 260–263</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BN. Walker, J. Lindsay, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Walker BN, Lindsay J (2006) Navigation performance with a virtual auditory display: Navigation performance wit" /><p class="c-article-references__text" id="ref-CR69">Walker BN, Lindsay J (2006) Navigation performance with a virtual auditory display: Navigation performance with a virtual auditory display: effects of beacon sound, capture radius, and practice. Hum Factors: J Hum Fact Ergonomics Soci Sum 48(2):265–278</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 63 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Navigation%20performance%20with%20a%20virtual%20auditory%20display%3A%20Navigation%20performance%20with%20a%20virtual%20auditory%20display%3A%20effects%20of%20beacon%20sound%2C%20capture%20radius%2C%20and%20practice.%20Hum%20Factors%3A%20J%20Hum%20Fact%20Ergonomics%20Soci%20Sum&amp;volume=48&amp;issue=2&amp;pages=265-278&amp;publication_year=2006&amp;author=Walker%2CBN&amp;author=Lindsay%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Walker BN, Nance A, Lindsay J (2006) Spearcons: speech-based earcons improve navigation performance in auditor" /><p class="c-article-references__text" id="ref-CR70">Walker BN, Nance A, Lindsay J (2006) Spearcons: speech-based earcons improve navigation performance in auditory menus. In: Proceedings of international conference on auditory display (ICAD2006), pp 95–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zheng J, Winstanley A, Pan Z, Coveney S (2009) Spatial characteristics of walking areas for pedestrian navigat" /><p class="c-article-references__text" id="ref-CR71">Zheng J, Winstanley A, Pan Z, Coveney S (2009) Spatial characteristics of walking areas for pedestrian navigation. Third International conference on multimedia and ubiquitous engineering, IEEE, China, pp 452–458</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-012-0213-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The NAVIG consortium includes IRIT, LIMSI, CerCo, SpikeNet Technology, NAVOCAP, CESDV - Institute for Young Blind, and the community of Grand Toulouse. This work was supported by the French National Research Agency (ANR) through the TecSan program (project NAVIG ANR-08-TECS-011) and the Midi-Pyrénées region through the APRRTT program. This research program has been labeled by the cluster Aerospace Valley.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LIMSI-CNRS, Université Paris Sud, 91403, Orsay, France</p><p class="c-article-author-affiliation__authors-list">Brian F. G. Katz, Gaëtan Parseihian, Malika Auvray &amp; Michel Denis</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">IRIT, CNRS &amp; Université Paul Sabatier, Toulouse, France</p><p class="c-article-author-affiliation__authors-list">Slim Kammoun, Olivier Gutierrez, Adrien Brilhault, Philippe Truillet &amp; Christophe Jouffrais</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">CerCo, CNRS &amp; Université Paul Sabatier, Toulouse, France</p><p class="c-article-author-affiliation__authors-list">Adrien Brilhault &amp; Simon Thorpe</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Brian_F__G_-Katz"><span class="c-article-authors-search__title u-h3 js-search-name">Brian F. G. Katz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Brian F. G.+Katz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Brian F. G.+Katz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brian F. G.+Katz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Slim-Kammoun"><span class="c-article-authors-search__title u-h3 js-search-name">Slim Kammoun</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Slim+Kammoun&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Slim+Kammoun" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Slim+Kammoun%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ga_tan-Parseihian"><span class="c-article-authors-search__title u-h3 js-search-name">Gaëtan Parseihian</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ga%C3%ABtan+Parseihian&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ga%C3%ABtan+Parseihian" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ga%C3%ABtan+Parseihian%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Olivier-Gutierrez"><span class="c-article-authors-search__title u-h3 js-search-name">Olivier Gutierrez</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Olivier+Gutierrez&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Olivier+Gutierrez" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Olivier+Gutierrez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Adrien-Brilhault"><span class="c-article-authors-search__title u-h3 js-search-name">Adrien Brilhault</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Adrien+Brilhault&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Adrien+Brilhault" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adrien+Brilhault%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Malika-Auvray"><span class="c-article-authors-search__title u-h3 js-search-name">Malika Auvray</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Malika+Auvray&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Malika+Auvray" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Malika+Auvray%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Philippe-Truillet"><span class="c-article-authors-search__title u-h3 js-search-name">Philippe Truillet</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Philippe+Truillet&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Philippe+Truillet" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Philippe+Truillet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Michel-Denis"><span class="c-article-authors-search__title u-h3 js-search-name">Michel Denis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Michel+Denis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michel+Denis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michel+Denis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Simon-Thorpe"><span class="c-article-authors-search__title u-h3 js-search-name">Simon Thorpe</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Simon+Thorpe&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Simon+Thorpe" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Simon+Thorpe%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Christophe-Jouffrais"><span class="c-article-authors-search__title u-h3 js-search-name">Christophe Jouffrais</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Christophe+Jouffrais&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christophe+Jouffrais" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christophe+Jouffrais%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-012-0213-6/email/correspondent/c1/new">Brian F. G. Katz</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=NAVIG%3A%20augmented%20reality%20guidance%20system%20for%20the%20visually%20impaired&amp;author=Brian%20F.%20G.%20Katz%20et%20al&amp;contentID=10.1007%2Fs10055-012-0213-6&amp;publication=1359-4338&amp;publicationDate=2012-06-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Katz, B.F.G., Kammoun, S., Parseihian, G. <i>et al.</i> NAVIG: augmented reality guidance system for the visually impaired.
                    <i>Virtual Reality</i> <b>16, </b>253–269 (2012). https://doi.org/10.1007/s10055-012-0213-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-012-0213-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-03-28">28 March 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-05-03">03 May 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-06-12">12 June 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-11">November 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-012-0213-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-012-0213-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Assisted navigation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Guidance</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Spatial audio</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visually impaired assistive device</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Need analysis</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0213-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=213;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

