<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Pick-by-vision: there is something to pick at the end of the augmented"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We report on the long process of exploring, evaluating and refining augmented reality-based methods to support the order picking process of logistics applications. Order picking means that workers..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Pick-by-vision: there is something to pick at the end of the augmented tunnel"/>

    <meta name="dc.source" content="Virtual Reality 2011 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-03-08"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We report on the long process of exploring, evaluating and refining augmented reality-based methods to support the order picking process of logistics applications. Order picking means that workers have to pick items out of numbered boxes in a warehouse, according to a work order. To support those workers, we have evaluated different HMD-based visualizations in six user studies, starting in a laboratory setup and continuing later in an industrial environment. This was a challenging task, as we had to conquer different kinds of navigation problems from very coarse to very fine granularity and accuracy. The resulting setup consists of a combined and adaptive visualization to precisely and efficiently guide the user even if the actual picking target is not always in the field of view of the HMD."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-03-08"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="213"/>

    <meta name="prism.endingPage" content="223"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0187-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0187-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0187-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0187-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Pick-by-vision: there is something to pick at the end of the augmented tunnel"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2011/03/08"/>

    <meta name="citation_firstpage" content="213"/>

    <meta name="citation_lastpage" content="223"/>

    <meta name="citation_article_type" content="SI: Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0187-9"/>

    <meta name="DOI" content="10.1007/s10055-011-0187-9"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0187-9"/>

    <meta name="description" content="We report on the long process of exploring, evaluating and refining augmented reality-based methods to support the order picking process of logistics appli"/>

    <meta name="dc.creator" content="Bj&#246;rn Schwerdtfeger"/>

    <meta name="dc.creator" content="Rupert Reif"/>

    <meta name="dc.creator" content="Willibald A. G&#252;nthner"/>

    <meta name="dc.creator" content="Gudrun Klinker"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Baudisch P, Rosenholtz R (2003) Halo: a technique for visualizing off-screen locations. In: Proceedings of CHI 2003, Fort Lauderdale, FL"/>

    <meta name="citation_reference" content="Biocca F, Tang A, Owen C, Xiao F (2006) Attention funnel: omnidirectional 3d cursor for mobile augmented reality platforms. In: CHI &#8217;06: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, pp 1115&#8211;1122"/>

    <meta name="citation_reference" content="citation_title=Statistik f&#252;r Human- und Sozialwissenschaftler; citation_publication_date=2005; citation_id=CR3; citation_author=J Bortz; citation_publisher=Springer Medizin Verlag"/>

    <meta name="citation_reference" content="Curtis D, Mizell D, Gruenbaum P, Janin A (1998) Several devils in the details: making an ar app work in the airplane factory. In: Proc. IEEE and ACM IWAR&#8217;98 (1. International Workshop on Augmented Reality). AK Peters, San Francisco, Nov 1998, pp 47&#8211;60"/>

    <meta name="citation_reference" content="Feiner S, MacIntyre B, Seligmann D (1993) Knowledge-based augmented reality. Commun ACM 36(7)"/>

    <meta name="citation_reference" content="citation_title=Logistik: Grundlagen, Strategien, Anwendungen; citation_publication_date=2005; citation_id=CR6; citation_author=T Gudehus; citation_publisher=Springer"/>

    <meta name="citation_reference" content="G&#252;nthner WAH (2007) In: Neue Wege in der Automobillogistik: Die Vision der Supra-Adaptivit&#228;t. Springer"/>

    <meta name="citation_reference" content="Henderson S, Feiner S (2009) Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret. In: International symposium on mixed and augmented reality (ISMAR &#8217;09)"/>

    <meta name="citation_reference" content="Gabbard J, Swan E, Hix D (2006) The effects of text drawing styles, background textures, and natural lighting on text legibility in outdoor augmented reality. Presence Teleoper Virtual Environ 15(1)"/>

    <meta name="citation_reference" content="Kramer LJ, Prinzel LJ III, Arthur JJ III, Bailey RE (2004) Pathway design effects on synthetic vision headup displays. In: Proceedings of SPIE&#8212;vol 5424, enhanced and synthetic vision"/>

    <meta name="citation_reference" content="Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics applications. In: Proceedings of 16th international conference on artificial reality and telexistence (ICAT 2006), Nov 2006"/>

    <meta name="citation_reference" content="Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picking, 2008. Technische Universit&#228;t M&#252;nchen, Report TUM-I-08-19"/>

    <meta name="citation_reference" content="Schwerdtfeger B, Klinker G (2008) Supporting order picking with augmented reality. In: Proceedings of the 6th international symposium on mixed and augmented reality (ISMAR), Sept 2008"/>

    <meta name="citation_reference" content="Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of the 7th international symposium on mixed and augmented reality (ISMAR), Oct 2009"/>

    <meta name="citation_author" content="Bj&#246;rn Schwerdtfeger"/>

    <meta name="citation_author_institution" content="Technische Universit&#228;t M&#252;nchen, Munich, Germany"/>

    <meta name="citation_author" content="Rupert Reif"/>

    <meta name="citation_author_institution" content="Technische Universit&#228;t M&#252;nchen, Munich, Germany"/>

    <meta name="citation_author" content="Willibald A. G&#252;nthner"/>

    <meta name="citation_author_institution" content="Technische Universit&#228;t M&#252;nchen, Munich, Germany"/>

    <meta name="citation_author" content="Gudrun Klinker"/>

    <meta name="citation_author_email" content="klinker@in.tum.de"/>

    <meta name="citation_author_institution" content="Technische Universit&#228;t M&#252;nchen, Munich, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0187-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0187-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Pick-by-vision: there is something to pick at the end of the augmented tunnel"/>
        <meta property="og:description" content="We report on the long process of exploring, evaluating and refining augmented reality-based methods to support the order picking process of logistics applications. Order picking means that workers have to pick items out of numbered boxes in a warehouse, according to a work order. To support those workers, we have evaluated different HMD-based visualizations in six user studies, starting in a laboratory setup and continuing later in an industrial environment. This was a challenging task, as we had to conquer different kinds of navigation problems from very coarse to very fine granularity and accuracy. The resulting setup consists of a combined and adaptive visualization to precisely and efficiently guide the user even if the actual picking target is not always in the field of view of the HMD."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Pick-by-vision: there is something to pick at the end of the augmented tunnel | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0187-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, User studies, Order picking, Logistics, Tracked head-mounted display, Guidance","kwrd":["Augmented_reality","User_studies","Order_picking","Logistics","Tracked_head-mounted_display","Guidance"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0187-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0187-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=187;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0187-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Pick-by-vision: there is something to pick at the end of the augmented tunnel
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0187-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0187-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-03-08" itemprop="datePublished">08 March 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Pick-by-vision: there is something to pick at the end of the augmented tunnel</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bj_rn-Schwerdtfeger" data-author-popup="auth-Bj_rn-Schwerdtfeger">Björn Schwerdtfeger</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität München" /><meta itemprop="address" content="grid.6936.a, 0000000123222966, Technische Universität München, Munich, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rupert-Reif" data-author-popup="auth-Rupert-Reif">Rupert Reif</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität München" /><meta itemprop="address" content="grid.6936.a, 0000000123222966, Technische Universität München, Munich, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Willibald_A_-G_nthner" data-author-popup="auth-Willibald_A_-G_nthner">Willibald A. Günthner</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität München" /><meta itemprop="address" content="grid.6936.a, 0000000123222966, Technische Universität München, Munich, Germany" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gudrun-Klinker" data-author-popup="auth-Gudrun-Klinker" data-corresp-id="c1">Gudrun Klinker<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität München" /><meta itemprop="address" content="grid.6936.a, 0000000123222966, Technische Universität München, Munich, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">213</span>–<span itemprop="pageEnd">223</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1510 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">29 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0187-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We report on the long process of exploring, evaluating and refining augmented reality-based methods to support the order picking process of logistics applications. Order picking means that workers have to pick items out of numbered boxes in a warehouse, according to a work order. To support those workers, we have evaluated different HMD-based visualizations in six user studies, starting in a laboratory setup and continuing later in an industrial environment. This was a challenging task, as we had to conquer different kinds of navigation problems from very coarse to very fine granularity and accuracy. The resulting setup consists of a combined and adaptive visualization to precisely and efficiently guide the user even if the actual picking target is not always in the field of view of the HMD.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>This research is conducted in the context of <i>supra-adaptive</i>
                        <sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> logistics applications (Günthner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Günthner WAH (2007) In: Neue Wege in der Automobillogistik: Die Vision der Supra-Adaptivität. Springer" href="/article/10.1007/s10055-011-0187-9#ref-CR7" id="ref-link-section-d80697e328">2007</a>). In such applications, workers have to adapt to new working conditions and environments quickly, frequently, and with minimal training. These are not artificially conceived scenarios, but rather the consequence of fast-developing markets in a globalized world. There exists the need to make workers improve under such working conditions without significantly increasing their stress level and while preventing them from making errors. To this end, they must be provided with just the right information at exactly the right time. Such a support system has to provide workers with detailed working instructions which have to be presented highly intuitively and very precisely. As a result, workers can then start executing arbitrary jobs efficiently and error-free—and without prior training. Augmented reality (AR) has the potential to provide this very functionally.</p><p>In the context of such supra-adaptive logistics applications, we have investigated AR support for the order picking process. We have iteratively developed and evaluated several mobile AR-based visualizations to support this process efficiently. This paper summarizes several empirical user studies that we have conducted to develop and improve our system. Overall, we have gathered data from more than 100 subjects picking altogether about 10,000 items out of different boxes in different warehouses, guided by one of our various AR-based visualizations. We have presented the system several times at public events, and we have used visitors’ reactions as well as observations of how they behave with different visualizations as further input to improve our system. This iterative process helped us develop a mature picking visualization and gain and exploit a lot of insights into the general use of head-mounted AR systems.
<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
                     </p><p>A picture of our final system is given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig1">1</a>. In this paper, we describe how it works and how we developed it. We present six consecutive user studies, interleaved with discussions on the challenges of designing systems to support the order picking process and what this means in the scope of Augmented Reality.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A photograph through a tracked head-mounted display (HMD—compare Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig3">3</a>a). The visualization shows a Tunnel twisting to the left, with a square Frame at the end highlighting a box in a warehouse from which a worker has to pick items</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec2">Order picking in logistics applications</h3><p>In order-picking tasks, workers collect sets of items from assortments in warehouses according to work orders. They deliver them to the next station in a precisely designed material flow process (Gudehus <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gudehus T (2005) Logistik: Grundlagen, Strategien, Anwendungen. Springer, Berlin" href="/article/10.1007/s10055-011-0187-9#ref-CR6" id="ref-link-section-d80697e374">2005</a>). Such scenarios occur a million times a day, e.g. whenever some books are ordered via a web portal, or in the supply chain of all car producers. This means that, if a few seconds could be saved for each picked item—while ensuring that the correct book or parts are always sent out—this could really save a lot of money.</p><p>The efficiency of such picking processes is divided into time measurements of four interleaved tasks: the <i>base time</i> for getting the next order information, the <i>dead time</i> during which a worker interprets and understands the order as a 3D navigation and picking task, the <i>way time</i> during which the user physically moves to the selected item and the <i>picking time</i> to actually grab the item. The base time, the way time and the picking time have already been subject to many process optimizations. The only time component which we can optimize with a Pick-by-Vision system is the dead time.</p><p>Traditionally, order picking is accomplished by providing workers with printed-out pick lists of articles, describing their position in the warehouse, the number of items to be collected, and short descriptions. To increase efficiency and to reduce the number of picking errors, current industrial setups mainly use two further (costly and expensive) techniques: <i>Pick-by-Light</i>, which offers visual aid for the worker by installing small lamps on each storage compartment and <i>Pick-by-Voice</i>, which supports the worker by providing all instructions through the computer’s speech output (Gudehus <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gudehus T (2005) Logistik: Grundlagen, Strategien, Anwendungen. Springer, Berlin" href="/article/10.1007/s10055-011-0187-9#ref-CR6" id="ref-link-section-d80697e401">2005</a>).</p><h3 class="c-article__sub-heading" id="Sec3">Supporting order picking with HMD-based augmented reality</h3><p>On first thought, it seems like an easy task to develop a <i>Pick-by-Vision</i> system—an AR system to visually support the order-picking process. Why should this be more complicated than augmenting the box to pick from with an arrow, such as the one in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>c? Yet, it took us several iterations of user studies to find a sophisticated solution.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Visualizations of test series 2: <b>a</b> The meta visualization: A Compass-like Arrow points toward the next relevant object. It is extended by a rubber band. <b>b</b>–<b>d</b> Different visualizations to highlight the box to pick from: <b>b</b> The Arrow <b>c</b> The Frame <b>d</b> The Tunnel</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The picking process consists of two navigation phases. In the first phase, the <i>coarse navigation</i>, workers have to find the way to the correct shelf. In the second phase, the <i>fine navigation</i>, they have to find the specific box (to pick from) on this shelf.</p><p>The first phase is not the critical task. Most logistics experts agree that the number of shelves is generally manageable. Nevertheless, we have reported on various visualizations (e.g AR traffic signs) to support this task (Schwerdtfeger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics applications. In: Proceedings of 16th international conference on artificial reality and telexistence (ICAT 2006), Nov 2006" href="/article/10.1007/s10055-011-0187-9#ref-CR11" id="ref-link-section-d80697e469">2006</a>).</p><p>Much more critical is the second phase, i.e., the fine navigation to one of the large number of boxes to pick from. It is not enough to just highlight the box with an augmentation. Due to the small field of view of current HMDs, such an augmentation actually is only rarely visible on the display. More often, it is outside the field of view because users are looking in the wrong direction. We then have to extend the actual visualization with a meta visualization to guide the users’ gaze toward the box—thereby covering the entire range of 4π steradians. The evolution of this meta visualization was the main challenge of our work. An example of such a combination of meta visualization and actual picking visualization is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>a, b.</p><h3 class="c-article__sub-heading" id="Sec4">Related work</h3><p>The problematic issue of guiding mobile users wearing a tracked HMD to objects in a wide-range environment has not yet been addressed extensively thus far.</p><p>One such concept is a <i>flight tunnel</i> to guide pilots in the air (Kramer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kramer LJ, Prinzel LJ III, Arthur JJ III, Bailey RE (2004) Pathway design effects on synthetic vision headup displays. In: Proceedings of SPIE—vol 5424, enhanced and synthetic vision" href="/article/10.1007/s10055-011-0187-9#ref-CR10" id="ref-link-section-d80697e491">2004</a>). Biocca et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Biocca F, Tang A, Owen C, Xiao F (2006) Attention funnel: omnidirectional 3d cursor for mobile augmented reality platforms. In: CHI ’06: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, pp 1115–1122" href="/article/10.1007/s10055-011-0187-9#ref-CR2" id="ref-link-section-d80697e494">2006</a>) have used this flight tunnel metaphor to direct a user’s view to a target object that has to be picked. They used the tunnel to visibly link the head-centered coordinate space directly to an object-centered coordinate space. The tunnel is constructed by aligning elements on a Bezier curve between the HMD and the object to pick. They have proved in a basic experiment that the tunnel is better than having no meta visualization. However, in their scenario they did not require exact navigation, as we do here.</p><p>In their famous wire bundle assembly application at Boeing, Curtis et al. have used a compass metaphor to guide users along several form boards to add another wire to the developing wire harness of an airplane (Curtis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Curtis D, Mizell D, Gruenbaum P, Janin A (1998) Several devils in the details: making an ar app work in the airplane factory. In: Proc. IEEE and ACM IWAR’98 (1. International Workshop on Augmented Reality). AK Peters, San Francisco, Nov 1998, pp 47–60" href="/article/10.1007/s10055-011-0187-9#ref-CR4" id="ref-link-section-d80697e500">1998</a>). Feiner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Feiner S, MacIntyre B, Seligmann D (1993) Knowledge-based augmented reality. Commun ACM 36(7)" href="/article/10.1007/s10055-011-0187-9#ref-CR5" id="ref-link-section-d80697e503">1993</a>) have guided users’ views to objects with a <i>rubber band</i>-like visualization, combined with highlighting the object. Henderson and Feiner (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Henderson S, Feiner S (2009) Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret. In: International symposium on mixed and augmented reality (ISMAR ’09)" href="/article/10.1007/s10055-011-0187-9#ref-CR8" id="ref-link-section-d80697e509">2009</a>) refined this visualization by displaying a three-dimensional <i>rubber band</i>-like arrow which fades out, when the actual object is in the field of view. When the user turns far away from the target, a screen fixed arrow indicates the shortest rotation distance to the target.</p><p>A more detailed overview of concepts toward overcoming the problems of order picking in large and complex warehouses by inexperienced workers has been discussed in Schwerdtfeger et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics applications. In: Proceedings of 16th international conference on artificial reality and telexistence (ICAT 2006), Nov 2006" href="/article/10.1007/s10055-011-0187-9#ref-CR11" id="ref-link-section-d80697e518">2006</a>).</p><p>Gabbard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gabbard J, Swan E, Hix D (2006) The effects of text drawing styles, background textures, and natural lighting on text legibility in outdoor augmented reality. Presence Teleoper Virtual Environ 15(1)" href="/article/10.1007/s10055-011-0187-9#ref-CR9" id="ref-link-section-d80697e525">2006</a>) evaluate the text legibility using optical see-through displays. They suggest that a virtual retinal display (the one we used) could potentially increase text legibility.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Exploring a wide space of options—test series 1</h2><div class="c-article-section__content" id="Sec5-content"><p>Our first approach toward supporting the order-picking process with computer visualizations (Pick-by-Vision) was to compare in an explorative study different displays (HMD vs. stationary monitors vs. PDAs), as well as several visualizations (1D, 2D, 3D) on each of these displays. In total, we compared 3 × 3 = 9 presentation schemes, each for two different scenarios (Schwerdtfeger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics applications. In: Proceedings of 16th international conference on artificial reality and telexistence (ICAT 2006), Nov 2006" href="/article/10.1007/s10055-011-0187-9#ref-CR11" id="ref-link-section-d80697e537">2006</a>).</p><p>In a formal test setup, users had to pick a specified number of items from boxes on a shelf according to work orders. We expected the AR-based presentation scheme (3D Visualization on an HMD) to outperform all other methods of information presentation. This did not happen. The two main observations w.r.t. the AR-based scheme were: A) People often picked items from the wrong box (one row too high or too low), indicating a problem with perceiving the depth of the displayed 3D Arrow (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>b), and B) most people needed a lot of time to familiarize themselves with AR. Altogether, the test persons were almost 50% faster in the third test cycle than in the first, due to learning effects and the high initial general fascination of first-time AR users.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Comparison between an augmented frame, arrow and tunnel—test series 2</h2><div class="c-article-section__content" id="Sec6-content"><p>We discussed the results of the first test series with several logistics experts and workers. They recommended that we focus on AR-based visualization of picking information rather than all the other further ways of information presentation of Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec5">2</a>. They further told us to investigate how to prevent users from making errors, i.e., to strive for a <i>zero error</i> system. We replaced the previously used Sony Glasstron HMD with a Nomad HMD (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig3">3</a>a) to benefit from the very high see-through capabilities and less occlusion of the peripheral field of view of the Nomad.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Setup of the test series 2 and 3. <b>a</b> Monocular, retinal Nomad HMD from Microvision—it just uses a small glass plate in the field of view. <b>b</b> The shelves consisting of 96 boxes (10 cm × 10 cm)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec7">Experimental setup</h3><p>We thus refined the experimental setup. Users were now placed directly in front of a shelf of small boxes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig3">3</a>b). Furthermore, we developed new metaphors for the user interface. In the new experiment, we compared three different visualization schemes, each consisting of a <i>meta visualization</i> to guide the user when the object was not within the field of view and a <i>picking visualization</i> that was shown when the object was in sight (Schwerdtfeger and Klinker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picking, 2008. Technische Universität München, Report TUM-I-08-19" href="/article/10.1007/s10055-011-0187-9#ref-CR12" id="ref-link-section-d80697e604">2008a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Schwerdtfeger B, Klinker G (2008) Supporting order picking with augmented reality. In: Proceedings of the 6th international symposium on mixed and augmented reality (ISMAR), Sept 2008" href="/article/10.1007/s10055-011-0187-9#ref-CR13" id="ref-link-section-d80697e607">b</a>). The different visualizations are presented next.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Arrow-based visualization</h4><p>In this scheme, the picking visualization consists of a horizontal Arrow (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>b) that points perpendicularly into the shelf (box) out of which one or more items have to be picked. To increase three-dimensional perception, the Arrow slightly immerses into the box. The Arrow is supplemented by a Compass-like second arrow as meta visualization. This second arrow is located statically about 30 cm in front of the user and thus is always visible. It points toward the next relevant augmentation and is extended by a Rubber Band (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>a). This combination of visualizations has already been used and evaluated in our first exploratory test (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec5">2</a>) and thus serves as a reference.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Frame-based visualization</h4><p>Several test participants of the first test recommended to “simply” surround the box in question by a rectangular Frame. We thus designed a virtual Frame supplemented by the already described Compass-like arrow as meta visualization. The implementation of this Compass-like arrow and Frame can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>a, c.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Tunnel-based visualization</h4><p>As a third variant, we developed a visual Tunnel linking the head-centered coordinate space (the HMD) with the coordinate space of the object to pick. Similarly to Biocca et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Biocca F, Tang A, Owen C, Xiao F (2006) Attention funnel: omnidirectional 3d cursor for mobile augmented reality platforms. In: CHI ’06: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, pp 1115–1122" href="/article/10.1007/s10055-011-0187-9#ref-CR2" id="ref-link-section-d80697e645">2006</a>), we aligned objects on a Bezier curve going from the front of the HMD to the object to pick. It actually looks like attaching a flexible hose of a vacuum cleaner from the HMD to the object. Since this visualization combines the picking visualization and the meta navigation, we did not need the Compass-like arrow from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>a. This visualization is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>d.</p><h3 class="c-article__sub-heading" id="Sec11">Test method</h3><p>To compare the different approaches, we designed the following experiment.</p><p>The single <i>independent</i> variable was the visualization metaphor, providing three levels: a) Arrow + Compass-like arrow, b) Frame + Compass-like arrow, and c) Tunnel. We had 34 test persons between 15 and 49 years of age (mean: 27.3, stddev: 5.8, 24 men, 10 women) in a within-subject design. Half of them were students from our campus and the other half were people from the city.</p><p>People were asked to stand in front of the shelf and pick items. It was not possible for them to see the entire shelf without moving their head to see the boxes to pick from—they thus had to make extensive use of the meta visualization. At startup, each test person received an introduction with three items for each visualization to play around with and to be able to ask questions about anything they did not fully understand (<i>try-and-ask phase</i>). We did this to compensate for the effect of fascination which had generated a high variability in the measured results of the first experiment.</p><p>During the actual experiment, the test persons had to perform 3 orders (9 items) for each visualization, in permuted sequence to compensate for learning effects. For system control, we used the Wizard of Oz technique, i.e., people had to say "I picked it!” whereupon we switched to the next visualization. We confirmed the switch with an acoustic feedback.</p><p>For this round of experimentation, we set up two formal hypotheses that there would be a difference between the visualizations, both with respect to time and errors. The rejection of the corresponding null-hypotheses were evaluated for the α niveau of 5%—a common assumption for these kinds of evaluation scenarios (Bortz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bortz J (2005) Statistik für Human- und Sozialwissenschaftler, 6th ed. Springer Medizin Verlag, Heidelberg" href="/article/10.1007/s10055-011-0187-9#ref-CR3" id="ref-link-section-d80697e680">2005</a>).</p><h3 class="c-article__sub-heading" id="Sec12">Results</h3><p>The <i>dependent</i> variables were picking time and errors. In contrast to the first test of Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec5">2</a>, we could not find a significant time difference (based on RM-ANOVA-GLM
<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>) between the three test cycles for each visualization. We are thus not able to claim the existence of a significant learning effect. Even though this does not prove that no learning effect exists. However, it suggests that the effect is reduced compared to the first experiment. We attribute this to the fact that we exposed test persons to an extensive try-and-ask phase. In consequence, we treat all orders equally in the following analysis.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Errors</h4><p>The mean error per pick amounted to 0.17 (stddev: 0.30) for the Arrow, 0.00 for the Frame and 0.06 (stddev: 0.04) for the Tunnel. The absolute error values (Arrow <i>n</i> = 49, Tunnel <i>n</i> = 3, and Frame <i>n</i> = 0) give a clear result and make further statistical analysis superfluous: a) The users had serious problems with the Arrow, at least in the beginning. b) The Tunnel produced a few problems. c) The Frame was perfect.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Picking times</h4><p>Picking times per item amounted to 4.25 s (stddev: 1.01s) for the Arrow, 3.58 s (stddev: 1.02 s) for the Frame and 4.08 s (stddev: 0.84 s) for the Tunnel. There were significant differences (<i>F</i>(2.99) = 11.373, <i>p</i> = 0.00, RM-ANOVA-GLM test). The test persons were significantly faster using the Frame than using the Arrow (average 0.67 s, stddev: 0.161s, LSD
<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup>, <i>p</i> = 0.00) and the Tunnel (average 0.5 s, stddev: 0.119 s, LSD, <i>p</i> = 0.00).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Subjective observations</h4><p>During the evaluations of the Arrow visualization, test persons clustered into two disjoint groups. One group moved their head around the augmented Arrow until they were absolutely convinced that they had found the correct box. They even got down on their knees to be on the same height with the Arrow. The other group immediately picked without examining of the position of the Arrow carefully. People from the last group were faster but made more mistakes.</p><p>Additionally, we observed that some people closed the eye which was not covered by the HMD. Those people felt uncomfortable and complained about cluttered vision. After telling them to open both eyes, they felt much better and were able to perform the test. More details can be found in Schwerdtfeger and Klinker (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picking, 2008. Technische Universität München, Report TUM-I-08-19" href="/article/10.1007/s10055-011-0187-9#ref-CR12" id="ref-link-section-d80697e753">2008</a>).</p><h3 class="c-article__sub-heading" id="Sec16">Conclusions</h3><p>The test persons were significantly slower using the Arrow than using the Frame. Furthermore, participants made most mistakes using the Arrow, followed by the Tunnel. The Frame-based visualization worked without mistakes.</p><p>Concerning the meta visualization, many people preferred the Tunnel over the Compass-like arrow. The Compass indicates the direction but does not indicate how far away the object is and how much the user still has to turn. The Tunnel does provide this information due to the bending of the curve. Furthermore, the bending gives users immediate feedback about getting closer or further away. Test persons used these cues to adjust their speed of rotation. The Tunnel was most helpful for coarse navigation. However, some people complained that it was not always the perfect indicator of directions. For fine navigation to the box, people preferred the Frame over Tunnel and Arrow.</p><p>Another important finding of our first and second test series was the fact that the quality of the HMD was very important. Using the same Arrow visualization, the test persons performed much better in the second series due to the fact that we switched from a Sony Glasstron HMD to the Nomad (with better see-through capabilities and much more peripheral vision). In fact, we were able to reduce the size of the shelves while keeping a very high success rate.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Occlusion problems between tunnel and frame—public evaluation at an exhibition</h2><div class="c-article-section__content" id="Sec17-content"><p>In reaction to the results of the second experiment, we tried to combine the respective advantages of the Frame and the Tunnel (thereby disregarding the Compass)—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig4">4</a>c. When we demonstrated this system at an open house event at our university, more than 30 visitors tried it. None of them made a single mistake picking items. However, some complained that the Tunnel confused them when they actually wanted to put their hand in the box and take an item. We observed again that some people were moving their heads around the box to see the visualization straight in front of the correct box (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig4">4</a>c). It seemed that some people had problems identifying the Frame behind the Tunnel. The problem: the Frame visualization was cluttered by the Tunnel visualization.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Picture <b>a</b> and <b>b</b> show different implementations of the Tunnel (<i>square</i> and <i>ring</i> based) Picture <b>c</b> and <b>d</b> show different behaviors of the Tunnel when the actual augmentation comes in the field of view. In <b>c</b> the Tunnel is opaque and occludes the Frame. The Frame is only hardly visible to the right of the Tunnel. In <b>d</b> the Tunnel fades out</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Search for a meta visualization scheme compatible with seeing the frame behind the tunnel—test series 3</h2><div class="c-article-section__content" id="Sec18-content"><p>Due to the observations at the public exhibition, we set up a third experiment with the main intention of speeding up the process by reducing the cluttering of the Tunnel. We tried to achieve this by dynamically fading out the meta visualization when it was not needed.</p><h3 class="c-article__sub-heading" id="Sec19">Experimental setup</h3><p>The experimental setup was the same as in the second test series (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec6">3</a>), except for the fact that we replaced the Wizard of Oz-based user input with a game show-like buzzer.</p><p>In the previous experiment and in the exhibition, people had never picked from a wrong box when the box was highlighted by a Frame (as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>c), whereas the simple Arrow and the Tunnel alone were no safe indicators for the correct box (compare Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>b, d). In consequence, we based all further developments on the Frame, exploring how to combine it with different meta visualizations to guide users until the correct box was within their field of view. We designed four new meta visualizations to go with the Frame.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">S-Tunnel (opaque) with frame</h4><p>As the first navigation scheme, we chose the square-based Tunnel in combination with the Frame (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig4">4</a>b, c). We included this scheme in our evaluation even though we already knew about the drawbacks of cluttering. Yet, we wanted to see how it performed compared to other, new visualizations.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">S-Tunnel (semi-transparent) with frame</h4><p>Since the Tunnel becomes superfluous when the Frame enters the field of view, in this visualization, the Tunnel is faded out when the Frame is in the user’s field of view—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig4">4</a>b, d.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Compass-like arrow with frame</h4><p>Since the second series (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec6">3</a>) gave good (though not perfect) results for the combination of Compass-like Arrow and Frame (first meta visualization—Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig2">2</a>a, c), we used it in this evaluation as a measure of ground truth.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">R-Tunnel (semi-transparent) with frame</h4><p>As we had problems with artifacts when overlaying several semi-transparent square corners during the fade-out phase of the Tunnel, we replaced the squares of the Tunnel with rings. Thus, we designed a Tunnel of rings in combination with the Frame. The rings faded to transparent according to the same dynamic function as the S-Tunnel (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig4">4</a>a, d).</p><h3 class="c-article__sub-heading" id="Sec24">Test method</h3><p>We used the same test setup (including the try-and-ask phase and the same orders) as in the second experiment—except for using manual user input rather than the Wizard-of-Oz method. The four levels of our independent variable were the just presented different visualizations. We used 14 test persons between 20 and 50 years (mean age: 27.7, stddev: 7.5) in a within-subject design. We again looked for the fastest visualization scheme, which—at the same time—had to support error-free picking. As the visualizations using the Frame from the last experiment showed to be error proof, we set up the hypothesis, that people would not make any errors. Additionally, we set up the undirected hypothesis that there would be a difference with respect to speed performance.</p><h3 class="c-article__sub-heading" id="Sec25">Results</h3><p>During pre-evaluations, we again could not measure significant learning effects between the three orders for each visualization. Thus, we again treat all orders the same. We again attribute this to the try-and-ask phase.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Errors</h4><p>During all 1,512 picks of this experiment, none of the test persons picked an item wrongly. Even though we had hypothesized this, we were still pleasantly surprised.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Picking times</h4><p>We measured the following picking times per item: 6.60 s (stddev:1.53 s) for the S-Tunnel (opaque), 6.27 s (stddev:1.21 s) for the S-Tunnel (semi-transparent), 6.04 s (stddev:2.33 s) for the Arrow, and 6.04 s (stddev:1.20 s) for the R-Tunnel (semi-transparent). There was a significant difference (<i>F</i>(3.52) = 3.215, <i>p</i> = 0.033, RM-ANOVA-GLM test). People performed significantly slower (LSD, <i>p</i> = 0.0) using the opaque S-Tunnel than using other visualizations. There are two seconds of difference in the average picking times, compared to the previous experiment (series 2). We lead these back to the change of the user input (Wizard of Oz versus game show buzzer): In this experiment, the users had to move the hand to the buzzer, whereas the in the previous experiment the Wizard of Oz operator preset the button as soon as the user put the item into the accumulation bin.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Subjective observations</h4><p>In a later interview, the test participants complained that the opaque S-Tunnel cluttered their view. Cluttering was experienced less for the semi-transparent Tunnels, but it was not yet perfect. The R-Tunnel was considered to be the best Tunnel, as users could distinguish the rings of the Tunnel more easily from the Frame (highlighting the box). We also realized that the function that faded Tunnels from opaque to transparent did not work perfectly from all perspectives.</p><p>Overall, the transparent R-Tunnel and the Arrow were rated best. The test persons liked the Arrow because it directly indicated where to look. Yet, it lacked distance information. Interestingly, the Tunnel was not considered to be a good directional indicator at the very beginning of the movement. Yet, later on during user motion, the Tunnel provided a better information about the remaining distance and movement to execute. This refines the findings of the second experiment.</p><p>Furthermore, people argued that sometimes they had to take one step back to see more elements of the Tunnel. Thus, the Tunnel was not a good visualization when users stood directly (about 0.5 m) in front of the shelf.</p><h3 class="c-article__sub-heading" id="Sec29">Conclusions</h3><p>In this iteration of user studies, we could prove the Frame visualization to be the best and clearest technique to indicate the box to pick from. Furthermore, we could improve the meta visualization. The idea of fading the Tunnel from opaque to semi-transparent when the target was in the field of view was shown to be good in principle, yet, the way we did it was not. A detailed analysis of this problem, as well as improvements are described in Schwerdtfeger and Klinker (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picking, 2008. Technische Universität München, Report TUM-I-08-19" href="/article/10.1007/s10055-011-0187-9#ref-CR12" id="ref-link-section-d80697e960">2008</a>).</p><p>The clear winners of this evaluation are the opaque R-Tunnel and the Arrow (both in combination with the Frame). The R-Tunnel performed very well because the test persons could distinguish between the elements of the Tunnel and the Frame due to the geometrical difference—and not yet due to the use of transparency (we were using a bad fading function). The Arrow was good for giving directional information, in particular when staying directly in front of the shelf. The Tunnel, on the other hand, gave good feedback about the remaining distance when the target was not yet within the field of view—thereby helping users to adjust their speed of rotational motion toward the target. The Tunnel was more helpful when standing a step back from the shelf such that the Tunnel shape was prolonged and well visible. These results were presented at ISMAR 08 (Schwerdtfeger and Klinker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Schwerdtfeger B, Klinker G (2008) Supporting order picking with augmented reality. In: Proceedings of the 6th international symposium on mixed and augmented reality (ISMAR), Sept 2008" href="/article/10.1007/s10055-011-0187-9#ref-CR13" id="ref-link-section-d80697e966">2008</a>).</p></div></div></section><section aria-labelledby="Sec30"><div class="c-article-section" id="Sec30-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec30">Bringing it to a warehouse: covering the full range of viewing directions—test series 4</h2><div class="c-article-section__content" id="Sec30-content"><p>To get a clearer understanding of the difference between the users’ performance using the Tunnel and the Arrow as meta visualizations, we moved our setup from the single shelf (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig3">3</a>b) to a realistic, warehouse-like setting consisting of four shelves and two lanes (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig5">5</a>). In the old setup, users basically had to find boxes in a single plane. They mostly had to turn their head by no more than 90 degrees to see the target. In our new setup, we exposed the users to a situation where they had to navigate between several shelves to find the boxes, and the boxes were rather high above them, low at their feet, very far to the left or right, or also on a shelf behind them. Meta visualizations thus had to cover nearly the entire range of 4π steradians. Furthermore, test persons had to walk from box to box instead of having everything just about reachable at arm’s length. Thus, the distances for the augmentations varied from rather close distances to about three meters. This is a challenge to the depth perception that needs to be provided by the visualization.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The warehouse with 4 shelves. The test person wears the Nomad HMD from the previous experiments. The WiFi-connected wireless PC is carried in a small backpack. As system control, we mounted the game show-like buzzer on the user’s belt. The <i>red circles</i> indicate the infrared-based optical camera system providing the tracking support</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The tracking was provided by an ART DTrack System.
<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> This tracking system, used in our wide-area setup, delivered a pose with an uncertainty of a few millimeters. The users sometimes reached the limits of the tracking functionality when the tracking target was partially occluded or when they bent down to the boxes close to the ground floor. The augmentations were then displaced by up to two centimeters. However, the users never felt irritated by this misalignment. Rather, they adapted to it.</p><p>Due to external constraints and requirements, we had to use our new setup to perform a comparative evaluation between our Pick-by-Vision system and a traditional paper-based list system, as well as a mixture of both systems (showing the information from the paper list on an HMD, without using a tracking system). This constraint was accompanied by the fact that we were going to use our system for the first time under realistic constraints. We knew that our Pick-by-Vision system was far from perfect, and we had a countless number of variants what the visualization could look like and how the system could behave.</p><h3 class="c-article__sub-heading" id="Sec31">Pre-evaluation</h3><p>After several discussions, we changed our approach toward evaluating user interfaces. We moved from the quantitative approach using rigid usability studies to a qualitative approach. With the new qualitative approach, we heavily reduced the number of test persons, but collected much more detailed data from the few test users by following them through every move and turn they made. We watched all interactions and all facial or body reactions extensively and discussed their reactions with them during and after the test.</p><p>We observed 8 subjects in-depth, each of them for about 2 h. Our subjects were 2 augmented reality experts, 2 operative order-picking workers, 2 logistics students and 2 casual students. These different types of users gave us elaborate input from all relevant areas. During the test, we presented the different visualizations to them and reacted to their comments immediately by changing the visualization ad-hoc. This was possible because the parameters of the visualization were designed to be changeable on the fly. This was extremely helpful, since we could discuss the changed approach immediately with the test persons. In retrospect, this change of evaluation policy was one of the best decisions we made in the entire process.</p><p>The main results of these informal pre-evaluations were that most people preferred the Tunnel over the Arrow. This was, among other things, due to the additional depth cues provided by the Tunnel. Furthermore, the test persons expressed similar positive preferences for the Tunnel as test persons did in previous tests. However, our most successful improvement (having the largest impact at the lowest cost) was reducing the thickness of the rings of the Tunnel to a minimum (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig1">1</a>). This made the Tunnel unobtrusive. Test persons could easily see through it. Thus, we did not have to fade out the Tunnel anymore—as we tried to do, in order not to occlude the Frame visualization behind it. This is important, because we realized that the users benefited from the Tunnel in the larger warehouse even when the picking visualization was, indeed, in the field of view: it provided quite important depth cues in the larger distances of the warehouse. Furthermore, the thin rings allowed the workers to have a clear view of their environment at any point in time. This is important in order to prevent workers from overlooking trip hazards and to prevent them from stumbling. Workers have to interact continuously with their environment. For example, they need to read article numbers from the items they just picked. This is problematic when the rings are too thick.</p><h3 class="c-article__sub-heading" id="Sec32">Evaluation</h3><p>We compared two non-AR-based techniques with our newly developed thin ring Tunnel visualization. We had 19 test persons between 18 and 45 years (mean age: 27.2, stddev: 6.78) in a within-subject design. We had people from academia as well as professional order-picking workers. For each visualization, they had to perform 6 orders, having 5 positions with 203 items per position (on average) to pick. The main result was that people did not make a single mistake using the new Pick-by-Vision system. However, they did made interactive mistakes, such as pressing the system control button twice—we did not provide a good mechanism to go one step back. Regarding the task-completion time, people were slightly faster using the Pick-by-Vision technology. More importantly, we were able to show that the Pick-by-Vision system generally worked well in a real warehouse-based setup. [For more detailed results, see Schwerdtfeger et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of the 7th international symposium on mixed and augmented reality (ISMAR), Oct 2009" href="/article/10.1007/s10055-011-0187-9#ref-CR14" id="ref-link-section-d80697e1047">2009</a>).] However, we realized that the Tunnel was not yet optimal. It performed well when people were within about 80 degrees from the box from which to pick. We call areas for these viewing directions FoV 1 and FoV 2 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig6">6</a>). For wider angles (FoV 3), the Tunnel was not able to indicate well in which direction the user had to start rotating. Even worse: if the box was much further away than about 110 degrees (FoV 4), the mathematically shortest path often went through the legs or over the head of the user. Unfortunately, this was not the fastest path for the user to turn toward the box.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0187-9/MediaObjects/10055_2011_187_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The Figure shows the different behaviors of the Tunnel. It is dependent on the angle alpha between the vector pointing from the box to the user and the viewing direction of the user. That means alpha is an indicator, how far the user has to turn to have the actual augmentation in its viewing center. The actual angular threshold depends on the field of view of the HMD. For the Nomad, we used 35°, 90° and 115°</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0187-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec33">Improved tunnel animation for FoV 2–FoV 4</h3><p>In summary, the Tunnel was a good indicator to guide the user in angles of less than 80 degrees (FoV 1 and FoV 2). For larger angles (FoV 3 and FoV 4) another meta-meta visualization is needed.</p><p>We decided against using a new type of meta-meta visualization such as additional arrows, since this wealth of different presentation styles might confuse the user. Rather, we modified the Tunnel animation, allowing the closest ring of the Tunnel to gradually slide away from the center of the display. For FoV 2, we arranged for an interpolated sliding motion of the ring toward the boundary of the display, in the direction of the target box. In FoV 3 and FoV 4, we no longer adhered to the geometrically correct interpolation but rather forced the ring to stick to one of the vertical borders—in order to prevent the tunnel from going through the users’ legs or above their heads. This means in FoV 3 that it interpolates from the geometrically correct border (FoV2) to the left or right border. In FoV 4, half of the ring sticks at the center of the left/right border—this implementation changed later, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec36">8</a>. Using this animation scheme, users always saw at least a little part of the Tunnel, helping them understand in which direction to turn in order to see more. The adapted Tunnel animations can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig6">6</a> for FoV 2 and FoV 3.</p></div></div></section><section aria-labelledby="Sec34"><div class="c-article-section" id="Sec34-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec34">Endurance test in the warehouse—test series 5</h2><div class="c-article-section__content" id="Sec34-content"><p>After several try-outs with the newly improved Tunnel visualization and some general system improvements, we submitted our system to a larger endurance test, comparing how people perform with Pick-by-Vision versus with a paper-based list. We tested again 8 test persons in a within-subject design in the same warehouse (Schwerdtfeger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of the 7th international symposium on mixed and augmented reality (ISMAR), Oct 2009" href="/article/10.1007/s10055-011-0187-9#ref-CR14" id="ref-link-section-d80697e1096">2009</a>). But this time, each test person had to work for 2 hours with each system. Every test person picked 300 items on average in each test. With the paper-based list, the users picked 134 (StdDev:27) order lines per hour and 145 (StdDev:39) with Pick-by-Vision. A significant difference could not be proven (Wilcoxon, <i>p</i> = .401). Users made 2.34% (StdDev: 2.08%) errors using the Pick-by-Vision system compared to 1.37% (StdDev:1.09%) with Pick-by-Paper. The main reason for the bad performance of the Pick-by-Vision is that it did not prevent users from picking the wrong amount or from accidentally skipping an order line. Yet, Pick-by-Vision always helped to find the correct box.</p><p>After a short introduction, people were able to work well with the Pick-by-Vision system, except for one fact: The visualization in the FoV 4 area was too static. The system did not help users realize whether they were getting closer to the target or not: the Tunnel stuck motionlessly to the border of the display. One subject with a mental left/right weakness actually confused the static ring at the left/right border and moved in the opposite direction, due to translating the visual instruction into an abstract orientational concept and then switching directions when transforming it into physical motion (according to her own description).</p></div></div></section><section aria-labelledby="Sec35"><div class="c-article-section" id="Sec35-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec35">Providing support even in the far-off field—test series 6, final visualization</h2><div class="c-article-section__content" id="Sec35-content"><p>In reaction to these observations, we animated the Tunnel visualization for FoV 4, gradually deforming the ring into an ellipse. We considered two options, ring contraction and ring expansion. Under <i>ring contraction</i>, the ring deforms horizontally, in fuzzy analogy to the idea that the center of the ring is stuck to the target, and its rim is pulled so much in the horizontal direction that it becomes increasingly flat (see the right pictures in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig6">6</a>). Under <i>ring expansion</i>, the ring deforms vertically, similar to the visualization by Baudisch (Baudisch and Rosenholtz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Baudisch P, Rosenholtz R (2003) Halo: a technique for visualizing off-screen locations. In: Proceedings of CHI 2003, Fort Lauderdale, FL" href="/article/10.1007/s10055-011-0187-9#ref-CR1" id="ref-link-section-d80697e1121">2003</a>), presenting an off-screen location in 2D maps on mobile phones. This scheme alludes to the idea that, again, the center of the ring lies on the target. Yet, the ring is not deformed but rather expands radially such that its outline is still partially visible on the display. Neither concept adheres to physically correct principles in three dimensions but works only as a weak analogy to a concept in which the target location has been projected perpendicularly onto the two-dimensional viewing plane of the display.</p><p>We discussed both alternatives with several experts and received votes of preference for either one. We thus conducted a further user study. Since the large warehouse (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig5">5</a>) was no longer available, we had to go back to the laboratory setup of the second test series (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0187-9#Sec6">3</a>; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig3">3</a>), consisting of two shelves side-by-side. The user was placed at arm’s length in front of the shelves, centered with one shelf to his left and the other to his right. We observed nine users (recruited from the hallway) in a within-subject design. The structure of the experiment followed our established approach: try-and-ask-introduction and 3 orders (with 9 articles) for each visualization. We hypothesized that it does not make a difference whether we expand or contract the ring, it only matters that the user receives continuous feedback according to his motion.</p><h3 class="c-article__sub-heading" id="Sec36">Results</h3><p>As expected, we did not measure significant differences in task performance time. However, we observed only nine users. In this exploratory study, the feedback from the users was more valuable than the quantitative analysis. Two users had problems getting comfortable with AR and HMDs in general. They preferred the expanding rings. Two users had no preferences. They just checked on which side of the display the ring was located. The other users preferred the contracting ring, due to the following two reasons.</p><p>Firstly, the contracting ring can give more detailed information about the magnitude of the required rotation. For the contracting ring, rotational motion dramatically changes the visual appearance of the ring on the display. The ellipse becomes flatter, yet protrudes more and more from the display boundary into the center of the display. In contrast, the curvature of the expanding ring becomes increasingly flat (vertical) and recedes to the boundary of the display. It thus becomes less visible and rotational changes are rarely discernible from the minimal change in the curvature of the ring. This effect is amplified by the fact that the display of the HMD has larger width than height.</p><p>Secondly, test persons found the transition from FoV 3 to FoV 4 confusing in the expanding case. They felt that it was counterintuitive for a metaphor to grow (expand) when the distance to the target increases. They preferred the idea that something stretches and deforms like rubber when one moves away and thus pulls it. The final visualization with the contracting circle (in FoV 4) is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0187-9#Fig6">6</a>.</p><p>We presented the final Tunnel visualization to several users. For all earlier tests, we had had to explain the visualization or at least give some small hints about how it works before users were able to use it. But with this new visualization, for the first time, we could just give users the HMD, start the demo. People knew without further instructions what to do. One comment from an expert was: “How else should it work!”
<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup>
                        </p></div></div></section><section aria-labelledby="Sec37"><div class="c-article-section" id="Sec37-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec37">Conclusions</h2><div class="c-article-section__content" id="Sec37-content"><p>We have presented the long and iterative story of developing a visualization for an industrial augmented reality system. It shows the complexity of designing a usable navigation and grasping aid for order picking. The final solution may seem obvious—yet, the research path toward finding this needle in a hay stack of alternatives is a long winding road.</p><p>We identified several facts to consider. A meta visualization must indicate at any point in time and for any user position, how far users still have to move to have the actual augmentation in the field of view. Without this, users move either too slowly or too fast. In the latter case, they often overshoot the mark and then have to turn back.</p><p>Second, the visualization must always (at any second and from every perspective) give a clear indication in which direction users have to turn. In doing so, attention has to be paid to the physical constraints of the user, i.e., the mathematically shortest way to turn is not always the best way to guide the user.</p><p>Furthermore, the meta visualization should not be obtrusive. Users should always be able to mentally just ignore and see through the meta visualization in order to focus on other real or virtual objects. This is quite important as the worker has to interact with the real environment intensively and must always be aware of the real dangers of industrial everyday life.</p><p>As a by-product, we learned that the feedback we got by observing the people in informal tests is at least as valuable as the feedback from the formal user studies. Furthermore, we realized that there exist large learning and fascination effects when using Augmented Reality for the first time—a problem which we conquered by providing extensive try-and-ask introduction phases before each test.</p><p>However, it is still a long way to bring this system to a real industrial environment. For example in Schwerdtfeger et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of the 7th international symposium on mixed and augmented reality (ISMAR), Oct 2009" href="/article/10.1007/s10055-011-0187-9#ref-CR14" id="ref-link-section-d80697e1178">2009</a>), we have shown that most people can work with the system over a longer period of time without being more strained than using a conventional paper-based picking system. But there is still problem that some people had serious trouble to read continuously from an HMD.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>To be able to adapt with minimal effort to global dynamic changes.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>And probably resulting in one of the most outworn HMDs.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>Repeated Measure ANOVA using a General Linear Model.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>Fisher’s Least Significant Difference test.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p><a href="http://www.ar-tracking.de">http://www.ar-tracking.de</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>Original statement in German: “Wie sollte es auch sonst funktionieren!”</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baudisch P, Rosenholtz R (2003) Halo: a technique for visualizing off-screen locations. In: Proceedings of CHI" /><p class="c-article-references__text" id="ref-CR1">Baudisch P, Rosenholtz R (2003) Halo: a technique for visualizing off-screen locations. In: Proceedings of CHI 2003, Fort Lauderdale, FL</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Biocca F, Tang A, Owen C, Xiao F (2006) Attention funnel: omnidirectional 3d cursor for mobile augmented reali" /><p class="c-article-references__text" id="ref-CR2">Biocca F, Tang A, Owen C, Xiao F (2006) Attention funnel: omnidirectional 3d cursor for mobile augmented reality platforms. In: CHI ’06: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, pp 1115–1122</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Bortz, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bortz J (2005) Statistik für Human- und Sozialwissenschaftler, 6th ed. Springer Medizin Verlag, Heidelberg" /><p class="c-article-references__text" id="ref-CR3">Bortz J (2005) Statistik für Human- und Sozialwissenschaftler, 6th ed. Springer Medizin Verlag, Heidelberg</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistik%20f%C3%BCr%20Human-%20und%20Sozialwissenschaftler&amp;publication_year=2005&amp;author=Bortz%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Curtis D, Mizell D, Gruenbaum P, Janin A (1998) Several devils in the details: making an ar app work in the ai" /><p class="c-article-references__text" id="ref-CR4">Curtis D, Mizell D, Gruenbaum P, Janin A (1998) Several devils in the details: making an ar app work in the airplane factory. In: Proc. IEEE and ACM IWAR’98 (1. International Workshop on Augmented Reality). AK Peters, San Francisco, Nov 1998, pp 47–60</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feiner S, MacIntyre B, Seligmann D (1993) Knowledge-based augmented reality. Commun ACM 36(7)" /><p class="c-article-references__text" id="ref-CR5">Feiner S, MacIntyre B, Seligmann D (1993) Knowledge-based augmented reality. Commun ACM 36(7)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Gudehus, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Gudehus T (2005) Logistik: Grundlagen, Strategien, Anwendungen. Springer, Berlin" /><p class="c-article-references__text" id="ref-CR6">Gudehus T (2005) Logistik: Grundlagen, Strategien, Anwendungen. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Logistik%3A%20Grundlagen%2C%20Strategien%2C%20Anwendungen&amp;publication_year=2005&amp;author=Gudehus%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Günthner WAH (2007) In: Neue Wege in der Automobillogistik: Die Vision der Supra-Adaptivität. Springer" /><p class="c-article-references__text" id="ref-CR7">Günthner WAH (2007) In: Neue Wege in der Automobillogistik: Die Vision der Supra-Adaptivität. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Henderson S, Feiner S (2009) Evaluating the benefits of augmented reality for task localization in maintenance" /><p class="c-article-references__text" id="ref-CR8">Henderson S, Feiner S (2009) Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret. In: International symposium on mixed and augmented reality (ISMAR ’09)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gabbard J, Swan E, Hix D (2006) The effects of text drawing styles, background textures, and natural lighting " /><p class="c-article-references__text" id="ref-CR9">Gabbard J, Swan E, Hix D (2006) The effects of text drawing styles, background textures, and natural lighting on text legibility in outdoor augmented reality. Presence Teleoper Virtual Environ 15(1)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kramer LJ, Prinzel LJ III, Arthur JJ III, Bailey RE (2004) Pathway design effects on synthetic vision headup d" /><p class="c-article-references__text" id="ref-CR10">Kramer LJ, Prinzel LJ III, Arthur JJ III, Bailey RE (2004) Pathway design effects on synthetic vision headup displays. In: Proceedings of SPIE—vol 5424, enhanced and synthetic vision</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics ap" /><p class="c-article-references__text" id="ref-CR11">Schwerdtfeger B, Frimor T, Pustka D, Klinker G (2006) Mobile information presentation schemes for logistics applications. In: Proceedings of 16th international conference on artificial reality and telexistence (ICAT 2006), Nov 2006</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picki" /><p class="c-article-references__text" id="ref-CR12">Schwerdtfeger B, Klinker G (2008) An evaluation of augmented reality visualizations to support the order picking, 2008. Technische Universität München, Report TUM-I-08-19</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwerdtfeger B, Klinker G (2008) Supporting order picking with augmented reality. In: Proceedings of the 6th " /><p class="c-article-references__text" id="ref-CR13">Schwerdtfeger B, Klinker G (2008) Supporting order picking with augmented reality. In: Proceedings of the 6th international symposium on mixed and augmented reality (ISMAR), Sept 2008</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR14">Schwerdtfeger B, Klinker G, Reif R, Tuemler J (2009) Pick-by-vision in an endurance test. In: Proceedings of the 7th international symposium on mixed and augmented reality (ISMAR), Oct 2009</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0187-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to thank T. Frimor, E. Yükselgil, M. Stadter, X. Pan, M. Stadtler, M. Meister and all our test persons. Furthermore, we thank ART GmbH, Germany for lending the equipment. This work was partially supported by the ForLog and trackframe projects of the Bayerische Forschungsstiftung (BFS).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Technische Universität München, Munich, Germany</p><p class="c-article-author-affiliation__authors-list">Björn Schwerdtfeger, Rupert Reif, Willibald A. Günthner &amp; Gudrun Klinker</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Bj_rn-Schwerdtfeger"><span class="c-article-authors-search__title u-h3 js-search-name">Björn Schwerdtfeger</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Bj%C3%B6rn+Schwerdtfeger&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Bj%C3%B6rn+Schwerdtfeger" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bj%C3%B6rn+Schwerdtfeger%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Rupert-Reif"><span class="c-article-authors-search__title u-h3 js-search-name">Rupert Reif</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Rupert+Reif&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rupert+Reif" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rupert+Reif%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Willibald_A_-G_nthner"><span class="c-article-authors-search__title u-h3 js-search-name">Willibald A. Günthner</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Willibald A.+G%C3%BCnthner&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Willibald A.+G%C3%BCnthner" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Willibald A.+G%C3%BCnthner%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gudrun-Klinker"><span class="c-article-authors-search__title u-h3 js-search-name">Gudrun Klinker</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gudrun+Klinker&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gudrun+Klinker" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gudrun+Klinker%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0187-9/email/correspondent/c1/new">Gudrun Klinker</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Pick-by-vision%3A%20there%20is%20something%20to%20pick%20at%20the%20end%20of%20the%20augmented%20tunnel&amp;author=Bj%C3%B6rn%20Schwerdtfeger%20et%20al&amp;contentID=10.1007%2Fs10055-011-0187-9&amp;publication=1359-4338&amp;publicationDate=2011-03-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Schwerdtfeger, B., Reif, R., Günthner, W.A. <i>et al.</i> Pick-by-vision: there is something to pick at the end of the augmented tunnel.
                    <i>Virtual Reality</i> <b>15, </b>213–223 (2011). https://doi.org/10.1007/s10055-011-0187-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0187-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-20">20 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-02-10">10 February 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-03-08">08 March 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0187-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0187-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User studies</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Order picking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Logistics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tracked head-mounted display</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Guidance</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0187-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=187;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

