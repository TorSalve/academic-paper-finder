<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Tangible props for scientific visualization: concept, requirements, ap"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In this paper, we explore the use of printed tangible props as input devices for scientific visualization. Three-dimensional printing technology is used to create a physical representation of data...."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/13/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Tangible props for scientific visualization: concept, requirements, application"/>

    <meta name="dc.source" content="Virtual Reality 2009 13:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2009-07-07"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In this paper, we explore the use of printed tangible props as input devices for scientific visualization. Three-dimensional printing technology is used to create a physical representation of data. The object is then used as a tangible input prop, which exactly matches the data. In addition, two-handed interaction with a stylus is performed on the prop without the use of buttons, instead relying on the detection of contact between the stylus and the prop through precise calibration and tracking. This allows the sense of touch to be harnessed to create a more efficient and natural interaction method for scientific visualizations in virtual and augmented reality. We explain the concept of tangible props and where it can be applied. We also consider the technical requirements of systems using such props. Finally, we present our example application, which uses printed tangible props for interactive measurement of marine coral data. The use of tangible props is found to improve the usability of the application."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2009-07-07"/>

    <meta name="prism.volume" content="13"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="235"/>

    <meta name="prism.endingPage" content="244"/>

    <meta name="prism.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-009-0126-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-009-0126-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-009-0126-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-009-0126-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Tangible props for scientific visualization: concept, requirements, application"/>

    <meta name="citation_volume" content="13"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2009/12"/>

    <meta name="citation_online_date" content="2009/07/07"/>

    <meta name="citation_firstpage" content="235"/>

    <meta name="citation_lastpage" content="244"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-009-0126-1"/>

    <meta name="DOI" content="10.1007/s10055-009-0126-1"/>

    <meta name="citation_doi" content="10.1007/s10055-009-0126-1"/>

    <meta name="description" content="In this paper, we explore the use of printed tangible props as input devices for scientific visualization. Three-dimensional printing technology is used to"/>

    <meta name="dc.creator" content="Krzysztof Jakub Kruszy&#324;ski"/>

    <meta name="dc.creator" content="Robert van Liere"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: SI3D &#8216;92 proceedings of the 1992 symposium on interactive 3D graphics, pp 183&#8211;188"/>

    <meta name="citation_reference" content="Couture N, Rivi&#232;re G, Reuter P (2008) Geotui: a tangible user interface for geoscience. In: TEI &#8216;08 proceedings of the 2nd international conference on tangible and embedded interaction, pp 89&#8211;96"/>

    <meta name="citation_reference" content="Fitzmaurice GW, Ishii H, Buxton WAS (1995) Bricks: laying the foundations for graspable user interfaces. In: CHI &#8216;95 proceedings of the SIGCHI conference on human factors in computing systems, pp 442&#8211;449"/>

    <meta name="citation_reference" content="Gillet A, Sanner M, Stoffler D, Goodsell D, Olson A (2004) Augmented reality with tangible auto-fabricated models for molecular biology applications. In: VIS &#8216;04 proceedings of the conference on visualization 2004, pp 235&#8211;242"/>

    <meta name="citation_reference" content="Hinckley K, Pausch R, Goble JC, Kassell NF (1994) Passive real-world interface props for neurosurgical visualization. In: CHI &#8216;94 proceedings of the SIGCHI conference on human factors in computing systems, pp 452&#8211;458"/>

    <meta name="citation_reference" content="Ishii H (2008) Tangible bits: beyond pixels. In: TEI &#8216;08 proceedings of the 2nd international conference on tangible and embedded interaction"/>

    <meta name="citation_reference" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI &#8216;97 proceedings of the SIGCHI conference on Human factors in computing systems, pp 234&#8211;241"/>

    <meta name="citation_reference" content="Kok AJ, van Liere R (2004) Co-location and tactile feedback for 2D widget manipulation. In: Proceedings IEEE conference on virtual reality 2004, pp 233&#8211;234"/>

    <meta name="citation_reference" content="Kruszy&#324;ski KJ, van Liere R (2008) Tangible interaction for 3D widget manipulation in virtual environments. In: EGVE &#8216;08 proceedings eurographics symposium on virtual environments 2008, pp 89&#8211;95"/>

    <meta name="citation_reference" content="citation_journal_title=Coral Reefs; citation_title=A computational method for quantifying morphological variation in scleractinian corals; citation_author=KJ Kruszy&#324;ski, JA Kaandorp, R Liere; citation_volume=26; citation_issue=4; citation_publication_date=2007; citation_pages=831-840; citation_doi=10.1007/s00338-007-0270-6; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Occlusion in mirror-based co-located augmented reality systems; citation_author=JD Mulder; citation_volume=15; citation_issue=1; citation_publication_date=2006; citation_pages=93-107; citation_doi=10.1162/pres.2006.15.1.93; citation_id=CR11"/>

    <meta name="citation_reference" content="Ortega M, Coquillart S (2005) Prop-based haptic interaction with co-location and immersion: an automotive application. In: HAVE 2005 Proceedings of the IEEE international workshop on haptic audio visual environments and their applications, 2005"/>

    <meta name="citation_author" content="Krzysztof Jakub Kruszy&#324;ski"/>

    <meta name="citation_author_email" content="chris.kruszynski@cwi.nl"/>

    <meta name="citation_author_institution" content="Centrum Wiskunde &amp; Informatica, Amsterdam, The Netherlands"/>

    <meta name="citation_author" content="Robert van Liere"/>

    <meta name="citation_author_institution" content="Centrum Wiskunde &amp; Informatica, Amsterdam, The Netherlands"/>

    <meta name="citation_author_institution" content="Technische Universiteit Eindhoven, Eindhoven, The Netherlands"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-009-0126-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2009/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-009-0126-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Tangible props for scientific visualization: concept, requirements, application"/>
        <meta property="og:description" content="In this paper, we explore the use of printed tangible props as input devices for scientific visualization. Three-dimensional printing technology is used to create a physical representation of data. The object is then used as a tangible input prop, which exactly matches the data. In addition, two-handed interaction with a stylus is performed on the prop without the use of buttons, instead relying on the detection of contact between the stylus and the prop through precise calibration and tracking. This allows the sense of touch to be harnessed to create a more efficient and natural interaction method for scientific visualizations in virtual and augmented reality. We explain the concept of tangible props and where it can be applied. We also consider the technical requirements of systems using such props. Finally, we present our example application, which uses printed tangible props for interactive measurement of marine coral data. The use of tangible props is found to improve the usability of the application."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Tangible props for scientific visualization: concept, requirements, application | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-009-0126-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Tangible user interfaces, Haptics, Augmented reality, Rapid prototyping","kwrd":["Tangible_user_interfaces","Haptics","Augmented_reality","Rapid_prototyping"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-009-0126-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-009-0126-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=126;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-009-0126-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Tangible props for scientific visualization: concept, requirements, application
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0126-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0126-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2009-07-07" itemprop="datePublished">07 July 2009</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Tangible props for scientific visualization: concept, requirements, application</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Krzysztof_Jakub-Kruszy_ski" data-author-popup="auth-Krzysztof_Jakub-Kruszy_ski" data-corresp-id="c1">Krzysztof Jakub Kruszyński<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centrum Wiskunde &amp; Informatica" /><meta itemprop="address" content="grid.6054.7, 0000000403694183, Centrum Wiskunde &amp; Informatica, Science Park 123, 1098 XG, Amsterdam, The Netherlands" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Robert-Liere" data-author-popup="auth-Robert-Liere">Robert van Liere</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centrum Wiskunde &amp; Informatica" /><meta itemprop="address" content="grid.6054.7, 0000000403694183, Centrum Wiskunde &amp; Informatica, Science Park 123, 1098 XG, Amsterdam, The Netherlands" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universiteit Eindhoven" /><meta itemprop="address" content="grid.6852.9, 0000000403988763, Technische Universiteit Eindhoven, Eindhoven, The Netherlands" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 13</b>, Article number: <span data-test="article-number">235</span> (<span data-test="article-publication-year">2009</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">317 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">12 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-009-0126-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In this paper, we explore the use of printed tangible props as input devices for scientific visualization. Three-dimensional printing technology is used to create a physical representation of data. The object is then used as a tangible input prop, which exactly matches the data. In addition, two-handed interaction with a stylus is performed on the prop without the use of buttons, instead relying on the detection of contact between the stylus and the prop through precise calibration and tracking. This allows the sense of touch to be harnessed to create a more efficient and natural interaction method for scientific visualizations in virtual and augmented reality. We explain the concept of tangible props and where it can be applied. We also consider the technical requirements of systems using such props. Finally, we present our example application, which uses printed tangible props for interactive measurement of marine coral data. The use of tangible props is found to improve the usability of the application.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Research in applying virtual reality (VR) for scientific visualization has made great progress throughout the years, and many success stories from various application domains have been reported. Undoubtedly, the greatest benefits of applying VR to scientific visualization are the enhanced feeling of presence and ease of interaction with the underlying scientific data provided to the user. These are further enhanced by the use of augmented reality (AR), combining the familiar real world with the application.</p><p>Two-dimensional (2D) and three-dimensional (3D) widgets are normally used for interacting with scientific data (Conner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: SI3D ‘92 proceedings of the 1992 symposium on interactive 3D graphics, pp 183–188" href="/article/10.1007/s10055-009-0126-1#ref-CR1" id="ref-link-section-d18795e328">1992</a>). These widgets provide an interface between the limited input device controlled by the users and a virtual object that performs specific actions on the data. Actions on widgets are triggered by pressing a button on the input device. Extensive sets of 2D and 3D widgets have been designed and have been developed for many diverse tasks. For example, 2D widgets can be used to modify visualization or simulation parameters. 3D widgets can be used to query values in the dataset, set seed points for flow visualization techniques, etc.</p><p>Input devices have been traditionally generic pointing devices, ranging from the two degrees-of-freedom (2DOF) mouse through six degrees-of-freedom (6DOF) devices (e.g. wands, gloves, etc.) and to haptic devices. Buttons are placed on the devices to trigger actions. In all cases, however, these devices are used to control a single point location in the 3D interaction space. It is our belief that providing users with only pointing devices results in non-intuitive and often difficult to use user interfaces. In our view, the main reason for this is that the physical devices serve only as a proxy for a virtual object, and the user has to rely on the generated visual or haptic feedback to interpret the effect of her actions. The actual interaction occurs in a remotely controlled and immaterial virtual world, distinct from the user’s familiar environment. Notwithstanding the use of expensive haptic devices, attempting to pick locations in thin air guided by nothing more than imperfect visual feedback is an inefficient method of interaction.</p><p>Tangible user interfaces provide real, physical props that can be manipulated by the user, and virtual counterparts of these props. The physical input props are identical, or at least similar in shape to their virtual counterparts, and both can be manipulated in precisely the same way. The user interacts by performing exactly those actions on the tangible prop which he would like to perform on the virtual representation. For example, a virtual object is moved by moving a physical, tangible handle. Such direct manipulation of physical objects provides a very natural and easy to use interface, since it taps into the same skills, which the user needs to sense and manipulate his physical environment in daily life (Ishii <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ishii H (2008) Tangible bits: beyond pixels. In: TEI ‘08 proceedings of the 2nd international conference on tangible and embedded interaction" href="/article/10.1007/s10055-009-0126-1#ref-CR6" id="ref-link-section-d18795e337">2008</a>). If the prop is a representation of the data in a visualization system then, in a sense, the user actually holds the data in his hands and can directly interact with it, instead of only manipulating it indirectly, on the screen.</p><p>Tangible interfaces are only limited by what can be physically created, and what kind of manipulations can be sensed and captured. The interface may include dials, sliders, buttons and switches, position and orientation tracking, pressure and tension sensors, or any other kind of sensing and measuring devices. The interface may even be combined with active haptic feedback systems.</p><p>In our approach, two-handed interaction using a stylus is performed on the props. The interface does not require buttons to indicate user actions, but instead uses contact between the stylus and the prop as the trigger. Data from the tracking system and precise calibration of the prop are used to detect when this contact occurs. This allows the interaction to occur in the same manner that simple everyday tools are used: a tool (the stylus) is picked up and is used on an object (the prop) simply by touching the object with the tool.</p><p>In this paper, we explore how tangible input props can be used to provide a better interface for scientific visualizations in virtual and in augmented reality. Our approach is to use rapid prototyping technology to print arbitrary shaped three-dimensional physical objects which are subsequently imported into the environment and used as tangible devices for interaction. The approach is iterative; so that more complex props can be used as more insight into the underlying simulation is gained. In the next section, we give an overview of the related work. Then, we explain the concept and where it can be applied, we consider the technical requirements of systems using tangible props, and finally we present our example application where we use printed tangible props for interactive measurement of marine coral data.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Many scientific visualization systems have been developed which use a variety of tangible props as input devices. These systems use either very generic abstract props, or detailed props, which are limited to a specific application or type of data. In these systems, physical props are mostly used for their resemblance to the virtual model and familiarity when touching the prop, while interaction is generally done with virtual devices and props. Haptic feedback comes from specialized devices like the Phantom, to which sometimes props are attached. Often mixed props are used in such a setup, where only the part, which is handled by the user, is an actual prop, while the rest of the prop exists only in the virtual world.</p><p>Hinckley et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Hinckley K, Pausch R, Goble JC, Kassell NF (1994) Passive real-world interface props for neurosurgical visualization. In: CHI ‘94 proceedings of the SIGCHI conference on human factors in computing systems, pp 452–458" href="/article/10.1007/s10055-009-0126-1#ref-CR5" id="ref-link-section-d18795e361">1994</a>) tracked passive props to create a natural interface for a neurosurgical visualization application. To create a linear trajectory into the patient’s brain a stylus was used, which controlled a ray, while the brain model was controlled with a rubber sphere. The trajectory was determined by the intersection of the ray with the brain model. A third prop consisting of a tracked piece of plexiglass was used to control a cutting plane to cut away part of the brain. Replacing the sphere with the head of a doll to provide a more realistic prop gave users false expectations, as the head prop was not exactly matched with the brain data. Placing the trajectory or cutting plane at a very specific place on the head prop, for example the eyes, would not result in the virtual tool being placed in the corresponding region of the brain model. This indicates that props should either be very abstract or an exact match of the data.</p><p>Couture et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Couture N, Rivière G, Reuter P (2008) Geotui: a tangible user interface for geoscience. In: TEI ‘08 proceedings of the 2nd international conference on tangible and embedded interaction, pp 89–96" href="/article/10.1007/s10055-009-0126-1#ref-CR2" id="ref-link-section-d18795e367">2008</a>) used tangible props as input devices with a tabletop projection system for a geoscience data analysis application. One or more tangible props of various kinds were moved around a flat projection to select cutting planes through a 3D volumetric model compiled from seismic data. More than one prop could be used simultaneously. A tangible box with physical buttons was used to initiate various actions. Each of these props corresponded to a virtual handle of a cutting plane, and as such, the prop only served as a handle for the actual tool. The actual shape of the props or any contact made between the props had no particular meaning.</p><p>Fitzmaurice et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Fitzmaurice GW, Ishii H, Buxton WAS (1995) Bricks: laying the foundations for graspable user interfaces. In: CHI ‘95 proceedings of the SIGCHI conference on human factors in computing systems, pp 442–449" href="/article/10.1007/s10055-009-0126-1#ref-CR3" id="ref-link-section-d18795e373">1995</a>) controlled virtual objects through tangible (‘graspable’) physical handles called ‘bricks’. Ishii and Ullmer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ‘97 proceedings of the SIGCHI conference on Human factors in computing systems, pp 234–241" href="/article/10.1007/s10055-009-0126-1#ref-CR7" id="ref-link-section-d18795e376">1997</a>) took this concept further with Tangible Bits, employing a more direct correspondence between physical and virtual objects in a context where almost every part of the environment could be used for interaction. However, the actual props used in this concept serve as physical handles for virtual tools, or are physical user interface elements. The data with which the interaction takes place only exists in the digital domain, and has no matching physical prop.</p><p>Gillet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Gillet A, Sanner M, Stoffler D, Goodsell D, Olson A (2004) Augmented reality with tangible auto-fabricated models for molecular biology applications. In: VIS ‘04 proceedings of the conference on visualization 2004, pp 235–242" href="/article/10.1007/s10055-009-0126-1#ref-CR4" id="ref-link-section-d18795e383">2004</a>) have used three-dimensional printed props of molecules in an AR application. The printed props included attachment surfaces for ARToolKit markers, which provided an implicit calibration of the props. Multiple ARToolKit markers were used for each prop, also to improve accuracy but mainly to prevent loss of tracking due to marker occlusion. The props were used to position and to orient the virtual models and were augmented with various kinds of information and visualizations, but no other input devices were used to interact with these props. As such, the props served only as physical handles for the virtual molecular models.</p><p>Ortega and Coquillart (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ortega M, Coquillart S (2005) Prop-based haptic interaction with co-location and immersion: an automotive application. In: HAVE 2005 Proceedings of the IEEE international workshop on haptic audio visual environments and their applications, 2005" href="/article/10.1007/s10055-009-0126-1#ref-CR12" id="ref-link-section-d18795e389">2005</a>) created an immersive industrial automotive application with a mixed tangible prop, attached to a string-based haptic system. The handle is a physical tangible prop, while the rest of the mixed prop existed only in the virtual world. The mixed prop is explicitly used to avoid calibration errors from using a full real prop. The focus is on the virtual part of the prop, so any mismatch between the real prop and the virtual model is of little consequence. The prop serves only as a physical handle for a virtual tool, and no physical form is used for the data, other than the haptic feedback.</p><p>Kok and van Liere (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kok AJ, van Liere R (2004) Co-location and tactile feedback for 2D widget manipulation. In: Proceedings IEEE conference on virtual reality 2004, pp 233–234" href="/article/10.1007/s10055-009-0126-1#ref-CR8" id="ref-link-section-d18795e395">2004</a>) controlled 2D widgets with 3D tangible props. They explored the use of passive haptic feedback as a means to detect that an action can be performed on a widget. The widgets were placed on a virtual cube, which was co-located with a tracked cube prop, and a tracked stylus was used to select the widget, but to actually start the interaction, a button had to be pressed, thus the tactile feedback from the surface of the cube only served to indicate that the stylus could interact with one of the widgets on the cube. The props only served as physical and mixed mode user interface elements.</p><p>In our previous work (Kruszyński and van Liere <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kruszyński KJ, van Liere R (2008) Tangible interaction for 3D widget manipulation in virtual environments. In: EGVE ‘08 proceedings eurographics symposium on virtual environments 2008, pp 89–95" href="/article/10.1007/s10055-009-0126-1#ref-CR9" id="ref-link-section-d18795e401">2008</a>), we used tangible interaction with a tracked stylus on a tracked cube prop to control 3D widgets, however, the interaction was not modeless but invoked by pressing a foot pedal, it was performed on only a single surface of the cube, and the cube prop was not an exact replica of the virtual counterpart. In addition, the tangible cube prop was not in any way related to the dataset with which interaction took place, but was purely a tangible element of the user interface.</p><p>All these systems use tangible props as input devices. However, the props serve as abstract physical handles or tools, and in most cases, they do not represent the data.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Concept</h2><div class="c-article-section__content" id="Sec3-content"><p>Tangible props add the possibility to use one’s sense of touch for the purpose of interaction. A physical representation of a data set held in the hand gives a very natural method of interaction, since people are accustomed to handling physical objects in their daily lives. Rapid prototyping technology provides the possibility to create any physical data representation. This printed object can then be used in conjunction with a tracking system to serve as a tangible input prop.</p><p>Some types of applications are more suited for use with printed tangible props than others. Especially applications which revolve around creating or analyzing structures or models, both mathematical and geometric, are suited for this approach.</p><h3 class="c-article__sub-heading" id="Sec4">Model development in iterations</h3><p>Models are developed in an iterative process. The first iteration is an initial rough basic model. Simulations or other calculations can then be carried out using the model, or the model itself is scrutinized. The results are then analyzed and the model is adapted where necessary, thus creating the next iteration. This iterative refinement process is continued until the final model is deemed satisfactory. This process is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The basic model refinement process. A crude model is created and is iteratively refined</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec5">Tangible prop</h3><p>A natural additional step in the process is to create a real-world version of the model in each iteration. A physical representation of the model can give better insights into the properties of the model. The representation can be created through various 3D printing methods, which can automatically produce a solid real-world object from the model. This technology is becoming increasingly affordable, and it is extremely likely that in a number of years 3D printers will be as ubiquitous as regular printers are today, and printed 3D objects will become disposable single-use items.</p><p>If the position and orientation of the printed model are tracked and related to the original virtual model then the print can be used as a tangible input prop, which corresponds exactly to the virtual model. It then becomes possible to use the sense of touch to explore the virtual model. This is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig2">2</a>. The visualization system can show additional visualizations and information, which is not visible on the physical prop, or show a magnified version of the model to examine small details.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Refinement process with tangible props. A crude model is created, and a tangible prop is made from this model. This prop then serves as an input device to create a more refined model. These steps can be repeated for several iterations</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The prop thus serves both as output and as input. It is the output of the last modeling iteration, a view of what the model looks like, and it can be annotated in an augmented reality system. It is also used as an input device to interact with the model.</p><p>Usually interaction is performed on virtual objects, and at most, a prop serves as a proxy for a virtual input object. However, by using tangible input props, the interaction actually takes place in the real world, and using a complex computer system becomes the same as using any physical everyday object. The computer system only responds to the actions, which the user performs in the real world, and it is no longer the primary focus of those actions.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Requirements</h2><div class="c-article-section__content" id="Sec6-content"><p>To use tangible props as input devices many requirements must be met.</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Accuracy—both the props and the tracking must be sufficiently accurate.</p>
                  </li>
                  <li>
                    <p>Response time—the system must have low latency.</p>
                  </li>
                  <li>
                    <p>Interaction—the types of possible interaction must be well-defined.</p>
                  </li>
                </ul>
                     <p>The props must be accurate representations of the object or data they represent. For some purposes, discrepancies between the printed prop and the original data must be determined. The prop must also be properly registered with its virtual representation. The method used to track the prop must also be sufficiently accurate for the application, and the latency of the system must be within reasonable limits. If the tangible prop is to be used in an augmented reality system, this presents some additional challenges. Finally, the methods of interaction with the prop need to be properly defined and be intuitive.</p><h3 class="c-article__sub-heading" id="Sec7">Accuracy of props</h3><p>Ideally, a printed object would correspond exactly to the original model. However, this is never the case, as the manufacturing processes have specific resolutions and tolerances. If the difference between the model and the prop becomes too large for the intended usage it must be compensated for.</p><p>Props can be printed with tracking markers or tracking sensor attachment points already in place, or they can be a pure representation of the model, with the tracking device being attached to a random location on the prop. In the latter case, it is necessary to determine where that location is and how the prop is oriented with respect to the tracking device, but even if the prop is created with a known attachment point it could be necessary to calibrate the prop.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Calibration and registration</h4><p>The printed prop will likely not be a 100% accurate representation of the model. These differences need to be determined through calibration. The prop can be skewed and it can have a slightly different aspect ratio or different dimensions. It is also possible that the prop is thicker or thinner than the original model; this manifests itself as an offset along the normal vector of the surface of the prop, and is the result of contraction or expansion of the material used to make the prop, or specific characteristics of the printing process or some kind of post-processing of the prop after printing (e.g. sanding, painting).</p><p>It is not only important to know the exact shape of the prop, but obviously also the positioning and orientation with respect to the tracking system. This allows for the proper positioning of the corresponding visualization and for properly registering the actual view of the prop with the rendered overlay in an augmented reality system.</p><h3 class="c-article__sub-heading" id="Sec9">Accuracy of tracking system</h3><p>Even a perfectly matching printed prop will be useless if the tracking system lacks sufficient accuracy. The required accuracy of the tracking system depends on the intended use, but also on the size of the printed prop. For augmented reality systems, the tracking should be at least accurate enough to register the prop with the augmentation within one pixel.</p><p>If the prop is used with a second input device, which is used to perform interaction on the prop, for example, to indicate a location, the accuracy must be sufficient to determine where exactly on the prop the interaction takes place. This in turn depends on the resolution of the original data, the size of the prop, and the size of the smallest relevant feature of interest. For example, in an educational molecular application, it could be sufficient if the system can determine which atom or link between atoms is indicated on the prop. However, a precise measurement application might need an accuracy of one millimeter. To reliably detect collisions between a prop and a stylus in a two-handed setup, an accuracy of less than 1 mm is necessary.</p><p>Tracking systems can suffer from distortion of the tracked volume. If this distortion is static, the distortion can be measured and a map can be created to compensate the error.</p><h3 class="c-article__sub-heading" id="Sec10">Latency</h3><p>Inevitably, there will be a delay between the immediate haptic feedback of the user manipulating a prop and the visual response of the system. If this latency is high, it causes the user to slow down. In augmented reality systems in which the user has a direct view of the interaction area (e.g. not video-based), there is a visual latency between the movement of the physical props and the virtual additions. In video-based augmented reality systems, on the other hand, there is latency between the immediate haptic feedback from the prop and the corresponding visual feedback from the augmented video, but there is no latency between the real and augmented parts of the scene.</p><p>Various experiments showed that detection of the presence and the amount of latency does not follow Weber’s law. This means that in a system with latency, the smallest amount of additional latency detectable by a user is not a fixed fraction of the base latency, but rather a constant amount. This indicates that people adapt to and compensate for the latency in a system. This compensation does cause the user to perform actions more slowly in order to remain accurate. Research indicates that 16 ms of visual latency can be noticed by users, and 50 ms of latency in the visual system leads to deterioration of performance. Latency in haptic feedback affects user performance already at 25 ms. However, for performing simple tasks for which only one type of feedback (visual or haptic) is sufficient, the brain is able to ignore the more delayed feedback and focus on the one which is faster.</p><p>Using tangible props for interaction reduces the effects of latency, since there is no haptic or visual delay from the ‘representations’ of the input devices when manipulating the props. The only latency is from the delay between an interaction event and the system response to the event. Keeping this in mind, user interaction should be designed in such a way that user dependency on computer feedback while performing a task is minimized.</p><h3 class="c-article__sub-heading" id="Sec11">Augmented reality</h3><p>Augmented reality systems combine the real world with computer visualizations. The real and virtual scene can be combined in a number of different ways, each of which has specific issues.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Video</h4><p>Video-based augmented reality systems use cameras to capture the real world and insert visualizations directly into the video stream. The user watches this video-feed and sees physical objects along with virtual additions. A head-mounted display (HMD) or a handheld device can be used to achieve co-location and mobility. There will inevitably be latency between a user action and the corresponding visual feedback due to video delay, but the real and virtual parts of the scene are fully synchronized.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Mirror/transparent display</h4><p>Here a semi-transparent mirror is situated in between the user’s head and his hands in the interaction area. A display, suspended somewhere above the mirror, is reflected in the mirror, on which the augmentation is rendered. The distance between the display and the mirror is chosen to match the distance between the mirror and the center of the interaction area behind it. This has the benefit that the focal distance of the user’s hands or other items in the interaction area and the focal distance of the virtual scene are matched. The use of head tracking is necessary for proper registration of the real and virtual scenes. Obviously, there is no delay from the real scene, but motion causes temporary misalignment between the real and virtual scenes.</p><p>There also exist see-through HMDs, which contain semi-transparent mirrors and displays, with special optics changing the apparent focal distance of the virtual scene.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Projection</h4><p>One or more projectors are used to project texture augmentations onto physical objects, or even onto whole rooms filled with objects. The objects onto which the augmentation is projected can be simple geometric shapes, but also highly complex and detailed models. The objects can also be movable and tracked, in which case the projection is modified to follow the object as it is moved around. Multiple projectors can be used to make the augmentation visible from all sides. By using stereo projection and head tracking, it is possible to add depth to the augmentation, for example making organs appear inside a dummy.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Occlusion in augmented reality</h4><p>A major problem in augmented reality is realistic occlusion. Occlusion is the single most powerful depth cue and it will override any other cue, such as stereo vision, focal depth or shadows. If the user’s hands or other items are in front of a virtual object then obviously part of the object should not be visible. Similarly if a virtual object is in front of a real object, then part of that real object should not be seen by the user. If these rules are not observed proper depth perception is very difficult. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig3">3</a> shows an augmented reality video feed where occlusion is not properly handled.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Improper occlusion effects in augmented reality using a video feed. The stylus is grasped in the hand, which occludes most of the stylus. However, the virtual representation of the stylus is not occluded at all and appears to be in front of the hand</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In video-based systems, occlusion can be performed using image segmentation of the video feed and knowledge of the shape and location of the props. Real objects can be occluded by virtual ones by simply rendering over the corresponding areas of the video feed, while occlusion of virtual object is handled by not rendering the occluded parts.</p><p>In mirror or transparent display systems performing occlusion is much more difficult, as there is no direct control over or view of what the user sees. It is possible to use a number of cameras to determine the location of the user’s hands and to avoid rendering occluded parts of the scene. One possibility is to use an LCD panel or other selective mirror technology to selectively make parts of the mirror or display completely opaque, thus occluding the real objects. More methods to handle occlusion can be found in (Mulder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Mulder JD (2006) Occlusion in mirror-based co-located augmented reality systems. Presence 15(1):93–107" href="/article/10.1007/s10055-009-0126-1#ref-CR11" id="ref-link-section-d18795e637">2006</a>).</p><p>A good way to attain proper occlusion of the hands is to use tangible props, which fully represent the extent of the virtual object. That way the hands are properly obscured by the prop itself. The occlusion of the augmentation of the prop by the user’s hands could then be ignored, since a person handling and examining an (augmented) item will usually instinctively avoid blocking his view with his hands as he would when examining any ordinary object.</p><h3 class="c-article__sub-heading" id="Sec16">Actions</h3><p>When using a single prop, the possible interaction will be limited to orientation and positioning of the prop and the corresponding visualizations. However, using a second device, such as a stylus, in conjunction with the prop enables the use of various interaction techniques, as locations on the prop can now be used for interaction.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Direct selection</h4><p>A pointing device, such as a stylus, can be used to interact with points on the surface. The basic tasks, which can then be performed, are selection and dragging.</p><p>A location on the surface is selected by touching the location with the stylus. The selected point can then be used for example for probing a value.</p><p>A selected point can also be dragged by moving the stylus across the surface. The dragging action can end when the stylus is lifted off the surface, or when the stylus is not moved for a short period. In the latter case, resuming the movement is seen as a new dragging action. The path of the stylus across the surface can be used as a whole, or only the start and end locations can be used, for example to indicate a direction.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Surface as proxy</h4><p>In addition to using the prop to interact directly with the surface, it is possible to use the surface as a proxy for an alternative representation of the data. The operations are then carried out on the alternative representation instead of on the surface of the virtual prop.</p><h5 class="c-article__sub-heading" id="Sec19">Higher-level features</h5><p>If features are identified in the data, which can be uniquely correlated with sections of the surface, a selection operation anywhere on the surface will select the correlated feature. For example, selecting any point on the wheel of a car selects the whole wheel.</p><h5 class="c-article__sub-heading" id="Sec20">Alternative representations</h5><p>If a mapping can be created which maps the surface of the virtual prop to an alternative representation, it becomes possible to perform operations on that alternative representation by interacting with the corresponding part of the surface. A simple example of this is a second iso-surface in the dataset created at a different isovalue and located below the surface used for the printed prop. Interaction with a point on the surface of the prop then results in interaction with the corresponding point on this second surface.</p><p>A point on a morphological skeleton, residing inside the prop, can be selected by mapping it to the closest points on the surface. Selecting a point anywhere on the surface then selects the corresponding point on the skeleton, and by dragging across the surface, the point is dragged to a different location on the skeleton, again corresponding to where the surface is touched by the stylus.</p><p>As an alternative, any point on the inside of the prop can be reached by first positioning a plane inside the prop and subsequently selecting a point on that plane by selecting a corresponding point on the surface of the prop.</p><h5 class="c-article__sub-heading" id="Sec21">Drawing</h5><p>Dragging can also be used for drawing and writing on objects. The dragged path can be recorded and can be visualized as lines on the prop. In this manner, it is possible to draw symbols or to write on the prop. If combined with handwriting recognition, this is a powerful tool.</p><p>In the same manner, a selection loop (lasso tool) can be created. For this, the first and the last recorded points are connected to form a closed loop. A possible use is to cut away or to make translucent the selected part of the surface to show some underlying structure.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Application: interactive measurements</h2><div class="c-article-section__content" id="Sec22-content"><p>We have created a system for measuring various shape metrics of branching marine corals, which uses tangible input props for interaction in virtual and augmented reality. These props are 3D prints of the surface of the coral, extracted from data obtained using Computed Tomography (CT) scanning of specimens collected from coral reefs, or geometry data resulting from numerical simulation of coral growth. A stylus is used for interaction with the props.</p><p>The volumetric coral data is processed by the application. A morphological skeleton is extracted from the data to identify the features, which are to be measured. Using the stylus, it is also possible to perform measurements interactively. Both the intermediate representation and the results of the measurements are visualized by the system.</p><p>The system can be used both in an augmented reality mode and in a virtual reality mode. The augmented mode can be used to interact and to visualize results in the context of the original object. The virtual reality mode is useful for examining magnified visualizations of these complex structures. The augmented reality mode was implemented using a video feed from a webcam.</p><p>There are many reasons to prefer printed replicas over the original coral specimens. First of all, coral is very fragile and must be handled very delicately, and could easily break already during an attempt to attach a tracker. In addition, many specimens are rather small, thus a larger than life size print would make interaction easier. In addition, having several copies enables remote collaboration with tangible props. Finally, in the case of data from numerical growth simulation, there is obviously no original specimen; here a printed coral can give the scientist a better feeling of how his simulated coral compares to real coral.</p><h3 class="c-article__sub-heading" id="Sec23">Printed coral</h3><p>To experiment with tangible props, we have adapted our existing interactive system for measuring CT scans of marine coral and visualizing the results (Kruszyński et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kruszyński KJ, Kaandorp JA, van Liere R (2007) A computational method for quantifying morphological variation in scleractinian corals. Coral Reefs 26(4):831–840" href="/article/10.1007/s10055-009-0126-1#ref-CR10" id="ref-link-section-d18795e733">2007</a>). In addition to CT data, the system can measure data resulting from numeric simulation of coral growth. We have printed three specimens of marine corals scaled at 100 and 150% of their original sizes. The diameter of the original specimens ranged from 10 to 15 cm, the resulting props thus had diameters of 10–22 cm. A Polhemus Fastrak tracker sensor was attached as close as possible to the center these props to minimize the effect of orientation uncertainty. Most of the props are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig4">4</a>, and another can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig5">5</a> with a sensor attached and used in conjunction with a tracked stylus.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Tangible props printed from coral data. The front left prop has a diameter of 10 cm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Tangible coral prop with attached sensor, held in the left hand. The right hand operates the stylus to indicate locations on the surface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The props have been printed using a Z Corporation Spectrum Z510 3D printer, which has a resolution of 600dpi in the X and Y directions and approximately 300dpi in the Z direction. The printer employs inkjet technology to inject a bonding agent into successive layers of fine powder, bonding the powder into a solid object. The powder particles are typically 0.05–0.1 mm in diameter. After the bonding agent is completely cured, the printed object is removed from the powder and impregnated with epoxy to increase structural strength. This results in an object with a fine grainy surface texture, which can be sanded or polished to create a smooth surface. The possibility exists that the object is slightly deformed during curing, and sanding will certainly reduce the thickness.</p><h3 class="c-article__sub-heading" id="Sec24">Calibration procedure</h3><p>The coral props were calibrated using a two-step procedure. In the first step, an initial orientation and position of the prop is found. In the second step, a more exact calibration is found using an iterative closest points approach.</p><p>A number of points (12) were defined on the surface of the model. For each point on the model, the corresponding point on the printed prop was then indicated with the tip of the stylus and the button was pressed. The position of the stylus tip relative to the prop sensor was then recorded, using the current position of the stylus and the orientation and position of the prop sensor. After all the points were recorded, the least squares optimal transformation was computed which would transform the pre-defined points to the recorded points by using only rotation and translation.</p><p>This initial transform was then refined using an iterative closest points (ICP) method. Random points on the surface of the prop were indicated with the stylus. These points were thus known to lie on the surface of the prop. These points were then used as source points in an iterative closest points search for an optimal transform from these points to the surface of the model (to which the initial transform was already applied). The model was used as the target data set for the ICP method.</p><h3 class="c-article__sub-heading" id="Sec25">Measuring and exploring</h3><p>The application is used to measure various shape parameters of the coral specimens. The props are used on one hand to indicate locations of interest, and on the other hand to render the measurements in the context of the original object.</p><p>The original application was used to measure the whole coral specimen, and the tangible prop can be used in the augmented reality setup to visualize and explore these results.</p><p>The tangible prop is also used in combination with the stylus to perform certain measurements in indicated locations.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Local branch thickness</h4><p>The stylus is used to select a point on the surface of the coral. The thickness of the branch corresponding to the surface location is then measured at the indicated point and visualized both as an inscribed sphere of corresponding thickness and as a numerical value.</p><p>When the button is used to change interaction state, the measured location changes with the movement of the stylus while the button is pressed, and after it is released the last location remains visible.</p><p>In buttonless mode, the location changes as long as the stylus touches the surface. The last measured location is shown after there is no more contact with the surface and while no new location is touched.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Branch distance measurement</h4><p>The distance between specific locations on the branches can be measured interactively. The stylus is used to indicate two points on the surface between which the distance is to be measured. This is visualized as a line annotated with the corresponding value as a number.</p><p>With the button, the first location is selected on the surface when the button is pressed. A line is then drawn from the selected location to the tip of the stylus. When the button is released, the second location is selected. The measured distance is shown until a new starting point is selected.</p><p>In buttonless mode, the first time the surface is touched a point is placed at that location. As long as the surface is touched, this point moves along with the stylus. When the surface is no longer touched, the last contact location is used as the starting point, and a line is drawn between this point and the stylus tip. The second point is selected in the same way, and the final location of the second point is again the last location where the stylus touched the surface.</p><p>The branch-measuring tool can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0126-1#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0126-1/MediaObjects/10055_2009_126_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Interactive coral measurement application. The user interacts with the small prop in his hand, while a greatly enlarged virtual representation is seen on the large 67-in. stereoscopic display</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0126-1/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec28">System accuracy</h3><p>The tracking system is a Polhemus Fastrak with a stylus and a standard sensor which is attached to the prop. The accuracy of this system is claimed to be 0.038 mm for position and 0.15 degrees for orientation. The sensors are sampled at a frequency of 60 Hz, but are not being sampled simultaneously. The latency between motion of a sensor and display update has been determined to be approximately 60 ms.</p><p>Calibration of one of the 150% scaled props yielded an average accuracy of approximately 0.7 mm, which is approximately one-third of the maximum attainable accuracy of the tracking system given the size of the prop, and about 1/13th of the diameter of the branches of the prop. The accuracy diminishes with increasing distance between the tip of the stylus and the sensor on the prop; this is mainly caused by the accuracy with which the system can track the orientation of the prop, as proper determination of the location of the surface depends on both position and orientation, and the effect of orientation uncertainty increases with distance from the sensor.</p><h3 class="c-article__sub-heading" id="Sec29">Usability</h3><p>We have subjectively compared the usability of different types of input for our measuring system. As a base for comparison, we use an interface with a tracked stylus for pointing and a second sensor for positioning and orienting the data. This is compared to two interfaces using a tangible printed prop. The first uses the button on the stylus for signaling interaction, the second uses collisions between the stylus and the tangible prop as the trigger. We will refer to these methods as S, T and TB, respectively.</p><p>Method S only provides visual feedback to the user. Attempting to pick a location in empty space guided only by visual feedback is inefficient, and it necessitates a tradeoff between speed and accuracy when using the method: the lack of haptic feedback means that for feedback the user has to rely completely on the visualization, with the associated latency. It is much more difficult to properly position the input devices to perform interaction in the desired location, often overshooting or undershooting the correct spot. Furthermore, pressing the button almost inevitably causes the stylus to move away from the intended interaction location, which sometimes severely affects accuracy. However, the lack of a prop, and thus dependency on any specific data set, makes the method very flexible. In addition, it is trivial to implement, and it requires no calibration.</p><p>Methods T and TB provide latency-free passive haptic and visual feedback through the prop, which makes it easy to quickly interact with the desired location. However, for each new coral, a new prop must be created, and switching data and props requires the calibration procedure to be repeated.</p><p>Method T is somewhat slower and less natural to use than method TB, as the passive haptic feedback resulting from touching the surface of the prop has to be responded to by the user by pressing the button in order to initiate interaction. This additional action increases the workload for the user, since the button press action has to be performed in addition to all the actions required in method TB. This is especially evident in tasks where a sequence of locations is selected: while method TB requires only a quick succession of movements, using method T the task is a repetition of a motion followed by a button press. In addition, the act of pressing the button causes some displacement of the input devices, potentially reducing the accuracy, but not nearly as much as with method S, since the physical contact between the tip of the stylus and the prop in most ensures the interaction location remains unchanged regardless of motion.</p><p>Finally, it must be said that pressing a button to perform an action (point-and-click) has become a common method of interaction for most people due to the prevalence of computer mice. This somewhat reduces the advantage of method TB over method T.</p><p>The findings are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-009-0126-1#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Findings of the subjective relative usability of three input methods: standard two-handed non-tangible input (S), input with a tangible prop using a button as an interaction trigger (T), and input with a tangible prop using contact between the stylus and the surface as the trigger (TB)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-009-0126-1/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec30"><div class="c-article-section" id="Sec30-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec30">Conclusion</h2><div class="c-article-section__content" id="Sec30-content"><p>In this paper, we have explored the use of tangible props as input devices to provide a graspable physical representation of data. This allows the use of the sense of touch when interacting with scientific visualizations in virtual and augmented reality systems. We have explained the concept and applicability of tangible props, considered the technical requirements of systems, which use such props, and presented an example application.</p><p>The interface which is thus created is much more natural to use than the ubiquitous standard pointing devices used in most systems. Instead of using input devices to merely remotely interact with virtual objects, with all the inherent latency issues, the input devices become the actual objects with which the user interacts, and which she can manipulate in the same natural manner as she would any other physical object. Thus, in addition to performance benefits, the application of tangible props increases the sense of immersion in virtual and augmented reality environments.</p><h3 class="c-article__sub-heading" id="Sec31">Future work</h3><p>An extension of the work would involve the use of modular props. With complex props consisting of one piece some of the inner parts are hard to reach with the stylus. This becomes much easier if parts of the prop can be removed. However, this might affect the accuracy, unless each part is tracked and calibrated separately.</p><p>Another direction for future development of this work is on the application side, for example using the tangible coral props to interact with real-time simulations, such a simulation of nutrient absorption by the coral.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: SI3" /><p class="c-article-references__text" id="ref-CR1">Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: SI3D ‘92 proceedings of the 1992 symposium on interactive 3D graphics, pp 183–188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Couture N, Rivière G, Reuter P (2008) Geotui: a tangible user interface for geoscience. In: TEI ‘08 proceeding" /><p class="c-article-references__text" id="ref-CR2">Couture N, Rivière G, Reuter P (2008) Geotui: a tangible user interface for geoscience. In: TEI ‘08 proceedings of the 2nd international conference on tangible and embedded interaction, pp 89–96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fitzmaurice GW, Ishii H, Buxton WAS (1995) Bricks: laying the foundations for graspable user interfaces. In: C" /><p class="c-article-references__text" id="ref-CR3">Fitzmaurice GW, Ishii H, Buxton WAS (1995) Bricks: laying the foundations for graspable user interfaces. In: CHI ‘95 proceedings of the SIGCHI conference on human factors in computing systems, pp 442–449</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gillet A, Sanner M, Stoffler D, Goodsell D, Olson A (2004) Augmented reality with tangible auto-fabricated mod" /><p class="c-article-references__text" id="ref-CR4">Gillet A, Sanner M, Stoffler D, Goodsell D, Olson A (2004) Augmented reality with tangible auto-fabricated models for molecular biology applications. In: VIS ‘04 proceedings of the conference on visualization 2004, pp 235–242</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Pausch R, Goble JC, Kassell NF (1994) Passive real-world interface props for neurosurgical visuali" /><p class="c-article-references__text" id="ref-CR5">Hinckley K, Pausch R, Goble JC, Kassell NF (1994) Passive real-world interface props for neurosurgical visualization. In: CHI ‘94 proceedings of the SIGCHI conference on human factors in computing systems, pp 452–458</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H (2008) Tangible bits: beyond pixels. In: TEI ‘08 proceedings of the 2nd international conference on ta" /><p class="c-article-references__text" id="ref-CR6">Ishii H (2008) Tangible bits: beyond pixels. In: TEI ‘08 proceedings of the 2nd international conference on tangible and embedded interaction</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ‘9" /><p class="c-article-references__text" id="ref-CR7">Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ‘97 proceedings of the SIGCHI conference on Human factors in computing systems, pp 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kok AJ, van Liere R (2004) Co-location and tactile feedback for 2D widget manipulation. In: Proceedings IEEE c" /><p class="c-article-references__text" id="ref-CR8">Kok AJ, van Liere R (2004) Co-location and tactile feedback for 2D widget manipulation. In: Proceedings IEEE conference on virtual reality 2004, pp 233–234</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kruszyński KJ, van Liere R (2008) Tangible interaction for 3D widget manipulation in virtual environments. In:" /><p class="c-article-references__text" id="ref-CR9">Kruszyński KJ, van Liere R (2008) Tangible interaction for 3D widget manipulation in virtual environments. In: EGVE ‘08 proceedings eurographics symposium on virtual environments 2008, pp 89–95</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KJ. Kruszyński, JA. Kaandorp, R. Liere, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Kruszyński KJ, Kaandorp JA, van Liere R (2007) A computational method for quantifying morphological variation " /><p class="c-article-references__text" id="ref-CR10">Kruszyński KJ, Kaandorp JA, van Liere R (2007) A computational method for quantifying morphological variation in scleractinian corals. Coral Reefs 26(4):831–840</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00338-007-0270-6" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20computational%20method%20for%20quantifying%20morphological%20variation%20in%20scleractinian%20corals&amp;journal=Coral%20Reefs&amp;volume=26&amp;issue=4&amp;pages=831-840&amp;publication_year=2007&amp;author=Kruszy%C5%84ski%2CKJ&amp;author=Kaandorp%2CJA&amp;author=Liere%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JD. Mulder, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Mulder JD (2006) Occlusion in mirror-based co-located augmented reality systems. Presence 15(1):93–107" /><p class="c-article-references__text" id="ref-CR11">Mulder JD (2006) Occlusion in mirror-based co-located augmented reality systems. Presence 15(1):93–107</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.2006.15.1.93" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2230820" aria-label="View reference 11 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Occlusion%20in%20mirror-based%20co-located%20augmented%20reality%20systems&amp;journal=Presence&amp;volume=15&amp;issue=1&amp;pages=93-107&amp;publication_year=2006&amp;author=Mulder%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ortega M, Coquillart S (2005) Prop-based haptic interaction with co-location and immersion: an automotive appl" /><p class="c-article-references__text" id="ref-CR12">Ortega M, Coquillart S (2005) Prop-based haptic interaction with co-location and immersion: an automotive application. In: HAVE 2005 Proceedings of the IEEE international workshop on haptic audio visual environments and their applications, 2005</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-009-0126-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was carried out in the context of the Virtual Laboratory for e-Science project (<a href="http://www.vl-e.nl">http://www.vl-e.nl</a>). This project is supported by a BSIK grant from the Dutch Ministry of Education, Culture and Science (OC&amp;W) and is part of the ICT innovation program of the Ministry of Economic Affairs (EZ).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Centrum Wiskunde &amp; Informatica, Science Park 123, 1098 XG, Amsterdam, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Krzysztof Jakub Kruszyński &amp; Robert van Liere</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Technische Universiteit Eindhoven, Eindhoven, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Robert van Liere</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Krzysztof_Jakub-Kruszy_ski"><span class="c-article-authors-search__title u-h3 js-search-name">Krzysztof Jakub Kruszyński</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Krzysztof Jakub+Kruszy%C5%84ski&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Krzysztof Jakub+Kruszy%C5%84ski" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Krzysztof Jakub+Kruszy%C5%84ski%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Robert-Liere"><span class="c-article-authors-search__title u-h3 js-search-name">Robert van Liere</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Robert+van+Liere&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robert+van+Liere" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robert+van+Liere%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-009-0126-1/email/correspondent/c1/new">Krzysztof Jakub Kruszyński</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Tangible%20props%20for%20scientific%20visualization%3A%20concept%2C%20requirements%2C%20application&amp;author=Krzysztof%20Jakub%20Kruszy%C5%84ski%20et%20al&amp;contentID=10.1007%2Fs10055-009-0126-1&amp;publication=1359-4338&amp;publicationDate=2009-07-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kruszyński, K.J., van Liere, R. Tangible props for scientific visualization: concept, requirements, application.
                    <i>Virtual Reality</i> <b>13, </b>235 (2009). https://doi.org/10.1007/s10055-009-0126-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-009-0126-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-02-28">28 February 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-06-20">20 June 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-07-07">07 July 2009</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-009-0126-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-009-0126-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Tangible user interfaces</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Rapid prototyping</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0126-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=126;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

