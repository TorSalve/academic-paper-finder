<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Assessing hands-free interactions for VR using eye gaze and electromyo"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Assessing hands-free interactions for VR using eye gaze and electromyography"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-11-20"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel techniques, such as eye tracking, are now commercially available. In our work, we investigate the use of physiological signals as input to enhance VR experiences. We present a system using gaze tracking and electromyography on a user&#8217;s forearm to make selection tasks in virtual spaces more efficient. In a study with 16 participants, we compared five different input techniques using a Fitts&#8217; law task: Using gaze tracking for cursor movement in combination with forearm contractions for making selections was superior to using an HTC Vive controller, Xbox gamepad, dwelling time, and eye-gaze dwelling time. To explore application scenarios and collect qualitative feedback, we further developed and evaluated a game with our input technique. Our findings inform the design of applications that use eye-gaze tracking and forearm muscle movements for effective user input in VR."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-11-20"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="119"/>

    <meta name="prism.endingPage" content="131"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0371-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0371-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0371-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0371-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Assessing hands-free interactions for VR using eye gaze and electromyography"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2019/06"/>

    <meta name="citation_online_date" content="2018/11/20"/>

    <meta name="citation_firstpage" content="119"/>

    <meta name="citation_lastpage" content="131"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0371-2"/>

    <meta name="DOI" content="10.1007/s10055-018-0371-2"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0371-2"/>

    <meta name="description" content="With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical control"/>

    <meta name="dc.creator" content="Yun Suen Pai"/>

    <meta name="dc.creator" content="Tilman Dingler"/>

    <meta name="dc.creator" content="Kai Kunze"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Muscle Nerve; citation_title=Acoustic and surface EMG diagnosis of pediatric muscle disease; citation_author=DT Barry, KE Gordon, GG Hinton; citation_volume=13; citation_issue=4; citation_publication_date=1990; citation_pages=286-290; citation_doi=10.1002/mus.880130403; citation_id=CR1"/>

    <meta name="citation_reference" content="Benko H, Saponas TS, Morris D, Tan D (2009) Enhancing input on and above the interactive surface with muscle sensing. In: Proceedings of the ACM international conference on interactive tabletops and surfaces&#8212;ITS &#8217;09, p 93. 
                    https://doi.org/10.1145/1731903.1731924
                    
                  . 
                    http://portal.acm.org/citation.cfm?doid=1731903.1731924
                    
                  
                        "/>

    <meta name="citation_reference" content="Cardoso J (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology. ACM, pp 319&#8211;320"/>

    <meta name="citation_reference" content="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131&#8211;138. 
                    https://doi.org/10.1145/2818346.2820752
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=J Rehabil Res Dev; citation_title=Integrated electromyogram and eye-gaze tracking cursor control system for computer users with motor disabilities; citation_author=CA Chin, A Barreto, JG Cremades, M Adjouadi; citation_volume=45; citation_issue=1; citation_publication_date=2008; citation_pages=161; citation_doi=10.1682/JRRD.2007.03.0050; citation_id=CR5"/>

    <meta name="citation_reference" content="Cloudhead G (2015) VR Navigation. 
                    http://cloudheadgames.com/cloudhead/vr-navigation/
                    
                  
                        "/>

    <meta name="citation_reference" content="Costanza E, Inverso SA, Allen R (2005) Toward subtle intimate interfaces for mobile devices using an EMG controller. In: Proceedings of the SIGCHI conference on Human factors in computing systems CHI 05:481. 
                    https://doi.org/10.1145/1054972.1055039
                    
                  . 
                    http://eprints.soton.ac.uk/270956/
                    
                  
                        "/>

    <meta name="citation_reference" content="Darken RP, Cockayne WR, Carmein D (1997) The omni-directional treadmill: a locomotion device for virtual worlds. In: Proceedings of the 10th annual ACM symposium on User interface software and technology&#8212;UIST &#8217;97, pp 213&#8211;221. 
                    https://doi.org/10.1145/263407.263550
                    
                  . 
                    http://dl.acm.org/citation.cfm?id=263550
                    
                  
                        "/>

    <meta name="citation_reference" content="Henze N, Rukzio E, Boll S (2011) 100,000,000 taps: analysis and improvement of touch performance in the large. In: Proceedings of the 13th international conference on human computer interaction with mobile devices and services. ACM, pp 133&#8211;142"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Bionics Biomech; citation_title=Development of a multi-DOF electromyography prosthetic system using the adaptive joint mechanism; citation_author=A Hernandez Arieta, R Katoh, H Yokoi, Y Wenwei; citation_volume=3; citation_issue=2; citation_publication_date=2006; citation_pages=101-111; citation_doi=10.1155/2006/741851; citation_id=CR10"/>

    <meta name="citation_reference" content="Hernandez-Rebollar JL, Kyriakopoulos N, Lindeman RW (2002) The AcceleGlove: a whole-hand input device for virtual reality. In: ACM SIGGRAPH 2002 conference abstracts and applications. ACM, p 259"/>

    <meta name="citation_reference" content="ISO (1998) Ergonomic requirements for office work with visual display terminals (VDTs)&#8212;part 11: guidance on usability. ISO 9241-11:1998, p 22"/>

    <meta name="citation_reference" content="Jacob RJK (1990) What you look at is what you get: eye movement-based interaction techniques. In: Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, pp 11&#8211;18"/>

    <meta name="citation_reference" content="Jacobsen SC, Jerard RB (1974) Computational requirements for control of the Utah arm. In: Proceedings of the 1974 annual conference, vol 1. ACM, pp 149&#8211;155"/>

    <meta name="citation_reference" content="Jones BR, Benko H, Ofek E, Wilson AD (2013) IllumiRoom: peripheral projected illusions for interactive experiences. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 869&#8211;878"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Fuzzy Syst; citation_title=Neuro-fuzzy control of a robotic exoskeleton with EMG signals; citation_author=K Kiguchi, T Tanaka, T Fukuda; citation_volume=12; citation_issue=4; citation_publication_date=2004; citation_pages=481-490; citation_doi=10.1109/TFUZZ.2004.832525; citation_id=CR16"/>

    <meta name="citation_reference" content="Kim YR, Kim GJ (2017) HoVR-type: smartphone as a typing interface in VR using hovering. In: 2017 IEEE international conference on consumer electronics (ICCE). IEEE, pp 200&#8211;203"/>

    <meta name="citation_reference" content="Lee JS, Lee SE, Jang SY, Park KS (2005) A simplified hand gesture interface for spherical manipulation in virtual environments. In: Proceedings of the 2005 international conference on Augmented tele-existence. ACM, p 284"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comput Interact; citation_title=Text entry for mobile computing: models and methods, theory and practice; citation_author=IS MacKenzie, RW Soukoreff; citation_volume=17; citation_issue=2&#8211;3; citation_publication_date=2002; citation_pages=147-198; citation_doi=10.1207/S15327051HCI172&amp;3_2; citation_id=CR19"/>

    <meta name="citation_reference" content="Miniotas D (2000) Application of Fitts&#8217; law to eye gaze interaction. In: CHI &#8217;00 extended abstracts on Human factors in computer systems&#8212;CHI &#8217;00, pp 339&#8211;340. 
                    https://doi.org/10.1145/633482.633496
                    
                  . 
                    http://portal.acm.org/citation.cfm?doid=633292.633496
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Am J Sports Med; citation_title=EMG analysis of the scapular muscles during a shoulder rehabilitation program; citation_author=JRJB Moseley, FW Jobe, M Pink, J Perry, J Tibone; citation_volume=20; citation_issue=2; citation_publication_date=1992; citation_pages=128-134; citation_doi=10.1177/036354659202000206; citation_id=CR21"/>

    <meta name="citation_reference" content="Myo (2013) Myo Gesture Control Armband. 
                    http://www.myo.com/techspecs
                    
                  
                        "/>

    <meta name="citation_reference" content="Oculus (2015) The Rift&#8217;s Recommended Spec, PC SDK 0.6 Released, and Mobile VR Jam Voting. 
                    https://www3.oculus.com/en-us/blog/the-rifts-recommended-spec-pc-sdk-0-6-released-and-mobile-vr-jam-voting/
                    
                  
                        "/>

    <meta name="citation_reference" content="Pai YS, Tag B, Outram B, Vontin N, Sugiura K, Kunze K (2016) GazeSim: simulating foveated rendering using depth in eye gaze for VR. In: ACM SIGGRAPH 2016 Posters. ACM, p 75"/>

    <meta name="citation_reference" content="Ramcharitar A, Teather RJ (2017) A Fitts&#8217; law evaluation of video game controllers: thumbstick, touchpad and gyrosensor. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, NY, CHI EA &#8217;17, pp 2860&#8211;2866. 
                    https://doi.org/10.1145/3027063.3053213
                    
                  . 
                    http://doi.acm.org/10.1145/3027063.3053213
                    
                  
                        "/>

    <meta name="citation_reference" content="Saponas TS, Tan DS, Morris D, Balakrishnan R, Turner J, Landay JA (2009) Enabling always-available input with muscle&#8211;computer interfaces. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology, pp 167&#8211;176. 
                    https://doi.org/10.1145/1622176.1622208
                    
                  
                        "/>

    <meta name="citation_reference" content="Saraiji Y (2016) PupilHMDCalibration. 
                    https://github.com/mrayy/PupilHMDCalibration
                    
                  
                        "/>

    <meta name="citation_reference" content="Saraiji Y, Sugimoto S, Fernando CL, Minamizawa K, Tachi S (2016) Layered telepresence: simultaneous multi presence experience using eye gaze based perceptual awareness blending. In: ACM SIGGRAPH 2016 posters. ACM, p 20"/>

    <meta name="citation_reference" content="Slambekova D, Bailey R, Geigel J (2012) Gaze and gesture based object manipulation in virtual worlds. In: Proceedings of the 18th ACM symposium on virtual reality software and technology. ACM, pp 203&#8211;204"/>

    <meta name="citation_reference" content="Unity (2015) User interfaces for VR. 
                    https://unity3d.com/learn/tutorials/topics/virtual-reality/user-interfaces-vr
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGCHI Bull; citation_title=An evaluation of an eye tracker as a device for computer input; citation_author=C Ware, HH Mikaelian; citation_volume=17; citation_issue=SI; citation_publication_date=1986; citation_pages=183-188; citation_doi=10.1145/30851.275627; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=Evaluation of walking in place on a Wii balance board to explore a virtual environment; citation_author=B Williams, S Bailey, G Narasimham, M Li, B Bodenheimer; citation_volume=8; citation_issue=3; citation_publication_date=2011; citation_pages=1-14; citation_doi=10.1145/2010325.2010329; citation_id=CR32"/>

    <meta name="citation_reference" content="Wilson AD, Cutrell E (2005) Flowmouse: a computer vision-based pointing and gesture input device. In: Human&#8211;computer interaction-INTERACT 2005, vol 3585, pp 565&#8211;578. 
                    https://doi.org/10.1007/11555261
                    
                  . 
                    http://www.springerlink.com/index/DJ6AENMGBTYBC0EE.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=Combining eye gaze input with a brain? Computer interface for touchless human? Computer interaction; citation_author=TO Zander, M Gaertner, C Kothe, R Vilimek; citation_volume=27; citation_issue=1; citation_publication_date=2010; citation_pages=38-51; citation_doi=10.1080/10447318.2011.535752; citation_id=CR34"/>

    <meta name="citation_reference" content="Zhang X, MacKenzie IS (2007) Evaluating eye tracking with ISO 9241&#8212;part 9. In: Proceedings of the 12th international conference on human-computer interaction: intelligent multimodal interaction environments, HCI&#8217;07. Springer, Berlin, pp 779&#8211;788. 
                    http://dl.acm.org/citation.cfm?id=1769590.1769678
                    
                  
                        "/>

    <meta name="citation_author" content="Yun Suen Pai"/>

    <meta name="citation_author_email" content="yspai1412@gmail.com"/>

    <meta name="citation_author_institution" content="Keio University Graduate School of Media Design, Yokohama, Japan"/>

    <meta name="citation_author" content="Tilman Dingler"/>

    <meta name="citation_author_institution" content="University of Melbourne, Melbourne, Australia"/>

    <meta name="citation_author" content="Kai Kunze"/>

    <meta name="citation_author_institution" content="Keio University Graduate School of Media Design, Yokohama, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0371-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0371-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Assessing hands-free interactions for VR using eye gaze and electromyography"/>
        <meta property="og:description" content="With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel techniques, such as eye tracking, are now commercially available. In our work, we investigate the use of physiological signals as input to enhance VR experiences. We present a system using gaze tracking and electromyography on a user’s forearm to make selection tasks in virtual spaces more efficient. In a study with 16 participants, we compared five different input techniques using a Fitts’ law task: Using gaze tracking for cursor movement in combination with forearm contractions for making selections was superior to using an HTC Vive controller, Xbox gamepad, dwelling time, and eye-gaze dwelling time. To explore application scenarios and collect qualitative feedback, we further developed and evaluated a game with our input technique. Our findings inform the design of applications that use eye-gaze tracking and forearm muscle movements for effective user input in VR."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Assessing hands-free interactions for VR using eye gaze and electromyography | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0371-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Physiological sensing, Eye gaze, Electromyography","kwrd":["Virtual_reality","Physiological_sensing","Eye_gaze","Electromyography"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0371-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0371-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=371;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0371-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Assessing hands-free interactions for VR using eye gaze and electromyography
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0371-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0371-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-11-20" itemprop="datePublished">20 November 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Assessing hands-free interactions for VR using eye gaze and electromyography</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yun_Suen-Pai" data-author-popup="auth-Yun_Suen-Pai" data-corresp-id="c1">Yun Suen Pai<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-6090-2837"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-6090-2837</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University Graduate School of Media Design" /><meta itemprop="address" content="0000 0004 1936 9959, grid.26091.3c, Keio University Graduate School of Media Design, Yokohama, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tilman-Dingler" data-author-popup="auth-Tilman-Dingler">Tilman Dingler</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Melbourne" /><meta itemprop="address" content="0000 0001 2179 088X, grid.1008.9, University of Melbourne, Melbourne, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kai-Kunze" data-author-popup="auth-Kai-Kunze">Kai Kunze</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University Graduate School of Media Design" /><meta itemprop="address" content="0000 0004 1936 9959, grid.26091.3c, Keio University Graduate School of Media Design, Yokohama, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">119</span>–<span itemprop="pageEnd">131</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">750 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0371-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel techniques, such as eye tracking, are now commercially available. In our work, we investigate the use of physiological signals as input to enhance VR experiences. We present a system using gaze tracking and electromyography on a user’s forearm to make selection tasks in virtual spaces more efficient. In a study with 16 participants, we compared five different input techniques using a Fitts’ law task: Using gaze tracking for cursor movement in combination with forearm contractions for making selections was superior to using an HTC Vive controller, Xbox gamepad, dwelling time, and eye-gaze dwelling time. To explore application scenarios and collect qualitative feedback, we further developed and evaluated a game with our input technique. Our findings inform the design of applications that use eye-gaze tracking and forearm muscle movements for effective user input in VR.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>VR as a platform is increasingly immersive as they began with tracking the user’s head direction and moved toward tracking their hands’ position, to allow for various forms of gesture and button-based inputs. This method has become the conventional input system, yet also the current limitation for VR; positionally tracked physical controllers often come with a directional pad for analog input and various buttons for each finger for digital input. Though reliable, we wish to delve deeper into the next step of input modalities for VR. We propose the use of physiological signals as an additional input method to enhance the overall VR experience. We chose to leverage the use of eye-gaze tracking and muscle activity sensing as a form of point and select for several reasons: Firstly, we wish to investigate hands-free methods of input to complement conventional hand controllers that currently exist today. Secondly, both of these signals allow reliable and explicit input compared to other forms of signals like electroencephalogram (EEG) and electrocardiogram (ECG) which measure brain and heart activity, respectively.</p><p>To validate our proposed method, we conducted a Fitts’ law experiment to evaluate its accuracy and speed. We compared it to five other conventional input methods; mouse, gaze direction with dwell time, eye-gaze direction with dwell time, gamepad input and using motion-tracked controllers. Motion-tracked controller input is treated as the gold standard baseline since it is the most wide-spread input method for VR now, whereas dwell time uses a timer of 1 s for activation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig1">1</a> illustrates a user using our proposed method as input for this study.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig1_HTML.jpg" alt="figure1" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Eye tracking with EMG technique</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We then conducted a second study, which applied our method as an input mechanic for participants to evaluate their gaming experience. The game we created allows pointing a gun using the eyes (selection) and firing it using arm muscle contraction (activation) to shoot several targets in a given time. Based on the participants’ qualitative feedback, we found that eye-gaze-based selection was deemed to be overall fast and intuitive for targeting purposes, whereas arm muscle contraction simulates the sensation of a gun recoil, thus creating an additional sense of realism. We, therefore, showed our interaction method’s potential to enhance the overall VR experience. Finally, we discuss several applications for our system in more generic tasks like pick-and-place, teleportation, user interface (UI) interaction, and gaming.</p><p>The contributions of this work are as follows:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>We investigated additional input modalities using a combination of eye gaze and EMG activation.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>We built a system to compare its effectiveness and accuracy.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>We validated our approach in a user study by comparing it to conventional input methods and implemented a game using our method based on physiological sensing.</p>
                    
                  </li>
                </ol></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Our approach is based on related research in VR interaction techniques, physiological sensing, and select-and-point task assessments. For VR interaction mechanics, we look into non-conventional input and sensing mechanics for VR like gesture-based detection and peripheral-based input devices. Physiological signals, however, have been gaining popularity as several related works in VR show. We, therefore, focus on point-and-select tasks in VR and how they are evaluated in terms of accuracy and performance.</p><h3 class="c-article__sub-heading" id="Sec3">Interaction techniques</h3><p>Several works of research in VR introduced novel methods for manipulation. For example, a spherical manipulation device was made using a tilt sensor, electric compass, and flex sensors that allows gesture recognition. This implementation combines a peripheral with gesture recognition, though the study was conducted only for spherical interfaces (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lee JS, Lee SE, Jang SY, Park KS (2005) A simplified hand gesture interface for spherical manipulation in virtual environments. In: Proceedings of the 2005 international conference on Augmented tele-existence. ACM, p 284" href="/article/10.1007/s10055-018-0371-2#ref-CR18" id="ref-link-section-d69949e444">2005</a>). This work is similar to the AcceleGlove, which presents a whole hand input device using accelerometers (Hernandez-Rebollar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Hernandez-Rebollar JL, Kyriakopoulos N, Lindeman RW (2002) The AcceleGlove: a whole-hand input device for virtual reality. In: ACM SIGGRAPH 2002 conference abstracts and applications. ACM, p 259" href="/article/10.1007/s10055-018-0371-2#ref-CR11" id="ref-link-section-d69949e447">2002</a>). The device was developed to recognize sign languages and can also be used as a substitute for a computer mouse for selection and activation. Another work combined gaze with gesture using a Microsoft Kinect with Mirametrix tracker for desktop use (Slambekova et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Slambekova D, Bailey R, Geigel J (2012) Gaze and gesture based object manipulation in virtual worlds. In: Proceedings of the 18th ACM symposium on virtual reality software and technology. ACM, pp 203–204" href="/article/10.1007/s10055-018-0371-2#ref-CR29" id="ref-link-section-d69949e450">2012</a>). Though no user study was conducted, it served as a proof of concept for a multimodal-based input that may increase the effectiveness of interactions in virtual space.</p><p>Other peripherals that were developed catered more toward a specific use of input, such as locomotion. For example, the omnidirectional treadmill was developed for training the army in virtual environments (Darken et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Darken RP, Cockayne WR, Carmein D (1997) The omni-directional treadmill: a locomotion device for virtual worlds. In: Proceedings of the 10th annual ACM symposium on User interface software and technology—UIST ’97, pp 213–221. &#xA;                    https://doi.org/10.1145/263407.263550&#xA;                    &#xA;                  . &#xA;                    http://dl.acm.org/citation.cfm?id=263550&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR8" id="ref-link-section-d69949e456">1997</a>). In this regard, it was extremely effective, and the cost and overall size of the device, however, prevented it from entering the mainstream market. Another work used a Wii balance board as a low-cost input device (Williams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Williams B, Bailey S, Narasimham G, Li M, Bodenheimer B (2011) Evaluation of walking in place on a Wii balance board to explore a virtual environment. ACM Trans Appl Percept 8(3):1–14. &#xA;                    https://doi.org/10.1145/2010325.2010329&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR32" id="ref-link-section-d69949e459">2011</a>). Besides locomotion, it could also be used for 3D object manipulation and application-specific task due to its discrete and continuous signals.</p><h3 class="c-article__sub-heading" id="Sec4">Physiological sensing in VR</h3><p>The usage of EMG may not be mainstream yet, but its feasibility as a daily interaction device is still highly promising. EMG is commonly applied in various medical applications such as muscular rehabilitation, muscular disease, and prosthetics control (Barry et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Barry DT, Gordon KE, Hinton GG (1990) Acoustic and surface EMG diagnosis of pediatric muscle disease. Muscle Nerve 13(4):286–290" href="/article/10.1007/s10055-018-0371-2#ref-CR1" id="ref-link-section-d69949e470">1990</a>; Hernandez Arieta et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hernandez Arieta A, Katoh R, Yokoi H, Wenwei Y (2006) Development of a multi-DOF electromyography prosthetic system using the adaptive joint mechanism. Appl Bionics Biomech 3(2):101–111" href="/article/10.1007/s10055-018-0371-2#ref-CR10" id="ref-link-section-d69949e473">2006</a>; Jacobsen and Jerard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Jacobsen SC, Jerard RB (1974) Computational requirements for control of the Utah arm. In: Proceedings of the 1974 annual conference, vol 1. ACM, pp 149–155" href="/article/10.1007/s10055-018-0371-2#ref-CR14" id="ref-link-section-d69949e476">1974</a>; Kiguchi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kiguchi K, Tanaka T, Fukuda T (2004) Neuro-fuzzy control of a robotic exoskeleton with EMG signals. IEEE Trans Fuzzy Syst 12(4):481–490" href="/article/10.1007/s10055-018-0371-2#ref-CR16" id="ref-link-section-d69949e479">2004</a>; Moseley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Moseley JRJB, Jobe FW, Pink M, Perry J, Tibone J (1992) EMG analysis of the scapular muscles during a shoulder rehabilitation program. Am J Sports Med 20(2):128–134" href="/article/10.1007/s10055-018-0371-2#ref-CR21" id="ref-link-section-d69949e482">1992</a>). During a feasibility analysis, it was found that an accuracy of up to 95% was achievable, implying that EMG interaction can be highly promising, especially given its discrete nature (Moseley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Moseley JRJB, Jobe FW, Pink M, Perry J, Tibone J (1992) EMG analysis of the scapular muscles during a shoulder rehabilitation program. Am J Sports Med 20(2):128–134" href="/article/10.1007/s10055-018-0371-2#ref-CR21" id="ref-link-section-d69949e486">1992</a>). EMG has the potential for gesture-based recognition such as fingers, hand, and arm motions. In terms of accuracy, however, EMG performs best simply with direct muscle contraction. Activities recognition like carrying a heavy bag or mug has a lower error percentage compared to recognition of pinching gestures (Saponas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Saponas TS, Tan DS, Morris D, Balakrishnan R, Turner J, Landay JA (2009) Enabling always-available input with muscle–computer interfaces. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology, pp 167–176. &#xA;                    https://doi.org/10.1145/1622176.1622208&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR26" id="ref-link-section-d69949e489">2009</a>). Furthermore, precise calibration is required for accurate gesture recognition due to the data being extremely user dependent.</p><p>Previous work used gesture-less EMG for various inputs by determining the length of muscle contraction time; however, relying solely on EMG greatly limits any form of selection (Costanza <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Costanza E, Inverso SA, Allen R (2005) Toward subtle intimate interfaces for mobile devices using an EMG controller. In: Proceedings of the SIGCHI conference on Human factors in computing systems CHI 05:481. &#xA;                    https://doi.org/10.1145/1054972.1055039&#xA;                    &#xA;                  . &#xA;                    http://eprints.soton.ac.uk/270956/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR7" id="ref-link-section-d69949e495">2005</a>). Using the length of contraction also suffers from the same issue with dwell time. One of the solutions for this is to pair EMG with other forms of input, thus creating a multimodal system. Multimodal inputs are no stranger to human–computer interface (Zander et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Zander TO, Gaertner M, Kothe C, Vilimek R (2010) Combining eye gaze input with a brain? Computer interface for touchless human? Computer interaction. Int J Hum Comput Interact 27(1):38–51. &#xA;                    https://doi.org/10.1080/10447318.2011.535752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR34" id="ref-link-section-d69949e498">2010</a>) and can certainly improve an interaction mechanic if done correctly. For example, coupling EMG with a touch screen allows the system to recognize which finger is touching the surface, as well as the amount of pressure exerted through muscle sensing without the use of a pressure sensor (Benko et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Benko H, Saponas TS, Morris D, Tan D (2009) Enhancing input on and above the interactive surface with muscle sensing. In: Proceedings of the ACM international conference on interactive tabletops and surfaces—ITS ’09, p 93. &#xA;                    https://doi.org/10.1145/1731903.1731924&#xA;                    &#xA;                  . &#xA;                    http://portal.acm.org/citation.cfm?doid=1731903.1731924&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR2" id="ref-link-section-d69949e501">2009</a>).</p><p>Besides EMG, other forms of multimodal input like combining eye gaze with hand gestures have also been explored (Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e507">2015</a>). Regarding eye tracking, it is a sensing mechanic that is very likely to be adopted into future VR headsets. In research, eye tracking has been used for foveated rendering (Pai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Pai YS, Tag B, Outram B, Vontin N, Sugiura K, Kunze K (2016) GazeSim: simulating foveated rendering using depth in eye gaze for VR. In: ACM SIGGRAPH 2016 Posters. ACM, p 75" href="/article/10.1007/s10055-018-0371-2#ref-CR24" id="ref-link-section-d69949e510">2016</a>), multi-user scenarios (Saraiji et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Saraiji Y, Sugimoto S, Fernando CL, Minamizawa K, Tachi S (2016) Layered telepresence: simultaneous multi presence experience using eye gaze based perceptual awareness blending. In: ACM SIGGRAPH 2016 posters. ACM, p 20" href="/article/10.1007/s10055-018-0371-2#ref-CR28" id="ref-link-section-d69949e513">2016</a>), or other forms of input modality. Coupling eye-gaze tracking with EMG is a novel proposal and useful input, seeing as how it has previously been used for motor disabilities (Chin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chin CA, Barreto A, Cremades JG, Adjouadi M (2008) Integrated electromyogram and eye-gaze tracking cursor control system for computer users with motor disabilities. J Rehabil Res Dev 45(1):161" href="/article/10.1007/s10055-018-0371-2#ref-CR5" id="ref-link-section-d69949e516">2008</a>). This proves that such a multimodal input can be beneficial given the right context.</p><h3 class="c-article__sub-heading" id="Sec5">Select-and-point task assessment</h3><p>Evaluating the performance of a new input method, whether it is for VR or any platform, requires the correct assessment method. A simpler method would measure the time required to complete a certain task, such as walking from point A to point B using the assigned input method (Cardoso <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Cardoso J (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology. ACM, pp 319–320" href="/article/10.1007/s10055-018-0371-2#ref-CR3" id="ref-link-section-d69949e527">2016</a>). However, a more thorough evaluation can be seen in text entry studies such as work related to keyboard input (MacKenzie and Soukoreff <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hum Comput Interact 17(2–3):147–198" href="/article/10.1007/s10055-018-0371-2#ref-CR19" id="ref-link-section-d69949e530">2002</a>). In HoVR type where the user developed a system for smartphone keyboard input in VR, they measured the time and accuracy to complete a given phrase or sentence, coupled with a general usability questionnaire (Kim and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kim YR, Kim GJ (2017) HoVR-type: smartphone as a typing interface in VR using hovering. In: 2017 IEEE international conference on consumer electronics (ICCE). IEEE, pp 200–203" href="/article/10.1007/s10055-018-0371-2#ref-CR17" id="ref-link-section-d69949e533">2017</a>). One of the more common evaluation methods is the Fitts’ law method as a relative measure of performance, which was also used in a study that combined gaze detection with gesture (MacKenzie and Soukoreff <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hum Comput Interact 17(2–3):147–198" href="/article/10.1007/s10055-018-0371-2#ref-CR19" id="ref-link-section-d69949e536">2002</a>; Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e539">2015</a>). Essentially, it is a selection and activation task for the user that includes a proposed solution being compared with a baseline. This was also utilized in a study to compare several gamepads (Ramcharitar and Teather <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ramcharitar A, Teather RJ (2017) A Fitts’ law evaluation of video game controllers: thumbstick, touchpad and gyrosensor. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, NY, CHI EA ’17, pp 2860–2866. &#xA;                    https://doi.org/10.1145/3027063.3053213&#xA;                    &#xA;                  . &#xA;                    http://doi.acm.org/10.1145/3027063.3053213&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR25" id="ref-link-section-d69949e543">2017</a>).</p><p>Based on the related works, it can be seen that there exist many forms of input and interaction techniques utilizing sensors, gestures, and currently available devices. Among these input methods, the use of physiological signals carries the benefit of being intuitive and more accurate depending on the context. To evaluate this, utilizing Fitts’ law is appropriate as it is a well-recognized evaluation method, though a user experience study would benefit for additional qualitative feedback.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Input through physiological sensors</h2><div class="c-article-section__content" id="Sec6-content"><p>Our proposed system integrates eye tracking into commercially available VR headsets as a form of selection, while coupling it with a muscle sensing device for activation. This opens several forms of use cases for the user, such as aiming and firing, pick and place, navigation, and so on.</p><h3 class="c-article__sub-heading" id="Sec7">Interaction mechanics</h3><p>The core interaction method proposed in this multimodal solution is to have the eye gaze used as a selection mechanic while the muscle contraction used as an activation. This can be similarly dubbed as the eye gaze being part of the target acquisition phase and the muscle contraction as the target action phase (Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e565">2015</a>). One of the core benefits as opposed to dwelling techniques is that the user is not required to focus on an object for an extended period of time which is fatiguing (Jacob <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Jacob RJK (1990) What you look at is what you get: eye movement-based interaction techniques. In: Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, pp 11–18" href="/article/10.1007/s10055-018-0371-2#ref-CR13" id="ref-link-section-d69949e568">1990</a>). Therefore, only muscle contraction is used for the target action phase. We will perform a comparison by using eye gaze for the action phase as well in our user study.</p><p>Special considerations are required when designing an interface that relies on eye tracking. For other forms of applications that are not VR based, determining the accuracy threshold of the eye tracker is important (Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e574">2015</a>). However, UI design should be scaled accordingly, which is also parallel with the design consideration for VR UI (Unity <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Unity (2015) User interfaces for VR. &#xA;                    https://unity3d.com/learn/tutorials/topics/virtual-reality/user-interfaces-vr&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR30" id="ref-link-section-d69949e577">2015</a>). Generally, if a UI is too small and requires really accurate eye tracking for a precise solution, then it is too small for proper viewing in a VR HMD. Most UIs are directly attached to the player camera; however, this does not apply for spatial UI or 3D objects that are placed in the virtual environment. This is because the user can physically move closer to the object for a better view for VR solutions that use spatial tracking.</p><p>For the target action phase, two kinds of interaction can be performed: a discrete action in the form of a single activation method like a lamp switch, whereas continuous action provides a stream of data for as long as a condition is true, such as holding down a keyboard button. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig2">2</a> depicts these interactions showing EMG data.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="362" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>EMG data visualization against time for no muscle contraction (left), discrete action (middle), and continuous action (right)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In this context, discrete action means a single muscle contraction, while continuous action means continuous muscle contraction. This allows for several interaction mechanics for the user in a VR environment. For instance, a discrete action is great for locomotion mechanic and activation of a menu. In a shooting game, this is akin to firing a semiautomatic pistol. Continuous action, on the other hand, is suitable for dragging objects around like a paintbrush, manipulating a graph bar, or firing a fully automatic rifle in VR. These two distinct methods of interaction create a taxonomy similarly proposed by Chatterjee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e606">2015</a>). More generic actions like pick-and-place or drag-and-drop may freely utilize either interaction mechanic.</p><h3 class="c-article__sub-heading" id="Sec8">Hardware and software</h3><p>All of the peripherals used in this study can be obtained off the shelf. Granted, the accuracy may not be as high as industrial- or medical-grade tool, but consumer electronics are the more accessible option for regular users. For the VR HMD, we use the HTC Vive VR headset. For the eye tracker, we used the IR trackers by Pupil Labs. These are a pair of 120 Hz cameras that can be bought with direct integration with the lenses of the Vive. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig3">3</a> shows the user’s eyes being tracked.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Eye tracking from the Pupil software</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>There are several eye-tracking solutions for VR; however, they can be quite expensive, or not available for consumers at this point of writing. Finally, for EMG sensing, we opt to use the Myo Armband, which contains eight medical-grade EMG sensors placed on the user’s forearm (Myo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Myo (2013) Myo Gesture Control Armband. &#xA;                    http://www.myo.com/techspecs&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR22" id="ref-link-section-d69949e640">2013</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig4">4</a> shows the EMG readings of each of the Myo sensors. Myo also comes equipped with an accelerometer and gyroscope for gesture input, but for the purpose of this study, we will be excluding these.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="449" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Visualization of EMG readings against iterations from the eight sensors on the Myo Armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In terms of software, we interfaced with the Unity Engine due to its seamless integration with VR. C# is the primary development language. To obtain the eye-tracking data from the Pupil trackers, we use Open Sound Control (OSC) and ZeroMQ to pipe the data over to Unity, allowing us to both obtain the raw data and toggle Pupil’s calibration directly from Unity (Saraiji <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Saraiji Y (2016) PupilHMDCalibration. &#xA;                    https://github.com/mrayy/PupilHMDCalibration&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR27" id="ref-link-section-d69949e666">2016</a>). For the Myo armband, Myo provided a Unity plug-in that directly pipes raw EMG data for each of the sensors. The machine used for this study is equipped with a Core i7-6700 processor, an Nvidia Geforce GTX 980 graphic card, and 8 GB of memory. It is important that the machine used is at least above or equal to the minimum requirement for VR suggested by Oculus (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Oculus (2015) The Rift’s Recommended Spec, PC SDK 0.6 Released, and Mobile VR Jam Voting. &#xA;                    https://www3.oculus.com/en-us/blog/the-rifts-recommended-spec-pc-sdk-0-6-released-and-mobile-vr-jam-voting/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR23" id="ref-link-section-d69949e669">2015</a>) to avoid any additional delays that may cause motion sickness.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">User study</h2><div class="c-article-section__content" id="Sec9-content"><p>The user study is divided into two parts: one for evaluating the performance and throughput, as well as the overall qualitative feedback in a use case scenario. Both parts of the study will be explained thoroughly in their respective subtopics. The participants first need to perform a brief calibration to map the tracking data into Unity’s 3D space, though the gaze data are only in 2D. The calibration is performed by looking at nine points that appear on the FOV sequentially. No calibration for the EMG sensor is necessary.</p><h3 class="c-article__sub-heading" id="Sec10">Participants</h3><p>A total of 16 participants were recruited for this user study, comprising of seven females and nine males aged between 22 and 38 (mean = 26.19, SD = 4.55), where their feedback and opinions were also collected at the end. All the participants had no prior knowledge of the association of EMG and eye tracking with VR, though some of them have experience with interaction in VR environments. For participants who wear glasses, it needs to be taken off in order for the eye trackers to detect the eyes accurately for calibration. Each participant also provided their informed consent, and no identifying information is provided in this study.</p><h3 class="c-article__sub-heading" id="Sec11">Method</h3><p>We used a Fitts’ law study shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig5">5</a> to evaluate the throughput of our proposed interaction based on a varying index of difficulties (ID) and input methods. This particular form of study has been used to evaluate multimodal input method before and has been shown to be a viable method for the evaluation of pointing techniques (Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e699">2015</a>). It is worth noting that Fitts’ law has been deemed suitable for gaze input as well (Miniotas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Miniotas D (2000) Application of Fitts’ law to eye gaze interaction. In: CHI ’00 extended abstracts on Human factors in computer systems—CHI ’00, pp 339–340. &#xA;                    https://doi.org/10.1145/633482.633496&#xA;                    &#xA;                  . &#xA;                    http://portal.acm.org/citation.cfm?doid=633292.633496&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR20" id="ref-link-section-d69949e702">2000</a>; Ware and Mikaelian <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Ware C, Mikaelian HH (1986) An evaluation of an eye tracker as a device for computer input. ACM SIGCHI Bull 17(SI):183–188. &#xA;                    https://doi.org/10.1145/30851.275627&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR31" id="ref-link-section-d69949e705">1986</a>; Wilson and Cutrell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wilson AD, Cutrell E (2005) Flowmouse: a computer vision-based pointing and gesture input device. In: Human–computer interaction-INTERACT 2005, vol 3585, pp 565–578. &#xA;                    https://doi.org/10.1007/11555261&#xA;                    &#xA;                  . &#xA;                    http://www.springerlink.com/index/DJ6AENMGBTYBC0EE.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR33" id="ref-link-section-d69949e708">2005</a>; Zhang and MacKenzie <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Zhang X, MacKenzie IS (2007) Evaluating eye tracking with ISO 9241—part 9. In: Proceedings of the 12th international conference on human-computer interaction: intelligent multimodal interaction environments, HCI’07. Springer, Berlin, pp 779–788. &#xA;                    http://dl.acm.org/citation.cfm?id=1769590.1769678&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR35" id="ref-link-section-d69949e712">2007</a>), though none has evaluated the use of EMG in a VR environment. We directly compare our method with gamepad input, motion controllers, dwell time, and dwell time with gaze, each explained thoroughly below. Even though mouse input has been considered for comparison due to most people’s comfort in using it, we deemed it unnecessary for VR as conventional VR input leans more toward a gamepad or motion controller.</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Gamepad: The gamepad was the first consumer VR input method before the introduction of motion controllers as most users are accustomed to its layout. Therefore, the gamepad’s analog stick is used for selection and a button (A-button on the Xbox gamepad) is used for activation.</p>
                    </li>
                    <li>
                      <p>Dwell time: Since one of the primary benefits of our proposed method is that it saves time, it is only fitting to compare it with a dwell time method that is currently being used for most hands-free interaction mechanics. Dwell time places a reticule in the middle of the FOV and requires the user to use head rotation to place the reticule over interactive objects for a period of time, in this case, 750 ms. Dwell time by eye gaze has shown that this value provides the highest throughput for dwell-based interactions (Zhang and MacKenzie <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Zhang X, MacKenzie IS (2007) Evaluating eye tracking with ISO 9241—part 9. In: Proceedings of the 12th international conference on human-computer interaction: intelligent multimodal interaction environments, HCI’07. Springer, Berlin, pp 779–788. &#xA;                    http://dl.acm.org/citation.cfm?id=1769590.1769678&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR35" id="ref-link-section-d69949e727">2007</a>). Though this input method does not rely on using eye gaze, a similar dwell time value is used.</p>
                    </li>
                    <li>
                      <p>Dwell time with eye gaze: Similar to conventional dwell time implementation mentioned above, this method uses eye gaze for selection instead. At the point of writing, this method has so far not been popular with consumers simply because eye tracking is not yet made mainstream to the average users. After a brief calibration, a similar dwell time of 750 ms is also assigned.</p>
                    </li>
                    <li>
                      <p>Eye gaze with EMG: Our proposed method replaces dwell time with muscle contraction activation. The user simply needs to contract the forearm muscle to enable activation at the point of gaze, akin to clicking a mouse button for where the cursor is placed.</p>
                    </li>
                    <li>
                      <p>Motion controllers: Motion controllers are essentially gamepads that are tracked in the virtual space. Since most current VR interfaces rely on them now, we used the conventional “laser pointer” interaction where a raycast is produced from a motion controller to point at selected targets. Activation is achieved by pushing the trigger button on the controller. We deem this input method as the gold standard baseline for VR input and interaction.</p>
                    </li>
                  </ul><p>Therefore, the independent variables are the five different input types, with six levels of IDs (2.81, 2.94, 3.07, 3.2, 3.33, and 3.46 bits). The dependent variables are the movement time (MT) as well as the effective index of difficulty (IDe) to calculate the final throughput (TP). We built the entire study in the Unity Engine, which is able to read the currently set independent variable through different modes that can be toggled with a keyboard trigger (1, 2, 3, 4, and 5 to change the input, and Q, W, E, R, T, and Y to change the ID). This means that all of the input devices are connected at the same time and can be toggled freely. The dependent variables, on the other hand, are logged into a comma-separated value (CSV) file so that they can be plotted and analyzed. The explanation for each of these variables will be further elaborated in the following subsection. We initiate the user study with a pilot study to determine the parameters for the gamepad, followed by study 1 based on Fitts’ law, and finally study 2 to obtain qualitative feedback and rough accuracy estimation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig6">6</a> shows the proposed framework of the link between the variables and the implementation of the system for the user study in general.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="611" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Screen of the user study, with the red point denoting the target</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="773" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Framework of the proposed user study showing the data from user input to data output</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Pilot study</h4><p>Among these five input methods, only the gamepad requires a preset cursor velocity as other methods depend on the efficiency of the user themselves. The default speed of a virtual object according to the maximum displacement of the analog stick is 120 units/s. We ran an informal pilot study with eight participants who are unrelated to the main study, with a varying amount of experience using a gamepad, to determine the most suitable speed of the cursor. Each participant is simply required to enter the VR environment and move a cursor between two points placed left and right at the opposite end of each other using the gamepad. Six levels of distances, which were equivalent to the amplitude (3, 3.2, 3.4, 3.6, 3.8, and 4 units), were set, and each participant is given the freedom to adjust the speed of the cursor until they find a comfortable speed balance between the distances. We found that users with little to no experience prefer the cursor to be slower around 96 units/s. because they have trouble positioning the cursor accurately, whereas participants with moderate experience in using a gamepad found that the lower speed is too slow, yet higher speed was very easy to overshoot. However, all participants agreed that cursor speed above 144 units/s was quite easy to overshoot, even at a maximum distance of 4 units. Therefore, we decided to use the default speed of 120 units/s without any speed multiplier.</p><p>For the main study, each participant is required to use each input method according to each preset ID, three times per ID. Therefore, each participant will run this study 90 times. To eliminate any ordering effect, the sequence of input method was counterbalanced according to the Latin Square. The sequence of ID was randomized for a more realistic range of difficulty (Ramcharitar and Teather <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ramcharitar A, Teather RJ (2017) A Fitts’ law evaluation of video game controllers: thumbstick, touchpad and gyrosensor. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, NY, CHI EA ’17, pp 2860–2866. &#xA;                    https://doi.org/10.1145/3027063.3053213&#xA;                    &#xA;                  . &#xA;                    http://doi.acm.org/10.1145/3027063.3053213&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR25" id="ref-link-section-d69949e798">2017</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Apparatus</h4><p>The ISO 9241-9 Fitts’ study method was used for this experiment (Chatterjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. &#xA;                    https://doi.org/10.1145/2818346.2820752&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR4" id="ref-link-section-d69949e809">2015</a>; ISO <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="ISO (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidance on usability. ISO 9241-11:1998, p 22" href="/article/10.1007/s10055-018-0371-2#ref-CR12" id="ref-link-section-d69949e812">1998</a>). According to Fitts’ law, the index of difficulty, ID is influenced by the distance from the center point to any of the targets and the width of the said targets.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{ID}} = {\text{log}}_2\left( \frac{A}{W} + 1\right)$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>A</i> is the amplitude or distance and <i>W</i> is the width of the target, in unity units. The targets in this experiment are modeled as green spheres in a 3D environment. When the experiment begins, a green target will be rendered red and the participant is required to activate the sphere (depending on the input method), followed by the next target which is opposite the previously selected target until each of the targets is activated. The trial ends when the last sphere is the same as the first. We also record the movement time (MT) of the whole trial for each input method. Since it refers to the time the user spends moving a pointing device, we exclude the dwell time for dwell-based interactions as it is assumed that the pointer is static. At the beginning of each trial round, the position of the cursor is reset to the middle of the scene. At the point of selection, we also measure the standard deviation (SD) of overshoots and undershoots from the center of the target spheres. We use the SD values to calculate the effective width, We, shown below:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{We}} = 4.133 * {\text{SD}}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>This allows us to compute the effective index of difficulty, IDe, and the final throughput, TP.</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{IDe}}= {\text{log}}_2\left( \frac{A}{{{\rm We}}} + 1\right)$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{TP}}= \frac{{\rm IDe}}{{\rm MT}}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>During the entirety of the study, we employed a think-aloud protocol where at any time, the participants are free to express their opinions and provide constructive feedback, which is recorded. Finally, after each participant experiences all the IDs of an input method, we ask them to answer the NASA Task Load questionnaire for each input to understand their perceived load.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Qualitative study</h4><p>The second study focuses on qualitative feedback from participants where we obtain informal feedback regarding the proposed input method in a simple game (Jones et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Jones BR, Benko H, Ofek E, Wilson AD (2013) IllumiRoom: peripheral projected illusions for interactive experiences. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 869–878" href="/article/10.1007/s10055-018-0371-2#ref-CR15" id="ref-link-section-d69949e994">2013</a>). The game places the participant in a room, equipped with a pistol that aims in the direction the participant looks, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig7">7</a>. The participant is simply required to look at targets that spawn in front of them and fire a bullet based on muscle contraction.</p><p>To complement the main study, we also calculated the accuracy of each bullet shot, by finding the distance between where the bullet lands and the center point of the target, and evaluated the error rate (Henze et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Henze N, Rukzio E, Boll S (2011) 100,000,000 taps: analysis and improvement of touch performance in the large. In: Proceedings of the 13th international conference on human computer interaction with mobile devices and services. ACM, pp 133–142" href="/article/10.1007/s10055-018-0371-2#ref-CR9" id="ref-link-section-d69949e1003">2011</a>). However, we did not compare this with other input method and simply treat this as a minor performance evaluation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="609" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Shooting game to obtain qualitative feedback, score, and accuracy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Results</h2><div class="c-article-section__content" id="Sec15-content"><p>We assessed the Fitts’ law index of performance (IP), recorded subjective workload via a NASA TLX questionnaire, and collected qualitative feedback from a simple shooting game.</p><h3 class="c-article__sub-heading" id="Sec16">Target selection</h3><p>We conducted a two-way repeated measure ANOVA with the dependent variable throughput and the two-fixed factor target selection method and level of difficulty. An interaction between condition and level of difficulty could not be demonstrated, <span class="mathjax-tex">\(F(20,450) = .241, p = 1.0\)</span>. There was a statistically significant effect of the target selection method on throughput, <span class="mathjax-tex">\(F(4,450) = 89.211, p &lt; .0001\)</span>. With <span class="mathjax-tex">\(\eta _{p}^{2} = .442\)</span> this is a large effect accounting for 41.5% (adjusted <span class="mathjax-tex">\({\mathcal{R}}^2\)</span>) of the variance in throughput. Level of difficulty did not have a statistically significant effect on throughput, <span class="mathjax-tex">\(F(5,450) = 1.433, p = .211\)</span>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig8">8</a> visualizes the estimated marginal means of throughput and shows the dependency on the target selection method. Post hoc analysis conducted with a Tukey’s range test showed that target selection by motion yielded significantly higher throughput than all other methods (<span class="mathjax-tex">\(p &lt; .0001\)</span>). Dwell lead to significantly higher throughput than gamepad (<span class="mathjax-tex">\(p = .011\)</span>) and gaze (<span class="mathjax-tex">\(p &lt; .0001\)</span>), whereas gaze and EMG resulted in significantly higher throughput than gaze alone (<span class="mathjax-tex">\(p = .001\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="617" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Throughput is highly dependent on the target selection method (lines), whereas level of difficulty does not yield a statistically significant effect (slope). There is no interaction effect between the two-fixed factor selection method and level of difficulty</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec17">Workload</h3><p>To assess the mental workload perceived by participants, we calculated the raw TLX score after each condition and applied a Friedman test on the five control conditions. Here, we found a statistically significant difference between conditions, <span class="mathjax-tex">\({\chi }^2 (2) = 25.509,p &lt; .0001\)</span>). We conducted the post hoc analysis with Wilcoxon signed-rank tests applying a Bonferroni correction, which resulted in a significance level of <span class="mathjax-tex">\(p &lt; .005\)</span>. Median perceived work load scores for Dwell were 40 (<span class="mathjax-tex">\({\text{SD}}=17.6\)</span>), for Dwell and Gaze 51.7 (<span class="mathjax-tex">\({\text{SD}}=12.3\)</span>), for Gaze and EMG 29.2 (<span class="mathjax-tex">\({\text{SD}}=12.7\)</span>), for gamepad 25.8 (<span class="mathjax-tex">\({\text{SD}}=15.2\)</span>), and for Motion 29.6 (<span class="mathjax-tex">\({\text{SD}}=10.3\)</span>) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig9">9</a>). There was a statistically significant reduction in perceived work load when using Gaze and EMG (<span class="mathjax-tex">\(Z = -\,3.124, p = .002\)</span>), the Gamepad (<span class="mathjax-tex">\(Z = -\,3.362, p = .001\)</span>), or Motion (<span class="mathjax-tex">\(Z = -\,3.465, p = .001\)</span>) compared to Dwell and Gaze. There was no statistically significant difference between the remaining conditions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig9_HTML.png?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig9_HTML.png" alt="figure9" loading="lazy" width="685" height="585" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Perceived work load as a result of the NASA raw TLX scores: target selection with Dwell and Gaze was significantly more strenuous than using gaze and EMG, the controller, or motion</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Discussion</h2><div class="c-article-section__content" id="Sec18-content"><p>Overall, we demonstrated the feasibility of combining eye gaze with EMG for VR interactions. Based on the results, we found that dwell gaze performed the worst, which is to be expected because the participants were required to fixate on a point for a period of time, which can be uncomfortable or straining, especially given the length of time and number of trials they were required to do so. However, by removing fixation and replacing it with instant activation using EMG, the performance significantly improves. Using the motion controller still outperforms other input methods which can be due to several factors, namely due to how it feels like a direct pointing gesture, though it is not a hands-free solution and therefore could be more limiting regarding introducing additional input or for mobile VR solutions. Finally, the gamepad was the slowest since unlike eye, head, or arm-based selection, using the gamepad requires a preset maximum speed and thus cannot be as fast as the participant wishes it to be.</p><p>When asked to provide qualitative feedback during the Fitts’ study, a total of seven participants preferred to use the combination of eye gaze with EMG, eight participants preferred using the motion controller, and one participant preferred using the conventional gamepad the most. When we interviewed the one participant that chose gamepad input, he or she mentioned that it was the most relaxing and least taxing, requiring near to zero physical movements, thus reducing motion sickness because he or she did not have enough rest the previous night and was feeling slight nausea and disorientation prior to participating in our study.</p><p>For the eight participants who preferred the motion controller, three of them mentioned that it was simply more fun since the virtual laser pointer from the tip of the motion controller looked visually similar to a fictional sword in a popular science fiction movie, thus increasing the enjoyment. It was also overall fun for these three participants who had never experienced a motion-based controller for VR before. The remaining five participants who chose motion controller was simply due to its balance between speed and stability. For lower ID, a wrist movement was enough to perform the selection, though, at higher ID, arm movement was necessary, thus increasing fatigue over time. Furthermore, all eight of these participants experienced some difficulty in calibration for the eye tracking, thus reducing the overall score of both the eye-gaze dwell and eye gaze with EMG. For the participants who preferred eye gaze with EMG, five of them stated that it was extremely fast, since the selection was achieved using eye gaze which feels natural, and activation was using arm muscle without much force, thus negating any fatigue. The other three participants enjoyed the novelty of the interaction over other more conventional methods since these participants also have moderate to expert experience in VR interactions.</p><p>Among other comments, one of the participants mentioned that the loading time used for dwell and dwell gaze is frustrating. Another participant had no issue with the loading time, but dwell solution was tiring for the neck, especially for higher ID. A participant who tried the gaze solutions first followed by dwell found that he or she felt strange that activation did not start after looking at the points with the eye, possibly due to ordering effect.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Limitations</h2><div class="c-article-section__content" id="Sec19-content"><p>One of the main limitations at this point in time is, in fact, the accuracy of eye-gaze tracking in VR. With precise calibration, it is possible to obtain near-perfect tracking. However, in most cases, participants need to calibrate several times to achieve the desired level of accuracy. Furthermore, eye tracking is a very delicate method that uses IR cameras. The occurrence of slippage in the HMD after calibration will completely render the tracking unusable and further calibration is required. This means that the user cannot make any sudden or extreme head movement to maintain eye-tracking precision. However, it is worth noting that as this technology becomes more mainstream, the tracking will certainly improve. Another common issue was that since the position of the trackers is fixed in the HMD, it largely depends on the participant’s compatibility with the trackers. From our study, we found that the eye trackers have difficulty detecting the eyes of Asian participants, whereas they seem to work relatively well for European participants. This can be due to several factors such as eye color, size, and relative position of the eyes. As of this moment, the currently used setup is suitable for our user studies; however, a separate MacBook was required to operate the eye trackers. Nevertheless, we believe eye tracking will be the next evolution for VR and AR technology once it matures. Regarding the utilization of EMG, since minor muscle contraction can be easily detected by the sensor, it is possible for accidental activation to still occur. This is a common problem for input methods that do not involve buttons. Nevertheless, software tweaks on the UI behavior or increasing the activation threshold can easily circumvent this issue. High muscle contraction only even happens if the user is lifting something heavy or performing an extreme motion like punching unless done on purpose. Comparing to interactions like arm or finger gesture, or pressure sensors on certain body parts, EMG activation is less likely to accidentally occur.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Proposed applications</h2><div class="c-article-section__content" id="Sec20-content"><p>Based on the obtained results, we now understand the potential of using eye gaze with EMG for VR interaction. However, the Fitts’ law study merely demonstrates its interactivity with context to a point-and-click task, as opposed to real-world applications like gaming or interacting with objects. In this section, we designed several VR applications that can fully benefit from the proposed interaction. Since VR is a diverse platform, the applications are divided into their respective fields for interior and engineering design, entertainment, and gaming, as well as UI interface selection and media consumption. For each of these applications, we show how the user can easily select and activate elements that are present depending on the application that is hands-free, time-saving, and unobtrusive. These applications serve as a proof of concept on how eye gaze with EMG can provide a unique alternative to interaction, whether it is gaming on the go or have a VR business conference. Each of this applications will be explained through its categorization for both the target acquisition phase and target action phase. We explored four application use cases based on our novel input modality: (1) interior design explorations, (2) gaming, and (3) text input.</p><h3 class="c-article__sub-heading" id="Sec21">Interior design</h3><p>One of the core benefits when it comes to VR is the ability to place users in a virtual environment as though they are actually there. For professional use, one such usage would be assessing an interior design. In such an environment, minimalistic UI is best so that it will not obstruct the designer’s view to allow careful observation and assessment of the environment. In our implemented prototype, navigation and pick-and-place are provided through a discrete action of the target action phase. If the user wishes to navigate to a position, they can simply look at the ground of that point and perform a muscle contraction to teleport to that location. Teleportation navigation is increasingly more popular as it negates the effect of motion sickness (Cloudhead <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Cloudhead G (2015) VR Navigation. &#xA;                    http://cloudheadgames.com/cloudhead/vr-navigation/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0371-2#ref-CR6" id="ref-link-section-d69949e1798">2015</a>). We also implemented a pick-and-place tool shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig10">10</a>, allowing the user to look at a particular object of interest, pick it up via short muscle contraction, and place it at any designated spot by performing another muscle contraction. To summarize, this particular application is a pick-and-place scenario by moving furniture while navigating interior spaces.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig10_HTML.png?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig10_HTML.png" alt="figure10" loading="lazy" width="685" height="384" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Pick-and-place task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec22">Gaming</h3><p>This generation of VR products focus on gaming; therefore it is only natural to consider some kind of gaming function. We created a simple shooting scene shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig11">11</a> where the user views the world in a first-person view with the gun placed at the lower right corner, similar to most first-person shooter (FPS) games. We equip the user with a pistol and allow the user to toggle between full-auto and semi-auto firing. Full-auto allows the user to continuously fire the rifle for as long as their muscle contracts, while semi-auto fires a single bullet per muscle contraction. This effectively switches between both kinds of target action phase. One of the interesting benefits of using physiological signals is that in this use case, contraction of the arm muscle could simulate the recoil of a gun, whereas aiming with the eyes simply feels natural, leading to a greater level of immersion in gaming though further studies are required to test this hypothesis.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig11_HTML.png?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig11_HTML.png" alt="figure11" loading="lazy" width="685" height="383" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Firing a gun in full-auto and semi-auto</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec23">Input interfaces: number pads and keyboards</h3><p>We created a generalized UI system to demonstrate the feasibility of the proposed interaction mechanic. The first UI is a number pad illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0371-2#Fig12">12</a> that allows the user to simply look at a number and contract their muscle to select it. The second prototype is a series of knobs and graphs. Each of this interface depends on the user’s amount of muscle contraction. Maximum contraction will maximize the knobs and graphs while relaxing the muscle reduces it back to 0. These UIs serve as a proof of concept for them to be applied in various other usages like menu selection, scrolling, and media control without the presence of physical buttons or other devices that occupy the users’ hands. To evaluate this, a text entry study by reporting the words per minute (WPM) and error rate would prove useful.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig12_HTML.png?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0371-2/MediaObjects/10055_2018_371_Fig12_HTML.png" alt="figure12" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Number pad interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0371-2/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusion and future works</h2><div class="c-article-section__content" id="Sec24-content"><p>We present a novel multimodal assessment for VR interaction using both eye gaze and EMG for selection and activation. In two user studies, we have shown the applicability of these input methods for future VR solutions, as machines become more portable, powerful, and well equipped with the necessary sensors. The hands-free and time-saving nature of this interaction mechanic provides insights into its feasibility, effectiveness, and reliability for VR applications. As these input modalities become seamlessly integrated with the next generation of VR tools, more users may find themselves indulging in VR anytime, anywhere. For future works, we plan to further evaluate the proposed application scenarios by performing an immersion study for gaming, text entry study for input interfaces, and user feedback in pick-and-place and navigation scenarios.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DT. Barry, KE. Gordon, GG. Hinton, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Barry DT, Gordon KE, Hinton GG (1990) Acoustic and surface EMG diagnosis of pediatric muscle disease. Muscle N" /><p class="c-article-references__text" id="ref-CR1">Barry DT, Gordon KE, Hinton GG (1990) Acoustic and surface EMG diagnosis of pediatric muscle disease. Muscle Nerve 13(4):286–290</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fmus.880130403" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20and%20surface%20EMG%20diagnosis%20of%20pediatric%20muscle%20disease&amp;journal=Muscle%20Nerve&amp;volume=13&amp;issue=4&amp;pages=286-290&amp;publication_year=1990&amp;author=Barry%2CDT&amp;author=Gordon%2CKE&amp;author=Hinton%2CGG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benko H, Saponas TS, Morris D, Tan D (2009) Enhancing input on and above the interactive surface with muscle s" /><p class="c-article-references__text" id="ref-CR2">Benko H, Saponas TS, Morris D, Tan D (2009) Enhancing input on and above the interactive surface with muscle sensing. In: Proceedings of the ACM international conference on interactive tabletops and surfaces—ITS ’09, p 93. <a href="https://doi.org/10.1145/1731903.1731924">https://doi.org/10.1145/1731903.1731924</a>. <a href="http://portal.acm.org/citation.cfm?doid=1731903.1731924">http://portal.acm.org/citation.cfm?doid=1731903.1731924</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cardoso J (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR3">Cardoso J (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology. ACM, pp 319–320</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interact" /><p class="c-article-references__text" id="ref-CR4">Chatterjee I, Xiao R, Harrison C (2015) Gaze + gesture : expressive , precise and targeted free-space interactions. In: Proceedings of the 2015 ACM on international conference on multimodal interaction (C), pp 131–138. <a href="https://doi.org/10.1145/2818346.2820752">https://doi.org/10.1145/2818346.2820752</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CA. Chin, A. Barreto, JG. Cremades, M. Adjouadi, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chin CA, Barreto A, Cremades JG, Adjouadi M (2008) Integrated electromyogram and eye-gaze tracking cursor cont" /><p class="c-article-references__text" id="ref-CR5">Chin CA, Barreto A, Cremades JG, Adjouadi M (2008) Integrated electromyogram and eye-gaze tracking cursor control system for computer users with motor disabilities. J Rehabil Res Dev 45(1):161</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1682%2FJRRD.2007.03.0050" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrated%20electromyogram%20and%20eye-gaze%20tracking%20cursor%20control%20system%20for%20computer%20users%20with%20motor%20disabilities&amp;journal=J%20Rehabil%20Res%20Dev&amp;volume=45&amp;issue=1&amp;publication_year=2008&amp;author=Chin%2CCA&amp;author=Barreto%2CA&amp;author=Cremades%2CJG&amp;author=Adjouadi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cloudhead G (2015) VR Navigation. http://cloudheadgames.com/cloudhead/vr-navigation/&#xA;                        " /><p class="c-article-references__text" id="ref-CR6">Cloudhead G (2015) VR Navigation. <a href="http://cloudheadgames.com/cloudhead/vr-navigation/">http://cloudheadgames.com/cloudhead/vr-navigation/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Costanza E, Inverso SA, Allen R (2005) Toward subtle intimate interfaces for mobile devices using an EMG contr" /><p class="c-article-references__text" id="ref-CR7">Costanza E, Inverso SA, Allen R (2005) Toward subtle intimate interfaces for mobile devices using an EMG controller. In: Proceedings of the SIGCHI conference on Human factors in computing systems CHI 05:481. <a href="https://doi.org/10.1145/1054972.1055039">https://doi.org/10.1145/1054972.1055039</a>. <a href="http://eprints.soton.ac.uk/270956/">http://eprints.soton.ac.uk/270956/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darken RP, Cockayne WR, Carmein D (1997) The omni-directional treadmill: a locomotion device for virtual world" /><p class="c-article-references__text" id="ref-CR8">Darken RP, Cockayne WR, Carmein D (1997) The omni-directional treadmill: a locomotion device for virtual worlds. In: Proceedings of the 10th annual ACM symposium on User interface software and technology—UIST ’97, pp 213–221. <a href="https://doi.org/10.1145/263407.263550">https://doi.org/10.1145/263407.263550</a>. <a href="http://dl.acm.org/citation.cfm?id=263550">http://dl.acm.org/citation.cfm?id=263550</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Henze N, Rukzio E, Boll S (2011) 100,000,000 taps: analysis and improvement of touch performance in the large." /><p class="c-article-references__text" id="ref-CR9">Henze N, Rukzio E, Boll S (2011) 100,000,000 taps: analysis and improvement of touch performance in the large. In: Proceedings of the 13th international conference on human computer interaction with mobile devices and services. ACM, pp 133–142</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hernandez Arieta, R. Katoh, H. Yokoi, Y. Wenwei, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Hernandez Arieta A, Katoh R, Yokoi H, Wenwei Y (2006) Development of a multi-DOF electromyography prosthetic s" /><p class="c-article-references__text" id="ref-CR10">Hernandez Arieta A, Katoh R, Yokoi H, Wenwei Y (2006) Development of a multi-DOF electromyography prosthetic system using the adaptive joint mechanism. Appl Bionics Biomech 3(2):101–111</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1155%2F2006%2F741851" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20multi-DOF%20electromyography%20prosthetic%20system%20using%20the%20adaptive%20joint%20mechanism&amp;journal=Appl%20Bionics%20Biomech&amp;volume=3&amp;issue=2&amp;pages=101-111&amp;publication_year=2006&amp;author=Hernandez%20Arieta%2CA&amp;author=Katoh%2CR&amp;author=Yokoi%2CH&amp;author=Wenwei%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hernandez-Rebollar JL, Kyriakopoulos N, Lindeman RW (2002) The AcceleGlove: a whole-hand input device for virt" /><p class="c-article-references__text" id="ref-CR11">Hernandez-Rebollar JL, Kyriakopoulos N, Lindeman RW (2002) The AcceleGlove: a whole-hand input device for virtual reality. In: ACM SIGGRAPH 2002 conference abstracts and applications. ACM, p 259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ISO (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidance on us" /><p class="c-article-references__text" id="ref-CR12">ISO (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidance on usability. ISO 9241-11:1998, p 22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacob RJK (1990) What you look at is what you get: eye movement-based interaction techniques. In: Proceedings " /><p class="c-article-references__text" id="ref-CR13">Jacob RJK (1990) What you look at is what you get: eye movement-based interaction techniques. In: Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, pp 11–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacobsen SC, Jerard RB (1974) Computational requirements for control of the Utah arm. In: Proceedings of the 1" /><p class="c-article-references__text" id="ref-CR14">Jacobsen SC, Jerard RB (1974) Computational requirements for control of the Utah arm. In: Proceedings of the 1974 annual conference, vol 1. ACM, pp 149–155</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jones BR, Benko H, Ofek E, Wilson AD (2013) IllumiRoom: peripheral projected illusions for interactive experie" /><p class="c-article-references__text" id="ref-CR15">Jones BR, Benko H, Ofek E, Wilson AD (2013) IllumiRoom: peripheral projected illusions for interactive experiences. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 869–878</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Kiguchi, T. Tanaka, T. Fukuda, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kiguchi K, Tanaka T, Fukuda T (2004) Neuro-fuzzy control of a robotic exoskeleton with EMG signals. IEEE Trans" /><p class="c-article-references__text" id="ref-CR16">Kiguchi K, Tanaka T, Fukuda T (2004) Neuro-fuzzy control of a robotic exoskeleton with EMG signals. IEEE Trans Fuzzy Syst 12(4):481–490</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTFUZZ.2004.832525" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuro-fuzzy%20control%20of%20a%20robotic%20exoskeleton%20with%20EMG%20signals&amp;journal=IEEE%20Trans%20Fuzzy%20Syst&amp;volume=12&amp;issue=4&amp;pages=481-490&amp;publication_year=2004&amp;author=Kiguchi%2CK&amp;author=Tanaka%2CT&amp;author=Fukuda%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim YR, Kim GJ (2017) HoVR-type: smartphone as a typing interface in VR using hovering. In: 2017 IEEE internat" /><p class="c-article-references__text" id="ref-CR17">Kim YR, Kim GJ (2017) HoVR-type: smartphone as a typing interface in VR using hovering. In: 2017 IEEE international conference on consumer electronics (ICCE). IEEE, pp 200–203</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee JS, Lee SE, Jang SY, Park KS (2005) A simplified hand gesture interface for spherical manipulation in virt" /><p class="c-article-references__text" id="ref-CR18">Lee JS, Lee SE, Jang SY, Park KS (2005) A simplified hand gesture interface for spherical manipulation in virtual environments. In: Proceedings of the 2005 international conference on Augmented tele-existence. ACM, p 284</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IS. MacKenzie, RW. Soukoreff, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hu" /><p class="c-article-references__text" id="ref-CR19">MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hum Comput Interact 17(2–3):147–198</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15327051HCI172%263_2" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Text%20entry%20for%20mobile%20computing%3A%20models%20and%20methods%2C%20theory%20and%20practice&amp;journal=Hum%20Comput%20Interact&amp;volume=17&amp;issue=2%E2%80%933&amp;pages=147-198&amp;publication_year=2002&amp;author=MacKenzie%2CIS&amp;author=Soukoreff%2CRW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Miniotas D (2000) Application of Fitts’ law to eye gaze interaction. In: CHI ’00 extended abstracts on Human f" /><p class="c-article-references__text" id="ref-CR20">Miniotas D (2000) Application of Fitts’ law to eye gaze interaction. In: CHI ’00 extended abstracts on Human factors in computer systems—CHI ’00, pp 339–340. <a href="https://doi.org/10.1145/633482.633496">https://doi.org/10.1145/633482.633496</a>. <a href="http://portal.acm.org/citation.cfm?doid=633292.633496">http://portal.acm.org/citation.cfm?doid=633292.633496</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JRJB. Moseley, FW. Jobe, M. Pink, J. Perry, J. Tibone, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Moseley JRJB, Jobe FW, Pink M, Perry J, Tibone J (1992) EMG analysis of the scapular muscles during a shoulder" /><p class="c-article-references__text" id="ref-CR21">Moseley JRJB, Jobe FW, Pink M, Perry J, Tibone J (1992) EMG analysis of the scapular muscles during a shoulder rehabilitation program. Am J Sports Med 20(2):128–134</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F036354659202000206" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=EMG%20analysis%20of%20the%20scapular%20muscles%20during%20a%20shoulder%20rehabilitation%20program&amp;journal=Am%20J%20Sports%20Med&amp;volume=20&amp;issue=2&amp;pages=128-134&amp;publication_year=1992&amp;author=Moseley%2CJRJB&amp;author=Jobe%2CFW&amp;author=Pink%2CM&amp;author=Perry%2CJ&amp;author=Tibone%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Myo (2013) Myo Gesture Control Armband. http://www.myo.com/techspecs&#xA;                        " /><p class="c-article-references__text" id="ref-CR22">Myo (2013) Myo Gesture Control Armband. <a href="http://www.myo.com/techspecs">http://www.myo.com/techspecs</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oculus (2015) The Rift’s Recommended Spec, PC SDK 0.6 Released, and Mobile VR Jam Voting. https://www3.oculus." /><p class="c-article-references__text" id="ref-CR23">Oculus (2015) The Rift’s Recommended Spec, PC SDK 0.6 Released, and Mobile VR Jam Voting. <a href="https://www3.oculus.com/en-us/blog/the-rifts-recommended-spec-pc-sdk-0-6-released-and-mobile-vr-jam-voting/">https://www3.oculus.com/en-us/blog/the-rifts-recommended-spec-pc-sdk-0-6-released-and-mobile-vr-jam-voting/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pai YS, Tag B, Outram B, Vontin N, Sugiura K, Kunze K (2016) GazeSim: simulating foveated rendering using dept" /><p class="c-article-references__text" id="ref-CR24">Pai YS, Tag B, Outram B, Vontin N, Sugiura K, Kunze K (2016) GazeSim: simulating foveated rendering using depth in eye gaze for VR. In: ACM SIGGRAPH 2016 Posters. ACM, p 75</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ramcharitar A, Teather RJ (2017) A Fitts’ law evaluation of video game controllers: thumbstick, touchpad and g" /><p class="c-article-references__text" id="ref-CR25">Ramcharitar A, Teather RJ (2017) A Fitts’ law evaluation of video game controllers: thumbstick, touchpad and gyrosensor. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, NY, CHI EA ’17, pp 2860–2866. <a href="https://doi.org/10.1145/3027063.3053213">https://doi.org/10.1145/3027063.3053213</a>. <a href="http://doi.acm.org/10.1145/3027063.3053213">http://doi.acm.org/10.1145/3027063.3053213</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saponas TS, Tan DS, Morris D, Balakrishnan R, Turner J, Landay JA (2009) Enabling always-available input with " /><p class="c-article-references__text" id="ref-CR26">Saponas TS, Tan DS, Morris D, Balakrishnan R, Turner J, Landay JA (2009) Enabling always-available input with muscle–computer interfaces. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology, pp 167–176. <a href="https://doi.org/10.1145/1622176.1622208">https://doi.org/10.1145/1622176.1622208</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saraiji Y (2016) PupilHMDCalibration. https://github.com/mrayy/PupilHMDCalibration&#xA;                        " /><p class="c-article-references__text" id="ref-CR27">Saraiji Y (2016) PupilHMDCalibration. <a href="https://github.com/mrayy/PupilHMDCalibration">https://github.com/mrayy/PupilHMDCalibration</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saraiji Y, Sugimoto S, Fernando CL, Minamizawa K, Tachi S (2016) Layered telepresence: simultaneous multi pres" /><p class="c-article-references__text" id="ref-CR28">Saraiji Y, Sugimoto S, Fernando CL, Minamizawa K, Tachi S (2016) Layered telepresence: simultaneous multi presence experience using eye gaze based perceptual awareness blending. In: ACM SIGGRAPH 2016 posters. ACM, p 20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Slambekova D, Bailey R, Geigel J (2012) Gaze and gesture based object manipulation in virtual worlds. In: Proc" /><p class="c-article-references__text" id="ref-CR29">Slambekova D, Bailey R, Geigel J (2012) Gaze and gesture based object manipulation in virtual worlds. In: Proceedings of the 18th ACM symposium on virtual reality software and technology. ACM, pp 203–204</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Unity (2015) User interfaces for VR. https://unity3d.com/learn/tutorials/topics/virtual-reality/user-interface" /><p class="c-article-references__text" id="ref-CR30">Unity (2015) User interfaces for VR. <a href="https://unity3d.com/learn/tutorials/topics/virtual-reality/user-interfaces-vr">https://unity3d.com/learn/tutorials/topics/virtual-reality/user-interfaces-vr</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Ware, HH. Mikaelian, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Ware C, Mikaelian HH (1986) An evaluation of an eye tracker as a device for computer input. ACM SIGCHI Bull 17" /><p class="c-article-references__text" id="ref-CR31">Ware C, Mikaelian HH (1986) An evaluation of an eye tracker as a device for computer input. ACM SIGCHI Bull 17(SI):183–188. <a href="https://doi.org/10.1145/30851.275627">https://doi.org/10.1145/30851.275627</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F30851.275627" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20evaluation%20of%20an%20eye%20tracker%20as%20a%20device%20for%20computer%20input&amp;journal=ACM%20SIGCHI%20Bull&amp;doi=10.1145%2F30851.275627&amp;volume=17&amp;issue=SI&amp;pages=183-188&amp;publication_year=1986&amp;author=Ware%2CC&amp;author=Mikaelian%2CHH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Williams, S. Bailey, G. Narasimham, M. Li, B. Bodenheimer, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Williams B, Bailey S, Narasimham G, Li M, Bodenheimer B (2011) Evaluation of walking in place on a Wii balance" /><p class="c-article-references__text" id="ref-CR32">Williams B, Bailey S, Narasimham G, Li M, Bodenheimer B (2011) Evaluation of walking in place on a Wii balance board to explore a virtual environment. ACM Trans Appl Percept 8(3):1–14. <a href="https://doi.org/10.1145/2010325.2010329">https://doi.org/10.1145/2010325.2010329</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2010325.2010329" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20walking%20in%20place%20on%20a%20Wii%20balance%20board%20to%20explore%20a%20virtual%20environment&amp;journal=ACM%20Trans%20Appl%20Percept&amp;doi=10.1145%2F2010325.2010329&amp;volume=8&amp;issue=3&amp;pages=1-14&amp;publication_year=2011&amp;author=Williams%2CB&amp;author=Bailey%2CS&amp;author=Narasimham%2CG&amp;author=Li%2CM&amp;author=Bodenheimer%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wilson AD, Cutrell E (2005) Flowmouse: a computer vision-based pointing and gesture input device. In: Human–co" /><p class="c-article-references__text" id="ref-CR33">Wilson AD, Cutrell E (2005) Flowmouse: a computer vision-based pointing and gesture input device. In: Human–computer interaction-INTERACT 2005, vol 3585, pp 565–578. <a href="https://doi.org/10.1007/11555261">https://doi.org/10.1007/11555261</a>. <a href="http://www.springerlink.com/index/DJ6AENMGBTYBC0EE.pdf">http://www.springerlink.com/index/DJ6AENMGBTYBC0EE.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TO. Zander, M. Gaertner, C. Kothe, R. Vilimek, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Zander TO, Gaertner M, Kothe C, Vilimek R (2010) Combining eye gaze input with a brain? Computer interface for" /><p class="c-article-references__text" id="ref-CR34">Zander TO, Gaertner M, Kothe C, Vilimek R (2010) Combining eye gaze input with a brain? Computer interface for touchless human? Computer interaction. Int J Hum Comput Interact 27(1):38–51. <a href="https://doi.org/10.1080/10447318.2011.535752">https://doi.org/10.1080/10447318.2011.535752</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10447318.2011.535752" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20eye%20gaze%20input%20with%20a%20brain%3F%20Computer%20interface%20for%20touchless%20human%3F%20Computer%20interaction&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1080%2F10447318.2011.535752&amp;volume=27&amp;issue=1&amp;pages=38-51&amp;publication_year=2010&amp;author=Zander%2CTO&amp;author=Gaertner%2CM&amp;author=Kothe%2CC&amp;author=Vilimek%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang X, MacKenzie IS (2007) Evaluating eye tracking with ISO 9241—part 9. In: Proceedings of the 12th interna" /><p class="c-article-references__text" id="ref-CR35">Zhang X, MacKenzie IS (2007) Evaluating eye tracking with ISO 9241—part 9. In: Proceedings of the 12th international conference on human-computer interaction: intelligent multimodal interaction environments, HCI’07. Springer, Berlin, pp 779–788. <a href="http://dl.acm.org/citation.cfm?id=1769590.1769678">http://dl.acm.org/citation.cfm?id=1769590.1769678</a>
                        </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0371-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by the JSPS KAKENHI Grant Number 18H03278.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Keio University Graduate School of Media Design, Yokohama, Japan</p><p class="c-article-author-affiliation__authors-list">Yun Suen Pai &amp; Kai Kunze</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">University of Melbourne, Melbourne, Australia</p><p class="c-article-author-affiliation__authors-list">Tilman Dingler</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Yun_Suen-Pai"><span class="c-article-authors-search__title u-h3 js-search-name">Yun Suen Pai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yun Suen+Pai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yun Suen+Pai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yun Suen+Pai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tilman-Dingler"><span class="c-article-authors-search__title u-h3 js-search-name">Tilman Dingler</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tilman+Dingler&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tilman+Dingler" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tilman+Dingler%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kai-Kunze"><span class="c-article-authors-search__title u-h3 js-search-name">Kai Kunze</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kai+Kunze&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kai+Kunze" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kai+Kunze%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0371-2/email/correspondent/c1/new">Yun Suen Pai</a>.</p></div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec25-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv6bmnfX76vvhTjQc7fhx3AD"><div mi24-video-player="true" video-id="6bmnfX76vvhTjQc7fhx3AD" player-id="8PcXmCm9nWqE6posBEkd1h" config-type="vmpro" flash-path="//e.video-cdn.net/v2/" api-url="//d.video-cdn.net/play"></div><script src="//e.video-cdn.net/v2/embed.js"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>Supplementary material 1 (mp4 37248 KB)</p></div></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Assessing%20hands-free%20interactions%20for%20VR%20using%20eye%20gaze%20and%20electromyography&amp;author=Yun%20Suen%20Pai%20et%20al&amp;contentID=10.1007%2Fs10055-018-0371-2&amp;publication=1359-4338&amp;publicationDate=2018-11-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0371-2" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0371-2" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Pai, Y.S., Dingler, T. &amp; Kunze, K. Assessing hands-free interactions for VR using eye gaze and electromyography.
                    <i>Virtual Reality</i> <b>23, </b>119–131 (2019). https://doi.org/10.1007/s10055-018-0371-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0371-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-16">16 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-10-30">30 October 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11-20">20 November 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-06-01">01 June 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0371-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0371-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Physiological sensing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Eye gaze</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Electromyography</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0371-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=371;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

