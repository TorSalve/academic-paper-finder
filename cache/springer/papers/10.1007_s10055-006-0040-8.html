<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Multi-modal virtual environments for education with haptic and olfacto"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="It has been suggested that immersive virtual reality (VR) technology allows knowledge-building experiences and in this way provides an alternative educational process. Important key features of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Multi-modal virtual environments for education with haptic and olfactory feedback"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-10-17"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="It has been suggested that immersive virtual reality (VR) technology allows knowledge-building experiences and in this way provides an alternative educational process. Important key features of constructivist educational computer-based environments for science teaching and learning, include interaction, size, transduction and reification. Indeed, multi-sensory VR technology suits very well the needs of sciences that require a higher level of visualization and interaction. Haptics that refers to physical interactions with virtual environments (VEs) may be coupled with other sensory modalities such as vision and audition but are hardly ever associated with other feedback channels, such as olfactory feedback. A survey of theory and existing VEs including haptic or olfactory feedback, especially in the field of education is provided. Our multi-modal human-scale VE VIREPSE (virtual reality platform for simulation and experimentation) that provides haptic interaction using a string-based interface called SPIDAR (space interface device for artificial reality), olfactory and auditory feedbacks is described. An application that allows students experiencing the abstract concept of the Bohr atomic model and the quantization of the energy levels has been developed. Different configurations that support interaction, size and reification through the use of immersive and multi-modal (visual, haptic, auditory and olfactory) feedback are proposed for further evaluation. Haptic interaction is achieved using different techniques ranging from desktop pseudo-haptic feedback to human-scale haptic interaction. Olfactory information is provided using different fan-based olfactory displays (ODs). Significance of developing such multi-modal VEs for education is discussed."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-10-17"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="207"/>

    <meta name="prism.endingPage" content="225"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0040-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0040-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0040-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0040-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Multi-modal virtual environments for education with haptic and olfactory feedback"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2006/12"/>

    <meta name="citation_online_date" content="2006/10/17"/>

    <meta name="citation_firstpage" content="207"/>

    <meta name="citation_lastpage" content="225"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0040-8"/>

    <meta name="DOI" content="10.1007/s10055-006-0040-8"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0040-8"/>

    <meta name="description" content="It has been suggested that immersive virtual reality (VR) technology allows knowledge-building experiences and in this way provides an alternative educatio"/>

    <meta name="dc.creator" content="E. Richard"/>

    <meta name="dc.creator" content="A. Tijou"/>

    <meta name="dc.creator" content="P. Richard"/>

    <meta name="dc.creator" content="J.-L. Ferrier"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Viewing geometric protein structures from inside a CAVE&#8482;; citation_author=N Akkiraju, H Edelsbrunner, F Ping., J Qian; citation_volume=16; citation_issue=4; citation_publication_date=1996; citation_pages=58-61; citation_doi=10.1109/38.511855; citation_id=CR1"/>

    <meta name="citation_reference" content="Ammi M, Ferreira A (2004) Virtualized reality interfaces for micro and nanomanipulation. Proc. IEEE Int. Conf. on robotics and automation (ICRA&#8217;04) New Orleans, LA"/>

    <meta name="citation_reference" content="citation_journal_title=J. Psychol; citation_title=Olfactory memory: a case study in cognitive psychology; citation_author=J Annett; citation_volume=130; citation_issue=3; citation_publication_date=1996; citation_pages=309-319; citation_id=CR3"/>

    <meta name="citation_reference" content="Arai F, Ando D, Fukuda T, Nonoda Y, Oota T (1995) Micro manipulation based on micro physics, strategy based on attractive force reduction and stress measurement. Proc. IEEE Int. Conf. on robotics and automation (ICRA&#8217;95); 236&#8211;241"/>

    <meta name="citation_reference" content="citation_journal_title=Workshop on Space Telerobotics; citation_title=Sensory substitution for space gloves and for space robots; citation_author=P Bach-y-Rita, J Webster, W Thompkins, T Crabb; citation_volume=2; citation_publication_date=1987; citation_pages=51-57; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators and Virtual Environments; citation_title=Comments on the use of olfactory displays for virtual environments; citation_author=W Barfield, E Danas; citation_volume=5; citation_issue=1; citation_publication_date=1996; citation_pages=109-121; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=J Appl Soc Psychol; citation_title=Environmentally induced positive effects: its impact on self-efficacy, task performance, negotiation and conflict; citation_author=R Baron; citation_volume=20; citation_publication_date=1990; citation_pages=368-384; citation_doi=10.1111/j.1559-1816.1990.tb00417.x; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput-Hum Interact (TOCHI); citation_title=An experimental study on the role of touch in shared virtual environments; citation_author=C Basdogan, C-H Ho, M Srinivasan, M Slater; citation_volume=7; citation_issue=4; citation_publication_date=2000; citation_pages=443-460; citation_doi=10.1145/365058.365082; citation_id=CR8"/>

    <meta name="citation_reference" content="Biggs S, Srinivasan M (2002) Haptic Interfaces, In: Stanney K. M. (Ed) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London 5; 93&#8211;116"/>

    <meta name="citation_reference" content="citation_journal_title=J Struct Biol; citation_title=Interactive fitting augmented by force feedback and virtual reality; citation_author=S Birmanns, W Wriggers; citation_volume=144; citation_publication_date=2003; citation_pages=123-131; citation_doi=10.1016/j.jsb.2003.09.018; citation_id=CR10"/>

    <meta name="citation_reference" content="Bouzit M, Popescu V, Burdea G, Boian R (2002) The Rutgers Master II-ND Force Feedback Glove. Proc. IEEE virtual reality conf. 2002 (VRC&#8217;02) Haptics symposium, Orlando FL, March"/>

    <meta name="citation_reference" content="Brady R, Pixton J, Baxter G, Moran P, Potter C, Carragher B, Belmont A (1995) Crumbs: a virtual environment tracking tool for biological imaging. Proc. IEEE Symp. on Frontiers in biomedical visualization, IEEE Computer Society Press, Los Alamitos, USA; 18&#8211;25"/>

    <meta name="citation_reference" content="citation_journal_title=Computer Graphics (ACM); citation_title=GROPE: haptic displays for scientific visualization; citation_author=F Brooks, O-Y Ming, J Batter, P Kilpatrick; citation_volume=24; citation_issue=4; citation_publication_date=1990; citation_pages=177-185; citation_id=CR13"/>

    <meta name="citation_reference" content="Bryson S (1996) Virtual reality in scientific visualization, communications of the ACM 39(5):62&#8211;71"/>

    <meta name="citation_reference" content="Burdea G (1996) Force and touch feedback for virtual reality. Wiley, New York"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoperators and Virtual Environments; citation_title=A portable dextrous master with force feedback; citation_author=G Burdea, J Zhuang, E Roskos, D Silver, N Langrana; citation_volume=1; citation_publication_date=1992; citation_pages=18-28; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Int. J. of Human-Computer Interaction (IJHCI), Special Issue on Human-Virtual Environment Interaction; citation_title=Integration of multi-modal I/Os for virtual environments; citation_author=G Burdea, P Coiffet, P Richard; citation_volume=1; citation_publication_date=1996; citation_pages=5-24; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Biomed Eng; citation_title=Virtual reality-based training for the diagnosis of prostate cancer,; citation_author=G Burdea, G Patounakis, V Popescu, RE Weiss; citation_volume=46; citation_issue=10; citation_publication_date=1999; citation_pages=1253-60; citation_doi=10.1109/10.790503; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_title=Virtual reality technology; citation_publication_date=1994; citation_id=CR19; citation_author=G Burdea; citation_author=P Coiffet; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="Burdea G, Coiffet P (2003) Virtual reality technology, 2nd Ed., Wiley, Hoboken, New Jersey, 444 p"/>

    <meta name="citation_reference" content="Byrne C (1996) Water on tap-the use of virtual reality as an educational tool, Ph. D. thesis, University of Washington"/>

    <meta name="citation_reference" content="citation_journal_title=The IEICE Transactions, E80-D; citation_title=A human-scale direct motion instruction system device for education systems; citation_author=Y Cai, S Wang, M Sato; citation_volume=2; citation_publication_date=1997; citation_pages=212-217; citation_id=CR22"/>

    <meta name="citation_reference" content="Castelino K (2002) Biological object nanomanipulation. Review report, University of California, Berkeley"/>

    <meta name="citation_reference" content="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781"/>

    <meta name="citation_reference" content="ChangHoon P, Heedong K, Ig-Jae K, Sang Chul A, Yong-Moo K, Hyoung-Gon K (2002) The making of Kyongju VR theatre. Proc. IEEE virtual reality conf. 2002 (VRC&#8217;02); 269&#8211;273"/>

    <meta name="citation_reference" content="citation_journal_title=Proc. of the FGR&#8217;04; citation_title=A development and evaluation of reactive motion capture system with haptic feedback; citation_author=W Choi, S-J Jeong, N Hashimoto, S Hasegawa, Y Koike, M Sato; citation_volume=37; citation_publication_date=2004; citation_pages=851-856; citation_id=CR26"/>

    <meta name="citation_reference" content="Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, In: Stanney M (eds) Handbook of virtual environments: design, implementation, and applications, Lawrence Erlbaum Associates, London 46; 911&#8211;936"/>

    <meta name="citation_reference" content="Crison F, Lecuyer A, Mellet d&#8217;Huart D, Burkhardt J. &#8211;M, Michel G, Dautin J. &#8211;L. (2005) How to use milling machines with multi-sensory feedback in virtual reality. Proc. IEEE virtual reality conf. 2005 (VRC&#8217;05); 139&#8211;146"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Chem; citation_title=VIBE: a virtual biomolecular environment for interactive molecular modeling; citation_author=C Cruz-Neira, R Langley, P Bash; citation_volume=20; citation_issue=4; citation_publication_date=1996; citation_pages=469-477; citation_doi=10.1016/0097-8485(96)00009-5; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Intelligence; citation_title=What the nose knows: olfaction and cognitive abilities; citation_author=V Danthiir, R Roberts, G Pallier, L Stankov; citation_volume=29; citation_issue=4; citation_publication_date=2001; citation_pages=337-361; citation_doi=10.1016/S0160-2896(01)00061-7; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_title=Using virtual reality technology to convey abstract scientific concepts; citation_inbook_title=Learning the sciences of the 21 Century: research, design, and implementing advanced technology learning environments; citation_publication_date=1997; citation_id=CR31; citation_author=C Dede; citation_author=M Salzman; citation_author=B Loftin; citation_author=K Ash; citation_publisher=Lawrence Erlbaum Associates,"/>

    <meta name="citation_reference" content="citation_title=Multisensory immersion as a modeling environment for learning complex scientific concepts; citation_inbook_title=Modeling and simulation inscience and mathematics education; citation_publication_date=1999; citation_id=CR32; citation_author=C Dede; citation_author=M Salzman; citation_author=R Loftin; citation_author=D Prague; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Chem Senses; citation_title=Odors: implicit memory and performance effects; citation_author=J Degel, E K&#246;ster; citation_volume=26; citation_publication_date=1999; citation_pages=267-280; citation_doi=10.1093/chemse/26.3.267; citation_id=CR33"/>

    <meta name="citation_reference" content="Dinh H, Walker N, Hodges L, Chang S, Kobayashi A (1999) Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. Proc. IEEE virtual reality conf. 1999 (VRC&#8217;99) Houston, Texas; 222&#8211;228"/>

    <meta name="citation_reference" content="Duffy T, Jonassen D (1992) Constructivism; new implications for instructional technology. In: Duffy T, Jonassen D (eds) Constructivism and the technology of instruction. Lawrence Erlbaum Associates, New Jersey"/>

    <meta name="citation_reference" content="Emerson T, Revere D (1997) Virtual reality in training and education: resource guide to citations and online information, University of Washington, HITL, Seattle, WA, technical publication: B-94&#8211;1"/>

    <meta name="citation_reference" content="citation_title=The perception of odors; citation_publication_date=1982; citation_id=CR37; citation_author=T Engen; citation_publisher=Academic Press"/>

    <meta name="citation_reference" content="citation_journal_title=J Nanoparticle Res; citation_title=Mechanics and friction at the nanometer scale; citation_author=M Falvo, R Superfine; citation_volume=2; citation_publication_date=2002; citation_pages=237-248; citation_doi=10.1023/A:1010017130136; citation_id=CR38"/>

    <meta name="citation_reference" content="Ferreira A, Mavroidis C (2006) Virtual reality and haptics for nano robotics: a review study. IEEE robotics and automation magazine (in press) "/>

    <meta name="citation_reference" content="Fiolhais C, Trindade J (1999) Use of computers in physics education. In Ferrari A (ed.) Proc. Euroconference&#8217;98 -new technologies for higher education, Aveiro"/>

    <meta name="citation_reference" content="Fjeld M, Voegtli M (2002) Augmented chemistry&#160;: an interactive educational workbench. ISMAR&#8217;02"/>

    <meta name="citation_reference" content="Fuchs P, Moreau G, Burkhardt JM-, Coquillart S (2006a) L&#8217;interfa&#231;age, l&#8217;immersion et l&#8217;interaction en environnement virtuel, In: Le trait&#233; de la R&#233;alit&#233; Virtuelle, Vol. 2, Presses de l&#8217;Ecole des Mines, Paris, 520 p"/>

    <meta name="citation_reference" content="Fuchs P, Moreau G, Arnaldi B, Guitton P (2006b) Les applications de la r&#233;alit&#233; virtuelle, In: Le trait&#233; de la R&#233;alit&#233; Virtuelle, Vol. 4, Presses de l&#8217;Ecole des Mines, Paris, 520 p"/>

    <meta name="citation_reference" content="Fuchs P, Papin J. &#8211;P, Richard P, Tijou A (2006c) Les interfaces olfactives, In: Fuchs P, Moreau G (eds) Le Trait&#233; de R&#233;alit&#233; Virtuelle, Vol. 2: L&#8217;interfa&#231;age, l&#8217;immersion et l&#8217;interaction en environnement virtuel, Presse de l&#8217;Ecole des Mines 4 11"/>

    <meta name="citation_reference" content="Gallina P, Rossi A, Williams II R (2000) Planar cable-direct-driven robot. Part I &amp; II. ASME design tech. Conf., Pittsburgh"/>

    <meta name="citation_reference" content="Garratt J, Clow D, Hodgson A, Tomlinson A (1999) Computer simulation and chemical education&#8212;a review of project elaborate. Chem Educ Rev; 51&#8211;73"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality Special Report; citation_title=Is virtual reality a good teaching tool?; citation_author=E Gay; citation_volume=1; citation_publication_date=1994; citation_pages=51-60; citation_id=CR47"/>

    <meta name="citation_reference" content="Gomez D, Burdea G, Langrana N (1995) Modeling of the Rutgers master II haptic display. Proc. 4th ann. symp. on haptic interfaces for virtual environments and teleoperator systems, ASME; 727&#8211;734"/>

    <meta name="citation_reference" content="Gutierrez-Osuna R (2004) Olfactory Interaction, In: Bainbridge W (ed) Encyclopedia of human-computer interaction. Berkshire Pub, pp. 507&#8211;511"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Biol Chem; citation_title=Towards an odor communication system; citation_author=D Harel, L Carmel, D Lancet; citation_volume=27; citation_publication_date=2003; citation_pages=121-133; citation_doi=10.1016/S1476-9271(02)00092-0; citation_id=CR50"/>

    <meta name="citation_reference" content="Hashimoto M, Morioka S, Yamamoyo R. (1997) Force display for atomic bonds. Proc. IEEE int. conf. on robotics and automation (ICRA&#8217;97)"/>

    <meta name="citation_reference" content="citation_journal_title=Proc. PCM&#8217;04; citation_title=Human-scale interaction with a multi-projector display and multimodal interfaces; citation_author=N Hashimoto, J Ryu, S.-J Jeong, M Sato; citation_volume=3; citation_publication_date=2004; citation_pages=23-30; citation_id=CR52"/>

    <meta name="citation_reference" content="Heilig M (1962) US Patent 3,050,870 Sensorama stimulator. August 28"/>

    <meta name="citation_reference" content="citation_title=Commentary and Envoi; citation_inbook_title=Memory for odors; citation_publication_date=1995; citation_pages=159-175; citation_id=CR54; citation_author=R Herz; citation_author=E Eich; citation_publisher=Lawrence Erlbaum Associates"/>

    <meta name="citation_reference" content="citation_journal_title=Psychon Bull Rev; citation_title=Odor memory: review and analysis; citation_author=R Herz, T Engen; citation_volume=3; citation_publication_date=1996; citation_pages=300-313; citation_id=CR55"/>

    <meta name="citation_reference" content="citation_journal_title=Annals of the New York Academy of Sciences; citation_title=Are odors the best cues to memory? A cross-modal comparison of associative memory stimuli; citation_author=R Herz; citation_volume=855; citation_publication_date=1998; citation_pages=670-674; citation_id=CR56"/>

    <meta name="citation_reference" content="Hinckley K, Pausch R, Goble J, Kassell N (1994) Passive real-world interface props for neurosurgical visualization. ACM CHI; 452&#8211;458"/>

    <meta name="citation_reference" content="Hirata Y, Sato M (1992) 3-dimensional interface device for virtual work space. Proc. 1992 IEEE/RSJ Int. Conf. on IROS 2:889&#8211;896"/>

    <meta name="citation_reference" content="citation_journal_title=J Mol Mod; citation_title=Virtual Reality in Chemistry; citation_author=W Ihlenfeldt; citation_volume=3; citation_publication_date=1997; citation_pages=386-402; citation_doi=10.1007/s008940050056; citation_id=CR59"/>

    <meta name="citation_reference" content="Inglese F-X, Jeong S-J, Richard P, Sato M (2005) A multi-modal virtual environment. Proc. Int. Conf. Virtual Concept&#8217;05, Biarritz, France, 8&#8211;10 November 2005"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators and Virtual Environments; citation_title=3D spatial interface device using tensed strings; citation_author=M Ishii, M Sato; citation_volume=3; citation_issue=1; citation_publication_date=1994; citation_pages=81-86; citation_id=CR61"/>

    <meta name="citation_reference" content="Jansson G, Petrie H, Colwell C, Kornbrot D, F&#228;nger J, K&#246;nig H, Billberger K, Hardwick A, Furner S (1999) Haptic virtual environments for blind people: exploratory experiments with two devices. Int. J. Virtual Real 4 1"/>

    <meta name="citation_reference" content="Jones A, Scanlon E, Blake, C (1998) Reflections on a model for evaluating learning technologies, In: Oliver M. (ed) Innovation in the evaluation of learning technology. University of North London; 25&#8211;41"/>

    <meta name="citation_reference" content="Kalawsky R (1993) The science of virtual reality and virtual environments, Addison-Wesley, Pub. Co"/>

    <meta name="citation_reference" content="citation_journal_title=Trends Genet; citation_title=Virtual biology in the CAVE; citation_author=T Karr, R Brady; citation_volume=16; citation_publication_date=2000; citation_pages=231-232; citation_doi=10.1016/S0168-9525(00)01996-X; citation_id=CR65"/>

    <meta name="citation_reference" content="Kaye J (2001) Symbolic olfactory display. Master&#8217;s Thesis, MIT Media Lab"/>

    <meta name="citation_reference" content="citation_title=Transmission of olfactory information for telemedicine,; citation_inbook_title=Interactive technology and the new paradigm for healthcare; citation_publication_date=1995; citation_pages=168-172; citation_id=CR67; citation_author=P Keller; citation_author=R Kouzes; citation_author=L Kangas; citation_author=S Hashem; citation_publisher=IOS Press and Ohmsha"/>

    <meta name="citation_reference" content="Kim S, Hasegawa Y, Koike M, Sato M (2002) Tension based 7 DOF force feedback device: SPIDAR-G. Proc. IEEE virtual reality conf. (VRC&#8217;02)"/>

    <meta name="citation_reference" content="K&#246;ster E (2002) The specific characteristics of the sense of smell, In: Rouby C, Schaal B, Dubois D, Gervay R, Holley A (eds) Olfaction, taste, and cognition. Cambridge Univ. Press 3, pp. 27&#8211;43"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Human performance using virtual reality tumor palpation simulation; citation_author=N Langrana; citation_volume=21; citation_issue=4; citation_publication_date=1997; citation_pages=451-458; citation_doi=10.1016/S0097-8493(97)00021-6; citation_id=CR70"/>

    <meta name="citation_reference" content="L&#233;cuyer A, Coquillart S, Kheddar A, Richard P, Coiffet P (2000) Pseudo-haptic feedback: can isometric input devices simulate force feedback ?, Proc. IEEE virtual reality conf. 2000 (VR&#8217;00), New Brunswick, New Jersey; 83&#8211;90"/>

    <meta name="citation_reference" content="L&#233;cuyer A, Burkhardt J.-M, Etienne L (2004) Feeling bumps and holes without a haptic interface: the perception of pseudo-haptic textures. Proc. CHI 2004; 239&#8211;247"/>

    <meta name="citation_reference" content="citation_journal_title=Educ Inf Technol; citation_title=Student&#8217;s attitudes towards educational virtual environments; citation_author=T Mikropoulos, A Chalkidis, A Katsikis, A Emvalotis; citation_volume=3; citation_publication_date=1998; citation_pages=137-148; citation_doi=10.1023/A:1009687025419; citation_id=CR73"/>

    <meta name="citation_reference" content="citation_journal_title=J Biol Educ; citation_title=Virtual environments in biology teaching; citation_author=T. A Mikropoulos, A Katsikis, E Nikolou, P Tsakalis; citation_volume=37; citation_issue=4; citation_publication_date=2003; citation_pages=176-181; citation_id=CR74"/>

    <meta name="citation_reference" content="citation_journal_title=Proc. SCI&#8217;04, Kyoto, Japan; citation_title=An olfactory display device linked with human gesture; citation_author=A Mochizuki, T Amada, S Sawa, T Takeda, S Motoyashiki, K Kohyama, M Imura, K Chihara; citation_volume=48; citation_issue=6004; citation_publication_date=2004; citation_pages=531-532; citation_id=CR75"/>

    <meta name="citation_reference" content="Morie J, Iyer K, Valanejad K, Sadek R, Miraglia D, Milam D, Williams J, Luigi D.-P, Leshin J (2003) Sensory design for virtual environments, SIGGRAPH 2003 Sketch, San Diego, CA, July"/>

    <meta name="citation_reference" content="Nikolou E, Mikropoulos T, Katsikis A (1997) Virtual realities in biology teaching, In: Bevan M (ed) Proc. Int. conference virtual reality in education and training. Loughborough, UK; 59&#8211;63"/>

    <meta name="citation_reference" content="Ouh-Young G, Pique M, Hughes J, Srinivasan N, Brooks Jr. F (1988) Using a manipulator for force display in molecular docking. Proc. IEEE robotics and automation conference, Philadelphia, PA; 1824&#8211;1829"/>

    <meta name="citation_reference" content="Ouh-Young M, Beard D, Brooks F (1989) Force display performs better than visual display in a simple 6-D docking task. Proc. IEEE Int. Conf. on robotics and automation (ICRA&#8217;89)"/>

    <meta name="citation_reference" content="Paljic A, Tarrin N, Coquillart S, Bouguila L, Sato M (2004) The passive stringed haptic spidar for the worlkbench. EuroGraphics&#8217;04, Grenoble, France"/>

    <meta name="citation_reference" content="Papin J.-P, Bouallagui M, Ouali A, Richard P, Tijou A, Poisson P, Bartoli W (2003) DIODE: Smell-diffusion in real and virtual environments. Proc. 5th Int. Conf. on virtual reality (VRIC&#8217;03). Laval, France, May; 113&#8211;117"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators and Virtual Environments; citation_title=Effect of frame rate and force feedback on virtual objects manipulation; citation_author=P Richard, G Birebent, G Burdea, D Gomez, N Langrana, P Coiffet; citation_volume=15; citation_publication_date=1996; citation_pages=95-108; citation_id=CR82"/>

    <meta name="citation_reference" content="Richard P, Allain P, Richard E, Le Gall D (2006a) Projet EVACOG&#8211;Environnements Virtuels Appliqu&#233;s aux Sciences Cognitives. Handicap 2006, Proc. 4th Conf. &quot;Nouvelles Technologies au service de l&#39;homme&quot;, Handicap 2006, Paris, France, 7&#8211;9 June 2006, pp 233&#8211;239"/>

    <meta name="citation_reference" content="Richard P, Chamaret D, Inglese F-X, Lucidarme P, Ferrier J-L (2006b) Human-scale haptic virtual environment for product design: effect of sensory substitution. Int J Virtual Real (in press)"/>

    <meta name="citation_reference" content="Richard P, Coiffet P (1995) Human perceptual issues in virtual environments : sensory substitution and information redundancy. Proc. of the IEEE Int. work. on robot and human communication, Tokyo, Japan"/>

    <meta name="citation_reference" content="citation_journal_title=LNCS; citation_title=A multi-scale virtual reality approach to chemical experiments; citation_author=A Riganelli, O Gervasi, A Lagan&#224;, M Alberti; citation_volume=2658; citation_publication_date=2003; citation_pages=324-330; citation_id=CR85"/>

    <meta name="citation_reference" content="Rizzo A (2005) Development of a virtual reality therapy application for Iraq war veterans with PTSD, virtual reality. Associated technologies and rehabilitation, Three-day symposium, University of Haifa, Israel, March 7&#8211;9"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators and Virtual Environments; citation_title=SWOT analysis of the field of virtual reality rehabilitation and therapy; citation_author=A Rizzo, A Jounghyun; citation_volume=14; citation_issue=2; citation_publication_date=2005; citation_pages=119-146; citation_doi=10.1162/1054746053967094; citation_id=CR87"/>

    <meta name="citation_reference" content="Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 education, AERA Roundtable session at the American Educational Research Association annual meeting, San Diego, US, April"/>

    <meta name="citation_reference" content="citation_journal_title=LNCS; citation_title=Computer-assisted learning of chemical experiments through a 3D virtual laboratory; citation_author=I Ruiz, E Espinosa, G Garcia, M G&#243;mez-Nieto; citation_volume=2329; citation_publication_date=2002; citation_pages=704-712; citation_id=CR89"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators and Virtual Environments; citation_title=a model for understanding how virtual reality aids complex conceptual learning; citation_author=M Salzman, C Dede, R Loftin, J Chen; citation_volume=8; citation_issue=3; citation_publication_date=1999; citation_pages=293-316; citation_doi=10.1162/105474699566242; citation_id=CR90"/>

    <meta name="citation_reference" content="Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural molecular biology. Proc. 11th Symp. on Haptic interfaces for virtual environment and teleoperator systems. Los Angeles, CA; 365"/>

    <meta name="citation_reference" content="Sato M (2001) Evolution of SPIDAR. Proc. 3rd Int. virtual reality conf. (VRIC&#8217;01) Laval, May, France"/>

    <meta name="citation_reference" content="citation_journal_title=Proc. EuroHaptics&#8217;04, Munich, Germany; citation_title=Virtual environment for exploring atomic bonding; citation_author=C Sauer, W Hastings, A Okamura; citation_volume=15; citation_publication_date=2004; citation_pages=232-239; citation_id=CR93"/>

    <meta name="citation_reference" content="Schiffman S, Pearce T (2002) Introduction to olfaction: perception, anatomy, physiology, and molecular biology. In: Pearce T, Schiffman S, Nagle H, Gardner JW (eds) Handbook of machine olfaction: Electronic Nose Technology. Wiley-VCH"/>

    <meta name="citation_reference" content="Shaffer D, Meglan D, Ferrell M, Dawson S (1999) Virtual rounds: simulation-based education in procedural medicine. Proc. 1999 SPIE Battlefield Biomedical Technologies Conf., Orlando, FL 3712:99&#8211;108"/>

    <meta name="citation_reference" content="Sharma G, Mavroidis C, Ferreira A (2005) Virtual reality and haptics in nano- and bionanotechnology, In: Rieth M, Schommers W (eds) Handbook of theoretical and computational nanotechnology 10 40; 1&#8211;33"/>

    <meta name="citation_reference" content="citation_title=Haptic Interfaces; citation_inbook_title=Virtual reality: scientific and technical challenges; citation_publication_date=1995; citation_pages=161-187; citation_id=CR97; citation_author=M Srinivasan; citation_publisher=National Academic Press"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Haptics in virtual environments: taxonomy, research status, and challenges; citation_author=M Srinivasan, C Basadogan; citation_volume=21; citation_issue=4; citation_publication_date=1997; citation_pages=393-404; citation_doi=10.1016/S0097-8493(97)00030-7; citation_id=CR98"/>

    <meta name="citation_reference" content="citation_title=Handbook of the virtual environments: design, implementation and applications; citation_publication_date=2002; citation_id=CR99; citation_author=KM Stanney; citation_publisher=Lawrence Erlbaum Associates"/>

    <meta name="citation_reference" content="citation_journal_title=LNCS; citation_title=Haptic feedback: a brief history from telepresence to virtual reality; citation_author=R Stone; citation_volume=2058; citation_publication_date=2001; citation_pages=1-16; citation_id=CR100"/>

    <meta name="citation_reference" content="Stredney D, Wiet G, Yagel R, Sessanna D, Kurzion Y, Fontana M, Shareef N, Levin M, Martin K, Okamura A (1998) A comparative analysis of integrating visual representations with haptic displays, In: Westwood et al. (ed) Proc. MMVR6, IOS Press, Amsterdam; 20&#8211;26"/>

    <meta name="citation_reference" content="Sundgren H, Winquist F, Lundstrom I (1992) Artificial olfactory system based on field effect devices. Proc. interfaces to real and virtual worlds, Montpellier, France; 463&#8211;472"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Mater Sci; citation_title=Molecular simulations in the virtual material laboratory; citation_author=A Suzuki, M Kamiko, R Yamamoto, Y Tateizumi, M Hashimoto; citation_volume=14; citation_publication_date=1999; citation_pages=227-231; citation_doi=10.1016/S0927-0256(98)00135-9; citation_id=CR103"/>

    <meta name="citation_reference" content="Tarrin N, Coquillart S, Hasegawa S, Bouguila L, Sato M. (2003) The stringed haptic workbench: a new haptic workbench solution. EuroGraphics&#8217;03 22; 3"/>

    <meta name="citation_reference" content="Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. proceedings series: LNCS, Vol.&#160;3942, Pan, Z. et al, (eds), Technologies for e-learning and digital entertainment, first international conference, Edutainment 2006, Hangzhou, China; 1223&#8211;33"/>

    <meta name="citation_reference" content="Tijou A, Richard P, Papin J. &#8211;P (2006b) Diffusion d&#8217;odeurs dans les environnements virtuels: &#233;tude pr&#233;liminaire, IEEE Conf. Int. Francophone d&#8217;Automatique (CIFA&#8217;06), 30 mai-01 juin, Bordeaux, France"/>

    <meta name="citation_reference" content="Trindade J, Fiolhais C, Gil V (1999) Virtual water, an application of virtual environments as an education tool for physics and chemistry. In: Cumming G et al. (eds.) Advanced research in computers and communication in education. Proc. 7th Int. conf. on computers in education, ICCE&#8217;99, Chiba, Japan, IOS Press 2; 655&#8211;658"/>

    <meta name="citation_reference" content="citation_journal_title=Europhys News; citation_title=Visualizing atoms and molecules in on-line simulations and virtual reality; citation_author=J Trindade, J Paiva, C Fiolhais; citation_volume=32; citation_issue=11; citation_publication_date=2001; citation_pages=14-15; citation_doi=10.1051/epn:2001103; citation_id=CR108"/>

    <meta name="citation_reference" content="citation_journal_title=J Appl Physiol; citation_title=Olfactory analog to directional hearing; citation_author=G B&#233;k&#233;sy; citation_volume=19; citation_publication_date=1964; citation_pages=369-373; citation_id=CR109"/>

    <meta name="citation_reference" content="citation_journal_title=The IEICE Transactions, E84-D; citation_title=Two-handed multi-fingers string-based haptic interface device; citation_author=S Walairacht, M Ishii, Y Koike, M Sato; citation_volume=3; citation_publication_date=2001; citation_pages=365-373; citation_id=CR110"/>

    <meta name="citation_reference" content="citation_journal_title=Perfumer and Flavorist; citation_title=Effects on fragrance on vigilance, performance and stress; citation_author=J Warm, W Dember, R Parasuraman; citation_volume=15; citation_publication_date=1990; citation_pages=15-18; citation_id=CR111"/>

    <meta name="citation_reference" content="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3"/>

    <meta name="citation_reference" content="Washburn D, Jones L (2004) Could olfactory displays improve data visualization?. Computing in science and engineering, Nov-Dec; 80&#8211;83"/>

    <meta name="citation_reference" content="Williams II R, Chen M.-Y, Seaton J (2002) Haptics-augmented high school physics tutorials. Int J Virtual Real 5 1"/>

    <meta name="citation_reference" content="Williams II R, Srivastava M, Howell J, Conatser Jr, R, Eland D, Burns J, Chila A. (2004) The virtual haptic back for palpatory training. Proc. 6th Int. Conf. on multimodal interfaces, State College, PA, USA, October"/>

    <meta name="citation_reference" content="Williams II R.L (1999) Planar cable-suspended haptic interface: design for Wrench Exertion. Proc. 1999 ASME design tech. conf., 25th design automation conf., Las Vegas"/>

    <meta name="citation_reference" content="citation_journal_title=Neurosci Biobehav Rev; citation_title=Olfactory perceptual learning: the critical role of memory in odor discrimination; citation_author=D Wilson, R Stevenson; citation_volume=27; citation_publication_date=2003; citation_pages=307-328; citation_doi=10.1016/S0149-7634(03)00050-2; citation_id=CR117"/>

    <meta name="citation_reference" content="Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HITL, Seattle, WA, technical publication: R-93&#8211;9"/>

    <meta name="citation_reference" content="citation_journal_title=Themes Educ; citation_title=Learning science in virtual environments: the interplay of theory and experience; citation_author=W Winn, M Windschitl; citation_volume=1; citation_issue=4; citation_publication_date=2001; citation_pages=373-389; citation_id=CR119"/>

    <meta name="citation_reference" content="Wu W, Basdogan C, Srinivasan M (1999) The effect of perspective on visual-haptic perception of object size and compliance in virtual environments. Proc. ASME Dynamic systems and control division; 67"/>

    <meta name="citation_reference" content="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC&#8217;04) Chicago, March; 43&#8211;50"/>

    <meta name="citation_reference" content="Youngblut C, Johnson R, Nash S, Weinclaw R, Will C (1996) Review of virtual environment interface technology IDA paper P-3186 8:209&#8211;216"/>

    <meta name="citation_reference" content="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US"/>

    <meta name="citation_author" content="E. Richard"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Ing&#233;nierie des Syst&#232;mes Automatis&#233;s, Universit&#233; d&#8217;Angers &#8211; EA 4014, Angers, France"/>

    <meta name="citation_author" content="A. Tijou"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Ing&#233;nierie des Syst&#232;mes Automatis&#233;s, Universit&#233; d&#8217;Angers &#8211; EA 4014, Angers, France"/>

    <meta name="citation_author" content="P. Richard"/>

    <meta name="citation_author_email" content="emmanuelle.richard@univ-angers.fr"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Ing&#233;nierie des Syst&#232;mes Automatis&#233;s, Universit&#233; d&#8217;Angers &#8211; EA 4014, Angers, France"/>

    <meta name="citation_author" content="J.-L. Ferrier"/>

    <meta name="citation_author_institution" content="Laboratoire d&#8217;Ing&#233;nierie des Syst&#232;mes Automatis&#233;s, Universit&#233; d&#8217;Angers &#8211; EA 4014, Angers, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0040-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0040-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Multi-modal virtual environments for education with haptic and olfactory feedback"/>
        <meta property="og:description" content="It has been suggested that immersive virtual reality (VR) technology allows knowledge-building experiences and in this way provides an alternative educational process. Important key features of constructivist educational computer-based environments for science teaching and learning, include interaction, size, transduction and reification. Indeed, multi-sensory VR technology suits very well the needs of sciences that require a higher level of visualization and interaction. Haptics that refers to physical interactions with virtual environments (VEs) may be coupled with other sensory modalities such as vision and audition but are hardly ever associated with other feedback channels, such as olfactory feedback. A survey of theory and existing VEs including haptic or olfactory feedback, especially in the field of education is provided. Our multi-modal human-scale VE VIREPSE (virtual reality platform for simulation and experimentation) that provides haptic interaction using a string-based interface called SPIDAR (space interface device for artificial reality), olfactory and auditory feedbacks is described. An application that allows students experiencing the abstract concept of the Bohr atomic model and the quantization of the energy levels has been developed. Different configurations that support interaction, size and reification through the use of immersive and multi-modal (visual, haptic, auditory and olfactory) feedback are proposed for further evaluation. Haptic interaction is achieved using different techniques ranging from desktop pseudo-haptic feedback to human-scale haptic interaction. Olfactory information is provided using different fan-based olfactory displays (ODs). Significance of developing such multi-modal VEs for education is discussed."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Multi-modal virtual environments for education with haptic and olfactory feedback | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0040-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual Environment, Haptic interaction, Olfaction, Multi-modal feedback, Human scale, Education","kwrd":["Virtual_Environment","Haptic_interaction","Olfaction","Multi-modal_feedback","Human_scale","Education"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0040-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0040-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=40;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0040-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Multi-modal virtual environments for education with haptic and olfactory feedback
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0040-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0040-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-10-17" itemprop="datePublished">17 October 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Multi-modal virtual environments for education with haptic and olfactory feedback</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-E_-Richard" data-author-popup="auth-E_-Richard">E. Richard</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université d’Angers – EA 4014" /><meta itemprop="address" content="grid.7252.2, 0000000122483363, Laboratoire d’Ingénierie des Systèmes Automatisés, Université d’Angers – EA 4014, 62, Avenue Notre Dame du Lac, 49000, Angers, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-A_-Tijou" data-author-popup="auth-A_-Tijou">A. Tijou</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université d’Angers – EA 4014" /><meta itemprop="address" content="grid.7252.2, 0000000122483363, Laboratoire d’Ingénierie des Systèmes Automatisés, Université d’Angers – EA 4014, 62, Avenue Notre Dame du Lac, 49000, Angers, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-P_-Richard" data-author-popup="auth-P_-Richard" data-corresp-id="c1">P. Richard<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université d’Angers – EA 4014" /><meta itemprop="address" content="grid.7252.2, 0000000122483363, Laboratoire d’Ingénierie des Systèmes Automatisés, Université d’Angers – EA 4014, 62, Avenue Notre Dame du Lac, 49000, Angers, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J__L_-Ferrier" data-author-popup="auth-J__L_-Ferrier">J.-L. Ferrier</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université d’Angers – EA 4014" /><meta itemprop="address" content="grid.7252.2, 0000000122483363, Laboratoire d’Ingénierie des Systèmes Automatisés, Université d’Angers – EA 4014, 62, Avenue Notre Dame du Lac, 49000, Angers, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">207</span>–<span itemprop="pageEnd">225</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1393 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">59 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0040-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>It has been suggested that immersive virtual reality (VR) technology allows knowledge-building experiences and in this way provides an alternative educational process. Important key features of constructivist educational computer-based environments for science teaching and learning, include interaction, size, transduction and reification. Indeed, multi-sensory VR technology suits very well the needs of sciences that require a higher level of visualization and interaction. Haptics that refers to physical interactions with virtual environments (VEs) may be coupled with other sensory modalities such as vision and audition but are hardly ever associated with other feedback channels, such as olfactory feedback. A survey of theory and existing VEs including haptic or olfactory feedback, especially in the field of education is provided. Our multi-modal human-scale VE VIREPSE (virtual reality platform for simulation and experimentation) that provides haptic interaction using a string-based interface called SPIDAR (space interface device for artificial reality), olfactory and auditory feedbacks is described. An application that allows students experiencing the abstract concept of the Bohr atomic model and the quantization of the energy levels has been developed. Different configurations that support interaction, size and reification through the use of immersive and multi-modal (visual, haptic, auditory and olfactory) feedback are proposed for further evaluation. Haptic interaction is achieved using different techniques ranging from desktop pseudo-haptic feedback to human-scale haptic interaction. Olfactory information is provided using different fan-based olfactory displays (ODs). Significance of developing such multi-modal VEs for education is discussed.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Virtual reality (VR) is a technology that refers to computer-generated, interactive and three-dimensional environments into which users are immersed, or which add graphical information to the perceived natural environment that is updated according to the movements and position of the user (Augmented Reality). VR extends the traditional 3D graphics world in order to include stereoscopic, acoustic, haptic and even other feedbacks, like smell and taste to create a sense of immersion (Sundgren et al.<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Sundgren H, Winquist F, Lundstrom I (1992) Artificial olfactory system based on field effect devices. Proc. interfaces to real and virtual worlds, Montpellier, France; 463–472" href="/article/10.1007/s10055-006-0040-8#ref-CR102" id="ref-link-section-d93733e318">1992</a>; Kalawsky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Kalawsky R (1993) The science of virtual reality and virtual environments, Addison-Wesley, Pub. Co" href="/article/10.1007/s10055-006-0040-8#ref-CR64" id="ref-link-section-d93733e321">1993</a>; Burdea and Coiffet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Burdea G, Coiffet P (1994) Virtual reality technology. Wiley New York" href="/article/10.1007/s10055-006-0040-8#ref-CR19" id="ref-link-section-d93733e324">1994</a>; Fuchs et al.<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Fuchs P, Moreau G, Burkhardt JM-, Coquillart S (2006a) L’interfaçage, l’immersion et l’interaction en environnement virtuel, In: Le traité de la Réalité Virtuelle, Vol. 2, Presses de l’Ecole des Mines, Paris, 520 p" href="/article/10.1007/s10055-006-0040-8#ref-CR42" id="ref-link-section-d93733e327">2006a</a>). The scientific community is able to exploit VR for visualizing scientific data, modeling, animating complex engineering systems and for traditional applications such as medicine, education, arts, entertainment, defense and robotics (Stanney <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Stanney KM (2002) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London" href="/article/10.1007/s10055-006-0040-8#ref-CR99" id="ref-link-section-d93733e330">2002</a>; Fuchs et al.<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Fuchs P, Moreau G, Arnaldi B, Guitton P (2006b) Les applications de la réalité virtuelle, In: Le traité de la Réalité Virtuelle, Vol. 4, Presses de l’Ecole des Mines, Paris, 520 p" href="/article/10.1007/s10055-006-0040-8#ref-CR43" id="ref-link-section-d93733e334">2006b</a>; Burdea and Coiffet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Burdea G, Coiffet P (2003) Virtual reality technology, 2nd Ed., Wiley, Hoboken, New Jersey, 444 p" href="/article/10.1007/s10055-006-0040-8#ref-CR20" id="ref-link-section-d93733e337">2003</a>).</p><p>Since the 90s, research and development of VR applications in the field of learning and education has been increasing. VR incorporates characteristics that lend it significant potential: (a) immersion; (b) presence; (c) direct engagement; (d) autonomy; and (e) interactivity. VEs have a number of characteristics, such as self-directed activity, motivation and naturalistic learning (Winn <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HITL, Seattle, WA, technical publication: R-93–9" href="/article/10.1007/s10055-006-0040-8#ref-CR118" id="ref-link-section-d93733e343">1993</a>; Byrne <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Byrne C (1996) Water on tap-the use of virtual reality as an educational tool, Ph. D. thesis, University of Washington" href="/article/10.1007/s10055-006-0040-8#ref-CR21" id="ref-link-section-d93733e346">1996</a>; Emerson and Revere <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Emerson T, Revere D (1997) Virtual reality in training and education: resource guide to citations and online information, University of Washington, HITL, Seattle, WA, technical publication: B-94–1" href="/article/10.1007/s10055-006-0040-8#ref-CR36" id="ref-link-section-d93733e349">1997</a>). These features of VEs go some way in supporting learning according to a constructivist perspective for the acquisition of knowledge (Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific concepts. In: Jacobson MJ, Kozma RB (Eds) Learning the sciences of the 21st Century: research, design, and implementing advanced technology learning environments. Lawrence Erlbaum Associates, Hillsdale, NewJersey" href="/article/10.1007/s10055-006-0040-8#ref-CR31" id="ref-link-section-d93733e352">1997</a>; Youngblut <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US" href="/article/10.1007/s10055-006-0040-8#ref-CR123" id="ref-link-section-d93733e355">1998</a>; Roussos and Gillingham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 education, AERA Roundtable session at the American Educational Research Association annual meeting, San Diego, US, April" href="/article/10.1007/s10055-006-0040-8#ref-CR88" id="ref-link-section-d93733e359">1998</a>; Mikropoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Mikropoulos T, Chalkidis A, Katsikis A, Emvalotis A (1998) Student’s attitudes towards educational virtual environments. Educ Inf Technol 3:137–148" href="/article/10.1007/s10055-006-0040-8#ref-CR73" id="ref-link-section-d93733e362">1998</a>; Winn and Windschitl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Winn W, Windschitl M (2001) Learning science in virtual environments: the interplay of theory and experience. Themes Educ 1(4):373–389" href="/article/10.1007/s10055-006-0040-8#ref-CR119" id="ref-link-section-d93733e365">2001</a>).</p><p>VR technology suits very well the needs of sciences that require a higher level of visualization and interaction, the visualization of complex data and the building of more adequate conceptual models are possible (Bryson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Bryson S (1996) Virtual reality in scientific visualization, communications of the ACM 39(5):62–71" href="/article/10.1007/s10055-006-0040-8#ref-CR14" id="ref-link-section-d93733e371">1996</a>; Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning complex scientific concepts. In: Feurzeig W, Roberts N (eds) Modeling and simulation inscience and mathematics education. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-006-0040-8#ref-CR32" id="ref-link-section-d93733e374">1999</a>; Salzman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Salzman M, Dede C, Loftin R, Chen J (1999) a model for understanding how virtual reality aids complex conceptual learning. Presence: Teleoperators and Virtual Environments 8(3):293–316" href="/article/10.1007/s10055-006-0040-8#ref-CR90" id="ref-link-section-d93733e377">1999</a>). Moreover, VR allows educators to place their students into instructional environments, which is difficult or impossible to achieve otherwise (Ihlenfeldt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ihlenfeldt W (1997) Virtual Reality in Chemistry. J Mol Mod 3:386–402" href="/article/10.1007/s10055-006-0040-8#ref-CR59" id="ref-link-section-d93733e380">1997</a>; Nikolou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Nikolou E, Mikropoulos T, Katsikis A (1997) Virtual realities in biology teaching, In: Bevan M (ed) Proc. Int. conference virtual reality in education and training. Loughborough, UK; 59–63" href="/article/10.1007/s10055-006-0040-8#ref-CR77" id="ref-link-section-d93733e383">1997</a>; Trindade et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Trindade J, Fiolhais C, Gil V (1999) Virtual water, an application of virtual environments as an education tool for physics and chemistry. In: Cumming G et al. (eds.) Advanced research in computers and communication in education. Proc. 7th Int. conf. on computers in education, ICCE’99, Chiba, Japan, IOS Press 2; 655–658" href="/article/10.1007/s10055-006-0040-8#ref-CR107" id="ref-link-section-d93733e387">1999</a>; Sankaranarayanan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural molecular biology. Proc. 11th Symp. on Haptic interfaces for virtual environment and teleoperator systems. Los Angeles, CA; 365" href="/article/10.1007/s10055-006-0040-8#ref-CR91" id="ref-link-section-d93733e390">2003</a>; Sauer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sauer C, Hastings W, Okamura A (2004) Virtual environment for exploring atomic bonding. Proc. EuroHaptics’04, Munich, Germany 15:232–239" href="/article/10.1007/s10055-006-0040-8#ref-CR93" id="ref-link-section-d93733e393">2004</a>). Literature reports some applications that aim to explore the structure of matter at micro- or nanoscopic levels. By coupling an accurate molecular dynamics (MD) simulation to an immersive VR display with interactive capabilities and manual force feedback, “immersive” visualization of atoms, molecules, and orbitals could be improved (Sankaranarayanan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural molecular biology. Proc. 11th Symp. on Haptic interfaces for virtual environment and teleoperator systems. Los Angeles, CA; 365" href="/article/10.1007/s10055-006-0040-8#ref-CR91" id="ref-link-section-d93733e396">2003</a>). Some issues to be considered for the development of effective educational VR applications are related to both user’s immersion and multi-modal feedback. Indeed, integrating many senses into a single display system increases the feeling of presence in the environment and both cognitive and sensory-motor performance.</p><p>Previous studies have reported that compelling olfactory cues have the potential to enhance the sense of presence (Barfield and Danas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Barfield W, Danas E (1996) Comments on the use of olfactory displays for virtual environments. Presence: Teleoperators and Virtual Environments 5(1):109–121" href="/article/10.1007/s10055-006-0040-8#ref-CR6" id="ref-link-section-d93733e402">1996</a>; Dinh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dinh H, Walker N, Hodges L, Chang S, Kobayashi A (1999) Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. Proc. IEEE virtual reality conf. 1999 (VRC’99) Houston, Texas; 222–228" href="/article/10.1007/s10055-006-0040-8#ref-CR34" id="ref-link-section-d93733e405">1999</a>), invoke emotion, and provide salient spatial cues (Von Békésy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1964" title="Von Békésy G (1964) Olfactory analog to directional hearing. J Appl Physiol 19:369–373" href="/article/10.1007/s10055-006-0040-8#ref-CR109" id="ref-link-section-d93733e408">1964</a>). However, olfaction has received little note from the designers of multi-sensory VEs (Morie et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Morie J, Iyer K, Valanejad K, Sadek R, Miraglia D, Milam D, Williams J, Luigi D.-P, Leshin J (2003) Sensory design for virtual environments, SIGGRAPH 2003 Sketch, San Diego, CA, July" href="/article/10.1007/s10055-006-0040-8#ref-CR76" id="ref-link-section-d93733e411">2003</a>). So far, the integration of the sense of smell has been almost exclusively a research issue. Only a few applications providing smelling information have been developed in the field of education (Tijou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. proceedings series: LNCS, Vol. 3942, Pan, Z. et al, (eds), Technologies for e-learning and digital entertainment, first international conference, Edutainment 2006, Hangzhou, China; 1223–33" href="/article/10.1007/s10055-006-0040-8#ref-CR105" id="ref-link-section-d93733e414">2006a</a>).</p><p>The second and third sections of this paper give an overview of several learning applications in the field of science that include haptic and olfactory feedback. The fourth section describes our multi-modal human-scale virtual environment VIREPSE (virtual reality platform for simulation and experimentation) that is currently being evaluated for educational uses. Haptic interaction is provided using a string-based interface called SPIDAR (space interface device for artificial reality) (Ishii and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ishii M, Sato M (1994) 3D spatial interface device using tensed strings. Presence: Teleoperators and Virtual Environments 3(1):81–86" href="/article/10.1007/s10055-006-0040-8#ref-CR61" id="ref-link-section-d93733e421">1994</a>). Olfactory information is provided using different fan-based olfactory displays (ODs). In the fifth section, an educational application, “Haptic Atomic” that allows students to experience the abstract concept of the Bohr atomic model and the quantization of the orbital state energy levels is described. For further evaluation, different configurations are proposed ranging from desktop pseudo-haptic feedback to human-scale haptic interaction. In the last part, significance of developing such multi-modal VEs for education, within the framework of constructivism theory as well as practical educational applications is discussed.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Using haptics in learning science</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Haptics background and interfaces</h3><p>Since the 1950s, haptic rendering has been routinely employed in teleoperation, used for the remote manipulation of unreachable objects (Stone <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Stone R (2001) Haptic feedback: a brief history from telepresence to virtual reality. LNCS 2058:1–16" href="/article/10.1007/s10055-006-0040-8#ref-CR100" id="ref-link-section-d93733e436">2001</a>). In VR, the research in the field of haptics is intense (Srinivasan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Srinivasan M (1995) Haptic Interfaces. In: Durlach NI, Mavor AS (Eds) Virtual reality: scientific and technical challenges. National Academic Press, Washington DC, pp 161–187" href="/article/10.1007/s10055-006-0040-8#ref-CR97" id="ref-link-section-d93733e439">1995</a>; Srinivasan and Basdogan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Srinivasan M, Basadogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. Comput Graph 21(4):393–404" href="/article/10.1007/s10055-006-0040-8#ref-CR98" id="ref-link-section-d93733e442">1997</a>; Burdea <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Burdea G (1996) Force and touch feedback for virtual reality. Wiley, New York" href="/article/10.1007/s10055-006-0040-8#ref-CR15" id="ref-link-section-d93733e445">1996</a>; Burdea et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Burdea G, Coiffet P, Richard P (1996) Integration of multi-modal I/Os for virtual environments. Int. J. of Human-Computer Interaction (IJHCI), Special Issue on Human-Virtual Environment Interaction 1:5–24" href="/article/10.1007/s10055-006-0040-8#ref-CR17" id="ref-link-section-d93733e448">1996</a>; Biggs and Srinivasan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Biggs S, Srinivasan M (2002) Haptic Interfaces, In: Stanney K. M. (Ed) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London 5; 93–116" href="/article/10.1007/s10055-006-0040-8#ref-CR9" id="ref-link-section-d93733e452">2002</a>; Richard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Richard P, Chamaret D, Inglese F-X, Lucidarme P, Ferrier J-L (2006b) Human-scale haptic virtual environment for product design: effect of sensory substitution. Int J Virtual Real (in press)" href="/article/10.1007/s10055-006-0040-8#ref-CR124" id="ref-link-section-d93733e455">2006b</a>). Multiple disciplines such as biomechanics, neuroscience, psychophysics, robot design and control, mathematical modeling and simulation, converge to support haptics. Wide varieties of applications have emerged and span many areas of human needs such as product design, interactive computer applications, medical trainers, and rehabilitation. Other opportunities include the training of sensory-motor skills in general (Crison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Crison F, Lecuyer A, Mellet d’Huart D, Burkhardt J. –M, Michel G, Dautin J. –L. (2005) How to use milling machines with multi-sensory feedback in virtual reality. Proc. IEEE virtual reality conf. 2005 (VRC’05); 139–146" href="/article/10.1007/s10055-006-0040-8#ref-CR28" id="ref-link-section-d93733e458">2005</a>).</p><p>Several experimental studies established that haptic sensations provide essential information during interactive tasks. Indeed, they have demonstrated a dramatically improved human performance in terms of speed and precision when supported by force and touch feedback (Ouh-Young et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Ouh-Young M, Beard D, Brooks F (1989) Force display performs better than visual display in a simple 6-D docking task. Proc. IEEE Int. Conf. on robotics and automation (ICRA’89)" href="/article/10.1007/s10055-006-0040-8#ref-CR79" id="ref-link-section-d93733e464">1989</a>; Richard and Coiffet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Richard P, Coiffet P (1995) Human perceptual issues in virtual environments : sensory substitution and information redundancy. Proc. of the IEEE Int. work. on robot and human communication, Tokyo, Japan" href="/article/10.1007/s10055-006-0040-8#ref-CR84" id="ref-link-section-d93733e467">1995</a>; Birmanns and Wriggers <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Birmanns S, Wriggers W (2003) Interactive fitting augmented by force feedback and virtual reality. J Struct Biol 144:123–131" href="/article/10.1007/s10055-006-0040-8#ref-CR10" id="ref-link-section-d93733e470">2003</a>). Haptic rendering compensates common disadvantages of a pure visual representation, such as the misjudgment of the size of virtual objects (Wu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Wu W, Basdogan C, Srinivasan M (1999) The effect of perspective on visual-haptic perception of object size and compliance in virtual environments. Proc. ASME Dynamic systems and control division; 67" href="/article/10.1007/s10055-006-0040-8#ref-CR120" id="ref-link-section-d93733e473">1999</a>).</p><p>Haptic interfaces (HIs) are robotic devices that enable manual interactions with VEs or teleoperated remote systems, using the sense of touch. Recent advances in the development of force-reflecting haptic interface hardware as well as haptic rendering software have caused considerable excitement. HIs have been widely studied in the last decade and several easy-to-use interfaces for 3D input provide proportional outputs in 6 degrees of freedom (DOF). The latter are typically used for CAD and VR applications. Some of them are commercialized by companies such as Sensable Technologies (<a href="http://www.sensable.com/">http://www.sensable.com/</a>) and Immersion Corporation (<a href="http://www.immersion.com/">http://www.immersion.com/</a>).</p><p>A survey of the haptic interface devices developed in an academic setting can be found in literature (Srinivasan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Srinivasan M (1995) Haptic Interfaces. In: Durlach NI, Mavor AS (Eds) Virtual reality: scientific and technical challenges. National Academic Press, Washington DC, pp 161–187" href="/article/10.1007/s10055-006-0040-8#ref-CR97" id="ref-link-section-d93733e496">1995</a>; Biggs and Srinivasan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Biggs S, Srinivasan M (2002) Haptic Interfaces, In: Stanney K. M. (Ed) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London 5; 93–116" href="/article/10.1007/s10055-006-0040-8#ref-CR9" id="ref-link-section-d93733e499">2002</a>). Burdea et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Burdea G, Zhuang J, Roskos E, Silver D, Langrana N (1992) A portable dextrous master with force feedback. Presence Teleoperators and Virtual Environments 1:18–28" href="/article/10.1007/s10055-006-0040-8#ref-CR16" id="ref-link-section-d93733e502">1992</a> developed force feedback gloves, the Rutgers Masters I and II. They have pioneered a concept whereby pneumatic, force producing elements act on discrete areas inside a user’s hand. Portability makes the design adequate for use in conjunction with virtual reality gloves (Gomez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gomez D, Burdea G, Langrana N (1995) Modeling of the Rutgers master II haptic display. Proc. 4th ann. symp. on haptic interfaces for virtual environments and teleoperator systems, ASME; 727–734" href="/article/10.1007/s10055-006-0040-8#ref-CR48" id="ref-link-section-d93733e505">1995</a>; Bouzit et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bouzit M, Popescu V, Burdea G, Boian R (2002) The Rutgers Master II-ND Force Feedback Glove. Proc. IEEE virtual reality conf. 2002 (VRC’02) Haptics symposium, Orlando FL, March" href="/article/10.1007/s10055-006-0040-8#ref-CR11" id="ref-link-section-d93733e508">2002</a>).</p><p>In order to simulate haptic sensations without haptic interfaces, several researchers have thus proposed other solutions such as sensory substitution (Bach-y-Rita et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Bach-y-Rita P, Webster J, Thompkins W, Crabb T (1987) Sensory substitution for space gloves and for space robots. Workshop on Space Telerobotics 2:51–57" href="/article/10.1007/s10055-006-0040-8#ref-CR5" id="ref-link-section-d93733e515">1987</a>; Richard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Richard P, Birebent G, Burdea G, Gomez D, Langrana N, Coiffet P (1996) Effect of frame rate and force feedback on virtual objects manipulation. Presence: Teleoperators and Virtual Environments 15:95–108" href="/article/10.1007/s10055-006-0040-8#ref-CR82" id="ref-link-section-d93733e518">1996</a>) passive interfaces or “props” (Hinckley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Hinckley K, Pausch R, Goble J, Kassell N (1994) Passive real-world interface props for neurosurgical visualization. ACM CHI; 452–458" href="/article/10.1007/s10055-006-0040-8#ref-CR57" id="ref-link-section-d93733e521">1994</a>), and pseudo-haptic feedback which was initially obtained by combining the use of a passive input device with visual feedback. It was used to simulate haptic properties such as stiffness or friction (Lécuyer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lécuyer A, Coquillart S, Kheddar A, Richard P, Coiffet P (2000) Pseudo-haptic feedback: can isometric input devices simulate force feedback ?, Proc. IEEE virtual reality conf. 2000 (VR’00), New Brunswick, New Jersey; 83–90" href="/article/10.1007/s10055-006-0040-8#ref-CR71" id="ref-link-section-d93733e524">2000</a>). Lécuyer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lécuyer A, Burkhardt J.-M, Etienne L (2004) Feeling bumps and holes without a haptic interface: the perception of pseudo-haptic textures. Proc. CHI 2004; 239–247" href="/article/10.1007/s10055-006-0040-8#ref-CR72" id="ref-link-section-d93733e527">2004</a>) proposed a novel interaction technique to simulate textures in desktop by using a passive input device combined with visual feedback.</p><p>Other kinds of haptic interfaces are currently being evaluated in the context of education, entertainment and industrial applications (Cai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cai Y, Wang S, Sato M (1997) A human-scale direct motion instruction system device for education systems. The IEICE Transactions, E80-D 2:212–217" href="/article/10.1007/s10055-006-0040-8#ref-CR22" id="ref-link-section-d93733e533">1997</a>; Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Choi W, Jeong S-J, Hashimoto N, Hasegawa S, Koike Y, Sato M (2004) A development and evaluation of reactive motion capture system with haptic feedback. Proc. of the FGR’04 37:851–856" href="/article/10.1007/s10055-006-0040-8#ref-CR26" id="ref-link-section-d93733e536">2004</a>; Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Sato M (2001) Evolution of SPIDAR. Proc. 3rd Int. virtual reality conf. (VRIC’01) Laval, May, France" href="/article/10.1007/s10055-006-0040-8#ref-CR92" id="ref-link-section-d93733e539">2001</a>; Tarrin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Tarrin N, Coquillart S, Hasegawa S, Bouguila L, Sato M. (2003) The stringed haptic workbench: a new haptic workbench solution. EuroGraphics’03 22; 3" href="/article/10.1007/s10055-006-0040-8#ref-CR104" id="ref-link-section-d93733e542">2003</a>; Inglese et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Inglese F-X, Jeong S-J, Richard P, Sato M (2005) A multi-modal virtual environment. Proc. Int. Conf. Virtual Concept’05, Biarritz, France, 8–10 November 2005" href="/article/10.1007/s10055-006-0040-8#ref-CR60" id="ref-link-section-d93733e545">2005</a>). These are string-based haptic interfaces<b> (</b>Walairacht et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Walairacht S, Ishii M, Koike Y, Sato M (2001) Two-handed multi-fingers string-based haptic interface device. The IEICE Transactions, E84-D 3:365–373" href="/article/10.1007/s10055-006-0040-8#ref-CR110" id="ref-link-section-d93733e552">2001</a>; Gallina et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Gallina P, Rossi A, Williams II R (2000) Planar cable-direct-driven robot. Part I &amp; II. ASME design tech. Conf., Pittsburgh" href="/article/10.1007/s10055-006-0040-8#ref-CR45" id="ref-link-section-d93733e555">2000</a>; Hashimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hashimoto N, Ryu J, Jeong S.-J, Sato M (2004) Human-scale interaction with a multi-projector display and multimodal interfaces. Proc. PCM’04 3:23–30" href="/article/10.1007/s10055-006-0040-8#ref-CR52" id="ref-link-section-d93733e558">2004</a>; Hirata and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Hirata Y, Sato M (1992) 3-dimensional interface device for virtual work space. Proc. 1992 IEEE/RSJ Int. Conf. on IROS 2:889–896" href="/article/10.1007/s10055-006-0040-8#ref-CR58" id="ref-link-section-d93733e561">1992</a>; Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kim S, Hasegawa Y, Koike M, Sato M (2002) Tension based 7 DOF force feedback device: SPIDAR-G. Proc. IEEE virtual reality conf. (VRC’02)" href="/article/10.1007/s10055-006-0040-8#ref-CR68" id="ref-link-section-d93733e564">2002</a>; Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Williams II R.L (1999) Planar cable-suspended haptic interface: design for Wrench Exertion. Proc. 1999 ASME design tech. conf., 25th design automation conf., Las Vegas" href="/article/10.1007/s10055-006-0040-8#ref-CR116" id="ref-link-section-d93733e568">1999</a>). They are composed of actuators providing a force through a set of strings firmly linked together or to a manipulation tool. Such interfaces have very interesting properties, i.e., fixed-base, large workspace, and low intrusion. Additional properties like lightness, safeness, or low cost are also satisfied. However, these interfaces are complex to set-up and not easy to control. As an alternative, Paljic and Coquillart proposed a passive stringed-based haptic feedback system that can provide the user with grounded forces in a 3D manipulation space (Paljic et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Paljic A, Tarrin N, Coquillart S, Bouguila L, Sato M (2004) The passive stringed haptic spidar for the worlkbench. EuroGraphics’04, Grenoble, France" href="/article/10.1007/s10055-006-0040-8#ref-CR80" id="ref-link-section-d93733e571">2004</a>). </p><p>One of the first applications of haptics in VR was to improve tractor safety by training young rural drivers (Stredney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Stredney D, Wiet G, Yagel R, Sessanna D, Kurzion Y, Fontana M, Shareef N, Levin M, Martin K, Okamura A (1998) A comparative analysis of integrating visual representations with haptic displays, In: Westwood et al. (ed) Proc. MMVR6, IOS Press, Amsterdam; 20–26" href="/article/10.1007/s10055-006-0040-8#ref-CR101" id="ref-link-section-d93733e577">1998</a>). Haptics has also been applied to make virtual environments accessible to blind persons<b> (</b>Jansson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Jansson G, Petrie H, Colwell C, Kornbrot D, Fänger J, König H, Billberger K, Hardwick A, Furner S (1999) Haptic virtual environments for blind people: exploratory experiments with two devices. Int. J. Virtual Real 4 1" href="/article/10.1007/s10055-006-0040-8#ref-CR62" id="ref-link-section-d93733e583">1999</a>
                           <b>)</b>. The first existing papers relating to haptics and learning are those in the medical training field. The Interventional Cardiology Training Simulator links technical simulation with specific medical education content (Shaffer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Shaffer D, Meglan D, Ferrell M, Dawson S (1999) Virtual rounds: simulation-based education in procedural medicine. Proc. 1999 SPIE Battlefield Biomedical Technologies Conf., Orlando, FL 3712:99–108" href="/article/10.1007/s10055-006-0040-8#ref-CR95" id="ref-link-section-d93733e589">1999</a>). A virtual reality-based simulator prototype for the diagnosis of prostate cancer has been developed using the PHANToM<sup>®</sup> haptic interface (Burdea et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Burdea G, Patounakis G, Popescu V, Weiss RE (1999) Virtual reality-based training for the diagnosis of prostate cancer, IEEE Trans Biomed Eng 46(10):1253–60" href="/article/10.1007/s10055-006-0040-8#ref-CR18" id="ref-link-section-d93733e595">1999</a>). Another tumor palpation virtual reality (VR) simulation was developed by Langrana <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Langrana N (1997) Human performance using virtual reality tumor palpation simulation. Comput Graph 21(4):451–458" href="/article/10.1007/s10055-006-0040-8#ref-CR70" id="ref-link-section-d93733e598">1997</a>.</p><p>The next subsection reviews VR learning applications including haptics.</p><h3 class="c-article__sub-heading" id="Sec4">Overview of VR applications using haptics in learning science</h3><p>Williams et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Williams II R, Chen M.-Y, Seaton J (2002) Haptics-augmented high school physics tutorials. Int J Virtual Real 5 1" href="/article/10.1007/s10055-006-0040-8#ref-CR114" id="ref-link-section-d93733e611">2002</a>) at Ohio University have presented freely-available internet tutorials and haptics-augmented software activities to support the teaching and learning of high school physics, with pilot project results (<a href="http://www.ent.ohiou.edu/~bobw/html/HapEd/NASA/SimpMach/indexSM.htm">http://www.ent.ohiou.edu/∼bobw/html/HapEd/NASA/SimpMach/indexSM.htm</a>). The program includes five different haptics-augmented activities to reinforce concepts presented in a standard simple machine curriculum. The same group has also developed a haptic playback system which has the potential to improve virtual palpatory diagnosis training by allowing students to follow and feel an expert’s motions prior to performing their own palpatory tasks (Williams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Williams II R, Srivastava M, Howell J, Conatser Jr, R, Eland D, Burns J, Chila A. (2004) The virtual haptic back for palpatory training. Proc. 6th Int. Conf. on multimodal interfaces, State College, PA, USA, October" href="/article/10.1007/s10055-006-0040-8#ref-CR115" id="ref-link-section-d93733e621">2004</a>).</p><p>The “ScienceSpace Worlds” is the first immersive, multi-sensory, VE dedicated to the learning of abstract concepts of physics and chemistry using haptic navigation and feedback hardware: (a) Newton World to experience laws of the kinematics and the dynamics of one-dimensional motion; (b) Maxwell World to manipulate multiple representations of electrostatics leading up to the concept of Gauss’ Law; and (c) Pauling World to explore complex molecular structures and interactively investigate molecule interactions (Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific concepts. In: Jacobson MJ, Kozma RB (Eds) Learning the sciences of the 21st Century: research, design, and implementing advanced technology learning environments. Lawrence Erlbaum Associates, Hillsdale, NewJersey" href="/article/10.1007/s10055-006-0040-8#ref-CR31" id="ref-link-section-d93733e627">1997</a>; Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning complex scientific concepts. In: Feurzeig W, Roberts N (eds) Modeling and simulation inscience and mathematics education. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-006-0040-8#ref-CR32" id="ref-link-section-d93733e630">1999</a>; Salzman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Salzman M, Dede C, Loftin R, Chen J (1999) a model for understanding how virtual reality aids complex conceptual learning. Presence: Teleoperators and Virtual Environments 8(3):293–316" href="/article/10.1007/s10055-006-0040-8#ref-CR90" id="ref-link-section-d93733e633">1999</a>).</p><p>The literature reports some applications of VR in the field of chemistry education. The use of VR tools not only enhances the interaction of scientists with the experiment by offering an easy way of virtually exploring its components but also offers a solid base for rationalizing what is happening at micro- and nanoscopic levels by providing a VR representation of its elementary components (Ihlenfeldt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ihlenfeldt W (1997) Virtual Reality in Chemistry. J Mol Mod 3:386–402" href="/article/10.1007/s10055-006-0040-8#ref-CR59" id="ref-link-section-d93733e639">1997</a>). High-end systems that have been used for chemistry visualization, include interactive capabilities and manual force feedback (Sankaranarayanan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural molecular biology. Proc. 11th Symp. on Haptic interfaces for virtual environment and teleoperator systems. Los Angeles, CA; 365" href="/article/10.1007/s10055-006-0040-8#ref-CR91" id="ref-link-section-d93733e642">2003</a>; Birmanns and Wriggers <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Birmanns S, Wriggers W (2003) Interactive fitting augmented by force feedback and virtual reality. J Struct Biol 144:123–131" href="/article/10.1007/s10055-006-0040-8#ref-CR10" id="ref-link-section-d93733e645">2003</a>). A well-known example is the GROPE project that was the first to demonstrate the usefulness of haptic interfaces for the perception of force fields, and in molecular docking tasks (Ouh-Young et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Ouh-Young G, Pique M, Hughes J, Srinivasan N, Brooks Jr. F (1988) Using a manipulator for force display in molecular docking. Proc. IEEE robotics and automation conference, Philadelphia, PA; 1824–1829" href="/article/10.1007/s10055-006-0040-8#ref-CR78" id="ref-link-section-d93733e648">1988</a>; Brooks Jr et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Brooks Jr F, Ming O-Y, Batter J, Kilpatrick P (1990) Project GROPE: haptic displays for scientific visualization. Computer Graphics (ACM) 24(4):177–185" href="/article/10.1007/s10055-006-0040-8#ref-CR13" id="ref-link-section-d93733e651">1990</a>).</p><p>At the University of Coimbra, Trindade et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Trindade J, Paiva J, Fiolhais C (2001) Visualizing atoms and molecules in on-line simulations and virtual reality. Europhys News 32(11):14–15" href="/article/10.1007/s10055-006-0040-8#ref-CR108" id="ref-link-section-d93733e657">2001</a>) have developed the “Molecularium” VR project which gathers computational simulations dealing with various physical and chemical processes acting in the microscopic world using a HMD (head mounted display) and a CyberGlove<sup>®.</sup> Their “Virtual Water” environment was the first application designed to illustrate scientific concepts such as atomic and molecular orbitals, electron densities, molecular geometry, and water molecular dynamics for the liquid and gaseous phases transitions (Fiolhais and Trindade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fiolhais C, Trindade J (1999) Use of computers in physics education. In Ferrari A (ed.) Proc. Euroconference’98 -new technologies for higher education, Aveiro" href="/article/10.1007/s10055-006-0040-8#ref-CR40" id="ref-link-section-d93733e662">1999</a>).</p><p>Suzuki et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Suzuki A, Kamiko M, Yamamoto R, Tateizumi Y, Hashimoto M (1999) Molecular simulations in the virtual material laboratory. Comput Mater Sci 14:227–231" href="/article/10.1007/s10055-006-0040-8#ref-CR103" id="ref-link-section-d93733e669">1999</a>) developed a VR system for atomic behavior in materials testing. They have built a force feedback device with 1° of freedom and have examined virtual experiments on the atomic bonds between two atoms (Hashimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hashimoto M, Morioka S, Yamamoyo R. (1997) Force display for atomic bonds. Proc. IEEE int. conf. on robotics and automation (ICRA’97)" href="/article/10.1007/s10055-006-0040-8#ref-CR51" id="ref-link-section-d93733e672">1997</a>). The force exerted by the atoms calculated by the simulation server can be used as a feedback calculator.</p><p>Fjeld and Voegtli (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Fjeld M, Voegtli M (2002) Augmented chemistry : an interactive educational workbench. ISMAR’02" href="/article/10.1007/s10055-006-0040-8#ref-CR41" id="ref-link-section-d93733e678">2002</a>) proposed an interactive educational workbench using augmented reality (AR), to display and manipulate 3D molecular models. The system called “Augmented Chemistry” allows, using a specialized select- and rotate-tool, to pick up, to position single atoms and to assemble them into complete molecules. Chemical rules can be formulated and integrated into the application.</p><p>Sauer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sauer C, Hastings W, Okamura A (2004) Virtual environment for exploring atomic bonding. Proc. EuroHaptics’04, Munich, Germany 15:232–239" href="/article/10.1007/s10055-006-0040-8#ref-CR93" id="ref-link-section-d93733e684">2004</a>) created a program for the beginning user designed to teach the fundamentals of atoms and molecules in a virtual world. A PHANToM<sup>®</sup> allows the user to experience a recreation of the forces that would be present during real world atomic interactions.</p><p>Quite a few VEs have been developed for the support of biology and biochemistry teaching (Nikolou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Nikolou E, Mikropoulos T, Katsikis A (1997) Virtual realities in biology teaching, In: Bevan M (ed) Proc. Int. conference virtual reality in education and training. Loughborough, UK; 59–63" href="/article/10.1007/s10055-006-0040-8#ref-CR77" id="ref-link-section-d93733e692">1997</a>; Gay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Gay E (1994) Is virtual reality a good teaching tool? Virtual Reality Special Report 1:51–60" href="/article/10.1007/s10055-006-0040-8#ref-CR47" id="ref-link-section-d93733e695">1994</a>; Mikropoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Mikropoulos T. A, Katsikis A, Nikolou E, Tsakalis P (2003) Virtual environments in biology teaching. J Biol Educ 37(4):176–181" href="/article/10.1007/s10055-006-0040-8#ref-CR74" id="ref-link-section-d93733e698">2003</a>). Some of them include haptic feedback to provide a higher level of interaction (Brady et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Brady R, Pixton J, Baxter G, Moran P, Potter C, Carragher B, Belmont A (1995) Crumbs: a virtual environment tracking tool for biological imaging. Proc. IEEE Symp. on Frontiers in biomedical visualization, IEEE Computer Society Press, Los Alamitos, USA; 18–25" href="/article/10.1007/s10055-006-0040-8#ref-CR12" id="ref-link-section-d93733e701">1995</a>; Karr and Brady <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Karr T, Brady R (2000) Virtual biology in the CAVE. Trends Genet 16:231–232" href="/article/10.1007/s10055-006-0040-8#ref-CR65" id="ref-link-section-d93733e704">2000</a>). Karr and Brady (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Karr T, Brady R (2000) Virtual biology in the CAVE. Trends Genet 16:231–232" href="/article/10.1007/s10055-006-0040-8#ref-CR65" id="ref-link-section-d93733e708">2000</a>) have developed databases and additional software and user interfaces to refine the CAVE (cave automatic virtual environment) for undergraduate teaching of developmental biology. CAVEs have already been exploited for the visualization of protein surfaces using structural representations, e.g., space-filling, solvent-accessible surface, molecular surface and the alpha complex algorithm (Akkiraju et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Akkiraju N, Edelsbrunner H, Ping. F, Qian J (1996) Viewing geometric protein structures from inside a CAVE™. IEEE Comput Graph Appl 16(4):58–61" href="/article/10.1007/s10055-006-0040-8#ref-CR1" id="ref-link-section-d93733e711">1996</a>). Therefore, the virtual biomolecular environment (VIBE) was successfully used to steer a cyclic urea compound into the active site of the HIV protease (Cruz-Neira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Cruz-Neira C, Langley R, Bash P (1996) VIBE: a virtual biomolecular environment for interactive molecular modeling. Comput Chem 20(4):469–477" href="/article/10.1007/s10055-006-0040-8#ref-CR29" id="ref-link-section-d93733e714">1996</a>).</p><p>The most recent step of this ongoing process of making analogies of the real world is the use of VR at both meter and nanometer levels (Garratt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Garratt J, Clow D, Hodgson A, Tomlinson A (1999) Computer simulation and chemical education—a review of project elaborate. Chem Educ Rev; 51–73" href="/article/10.1007/s10055-006-0040-8#ref-CR46" id="ref-link-section-d93733e720">1999</a>; Ruiz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ruiz I, Espinosa E, Garcia G, Gómez-Nieto M (2002) Computer-assisted learning of chemical experiments through a 3D virtual laboratory. LNCS 2329:704–712" href="/article/10.1007/s10055-006-0040-8#ref-CR89" id="ref-link-section-d93733e723">2002</a>; Riganelli et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Riganelli A, Gervasi O, Laganà A, Alberti M (2003) A multi-scale virtual reality approach to chemical experiments. LNCS 2658:324–330" href="/article/10.1007/s10055-006-0040-8#ref-CR85" id="ref-link-section-d93733e726">2003</a>). Thus, various projects using haptics to enhance the human interface of imaging instruments such as scanning tunneling and atomic force microscopes exist (Ferreira and Mavroidis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ferreira A, Mavroidis C (2006) Virtual reality and haptics for nano robotics: a review study. IEEE robotics and automation magazine (in press) " href="/article/10.1007/s10055-006-0040-8#ref-CR39" id="ref-link-section-d93733e729">2006</a>). Truly interactive VEs that incorporate efficient, dynamic, physical- and realistic-based simulations (adhesive, repulsive, attractive and frictional nanoforces) and motion-planning techniques applicable to complex nanomanipulation tasks have already been experienced (Ammi and Ferreira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ammi M, Ferreira A (2004) Virtualized reality interfaces for micro and nanomanipulation. Proc. IEEE Int. Conf. on robotics and automation (ICRA’04) New Orleans, LA" href="/article/10.1007/s10055-006-0040-8#ref-CR2" id="ref-link-section-d93733e732">2004</a>; Arai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Arai F, Ando D, Fukuda T, Nonoda Y, Oota T (1995) Micro manipulation based on micro physics, strategy based on attractive force reduction and stress measurement. Proc. IEEE Int. Conf. on robotics and automation (ICRA’95); 236–241" href="/article/10.1007/s10055-006-0040-8#ref-CR4" id="ref-link-section-d93733e736">1995</a>; Castelino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Castelino K (2002) Biological object nanomanipulation. Review report, University of California, Berkeley" href="/article/10.1007/s10055-006-0040-8#ref-CR23" id="ref-link-section-d93733e739">2002</a>; Falvo and Superfine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Falvo M, Superfine R (2002) Mechanics and friction at the nanometer scale. J Nanoparticle Res 2:237–248" href="/article/10.1007/s10055-006-0040-8#ref-CR38" id="ref-link-section-d93733e742">2002</a>). For example, a PHANToM<sup>®</sup> and a HMD with a head-tracking system are used to enable the user skills, and an automatic motion planner to cooperatively solve a motion task query physically placing the scientist at the scale of a nanopart (Ammi and Ferreira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ammi M, Ferreira A (2004) Virtualized reality interfaces for micro and nanomanipulation. Proc. IEEE Int. Conf. on robotics and automation (ICRA’04) New Orleans, LA" href="/article/10.1007/s10055-006-0040-8#ref-CR2" id="ref-link-section-d93733e747">2004</a>). Recently, Sharma et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sharma G, Mavroidis C, Ferreira A (2005) Virtual reality and haptics in nano- and bionanotechnology, In: Rieth M, Schommers W (eds) Handbook of theoretical and computational nanotechnology 10 40; 1–33" href="/article/10.1007/s10055-006-0040-8#ref-CR96" id="ref-link-section-d93733e750">2005</a>) have published a review of such applications for the benefit of students and researchers alike.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Using olfactory channel in learning science</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">Olfactory background</h3><p>Olfaction is commonly regarded as a minor sensory modality. In this subsection, specific characteristics of the sense of smell are discussed and some review of VEs using olfactory channel is given.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Characteristics of olfaction</h4><p>Olfaction, the sense of smell, appears to be separate from visual-spatial or verbal-auditory modalities. It differs from vision and hearing in terms of a number of factors (Köster <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Köster E (2002) The specific characteristics of the sense of smell, In: Rouby C, Schaal B, Dubois D, Gervay R, Holley A (eds) Olfaction, taste, and cognition. Cambridge Univ. Press 3, pp. 27–43" href="/article/10.1007/s10055-006-0040-8#ref-CR69" id="ref-link-section-d93733e772">2002</a>). For the case of vision, there is a physiological validity in compressing color information: the continuous spectrum to the mixture of three primary colors (red, green and blue). However, there is no such mechanism found in olfaction. The number of detectors of human olfaction is considered to be thousands, so it is difficult to code the odorants as the mixture of a small number of “primary odors”. As an ambient or peripheral medium, olfaction does not require manipulation or eye contact (Schiffman and Pearce <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Schiffman S, Pearce T (2002) Introduction to olfaction: perception, anatomy, physiology, and molecular biology. In: Pearce T, Schiffman S, Nagle H, Gardner JW (eds) Handbook of machine olfaction: Electronic Nose Technology. Wiley-VCH" href="/article/10.1007/s10055-006-0040-8#ref-CR94" id="ref-link-section-d93733e775">2002</a>).</p><p>According to Köster (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Köster E (2002) The specific characteristics of the sense of smell, In: Rouby C, Schaal B, Dubois D, Gervay R, Holley A (eds) Olfaction, taste, and cognition. Cambridge Univ. Press 3, pp. 27–43" href="/article/10.1007/s10055-006-0040-8#ref-CR69" id="ref-link-section-d93733e781">2002</a>), the sense of smell has specific characteristics: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(a)</span>
                        
                          <p>Olfaction provides us simple “nominal” data about the presence of qualitatively different odors in our surroundings. Olfaction combines good absolute sensitivity with excellent quality discrimination that enables us to distinguish almost any two odorous compounds. Moreover, odor quality discrimination is much more important than identification. At the same time, olfaction has rather poor intensity discrimination (around 20%). When compared to intensity estimation, qualitative description of an odorant is a very difficult task. It is estimated that humans have the ability to discriminate up to 10,000 different odorants, though most of us only experience a fraction of these in our lifetime. Various schemes, such as Henning’s odor prism (flowery, putrid, fruity, spicy, burned and resinous), have been proposed in the past in an attempt to classify odors into a small number of dimensions (Engen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Engen T (1982) The perception of odors. Academic Press, New York" href="/article/10.1007/s10055-006-0040-8#ref-CR37" id="ref-link-section-d93733e794">1982</a>; Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e797">2001</a>). Current approaches employ odor profiling techniques, in which a large number of verbal descriptors are used to describe individual odors (Schiffman and Pearce <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Schiffman S, Pearce T (2002) Introduction to olfaction: perception, anatomy, physiology, and molecular biology. In: Pearce T, Schiffman S, Nagle H, Gardner JW (eds) Handbook of machine olfaction: Electronic Nose Technology. Wiley-VCH" href="/article/10.1007/s10055-006-0040-8#ref-CR94" id="ref-link-section-d93733e800">2002</a>; Harel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Harel D, Carmel L, Lancet D (2003) Towards an odor communication system. Comput Biol Chem 27:121–133" href="/article/10.1007/s10055-006-0040-8#ref-CR50" id="ref-link-section-d93733e803">2003</a>);</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(b)</span>
                        
                          <p>Olfaction is a “near” sense. This means that all information about the quality of an object is contained in the molecules that make direct contact with the nose’s receptors;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(c)</span>
                        
                          <p>Olfaction is a “hidden sense”, indeed, unlike vision it is seldom the focus of attention. Previous studies have shown that odors, in some cases, can influence performance on vigilance tasks (Warm et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Warm J, Dember W, Parasuraman R (1990) Effects on fragrance on vigilance, performance and stress. Perfumer and Flavorist 15:15–18" href="/article/10.1007/s10055-006-0040-8#ref-CR111" id="ref-link-section-d93733e827">1990</a>), and mathematical tasks (Baron <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Baron R (1990) Environmentally induced positive effects: its impact on self-efficacy, task performance, negotiation and conflict. J Appl Soc Psychol 20:368–384" href="/article/10.1007/s10055-006-0040-8#ref-CR7" id="ref-link-section-d93733e830">1990</a>), even when the subjects are unaware of the presence of the odor (Degel and Köster <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Degel J, Köster E (1999) Odors: implicit memory and performance effects. Chem Senses 26:267–280" href="/article/10.1007/s10055-006-0040-8#ref-CR33" id="ref-link-section-d93733e833">1999</a>);</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(d)</span>
                        
                          <p>Sense of smell is an “associative” and “emotional” sense. It has developed to sense information about the world around us. Scents are extremely evocative, and can shift attention, add novelty, enhance mental state. The hedonic tone is a qualitative property related to the pleasantness of an odorant. It is highly subjective, and is influenced by cultural factors and emotional associations (Wilson and Stevenson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wilson D, Stevenson R (2003) Olfactory perceptual learning: the critical role of memory in odor discrimination. Neurosci Biobehav Rev 27:307–328" href="/article/10.1007/s10055-006-0040-8#ref-CR117" id="ref-link-section-d93733e847">2003</a>);</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(e)</span>
                        
                          <p>Olfaction has a “special memory”. Herz and Eich (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Herz R, Eich E (1995) Commentary and Envoi. In: Schab F, Crowder R (Eds) Memory for odors. Lawrence Erlbaum Associates, Mahwah, New Jersey, pp159–175" href="/article/10.1007/s10055-006-0040-8#ref-CR54" id="ref-link-section-d93733e861">1995</a>) presented a number of arguments to support the view that olfactory memory is different from verbal memory, it is episodic and not semantic in nature. Previous work indicates that, contrary to popular belief, odorants are not more effective than other sensory stimuli at recalling purely factual information (Herz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Herz R (1998) Are odors the best cues to memory? A cross-modal comparison of associative memory stimuli. Annals of the New York Academy of Sciences 855; 670–674" href="/article/10.1007/s10055-006-0040-8#ref-CR56" id="ref-link-section-d93733e864">1998</a>). Forgetting seems to follow a different time course for olfaction than for verbal memory. A few odors that are remembered, seem to linger on in memory almost forever, whereas words are gradually forgotten completely. Smell-related memories have a stronger emotional content than those triggered by other sensory modalities. Olfactory memory is considered to have reliable qualities, commonly known as “Proustian characteristics” which include resistance to interference, uniqueness, and independence from other modalities (Herz and Engen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Herz R, Engen T (1996) Odor memory: review and analysis. Psychon Bull Rev 3:300–313" href="/article/10.1007/s10055-006-0040-8#ref-CR55" id="ref-link-section-d93733e867">1996</a>; Annett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Annett J (1996) Olfactory memory: a case study in cognitive psychology. J. Psychol 130(3):309–319" href="/article/10.1007/s10055-006-0040-8#ref-CR3" id="ref-link-section-d93733e870">1996</a>; Danthiir et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Danthiir V, Roberts R, Pallier G, Stankov L (2001) What the nose knows: olfaction and cognitive abilities. Intelligence 29(4):337–361" href="/article/10.1007/s10055-006-0040-8#ref-CR30" id="ref-link-section-d93733e873">2001</a>).</p>
                        
                      </li>
                    </ol><p>Additionally, odors have been found to activate the cerebellum, which is involved in motor learning. Further research is needed to test the impact of olfaction on tasks that already involve dual-modalities. It is unclear whether olfaction would increase interference with either the visual/spatial or verbal/auditory modalities commonly used during training. According to Youngblut et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Youngblut C, Johnson R, Nash S, Weinclaw R, Will C (1996) Review of virtual environment interface technology IDA paper P-3186 8:209–216" href="/article/10.1007/s10055-006-0040-8#ref-CR122" id="ref-link-section-d93733e880">1996</a>), “odors can be used to manipulate mood, increase vigilance, decrease stress, and improve retention and recall of learned materials.” Moreover, the roles of olfaction in learning and memory have been found to be useful in training people in various areas. Examples include the recognition of chemical reactions by olfaction rather than sight and for increasing recall, recognition, attention, performance, and productivity (Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e883">2001</a>). Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e886">2001</a>) views olfactory cues as iconic representations. These odor cues may be semantically related to the represented object, such as the smell of burnt rubber in a car-racing VR game, or they could be related only in an abstract manner, such as using a puff of scent to remind the user to attend a meeting. Similarly, scent can also be used to convey representative pieces of information about a status, situation or event.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Olfactory channel in VR</h4><p>The grandfather of all virtual reality experiences using olfactory channel is Heilig’s Sensorama (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1962" title="Heilig M (1962) US Patent 3,050,870 Sensorama stimulator. August 28" href="/article/10.1007/s10055-006-0040-8#ref-CR53" id="ref-link-section-d93733e897">1962</a>). About 30 years later, Barfield and Danas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Barfield W, Danas E (1996) Comments on the use of olfactory displays for virtual environments. Presence: Teleoperators and Virtual Environments 5(1):109–121" href="/article/10.1007/s10055-006-0040-8#ref-CR6" id="ref-link-section-d93733e900">1996</a>) reviewed the physiological and psychological aspects of olfaction in the context of VEs, discussed mechanisms for the presentation of odorants, and enumerated a number of olfactory analogs to visual parameters, such as field of smell, the great variety of smells, and spatial resolution.</p><p>Since the late 90’s, the olfactory channel has been brought into use in VR-related experiments. Potential applications of olfaction in VR are numerous: high-end games and entertainment (Digital Tech Frontier LLC homepage: <a href="http://www.hightechentertainment.com">http://www.hightechentertainment.com</a>; Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e913">2001</a>; ChangHoon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="ChangHoon P, Heedong K, Ig-Jae K, Sang Chul A, Yong-Moo K, Hyoung-Gon K (2002) The making of Kyongju VR theatre. Proc. IEEE virtual reality conf. 2002 (VRC’02); 269–273" href="/article/10.1007/s10055-006-0040-8#ref-CR25" id="ref-link-section-d93733e916">2002</a>), training (Cater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781" href="/article/10.1007/s10055-006-0040-8#ref-CR24" id="ref-link-section-d93733e919">1994</a>; Washburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3" href="/article/10.1007/s10055-006-0040-8#ref-CR112" id="ref-link-section-d93733e922">2003</a>), medicine and tele-surgery (Keller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Keller P, Kouzes R, Kangas L, Hashem S (1995) Transmission of olfactory information for telemedicine, In: Morgan K, Satava R, Sieburg H, Matteus R, Christensen J. (eds) Interactive technology and the new paradigm for healthcare. IOS Press and Ohmsha, Amsterdam, pp 168–172" href="/article/10.1007/s10055-006-0040-8#ref-CR67" id="ref-link-section-d93733e926">1995</a>), rehabilitation and therapy (Rizzo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rizzo A (2005) Development of a virtual reality therapy application for Iraq war veterans with PTSD, virtual reality. Associated technologies and rehabilitation, Three-day symposium, University of Haifa, Israel, March 7–9" href="/article/10.1007/s10055-006-0040-8#ref-CR86" id="ref-link-section-d93733e929">2005</a>; Rizzo and Jounghyun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rizzo A, Jounghyun A (2005) SWOT analysis of the field of virtual reality rehabilitation and therapy. Presence: Teleoperators and Virtual Environments 14(2):119–146" href="/article/10.1007/s10055-006-0040-8#ref-CR87" id="ref-link-section-d93733e932">2005</a>) and also learning and education (Nice-smelling Interactive Multimedia Alphabet” project homepage: <a href="http://www.exhalia.com/">http://www.exhalia.com/</a>; Tijou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. proceedings series: LNCS, Vol. 3942, Pan, Z. et al, (eds), Technologies for e-learning and digital entertainment, first international conference, Edutainment 2006, Hangzhou, China; 1223–33" href="/article/10.1007/s10055-006-0040-8#ref-CR105" id="ref-link-section-d93733e942">2006a</a>).</p><p>In 2003, we developed the DIODE project that enables olfactory feedback (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig1">1</a>a) (Papin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Papin J.-P, Bouallagui M, Ouali A, Richard P, Tijou A, Poisson P, Bartoli W (2003) DIODE: Smell-diffusion in real and virtual environments. Proc. 5th Int. Conf. on virtual reality (VRIC’03). Laval, France, May; 113–117" href="/article/10.1007/s10055-006-0040-8#ref-CR81" id="ref-link-section-d93733e951">2003</a>). The user, wearing a HMD, navigates through the Vendôme Square in Paris. Three nice-smelling entities are placed in the environment: a Christmas tree, a rose and an orange tree, in the center of olfactive “spheres” (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig1">1</a>b). As the user penetrates inside a sphere, the smell corresponding to the entity is generated by olfactory display together with a visual representation of smelling particles.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>DIODE Project : experimental configuration (<b>a</b>); and snapshot of Vendôme square VE with an olfactive sphere (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>A more recent entertainment application called “Fragra” (“Fragra” project homepage: <a href="http://www.chihara.aist-nara.ac.jp/ivrc2003/index.html">http://www.chihara.aist-nara.ac.jp/ivrc2003/index.html</a>) uses either a head wearable or an arm-mounted interactive OD, which has been developed at Nara Institute of Advanced Science and Technology (Mochizuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mochizuki A, Amada T, Sawa S, Takeda T, Motoyashiki S, Kohyama K, Imura M, Chihara K (2004) An olfactory display device linked with human gesture. Proc. SCI’04, Kyoto, Japan 48(6004):531–532" href="/article/10.1007/s10055-006-0040-8#ref-CR75" id="ref-link-section-d93733e992">2004</a>). The user has to reach for, grasp and bring near his face a fruit in order to smell it.</p><p>Other researchers speculated on the utility of olfactory information in the field of training applications. At the Deep Immersion Virtual Environment Laboratory, Cater (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781" href="/article/10.1007/s10055-006-0040-8#ref-CR24" id="ref-link-section-d93733e999">1994</a>) has built a backpack-mounted fire-fighter training device, with scents delivered through the oxygen mask that is standard fire-fighter equipment. More recently, the Institute for Creative Technologies (ICT) at the University of Southern California has been involved in several projects exploring the usefulness of olfaction in VR training military applications. Washburn and Jones recommended using a scent to complement the other modality cues in such experiments (Washburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3" href="/article/10.1007/s10055-006-0040-8#ref-CR112" id="ref-link-section-d93733e1002">2003</a>). The AnthroTronix project, in collaboration with ICT, explores the feasibility and usefulness of an immersive VE that includes a scent necklace (AnthroTronix homepage : <a href="http://www.anthrotronix.com">http://www.anthrotronix.com</a>) (Washburn and Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Washburn D, Jones L (2004) Could olfactory displays improve data visualization?. Computing in science and engineering, Nov-Dec; 80–83" href="/article/10.1007/s10055-006-0040-8#ref-CR113" id="ref-link-section-d93733e1012">2004</a>). Another project is planned to help veterans come to terms with what they have experienced in places like Iraq and Afghanistan. The idea is to re-introduce the patients into the experiences that triggered the trauma, gradually, by immersing them in the sights, sounds and smells, until the memory no longer incapacitates them (Rizzo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rizzo A (2005) Development of a virtual reality therapy application for Iraq war veterans with PTSD, virtual reality. Associated technologies and rehabilitation, Three-day symposium, University of Haifa, Israel, March 7–9" href="/article/10.1007/s10055-006-0040-8#ref-CR86" id="ref-link-section-d93733e1015">2005</a>; Rizzo and Jounghyun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rizzo A, Jounghyun A (2005) SWOT analysis of the field of virtual reality rehabilitation and therapy. Presence: Teleoperators and Virtual Environments 14(2):119–146" href="/article/10.1007/s10055-006-0040-8#ref-CR87" id="ref-link-section-d93733e1019">2005</a>).</p><h3 class="c-article__sub-heading" id="Sec9">Olfactory displays (ODs)</h3><p>Currently, there is a large number of companies involved in the production of olfactory displays (ODs) that emit scent, under some variety of computer control (Osmooze homepage: <a href="http://www.osmooze.com/">http://www.osmooze.com/</a>; AromaJet homepage: <a href="http://www.aromajet.com">http://www.aromajet.com</a>; TriSenx homepage: <a href="http://www.trisenx.com">http://www.trisenx.com</a>; ScentAir Technologies homepage: <a href="http://www.scentair.com">http://www.scentair.com</a>; Ruetz Technologies homepage: <a href="http://www.ruetz.de">http://www.ruetz.de</a>). Furthermore, there are a few recent prototypes developed in an academic setting (Mochizuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mochizuki A, Amada T, Sawa S, Takeda T, Motoyashiki S, Kohyama K, Imura M, Chihara K (2004) An olfactory display device linked with human gesture. Proc. SCI’04, Kyoto, Japan 48(6004):531–532" href="/article/10.1007/s10055-006-0040-8#ref-CR75" id="ref-link-section-d93733e1067">2004</a>; Washburn and Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Washburn D, Jones L (2004) Could olfactory displays improve data visualization?. Computing in science and engineering, Nov-Dec; 80–83" href="/article/10.1007/s10055-006-0040-8#ref-CR113" id="ref-link-section-d93733e1070">2004</a>; Yanagida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC’04) Chicago, March; 43–50" href="/article/10.1007/s10055-006-0040-8#ref-CR121" id="ref-link-section-d93733e1073">2004</a>). Additional systems are reviewed by Kaye (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e1076">2001</a>) and Fuchs et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006c" title="Fuchs P, Papin J. –P, Richard P, Tijou A (2006c) Les interfaces olfactives, In: Fuchs P, Moreau G (eds) Le Traité de Réalité Virtuelle, Vol. 2: L’interfaçage, l’immersion et l’interaction en environnement virtuel, Presse de l’Ecole des Mines 4 11" href="/article/10.1007/s10055-006-0040-8#ref-CR44" id="ref-link-section-d93733e1079">2006c</a>).</p><p>ODs are concerned with synthesizing odorants from a digital description. In its most general form, an OD consists of a palette of odorants, a flow delivery system and a control algorithm that determines the mixing ratios, concentration and timing of the stimulus (Gutierrez-Osuna <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Gutierrez-Osuna R (2004) Olfactory Interaction, In: Bainbridge W (ed) Encyclopedia of human-computer interaction. Berkshire Pub, pp. 507–511" href="/article/10.1007/s10055-006-0040-8#ref-CR49" id="ref-link-section-d93733e1085">2004</a>). Odorants can be stored in the liquid phase and released using inkjet printer technology, or microencapsulated in gels, as in the “scratch-n-sniff” cards, and released thermally or mechanically (Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e1088">2001</a>). Once released, odorants can be dispersed using a general air ventilation system, or delivered locally with a whiffer or head-mounted gear or a scent necklace (Cater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781" href="/article/10.1007/s10055-006-0040-8#ref-CR24" id="ref-link-section-d93733e1091">1994</a>; Washburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3" href="/article/10.1007/s10055-006-0040-8#ref-CR112" id="ref-link-section-d93733e1094">2003</a>; Mochizuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mochizuki A, Amada T, Sawa S, Takeda T, Motoyashiki S, Kohyama K, Imura M, Chihara K (2004) An olfactory display device linked with human gesture. Proc. SCI’04, Kyoto, Japan 48(6004):531–532" href="/article/10.1007/s10055-006-0040-8#ref-CR75" id="ref-link-section-d93733e1097">2004</a>).</p><p>However, there are many difficult problems to make interactive olfactory displays that can be efficiently used in VR applications (AnthroTronix homepage : <a href="http://www.anthrotronix.com">http://www.anthrotronix.com</a>). So far, the major problem with emitting scents has been that the odor diffuses to a wide area, and it does not dissipate quickly. Careful attention must also be paid to the issue of hypersensitivity to scents. A good understanding of the primary determinants of odor quality in single odorants and complex mixtures is required before a perceptually accurate stimulus can be generated from odorant palettes. The effects of various odorant delivery parameters (e.g., concentration, duration, frequency, flow rate) need to be thoroughly characterized (Barfield and Danas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Barfield W, Danas E (1996) Comments on the use of olfactory displays for virtual environments. Presence: Teleoperators and Virtual Environments 5(1):109–121" href="/article/10.1007/s10055-006-0040-8#ref-CR6" id="ref-link-section-d93733e1110">1996</a>; Kaye <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" href="/article/10.1007/s10055-006-0040-8#ref-CR66" id="ref-link-section-d93733e1113">2001</a>; Tijou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Tijou A, Richard P, Papin J. –P (2006b) Diffusion d’odeurs dans les environnements virtuels: étude préliminaire, IEEE Conf. Int. Francophone d’Automatique (CIFA’06), 30 mai-01 juin, Bordeaux, France" href="/article/10.1007/s10055-006-0040-8#ref-CR106" id="ref-link-section-d93733e1116">2006b</a>).</p><p>Some major theme parks have added olfaction to their attractions by using the theater seats to emit scents. Similar olfactory-seated systems are available from Digital Tech Frontier (Digital Tech Frontier LLC homepage: <a href="http://www.hightechentertainment.com">http://www.hightechentertainment.com</a>). ChangHoon et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="ChangHoon P, Heedong K, Ig-Jae K, Sang Chul A, Yong-Moo K, Hyoung-Gon K (2002) The making of Kyongju VR theatre. Proc. IEEE virtual reality conf. 2002 (VRC’02); 269–273" href="/article/10.1007/s10055-006-0040-8#ref-CR25" id="ref-link-section-d93733e1129">2002</a>) have built a VR theatre for the Kyongju World Culture EXPO 2000. For ventilation and aroma generation, air ducts are installed underneath the floor. The aroma generator has five containers for liquid fragrances and its controller is connected to an ONYX2 computer in the control room so that the computer can select the type of fragrance and controls the release time.</p><p>Because of their small size and individual nature, personalized ODs are preferred in training and memory performance VR applications. Dissipation and control of odors are much easier and they can be head-mounted or otherwise worn by the trainee (Cater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781" href="/article/10.1007/s10055-006-0040-8#ref-CR24" id="ref-link-section-d93733e1136">1994</a>; Washburn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3" href="/article/10.1007/s10055-006-0040-8#ref-CR112" id="ref-link-section-d93733e1139">2003</a>). Fragrance Technologies collaborated with Cater to provide a backpack-mounted fire-fighter training device, with scents delivered through the oxygen mask which is standard fire-fighter equipment (Cater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781" href="/article/10.1007/s10055-006-0040-8#ref-CR24" id="ref-link-section-d93733e1142">1994</a>).</p><p>Some commercial devices are already available for this purpose such as the ScentKiosk Scent Dispenser that dispenses precise fragrance volumes direct to the user’s nose via a tube (ScentAir Technologies homepage: <a href="http://www.scentair.com">http://www.scentair.com</a>). The Sniffman<sup>®</sup> OD worked out by a German company Ruetz Technologies contains tiny alveoli (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig2">2</a>a). During the precise distribution of an image, the plug-in controlling the synchronization of the smell with the image sends its commands by radio signal: a droplet of precise flavor contained in the diffuser is released on a heating plate which then diffuses a tiny quantity of smell. The plug-in also drives the intensity of smell (Ruetz Technologies homepage: <a href="http://www.ruetz.de">http://www.ruetz.de</a>). AromaJet developed a prototype aroma-dispensing device. Users could wear or place a small device called Pinoke<sup>®</sup> in front of a monitor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig2">2</a>b) (AromaJet homepage: <a href="http://www.aromajet.com">http://www.aromajet.com</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Wearable ODs: Sniffman<sup>®</sup> from Ruetz Technologies (<b>a</b>); Pinoke<sup>®</sup> from AromaJet (<b>b</b>); Desktop ODs personalized SpotScent III from Yanagida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC’04) Chicago, March; 43–50" href="/article/10.1007/s10055-006-0040-8#ref-CR121" id="ref-link-section-d93733e1200">2004</a> (c); and Scent Dome<sup>®</sup> from Trisenx (<b>d</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>From the Japanese ATR Media Information Science Laboratories, Yanagida et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC’04) Chicago, March; 43–50" href="/article/10.1007/s10055-006-0040-8#ref-CR121" id="ref-link-section-d93733e1221">2004</a>) developed a projection-based olfactory display: the SpotScents system that uses scent projectors to deliver localized odors to human nose through free space by incorporating nose-tracking function. A scent projector is composed of an air cannon mounted on a pan-tilt platform. The air cannon launches vortex rings that can travel several meters. Different scents can be delivered within a short time frame without air conditioning equipment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig2">2</a>c) (Yanagida et al <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC’04) Chicago, March; 43–50" href="/article/10.1007/s10055-006-0040-8#ref-CR121" id="ref-link-section-d93733e1227">2004</a>).</p><p>The Scent Dome<sup>®</sup> device is small and has 20 different scents in its scent cartridge (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig2">2</a>d). Users can mix the scents in any amount to create different odors. The scents disperse from the top of the scent cartridge via a small fan, but the company does not have a way to direct the scent to the user for data visualization and VR applications (TriSenx homepage: <a href="http://www.trisenx.com">http://www.trisenx.com</a>).</p><h3 class="c-article__sub-heading" id="Sec10">VR applications using olfaction in learning science</h3><p>So far, in the field of education, VR applications including olfactory feedback have been few. The “Nice-smelling Interactive Multimedia Alphabet” project developed by the Souris Grise Company, under the specifications of France Telecom R&amp;D is a playful and educational application. The originality consists in the association of three sensorial modalities (vision, olfaction and sound) for the learning of letters of the alphabet and reading “Nice-smelling Interactive Multimedia Alphabet” project homepage: <a href="http://www.exhalia.com/">http://www.exhalia.com/</a>).</p><p>In our laboratory, we are currently investigating and developing the olfactory feedback channel, in several VR applications (Papin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Papin J.-P, Bouallagui M, Ouali A, Richard P, Tijou A, Poisson P, Bartoli W (2003) DIODE: Smell-diffusion in real and virtual environments. Proc. 5th Int. Conf. on virtual reality (VRIC’03). Laval, France, May; 113–117" href="/article/10.1007/s10055-006-0040-8#ref-CR81" id="ref-link-section-d93733e1263">2003</a>; Fuchs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006c" title="Fuchs P, Papin J. –P, Richard P, Tijou A (2006c) Les interfaces olfactives, In: Fuchs P, Moreau G (eds) Le Traité de Réalité Virtuelle, Vol. 2: L’interfaçage, l’immersion et l’interaction en environnement virtuel, Presse de l’Ecole des Mines 4 11" href="/article/10.1007/s10055-006-0040-8#ref-CR44" id="ref-link-section-d93733e1266">2006c</a>; Richard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Richard P, Allain P, Richard E, Le Gall D (2006a) Projet EVACOG–Environnements Virtuels Appliqués aux Sciences Cognitives. Handicap 2006, Proc. 4th Conf. &#34;Nouvelles Technologies au service de l'homme&#34;, Handicap 2006, Paris, France, 7–9 June 2006, pp 233–239" href="/article/10.1007/s10055-006-0040-8#ref-CR83" id="ref-link-section-d93733e1269">2006a</a>). For example, the “Olfactive Molecules” educational application is designed to investigate the effect of olfaction on learning, retention, and recall of complex 3D structures such as organic molecules. In particular, students will be able to explore the composition, 3D chemical structures, and stereochemistry of a few organic molecules using their senses of vision and olfaction according to different interaction techniques (Tijou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. proceedings series: LNCS, Vol. 3942, Pan, Z. et al, (eds), Technologies for e-learning and digital entertainment, first international conference, Edutainment 2006, Hangzhou, China; 1223–33" href="/article/10.1007/s10055-006-0040-8#ref-CR105" id="ref-link-section-d93733e1272">2006a</a>).</p><p>In the next section, we describe our human-scale multi-modal VE called VIREPSE that provides haptic interaction using the string-based interface SPIDAR, olfactory and auditory feedbacks. To our knowledge, this is, to date, the first virtual environment providing human-scale haptics coupled with olfactive feedback and used for educational applications.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">VIREPSE multi-modal virtual environment</h2><div class="c-article-section__content" id="Sec11-content"><p>The developed human-scale VE is based on the SPIDAR-H. This human-scale interface provides haptic sensation to both hands and displays various aspects of force feedback associated mainly with contact, weight, and inertia of manipulated objects. Most existing VEs do not provide the human operator with large-scale bimanual haptic feedback. Stereoscopic images are displayed on a rear-projected large screen (2 m × 2.5 m) and are viewed using polarized glasses. A 5.1 immersive sound system and olfactory displays are used for simulation realism, sensorial feedback and immersion.</p><h3 class="c-article__sub-heading" id="Sec12">Human-scale haptic interface</h3><p>In order to provide force feedback to both hands, a total of eight motors (1–8) are placed on non-adjacent corners of a cubic frame surrounding the user (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig3">3</a>a). Motors 2–4–5–7 are used to display force on user’s right hand while motors 1–3–6–8 are used to display force on user’s left hand. By controlling the tension and length of each string, the system generates appropriate force for both hands. The hand attachments are soft, so there is no risk of the user hurting himself (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig3">3</a>b). One end of the hand attachment is wrapped around a pulley driven by a DC motor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig3">3</a>c). Because it is a wire-based system, it has a transparent property so that the user can easily see the virtual world. It also provides a space where the user can freely move around. The system allows the user to manipulate virtual objects and to naturally convey objects’ physical properties to the user’s body.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Diagrams of the human-scale string-based haptic interface (<b>a</b>); hand attachments for both hands (<b>b</b>); and pulley, motor, encoder (<b>c</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">System workspace</h4><p>The workspace refers to the minimum space that contains all the reachable positions of the hand attachment. In the present case, the workspace could be divided into two spaces: reachable space that gathers every point users can reach with their hands (Tarrin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Tarrin N, Coquillart S, Hasegawa S, Bouguila L, Sato M. (2003) The stringed haptic workbench: a new haptic workbench solution. EuroGraphics’03 22; 3" href="/article/10.1007/s10055-006-0040-8#ref-CR104" id="ref-link-section-d93733e1336">2003</a>) and haptic space that gathers every point where the system can produce a force in any direction.</p><p>The global workspace is defined by the intersection of these two spaces. The workspace of the reachable space matches the cubic frame of the SPIDAR (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig3">3</a>a). Haptic spaces are illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig4">4</a>a (right hand) and Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig4">4</a>b (left hand).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Haptic space for right hand (<b>a</b>); and left hand (<b>b</b>); position measurement and resultant force (<b>c</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Position measurement</h4><p>Let the coordinates of the hand attachment position be <i>P(x,y,z)</i>, which represent both the hand position, and, <i>l</i>
                              <sub>
                      <i>i</i>
                    </sub>, the length of the <i>i</i>
                              <sup><i>th</i></sup> string (<i>i</i> = 0,1,2,3). To simplify the problem, let the four actuators (motor, pulley, encoder) be on four not adjacent vertices of the cubic frame, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig4">4</a>. Then <i>P(x,y,z)</i> must satisfy equations (1,2,3 and 4). After some mathematical manipulations, we can obtain the position of a hand attachment as the following equation, in function of the length <i>l</i>
                              <sub>
                      <i>i</i>
                    </sub>: </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ l_{0} ^{2} = (x + a)^{2} + (y + a)^{2} + (z + a)^{2} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                              <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ l_{1} ^{2} = (x - a)^{2} + (y - a)^{2} + (z + a)^{2} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                              <div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ l_{2} ^{2} = (x - a)^{2} + (y + a)^{2} + (z - a)^{2} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                              <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ l_{3} ^{2} = (x + a)^{2} + (y - a)^{2} + (z - a)^{2} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                              <div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \left\{ \begin{aligned}{} x &amp; = \frac{{(l_{0} ^{2} - l_{1} ^{2} - l_{2} ^{2} + l_{3} ^{2} )}} {{8a}} \\ y &amp; = \frac{{(l_{0} ^{2} - l_{1} ^{2} + l_{2} ^{2} - l_{3} ^{2} )}} {{8a}} \\ z &amp; = \frac{{(l_{0} ^{2} + l_{1} ^{2} - l_{2} ^{2} - l_{3} ^{2} )}} {{8a}} \\ \end{aligned} \right. $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>The length of the cube is 2<i>a</i>. The coordinates’ origin<i> i</i> is set at the center of the framework. The position measurement ranges for all x, y, and z within [−1.25 m, +1.25 m]. Inside the position measurement range, the absolute static position measurement error is less than 0.6 % of the workspace. The bandwidth is about 10 KHz. The maximum exerted force is 30 N. As discussed in Richard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Richard P, Chamaret D, Inglese F-X, Lucidarme P, Ferrier J-L (2006b) Human-scale haptic virtual environment for product design: effect of sensory substitution. Int J Virtual Real (in press)" href="/article/10.1007/s10055-006-0040-8#ref-CR124" id="ref-link-section-d93733e1498">2006b</a>), the maximum force that could be exerted in any direction depends to a great extent on the user’s hand position within the haptic workspace.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Force control</h4><p>The system uses the resultant force of tension from strings to provide force display. The hand attachment is suspended by four strings, giving certain tensions to each of them by means of motor. The resultant force occurs at the position of the hand attachment, where it is transmitted to and felt by the operator’s hand.</p><p>Let the resultant force be 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{f} \)</span> and unit vector of the tension be 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{u}_{i} \)</span> (<i>i</i> = 0,1,2,3), the resultant force is given by equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0040-8#Equ6">6</a>)</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \ifmmode\expandafter\vec\else\expandafter\vec\fi{f} = {\sum\limits_{i = 0}^3 {k_{i} \ifmmode\expandafter\vec\else\expandafter\vec\fi{u}_{i} \quad (\left. {k_{i} } \right &gt; 0)} } $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>k</i>
                              <sub>
                      <i>i</i>
                    </sub> represents the tension value of each string. By controlling all of the <i>k</i>
                              <sub>
                      <i>i</i>
                    </sub>, a resultant force in any direction can be composed.</p><h3 class="c-article__sub-heading" id="Sec16">Olfactory displays (ODs)</h3><p>Our human scale multi-modal VE provides smell diffusion using either the ONYX™ OD from Osmooze (Osmooze homepage: <a href="http://www.osmooze.com/">http://www.osmooze.com/</a>) or the LISA OD (LOD). Both devices are controlled using either C/C++ program or Virtools dev. 3.0 software through a USB interface. They are cold displays based on a fan ventilation system to disperse odorant molecules that are stored in a liquid state and can be composed of synthetic products as well as of natural extracts (essential oils). The range marketed by available smells is vast and varied: fruits (apple, pear, orange, lemon, strawberry, etc.), spices (thyme, rosemary, basil, cinnamon, vanillin, etc.), flowers (pink, jasmine, lily of the valley, etc.), vegetables (broccoli, carrot, etc.), and other original scents (ocean, etc.)</p><p>The ONYX™ olfactory display is a nebulization device that throws in the air, by using a mini-pump with piezoelectric resonators, olfactive liquid in the form of droplets. This mini-pump is simple and contains only a metal capillary and a resonator set in vibration by the signals resulting from an electronic control card (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig5">5</a>a). The device contains four mini-pumps that allow giving out several smells alternately or simultaneously. The mini-pump outflow is sufficient to emit a small quantity of nice-smelling concentrated substance (from one up to ten micrograms) to create an olfactive image near the user’s nose. Larger quantities can be easily emitted, according to the application, for example, to fill a large volume. This process restores with fidelity the smells and there is no risk of toxicity and saturation of the smell. Diffusion characteristics of the ONYX™ OD are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig5">5</a>b.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>ONYX™ olfactory display (<b>a</b>); and plots versus time of (<b>b</b>) the impulses of functioning of the mini-pump (plot 1), the activation of the stream of ventilation during the duration of the olfactive image (plot 2), the olfactive molecules concentration in air (plot 3), and the olfactive perception intensity level (plot 4)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The LOD is a fan-based prototype system that can provide up to four scents simultaneously (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig6">6</a>a). It allows displaying directional olfactive images and is located at 50 cm from the user.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Illustration of the ODs used in desktop configuration: LOD (<b>a</b>); Mad P@d™ (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>For desktop configuration, ODs used are either LOD or Mad P@d™ from Osmooze. This last one is based on the technology of gels and allows diffusion of up to six scents (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig6">6</a>b). Gels are conceived from essential oils and are stored in cartridges. They are dispersed by a light stream of air.</p><p>All of the described ODs are well adapted to real-time uses and easy to operate for educational applications that are devoted to a large number of students. Furthermore, the dispersal of olfactive molecules depends on the fan rotational velocity controlled in real-time according to events or user’s actions in VEs.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">HAPTIC ATOMIC application</h2><div class="c-article-section__content" id="Sec17-content"><h3 class="c-article__sub-heading" id="Sec18">Aim of the application</h3><p>The aim of this application is to allow students to experience the abstract concept of the Bohr atomic model, i.e., the quantization of the energy levels that is traditionally difficult for students to conceptualize. Students can explore the Bohr model, from the fundamental state <i>n </i>= 1 up to the <i>n </i>= 3 excited energy level and interactively visualize the corresponding atomic orbitals that describe the probability distribution of an electron around the nucleus, according to the Shrödinger model.</p><p>The present work tackles the haptical perception of the quantization of the orbitals’ state energy levels by means of different haptic feedback devices. Haptic-based interaction techniques range from pseudo-haptic illusion to human-scale haptic immersion. Students interact with the electron to make it jump from one allowed orbit to another. Concentric spheres surrounding the Bohr atom nucleus (proton) represent the different energy levels.</p><p>The Bohr model consists of four principles. (i) The neutrons and protons occupy a dense central region called the nucleus, and the electrons orbit the nucleus thanks to the attractive Coulomb force between the positively charged nucleus and the negatively charged electrons. Electrons assume only certain orbits around the nucleus. Each orbit has a discrete quantized energy associated with it (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig7">7</a>a). Orbits are stable and called “stationary” orbits. (ii) The laws of classical mechanics do not apply when electrons make the jump from one allowed orbit to another. (iii) When an electron makes a jump from one orbit to another, the energy difference is carried off (or supplied) by a single quantum of light, a photon. Electron transitions allowed when absorbing or emitting energy correspond exactly to the energy difference between the orbits (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig7">7</a>b). This energy may be calculated by dividing the product of the Planck’s constant<i> h</i> and the speed of light i.e., hc by the wavelength of the light. (iv) The allowed orbits depend on discrete values of orbital angular momentum <i>L</i>, according to the following equation where <i>n</i> = 1,2,3,... is the principal quantum number:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ L = n\hbar = n \cdot \frac{h} {{2\pi }}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>In the quantum model, atomic orbitals are three-dimensional mathematical functions that describe the probability distribution of an electron in an atom or molecule. Solution of the Schrödinger equation for the hydrogen atom produces a family of electron wave functions denoted <i>ψ(n,l,m).</i> These wave functions <i>ψ(n,l,m)</i> associated with each energy level E<sub>(n)</sub> depend on the four quantum numbers: n, l, m, and s according to the following equation:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \Psi _{{(n,l,m)}} = \psi (\text{r} ,\theta ,\varphi ) = \text{N}R_{{(n,l)}} (\text{r} )\text{Y} _{{(l,m)}} (\theta ,\varphi )\text{S} _{{(s)}} $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Bohr model of Hydrogen atom: energy levels (<i>E</i>
                                       <sub>(<i>n</i>)</sub> = 
<span class="mathjax-tex">\( \frac{{E_{{(1)}} }} {{n^{2} }} \)</span>en eV) and atomic radius (<i>r</i>
                                       <sub>(<i>n</i>)</sub> = 0,529n<sup>2</sup> en Å) (<b>a</b>); and electronic transition between two energy levels (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>with: <i>N</i>: Norm</p><p>
                           <i>R</i>
                           <sub>
                    <i>(n,l)</i>
                  </sub>
                           <i>(r)</i>: Radial function</p><p>
                           <i>Y</i>
                           <sub>(l,m)</sub>(θ, ϕ): Angular function</p><p>
                           <i>S</i>
                           <sub>
                              <i>(</i>s)</sub>: Electron spin function</p><p>The “Haptic Atomic” application allows linking of the two theoretical approaches (Bohr atom and quantum models) in the same user interface. The application was initially developed using 3D Studio Max 6.0 and Virtools Dev. 2.5 and 3.0 software. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig8">8</a> shows a snapshot of the user interface that allows the user to: (a) get some auditory information relative to the scientific data; (b) select the angular and magnetic quantum numbers (<i>l</i> and <i>m</i>) related to the actual energy state of the electron. Atomic orbitals <i>ψ(n,l,m)</i>, displayed as polygons or points were integrated from Orbital Viewer (Orbital Viewer homepage: <a href="http://www.orbitals.com/orb/ov.htm">http://www.orbitals.com/orb/ov.htm</a>); (c) choose a visual background; and (d) finally, select the haptic interface in the desktop configuration. The user can rotate and examine atomic orbitals using the mouse. A new version has been recently developed using the C/C++ language and OpenGL. In the immersive human-scale configuration, the user can operate the mouse cursor and interact with the menu by a camera-based interaction technique using pattern recognition.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Snapshot of the “Haptic Atomic” application user interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec19">Haptic interaction techniques</h3><p>Four different interaction techniques have been developed. These involve respectively the keyboard, the CadMan<sup>®</sup> motion controller (<a href="http://www.logitech.com">http://www.logitech.com</a>), the PHANToM<sup>®</sup> Omni™ (<a href="http://www.SensAble.com">http://www.SensAble.com</a>) and the SPIDAR haptic interface:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">(a)</span>
                      
                        <p>The first interaction technique allows the student to make the electron jump from one energy level to a higher one by pressing the space key. Each key press emulates an energy quantum and corresponds roughly to 1 eV. For example, to allow the electron jump from the fundamental level to the <i>n </i>= 2 level, the absorbed energy is 10.2 eV. So, in this case, ten key presses are required.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(b)</span>
                      
                        <p>The CadMan<sup>®</sup> motion controller is used to provide pseudo-haptic interaction (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig9">9</a>a). To allow the transition of the electron to higher levels, the user has to push on the upper part of the CadMan<sup>® </sup>up to the maximum and to maintain this position during a time interval proportional to the energy quantum (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig10">10</a>a). The electron gets back to the fundamental energy level as the student relaxes the effort on the device.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(c)</span>
                      
                        <p>The third interaction technique is based on the PHANToM<sup>®</sup> Omni™ (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig9">9</a>b). This interface allows the user to control the movements of the electron and to apply force feedback to the user’s hand depending on the proton-electron distance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig10">10</a>b).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(d)</span>
                      
                        <p>The last interaction technique, based on the SPIDAR system, is more immersive (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig9">9</a>c). Indeed, Bohr atoms and atomic orbitals are viewed stereoscopically on the large rear-projected screen. The proton nucleus is positioned on the center of the 3D frame of the SPIDAR. The radial force feedback is applied according to the plot illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig10">10</a>b.</p>
                      
                    </li>
                  </ol><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0040-8#Tab1">1</a> shows, for both haptic interaction techniques, the values of nucleus-electron distance according to the principal quantum number value. In the case of SPIDAR, the workspace is spherical and human-scaled. The third energy level <i>n </i>= 3 can be reached, as the electron is located in a 60 cm radius sphere (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig11">11</a>b).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Haptic interfaces used in “Haptic Atomic”application: CadMan<sup>®</sup> motion controller from 3DConnexion (<b>a</b>); PHANToM<sup>®</sup> Omnifrom Sensable Technologies (<b>b</b>); SPIDAR (<b>c</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Energy level versus duration of the applied maximum force on the CadMan<sup>®</sup> motion controller (<b>a</b>); Force feedback versus proton-electron distance for both PHANToM<sup>®</sup> Omni™ and SPIDAR (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Values of nucleus-electron distance according to n for PHANToM Omni and SPIDAR-H</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0040-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0040-8/MediaObjects/10055_2006_40_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>“Haptic Atomic” desktop configuration with PHANToM<sup>®</sup> Omni™ and Mad P@d™ (<b>a</b>); Immersive configuration with SPIDAR-H haptic display (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0040-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec20">Olfactory feedback</h3><p>In order to evaluate the effect of olfactory feedback on the learning process, on the information recall task, and on the capacity of mental associations, olfactory information may be added using fan-based ODs described in a previous section.</p><p>As a first approach, we propose to associate a specific scent with each energy level (<i>n </i>= 1 up to 3). Specific scents may also be associated with the second quantum number (l), i.e., with atomic orbital shapes along with the quantum model, whatever the energy level (<i>n</i>). Thus, the visualization of the ψ<sub>1s , </sub>ψ<sub>2s</sub> et ψ<sub>3s</sub> OA may be associated with a given odor (odor 1), then the visualization of the ψ<sub>2p </sub>and ψ<sub>3p </sub>OA may be associated with a second odor (odor 2) and finally, the visualization of the OA ψ<sub>3d </sub>may be coupled with a third odor (odor 3). Globally, in this case, a single odor will correspond to the fundamental energy level (<i>n </i>= 1), whereas two scents will correspond to the second energy level and three odors to the third energy level.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0040-8#Fig11">11</a> illustrates two VE experimental configurations of “Haptic Atomic” application including multi-modal interfaces.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Discussion</h2><div class="c-article-section__content" id="Sec21-content"><p>To date, evaluations of virtual learning environments have been limited by the size and focus of the research projects within which they have been based, and by the maturity of VEs. The majority of evaluation studies were informal and employed only one iteration of evaluation. Many of the early studies focused on the usability of the VEs, asking basic questions such as, “Can the VE be used by students?”, and which input devices are preferable? Many of the studies report attitudinal data to find out if the students enjoy using VEs to learn (Cobb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, In: Stanney M (eds) Handbook of virtual environments: design, implementation, and applications, Lawrence Erlbaum Associates, London 46; 911–936" href="/article/10.1007/s10055-006-0040-8#ref-CR27" id="ref-link-section-d93733e2278">2002</a>). One of the most relevant studies is that of Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning complex scientific concepts. In: Feurzeig W, Roberts N (eds) Modeling and simulation inscience and mathematics education. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-006-0040-8#ref-CR32" id="ref-link-section-d93733e2281">1999</a> who conducted experiments between Maxwell World and a similar 2-D software (EM Field). The Maxwell World group was found to develop more accurate and causal mental models than the EM group (Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning complex scientific concepts. In: Feurzeig W, Roberts N (eds) Modeling and simulation inscience and mathematics education. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-006-0040-8#ref-CR32" id="ref-link-section-d93733e2284">1999</a>).</p><p>Within the framework of our “Haptic Atomic” application, we describe different VE configurations that provide visual, auditory, haptic and olfactory feedback, that range from desktop to multi-modal human-scale VE.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0040-8#Tab2">2</a> illustrates the different VE configurations that we will investigate and compare by analyzing undergraduate student’s performance (cases marked with a circle). The student may interact with the Bohr atom model using four different interaction techniques ranging from desktop to human-scale haptic interaction. The other configurations are either difficult to exploit (i.e., SPIDAR in desktop configuration) or less interesting (i.e., keyboard and large screen display).
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Illustration of the different VE configurations that will be investigated (cases marked with circle) within the framework of “Haptic Atomic” application</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0040-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Using these configurations, the effect of key features such as immersion, haptic interaction techniques, interaction workspace, and sensory feedback on the learning process, the knowledge-building and recall of an abstract scientific concept as the Bohr atomic model may be investigated. Moreover, joint effects of these features could be also studied, such as human-scale haptic and olfactory feedback.</p><p>Recently, Sauer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sauer C, Hastings W, Okamura A (2004) Virtual environment for exploring atomic bonding. Proc. EuroHaptics’04, Munich, Germany 15:232–239" href="/article/10.1007/s10055-006-0040-8#ref-CR93" id="ref-link-section-d93733e2319">2004</a>) have created a program for the beginning user designed to teach the fundamentals of atoms and molecules in a virtual world. PHANToM<sup>®</sup> allows the user to experience a recreation of the forces that would be present during real world atomic interactions. A first order simulation of atomic interaction is described and an experiment has been designed to evaluate the benefits of haptic feedback in assembling molecules. Using haptics in the molecular model enables the user to feel the electrostatic forces between atoms and groups of atoms. The key information presented includes the strength of molecular bonding, properties of full and partially full valence shells in bonding, and an understanding of how these properties affect the geometric structure of the molecule. The results show that by providing force feedback, it is possible to decrease the amount of time required to create a simple molecule in a VE.</p><p>Many scientifically correct ideas are difficult to understand for students due to their complexity, abstract nature and/or contrast to common sense and experience. When difficult concepts and abstract ideas are involved, students need opportunities for direct observation and experimentation of the phenomena studied. The potential for learning may therefore be great as specific skills may be practised and observed from different viewpoints and information to be learned may be presented in meaningful and concrete ways (Cobb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, In: Stanney M (eds) Handbook of virtual environments: design, implementation, and applications, Lawrence Erlbaum Associates, London 46; 911–936" href="/article/10.1007/s10055-006-0040-8#ref-CR27" id="ref-link-section-d93733e2327">2002</a>). Constructivism has been discussed as a learning theory upon which to build an instructional technology for a number of years (Duffy and Jonassen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Duffy T, Jonassen D (1992) Constructivism; new implications for instructional technology. In: Duffy T, Jonassen D (eds) Constructivism and the technology of instruction. Lawrence Erlbaum Associates, New Jersey" href="/article/10.1007/s10055-006-0040-8#ref-CR35" id="ref-link-section-d93733e2330">1992</a>) and is the single most reported education theory supporting development of virtual environments for education (Youngblut <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US" href="/article/10.1007/s10055-006-0040-8#ref-CR123" id="ref-link-section-d93733e2333">1998</a>).</p><p>VR provides a new approach to learning as it increases the interest and provides in this way an alternative educational process (Winn <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HITL, Seattle, WA, technical publication: R-93–9" href="/article/10.1007/s10055-006-0040-8#ref-CR118" id="ref-link-section-d93733e2339">1993</a>; Byrne <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Byrne C (1996) Water on tap-the use of virtual reality as an educational tool, Ph. D. thesis, University of Washington" href="/article/10.1007/s10055-006-0040-8#ref-CR21" id="ref-link-section-d93733e2342">1996</a>; Emerson and Revere <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Emerson T, Revere D (1997) Virtual reality in training and education: resource guide to citations and online information, University of Washington, HITL, Seattle, WA, technical publication: B-94–1" href="/article/10.1007/s10055-006-0040-8#ref-CR36" id="ref-link-section-d93733e2345">1997</a>; Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific concepts. In: Jacobson MJ, Kozma RB (Eds) Learning the sciences of the 21st Century: research, design, and implementing advanced technology learning environments. Lawrence Erlbaum Associates, Hillsdale, NewJersey" href="/article/10.1007/s10055-006-0040-8#ref-CR31" id="ref-link-section-d93733e2348">1997</a>; Youngblut <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US" href="/article/10.1007/s10055-006-0040-8#ref-CR123" id="ref-link-section-d93733e2351">1998</a>; Roussos and Gillingham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 education, AERA Roundtable session at the American Educational Research Association annual meeting, San Diego, US, April" href="/article/10.1007/s10055-006-0040-8#ref-CR88" id="ref-link-section-d93733e2355">1998</a>; Mikropoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Mikropoulos T, Chalkidis A, Katsikis A, Emvalotis A (1998) Student’s attitudes towards educational virtual environments. Educ Inf Technol 3:137–148" href="/article/10.1007/s10055-006-0040-8#ref-CR73" id="ref-link-section-d93733e2358">1998</a>). Moreover, it allows educators to place their students into instructional environments which are difficult or impossible to achieve otherwise. It has been suggested that the main characteristics of VR which, when properly exploited, may lead to learning outcomes and do play an important role in knowledge construction (Winn <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HITL, Seattle, WA, technical publication: R-93–9" href="/article/10.1007/s10055-006-0040-8#ref-CR118" id="ref-link-section-d93733e2361">1993</a>). Winn proposed educational VEs for science teaching and learning, exploiting the following features (Winn and Windschitl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Winn W, Windschitl M (2001) Learning science in virtual environments: the interplay of theory and experience. Themes Educ 1(4):373–389" href="/article/10.1007/s10055-006-0040-8#ref-CR119" id="ref-link-section-d93733e2364">2001</a>):</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">(a)</span>
                    
                      <p>Interaction: manipulating virtual objects using controls similar to those of real objects, taking different viewpoints at will and hence operating with intuitive immediacy. Without meaningful object response to user actions, interaction is restricted to navigational tasks only.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(b)</span>
                    
                      <p>Size: ability to change VEs’ physical size, so the user can navigate and interact in macro- , micro- and even nano-worlds.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(c)</span>
                    
                      <p>Transduction: perception of non-perceptible signals such as ultrasound or infrared light.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(d)</span>
                    
                      <p>Reification: transformation of abstract ideas into perceptible representations</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(e)</span>
                    
                      <p>Autonomy: independence of the VE from the user’s actions. The laws of physics apply in space and time inside the VE.</p>
                    
                  </li>
                </ol><p>In the “Haptic Atomic” application, the student interacts with the electron to make it jump from one allowed orbit to another. The user haptically experiences the abstract concept of the quantization of the orbital state energy level using four different interaction techniques ranging from pseudo-haptic desktop to human-scale haptic interaction. Reification, in this case, refers to the abstract concept of the Bohr atom model and might lead students to understand that the electron is restricted to a discrete orbital level and that some external quantized quantity of energy is required to make it jump from the first energy level to a higher one.</p><p>Furthermore, we put forward the assumption of reification reinforcement through the association of a specific scent with each energy level linked to haptic feedback.</p><p>In a similar way, we make the hypothesis that a reification referring to the energy level degeneracy phenomenon and based on the association of scent with the secondary quantum number l, might support the conceptualization and shed light on the role of the second quantum number l in forming the orbital’s shape. The experiments carried out will allow us to test these hypotheses.</p><p>We have proposed four interaction techniques: the keyboard, the CadMan<sup>®</sup> motion controller, the PHANToM<sup>®</sup> Omni™ and the SPIDAR haptic interface.</p><p>The first one emulates energy quantum by pressing the space key. The last three interaction techniques provide haptic cues corresponding to the transition energy. In the case of the CadMan<sup>®</sup> motion controller, time interval proportional to the energy quantum is a metaphor for the required transition energy for jump. Moreover, both the PHANToM<sup>®</sup> Omni™ and SPIDAR devices respond by continuously updating force feedback to the user’s hand, creating a tangible representation of the electron-bounded state. This might allow students to understand that the electron is constrained in discrete orbitals and a given external quantity of energy is required to make it jump from an energy level to a higher one.</p><p>Moreover, we propose that human-scale haptic interaction (SPIDAR-H) leads to better acquisition and retention thanks to bigger physical and psychic/emotional implications of the user. Interacting with the SPIDAR-H interface allows the student to have a singular experience by directly interacting with the electron and by spending a certain physical energy to make it jump from an allowed orbit to another one.</p><p>Among human-scale haptic interfaces, SPIDAR-H is one of the most suited for education. It was initially designed to serve as “the next generation education system” (Cai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cai Y, Wang S, Sato M (1997) A human-scale direct motion instruction system device for education systems. The IEICE Transactions, E80-D 2:212–217" href="/article/10.1007/s10055-006-0040-8#ref-CR22" id="ref-link-section-d93733e2444">1997</a>). Stereoscopic visualization, the 5.1 immersive sound system and olfaction increase sensorial feedback and immersion. Immersion helps retention, by making important concepts more memorable, while also encouraging use of the learning environment as users are actually obliged to participate in order to receive feedback (Dede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific concepts. In: Jacobson MJ, Kozma RB (Eds) Learning the sciences of the 21st Century: research, design, and implementing advanced technology learning environments. Lawrence Erlbaum Associates, Hillsdale, NewJersey" href="/article/10.1007/s10055-006-0040-8#ref-CR31" id="ref-link-section-d93733e2447">1997</a>; Youngblut <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US" href="/article/10.1007/s10055-006-0040-8#ref-CR123" id="ref-link-section-d93733e2450">1998</a>; Roussos and Gillingham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 education, AERA Roundtable session at the American Educational Research Association annual meeting, San Diego, US, April" href="/article/10.1007/s10055-006-0040-8#ref-CR88" id="ref-link-section-d93733e2453">1998</a>).</p><p>There are issues to be considered for the development of effective VR educational applications. Some particularly interesting issues are related to user’s immersion and multi-modal feedback. Integrating many senses into a single display system increases the feeling of presence in the environment and both cognitive and sensory-motor performance.</p><p>Crison et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Crison F, Lecuyer A, Mellet d’Huart D, Burkhardt J. –M, Michel G, Dautin J. –L. (2005) How to use milling machines with multi-sensory feedback in virtual reality. Proc. IEEE virtual reality conf. 2005 (VRC’05); 139–146" href="/article/10.1007/s10055-006-0040-8#ref-CR28" id="ref-link-section-d93733e2461">2005</a>) designed an interactive manipulation of a cutter of a virtual milling machine, with visual, audio and haptic feedback for use in vocational training courses on using and programming numerically-controlled milling machines (Virtual Technical Trainer). Trainees can feel the cutting effort thanks to force feedback which varies as a function of different simulation parameters. Preliminary evaluation of the simulator showed that it could successfully help them to teach the basic principles of machining at the first stages of vocational training.</p><p>Some studies used large-scale VEs providing multi-modal interactions, in particular within the framework of collaborative virtual environments (CVEs) that allow many users to share the same virtual experience including the manipulation of common objects (Basdogan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Basdogan C, Ho C-H, Srinivasan M, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput-Hum Interact (TOCHI) 7(4):443–460" href="/article/10.1007/s10055-006-0040-8#ref-CR8" id="ref-link-section-d93733e2467">2000</a>). These projects include a combination of one or more technologies (3D graphics, acoustics, haptics) for industrial and scientific three-dimensional modeling and computer simulation tasks but do not yet include olfaction. Furthermore, haptic rendering is commonly achieved with desktop displays, such as the PHANToM<sup>®</sup>.</p><p>To date, VIREPSE is the first virtual environment providing human-scale haptics coupled with olfactive feedback and designed for educational applications.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Conclusion and future work</h2><div class="c-article-section__content" id="Sec22-content"><p>Virtual reality (VR) technology allows knowledge-building experiences and provides an alternative educational process. Haptic interaction is commonly used for exploration and extraction of information. It may be associated with other sensory modalities such as vision and audition but is hardly ever coupled with other feedback channels, such as olfactory feedback.</p><p>In the first part of this paper, a survey of theory and existing VEs including haptic or olfactory feedback, especially in the field of education was given. Then, we presented the architecture of our human-scale multi-modal platform VIREPSE that provides haptic interaction using a string-based interface called SPIDAR-H and olfactory feedback. Both of these are innovative in the field of education.</p><p>An application that allows students to experience the abstract concept of the Bohr atomic model and the quantization of the energy levels is described. Different configurations that support differences in interaction, size and reification through the use of immersive and multi-modal (visual, haptic, auditory and olfactory) feedback are proposed for further evaluation. Haptic feedback is achieved using different techniques ranging from desktop pseudo-haptic feedback to human-scale haptic interaction. Olfactory information is provided using different fan-based olfactory displays (ODs), according to the workspace size.</p><p>The same prototype application is presented according to several VE configurations. An appraisal of the state of the art revealed the very innovative aspect of our methodological approach, which consists of taking into account simultaneously different interaction techniques, multi-sensoriality and human-scale immersion. Assumptions are made concerning their influence and joint effects on the knowledge-building and learning processes. Experimentation will be carried out with undergraduate students of our institute and will allow us to support or invalidate these assumptions.</p><p>The “Haptic Atomic” application is an initial step in using non-traditional methods to teach undergraduate students the underlying principles of atomic concepts. The following step will be to verify the effectiveness of the simulation in comparison to traditional teaching methods and we may be able to understand the educational benefits of applying haptics to a human-scale way of learning. In a number of cases, virtual learning environments have been compared with traditional methods of teaching and learning. However, Jones et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Jones A, Scanlon E, Blake, C (1998) Reflections on a model for evaluating learning technologies, In: Oliver M. (ed) Innovation in the evaluation of learning technology. University of North London; 25–41" href="/article/10.1007/s10055-006-0040-8#ref-CR63" id="ref-link-section-d93733e2492">1998</a>) report that the educational experience often changes when educational technologies are used, meaning that it is impossible to directly compare this with traditional methods as something different is learnt. Comparative studies have shown that features of both VEs and traditional teaching methods should be combined in order to reap the benefits from both types of systems (Cobb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, In: Stanney M (eds) Handbook of virtual environments: design, implementation, and applications, Lawrence Erlbaum Associates, London 46; 911–936" href="/article/10.1007/s10055-006-0040-8#ref-CR27" id="ref-link-section-d93733e2495">2002</a>).</p><p>A few students who have tried both configurations and the different interaction techniques were able to understand the haptic representation, and were enthusiastic when experiencing SPIDAR-H. Students were pleasantly surprised and impressed by the additional olfactory information, although its usefulness was not apparent to them.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Akkiraju, H. Edelsbrunner, F. Ping., J. Qian, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Akkiraju N, Edelsbrunner H, Ping. F, Qian J (1996) Viewing geometric protein structures from inside a CAVE™. I" /><p class="c-article-references__text" id="ref-CR1">Akkiraju N, Edelsbrunner H, Ping. F, Qian J (1996) Viewing geometric protein structures from inside a CAVE™. IEEE Comput Graph Appl 16(4):58–61</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.511855" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Viewing%20geometric%20protein%20structures%20from%20inside%20a%20CAVE%E2%84%A2&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=16&amp;issue=4&amp;pages=58-61&amp;publication_year=1996&amp;author=Akkiraju%2CN&amp;author=Edelsbrunner%2CH&amp;author=Ping.%2CF&amp;author=Qian%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ammi M, Ferreira A (2004) Virtualized reality interfaces for micro and nanomanipulation. Proc. IEEE Int. Conf." /><p class="c-article-references__text" id="ref-CR2">Ammi M, Ferreira A (2004) Virtualized reality interfaces for micro and nanomanipulation. Proc. IEEE Int. Conf. on robotics and automation (ICRA’04) New Orleans, LA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Annett, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Annett J (1996) Olfactory memory: a case study in cognitive psychology. J. Psychol 130(3):309–319" /><p class="c-article-references__text" id="ref-CR3">Annett J (1996) Olfactory memory: a case study in cognitive psychology. J. Psychol 130(3):309–319</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20memory%3A%20a%20case%20study%20in%20cognitive%20psychology&amp;journal=J.%20Psychol&amp;volume=130&amp;issue=3&amp;pages=309-319&amp;publication_year=1996&amp;author=Annett%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Arai F, Ando D, Fukuda T, Nonoda Y, Oota T (1995) Micro manipulation based on micro physics, strategy based on" /><p class="c-article-references__text" id="ref-CR4">Arai F, Ando D, Fukuda T, Nonoda Y, Oota T (1995) Micro manipulation based on micro physics, strategy based on attractive force reduction and stress measurement. Proc. IEEE Int. Conf. on robotics and automation (ICRA’95); 236–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Bach-y-Rita, J. Webster, W. Thompkins, T. Crabb, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Bach-y-Rita P, Webster J, Thompkins W, Crabb T (1987) Sensory substitution for space gloves and for space robo" /><p class="c-article-references__text" id="ref-CR5">Bach-y-Rita P, Webster J, Thompkins W, Crabb T (1987) Sensory substitution for space gloves and for space robots. Workshop on Space Telerobotics 2:51–57</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%20substitution%20for%20space%20gloves%20and%20for%20space%20robots&amp;journal=Workshop%20on%20Space%20Telerobotics&amp;volume=2&amp;pages=51-57&amp;publication_year=1987&amp;author=Bach-y-Rita%2CP&amp;author=Webster%2CJ&amp;author=Thompkins%2CW&amp;author=Crabb%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Barfield, E. Danas, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Barfield W, Danas E (1996) Comments on the use of olfactory displays for virtual environments. Presence: Teleo" /><p class="c-article-references__text" id="ref-CR6">Barfield W, Danas E (1996) Comments on the use of olfactory displays for virtual environments. Presence: Teleoperators and Virtual Environments 5(1):109–121</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Comments%20on%20the%20use%20of%20olfactory%20displays%20for%20virtual%20environments&amp;journal=Presence%3A%20Teleoperators%20and%20Virtual%20Environments&amp;volume=5&amp;issue=1&amp;pages=109-121&amp;publication_year=1996&amp;author=Barfield%2CW&amp;author=Danas%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Baron, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Baron R (1990) Environmentally induced positive effects: its impact on self-efficacy, task performance, negoti" /><p class="c-article-references__text" id="ref-CR7">Baron R (1990) Environmentally induced positive effects: its impact on self-efficacy, task performance, negotiation and conflict. J Appl Soc Psychol 20:368–384</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1559-1816.1990.tb00417.x" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Environmentally%20induced%20positive%20effects%3A%20its%20impact%20on%20self-efficacy%2C%20task%20performance%2C%20negotiation%20and%20conflict&amp;journal=J%20Appl%20Soc%20Psychol&amp;volume=20&amp;pages=368-384&amp;publication_year=1990&amp;author=Baron%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Basdogan, C-H. Ho, M. Srinivasan, M. Slater, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Basdogan C, Ho C-H, Srinivasan M, Slater M (2000) An experimental study on the role of touch in shared virtual" /><p class="c-article-references__text" id="ref-CR8">Basdogan C, Ho C-H, Srinivasan M, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput-Hum Interact (TOCHI) 7(4):443–460</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365082" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20study%20on%20the%20role%20of%20touch%20in%20shared%20virtual%20environments&amp;journal=ACM%20Trans%20Comput-Hum%20Interact%20%28TOCHI%29&amp;volume=7&amp;issue=4&amp;pages=443-460&amp;publication_year=2000&amp;author=Basdogan%2CC&amp;author=Ho%2CC-H&amp;author=Srinivasan%2CM&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Biggs S, Srinivasan M (2002) Haptic Interfaces, In: Stanney K. M. (Ed) Handbook of the virtual environments: d" /><p class="c-article-references__text" id="ref-CR9">Biggs S, Srinivasan M (2002) Haptic Interfaces, In: Stanney K. M. (Ed) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London 5; 93–116</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Birmanns, W. Wriggers, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Birmanns S, Wriggers W (2003) Interactive fitting augmented by force feedback and virtual reality. J Struct Bi" /><p class="c-article-references__text" id="ref-CR10">Birmanns S, Wriggers W (2003) Interactive fitting augmented by force feedback and virtual reality. J Struct Biol 144:123–131</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jsb.2003.09.018" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20fitting%20augmented%20by%20force%20feedback%20and%20virtual%20reality&amp;journal=J%20Struct%20Biol&amp;volume=144&amp;pages=123-131&amp;publication_year=2003&amp;author=Birmanns%2CS&amp;author=Wriggers%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bouzit M, Popescu V, Burdea G, Boian R (2002) The Rutgers Master II-ND Force Feedback Glove. Proc. IEEE virtua" /><p class="c-article-references__text" id="ref-CR11">Bouzit M, Popescu V, Burdea G, Boian R (2002) The Rutgers Master II-ND Force Feedback Glove. Proc. IEEE virtual reality conf. 2002 (VRC’02) Haptics symposium, Orlando FL, March</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brady R, Pixton J, Baxter G, Moran P, Potter C, Carragher B, Belmont A (1995) Crumbs: a virtual environment tr" /><p class="c-article-references__text" id="ref-CR12">Brady R, Pixton J, Baxter G, Moran P, Potter C, Carragher B, Belmont A (1995) Crumbs: a virtual environment tracking tool for biological imaging. Proc. IEEE Symp. on Frontiers in biomedical visualization, IEEE Computer Society Press, Los Alamitos, USA; 18–25</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Brooks, O-Y. Ming, J. Batter, P. Kilpatrick, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Brooks Jr F, Ming O-Y, Batter J, Kilpatrick P (1990) Project GROPE: haptic displays for scientific visualizati" /><p class="c-article-references__text" id="ref-CR13">Brooks Jr F, Ming O-Y, Batter J, Kilpatrick P (1990) Project GROPE: haptic displays for scientific visualization. Computer Graphics (ACM) 24(4):177–185</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=GROPE%3A%20haptic%20displays%20for%20scientific%20visualization&amp;journal=Computer%20Graphics%20%28ACM%29&amp;volume=24&amp;issue=4&amp;pages=177-185&amp;publication_year=1990&amp;author=Brooks%2CF&amp;author=Ming%2CO-Y&amp;author=Batter%2CJ&amp;author=Kilpatrick%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bryson S (1996) Virtual reality in scientific visualization, communications of the ACM 39(5):62–71" /><p class="c-article-references__text" id="ref-CR14">Bryson S (1996) Virtual reality in scientific visualization, communications of the ACM 39(5):62–71</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burdea G (1996) Force and touch feedback for virtual reality. Wiley, New York" /><p class="c-article-references__text" id="ref-CR15">Burdea G (1996) Force and touch feedback for virtual reality. Wiley, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Burdea, J. Zhuang, E. Roskos, D. Silver, N. Langrana, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Burdea G, Zhuang J, Roskos E, Silver D, Langrana N (1992) A portable dextrous master with force feedback. Pres" /><p class="c-article-references__text" id="ref-CR16">Burdea G, Zhuang J, Roskos E, Silver D, Langrana N (1992) A portable dextrous master with force feedback. Presence Teleoperators and Virtual Environments 1:18–28</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20portable%20dextrous%20master%20with%20force%20feedback&amp;journal=Presence%20Teleoperators%20and%20Virtual%20Environments&amp;volume=1&amp;pages=18-28&amp;publication_year=1992&amp;author=Burdea%2CG&amp;author=Zhuang%2CJ&amp;author=Roskos%2CE&amp;author=Silver%2CD&amp;author=Langrana%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Burdea, P. Coiffet, P. Richard, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Burdea G, Coiffet P, Richard P (1996) Integration of multi-modal I/Os for virtual environments. Int. J. of Hum" /><p class="c-article-references__text" id="ref-CR17">Burdea G, Coiffet P, Richard P (1996) Integration of multi-modal I/Os for virtual environments. Int. J. of Human-Computer Interaction (IJHCI), Special Issue on Human-Virtual Environment Interaction 1:5–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integration%20of%20multi-modal%20I%2FOs%20for%20virtual%20environments&amp;journal=Int.%20J.%20of%20Human-Computer%20Interaction%20%28IJHCI%29%2C%20Special%20Issue%20on%20Human-Virtual%20Environment%20Interaction&amp;volume=1&amp;pages=5-24&amp;publication_year=1996&amp;author=Burdea%2CG&amp;author=Coiffet%2CP&amp;author=Richard%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Burdea, G. Patounakis, V. Popescu, RE. Weiss, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Burdea G, Patounakis G, Popescu V, Weiss RE (1999) Virtual reality-based training for the diagnosis of prostat" /><p class="c-article-references__text" id="ref-CR18">Burdea G, Patounakis G, Popescu V, Weiss RE (1999) Virtual reality-based training for the diagnosis of prostate cancer, IEEE Trans Biomed Eng 46(10):1253–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F10.790503" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality-based%20training%20for%20the%20diagnosis%20of%20prostate%20cancer%2C&amp;journal=IEEE%20Trans%20Biomed%20Eng&amp;volume=46&amp;issue=10&amp;pages=1253-60&amp;publication_year=1999&amp;author=Burdea%2CG&amp;author=Patounakis%2CG&amp;author=Popescu%2CV&amp;author=Weiss%2CRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Burdea, P. Coiffet, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Burdea G, Coiffet P (1994) Virtual reality technology. Wiley New York" /><p class="c-article-references__text" id="ref-CR19">Burdea G, Coiffet P (1994) Virtual reality technology. Wiley New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20technology&amp;publication_year=1994&amp;author=Burdea%2CG&amp;author=Coiffet%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burdea G, Coiffet P (2003) Virtual reality technology, 2nd Ed., Wiley, Hoboken, New Jersey, 444 p" /><p class="c-article-references__text" id="ref-CR20">Burdea G, Coiffet P (2003) Virtual reality technology, 2nd Ed., Wiley, Hoboken, New Jersey, 444 p</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Byrne C (1996) Water on tap-the use of virtual reality as an educational tool, Ph. D. thesis, University of Wa" /><p class="c-article-references__text" id="ref-CR21">Byrne C (1996) Water on tap-the use of virtual reality as an educational tool, Ph. D. thesis, University of Washington</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Cai, S. Wang, M. Sato, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Cai Y, Wang S, Sato M (1997) A human-scale direct motion instruction system device for education systems. The " /><p class="c-article-references__text" id="ref-CR22">Cai Y, Wang S, Sato M (1997) A human-scale direct motion instruction system device for education systems. The IEICE Transactions, E80-D 2:212–217</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20human-scale%20direct%20motion%20instruction%20system%20device%20for%20education%20systems&amp;journal=The%20IEICE%20Transactions%2C%20E80-D&amp;volume=2&amp;pages=212-217&amp;publication_year=1997&amp;author=Cai%2CY&amp;author=Wang%2CS&amp;author=Sato%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Castelino K (2002) Biological object nanomanipulation. Review report, University of California, Berkeley" /><p class="c-article-references__text" id="ref-CR23">Castelino K (2002) Biological object nanomanipulation. Review report, University of California, Berkeley</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems," /><p class="c-article-references__text" id="ref-CR24">Cater J (1994) Approximating the senses. Smell/taste: odors in virtual reality. Proc. IEEE Int. conf. systems, man and cybernetics, San Antonio 2; 1781</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ChangHoon P, Heedong K, Ig-Jae K, Sang Chul A, Yong-Moo K, Hyoung-Gon K (2002) The making of Kyongju VR theatr" /><p class="c-article-references__text" id="ref-CR25">ChangHoon P, Heedong K, Ig-Jae K, Sang Chul A, Yong-Moo K, Hyoung-Gon K (2002) The making of Kyongju VR theatre. Proc. IEEE virtual reality conf. 2002 (VRC’02); 269–273</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Choi, S-J. Jeong, N. Hashimoto, S. Hasegawa, Y. Koike, M. Sato, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Choi W, Jeong S-J, Hashimoto N, Hasegawa S, Koike Y, Sato M (2004) A development and evaluation of reactive mo" /><p class="c-article-references__text" id="ref-CR26">Choi W, Jeong S-J, Hashimoto N, Hasegawa S, Koike Y, Sato M (2004) A development and evaluation of reactive motion capture system with haptic feedback. Proc. of the FGR’04 37:851–856</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20development%20and%20evaluation%20of%20reactive%20motion%20capture%20system%20with%20haptic%20feedback&amp;journal=Proc.%20of%20the%20FGR%E2%80%9904&amp;volume=37&amp;pages=851-856&amp;publication_year=2004&amp;author=Choi%2CW&amp;author=Jeong%2CS-J&amp;author=Hashimoto%2CN&amp;author=Hasegawa%2CS&amp;author=Koike%2CY&amp;author=Sato%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, " /><p class="c-article-references__text" id="ref-CR27">Cobb S, Neale H, Crosier J, Wilson J (2002) Development and evaluation of virtual environments for education, In: Stanney M (eds) Handbook of virtual environments: design, implementation, and applications, Lawrence Erlbaum Associates, London 46; 911–936</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Crison F, Lecuyer A, Mellet d’Huart D, Burkhardt J. –M, Michel G, Dautin J. –L. (2005) How to use milling mach" /><p class="c-article-references__text" id="ref-CR28">Crison F, Lecuyer A, Mellet d’Huart D, Burkhardt J. –M, Michel G, Dautin J. –L. (2005) How to use milling machines with multi-sensory feedback in virtual reality. Proc. IEEE virtual reality conf. 2005 (VRC’05); 139–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cruz-Neira, R. Langley, P. Bash, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Cruz-Neira C, Langley R, Bash P (1996) VIBE: a virtual biomolecular environment for interactive molecular mode" /><p class="c-article-references__text" id="ref-CR29">Cruz-Neira C, Langley R, Bash P (1996) VIBE: a virtual biomolecular environment for interactive molecular modeling. Comput Chem 20(4):469–477</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0097-8485%2896%2900009-5" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=VIBE%3A%20a%20virtual%20biomolecular%20environment%20for%20interactive%20molecular%20modeling&amp;journal=Comput%20Chem&amp;volume=20&amp;issue=4&amp;pages=469-477&amp;publication_year=1996&amp;author=Cruz-Neira%2CC&amp;author=Langley%2CR&amp;author=Bash%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Danthiir, R. Roberts, G. Pallier, L. Stankov, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Danthiir V, Roberts R, Pallier G, Stankov L (2001) What the nose knows: olfaction and cognitive abilities. Int" /><p class="c-article-references__text" id="ref-CR30">Danthiir V, Roberts R, Pallier G, Stankov L (2001) What the nose knows: olfaction and cognitive abilities. Intelligence 29(4):337–361</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0160-2896%2801%2900061-7" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20the%20nose%20knows%3A%20olfaction%20and%20cognitive%20abilities&amp;journal=Intelligence&amp;volume=29&amp;issue=4&amp;pages=337-361&amp;publication_year=2001&amp;author=Danthiir%2CV&amp;author=Roberts%2CR&amp;author=Pallier%2CG&amp;author=Stankov%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Dede, M. Salzman, B. Loftin, K. Ash, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific conce" /><p class="c-article-references__text" id="ref-CR31">Dede C, Salzman M, Loftin B, Ash K (1997) Using virtual reality technology to convey abstract scientific concepts. In: Jacobson MJ, Kozma RB (Eds) Learning the sciences of the 21<sup>st</sup> Century: research, design, and implementing advanced technology learning environments. Lawrence Erlbaum Associates, Hillsdale, NewJersey</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20the%20sciences%20of%20the%2021st%20Century%3A%20research%2C%20design%2C%20and%20implementing%20advanced%20technology%20learning%20environments&amp;publication_year=1997&amp;author=Dede%2CC&amp;author=Salzman%2CM&amp;author=Loftin%2CB&amp;author=Ash%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Dede, M. Salzman, R. Loftin, D. Prague, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning com" /><p class="c-article-references__text" id="ref-CR32">Dede C, Salzman M, Loftin R, Prague D (1999) Multisensory immersion as a modeling environment for learning complex scientific concepts. In: Feurzeig W, Roberts N (eds) Modeling and simulation inscience and mathematics education. Springer, Berlin Heidelberg New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20and%20simulation%20inscience%20and%20mathematics%20education&amp;publication_year=1999&amp;author=Dede%2CC&amp;author=Salzman%2CM&amp;author=Loftin%2CR&amp;author=Prague%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Degel, E. Köster, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Degel J, Köster E (1999) Odors: implicit memory and performance effects. Chem Senses 26:267–280" /><p class="c-article-references__text" id="ref-CR33">Degel J, Köster E (1999) Odors: implicit memory and performance effects. Chem Senses 26:267–280</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fchemse%2F26.3.267" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Odors%3A%20implicit%20memory%20and%20performance%20effects&amp;journal=Chem%20Senses&amp;volume=26&amp;pages=267-280&amp;publication_year=1999&amp;author=Degel%2CJ&amp;author=K%C3%B6ster%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dinh H, Walker N, Hodges L, Chang S, Kobayashi A (1999) Evaluating the importance of multi-sensory input on me" /><p class="c-article-references__text" id="ref-CR34">Dinh H, Walker N, Hodges L, Chang S, Kobayashi A (1999) Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. Proc. IEEE virtual reality conf. 1999 (VRC’99) Houston, Texas; 222–228</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Duffy T, Jonassen D (1992) Constructivism; new implications for instructional technology. In: Duffy T, Jonasse" /><p class="c-article-references__text" id="ref-CR35">Duffy T, Jonassen D (1992) Constructivism; new implications for instructional technology. In: Duffy T, Jonassen D (eds) Constructivism and the technology of instruction. Lawrence Erlbaum Associates, New Jersey</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Emerson T, Revere D (1997) Virtual reality in training and education: resource guide to citations and online i" /><p class="c-article-references__text" id="ref-CR36">Emerson T, Revere D (1997) Virtual reality in training and education: resource guide to citations and online information, University of Washington, HITL, Seattle, WA, technical publication: B-94–1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Engen, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Engen T (1982) The perception of odors. Academic Press, New York" /><p class="c-article-references__text" id="ref-CR37">Engen T (1982) The perception of odors. Academic Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20perception%20of%20odors&amp;publication_year=1982&amp;author=Engen%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Falvo, R. Superfine, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Falvo M, Superfine R (2002) Mechanics and friction at the nanometer scale. J Nanoparticle Res 2:237–248" /><p class="c-article-references__text" id="ref-CR38">Falvo M, Superfine R (2002) Mechanics and friction at the nanometer scale. J Nanoparticle Res 2:237–248</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1010017130136" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mechanics%20and%20friction%20at%20the%20nanometer%20scale&amp;journal=J%20Nanoparticle%20Res&amp;volume=2&amp;pages=237-248&amp;publication_year=2002&amp;author=Falvo%2CM&amp;author=Superfine%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ferreira A, Mavroidis C (2006) Virtual reality and haptics for nano robotics: a review study. IEEE robotics an" /><p class="c-article-references__text" id="ref-CR39">Ferreira A, Mavroidis C (2006) Virtual reality and haptics for nano robotics: a review study. IEEE robotics and automation magazine (in press) </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fiolhais C, Trindade J (1999) Use of computers in physics education. In Ferrari A (ed.) Proc. Euroconference’9" /><p class="c-article-references__text" id="ref-CR40">Fiolhais C, Trindade J (1999) Use of computers in physics education. In Ferrari A (ed.) Proc. Euroconference’98 -new technologies for higher education, Aveiro</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fjeld M, Voegtli M (2002) Augmented chemistry : an interactive educational workbench. ISMAR’02" /><p class="c-article-references__text" id="ref-CR41">Fjeld M, Voegtli M (2002) Augmented chemistry : an interactive educational workbench. ISMAR’02</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs P, Moreau G, Burkhardt JM-, Coquillart S (2006a) L’interfaçage, l’immersion et l’interaction en environn" /><p class="c-article-references__text" id="ref-CR42">Fuchs P, Moreau G, Burkhardt JM-, Coquillart S (2006a) L’interfaçage, l’immersion et l’interaction en environnement virtuel, In: Le traité de la Réalité Virtuelle, Vol. 2, Presses de l’Ecole des Mines, Paris, 520 p</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs P, Moreau G, Arnaldi B, Guitton P (2006b) Les applications de la réalité virtuelle, In: Le traité de la " /><p class="c-article-references__text" id="ref-CR43">Fuchs P, Moreau G, Arnaldi B, Guitton P (2006b) Les applications de la réalité virtuelle, In: Le traité de la Réalité Virtuelle, Vol. 4, Presses de l’Ecole des Mines, Paris, 520 p</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs P, Papin J. –P, Richard P, Tijou A (2006c) Les interfaces olfactives, In: Fuchs P, Moreau G (eds) Le Tra" /><p class="c-article-references__text" id="ref-CR44">Fuchs P, Papin J. –P, Richard P, Tijou A (2006c) Les interfaces olfactives, In: Fuchs P, Moreau G (eds) Le Traité de Réalité Virtuelle, Vol. 2: L’interfaçage, l’immersion et l’interaction en environnement virtuel, Presse de l’Ecole des Mines 4 11</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gallina P, Rossi A, Williams II R (2000) Planar cable-direct-driven robot. Part I &amp; II. ASME design tech. Conf" /><p class="c-article-references__text" id="ref-CR45">Gallina P, Rossi A, Williams II R (2000) Planar cable-direct-driven robot. Part I &amp; II. ASME design tech. Conf., Pittsburgh</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Garratt J, Clow D, Hodgson A, Tomlinson A (1999) Computer simulation and chemical education—a review of projec" /><p class="c-article-references__text" id="ref-CR46">Garratt J, Clow D, Hodgson A, Tomlinson A (1999) Computer simulation and chemical education—a review of project elaborate. Chem Educ Rev; 51–73</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Gay, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Gay E (1994) Is virtual reality a good teaching tool? Virtual Reality Special Report 1:51–60" /><p class="c-article-references__text" id="ref-CR47">Gay E (1994) Is virtual reality a good teaching tool? Virtual Reality Special Report 1:51–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20virtual%20reality%20a%20good%20teaching%20tool%3F&amp;journal=Virtual%20Reality%20Special%20Report&amp;volume=1&amp;pages=51-60&amp;publication_year=1994&amp;author=Gay%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gomez D, Burdea G, Langrana N (1995) Modeling of the Rutgers master II haptic display. Proc. 4th ann. symp. on" /><p class="c-article-references__text" id="ref-CR48">Gomez D, Burdea G, Langrana N (1995) Modeling of the Rutgers master II haptic display. Proc. 4th ann. symp. on haptic interfaces for virtual environments and teleoperator systems, ASME; 727–734</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutierrez-Osuna R (2004) Olfactory Interaction, In: Bainbridge W (ed) Encyclopedia of human-computer interacti" /><p class="c-article-references__text" id="ref-CR49">Gutierrez-Osuna R (2004) Olfactory Interaction, In: Bainbridge W (ed) Encyclopedia of human-computer interaction. Berkshire Pub, pp. 507–511</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Harel, L. Carmel, D. Lancet, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Harel D, Carmel L, Lancet D (2003) Towards an odor communication system. Comput Biol Chem 27:121–133" /><p class="c-article-references__text" id="ref-CR50">Harel D, Carmel L, Lancet D (2003) Towards an odor communication system. Comput Biol Chem 27:121–133</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1047.92031" aria-label="View reference 50 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1476-9271%2802%2900092-0" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20an%20odor%20communication%20system&amp;journal=Comput%20Biol%20Chem&amp;volume=27&amp;pages=121-133&amp;publication_year=2003&amp;author=Harel%2CD&amp;author=Carmel%2CL&amp;author=Lancet%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hashimoto M, Morioka S, Yamamoyo R. (1997) Force display for atomic bonds. Proc. IEEE int. conf. on robotics a" /><p class="c-article-references__text" id="ref-CR51">Hashimoto M, Morioka S, Yamamoyo R. (1997) Force display for atomic bonds. Proc. IEEE int. conf. on robotics and automation (ICRA’97)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Hashimoto, J. Ryu, S.-J. Jeong, M. Sato, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Hashimoto N, Ryu J, Jeong S.-J, Sato M (2004) Human-scale interaction with a multi-projector display and multi" /><p class="c-article-references__text" id="ref-CR52">Hashimoto N, Ryu J, Jeong S.-J, Sato M (2004) Human-scale interaction with a multi-projector display and multimodal interfaces. Proc. PCM’04 3:23–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human-scale%20interaction%20with%20a%20multi-projector%20display%20and%20multimodal%20interfaces&amp;journal=Proc.%20PCM%E2%80%9904&amp;volume=3&amp;pages=23-30&amp;publication_year=2004&amp;author=Hashimoto%2CN&amp;author=Ryu%2CJ&amp;author=Jeong%2CS.-J&amp;author=Sato%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heilig M (1962) US Patent 3,050,870 Sensorama stimulator. August 28" /><p class="c-article-references__text" id="ref-CR53">Heilig M (1962) US Patent 3,050,870 Sensorama stimulator. August 28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Herz, E. Eich, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Herz R, Eich E (1995) Commentary and Envoi. In: Schab F, Crowder R (Eds) Memory for odors. Lawrence Erlbaum As" /><p class="c-article-references__text" id="ref-CR54">Herz R, Eich E (1995) Commentary and Envoi. In: Schab F, Crowder R (Eds) Memory for odors. Lawrence Erlbaum Associates, Mahwah, New Jersey, pp159–175</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Memory%20for%20odors&amp;pages=159-175&amp;publication_year=1995&amp;author=Herz%2CR&amp;author=Eich%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Herz, T. Engen, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Herz R, Engen T (1996) Odor memory: review and analysis. Psychon Bull Rev 3:300–313" /><p class="c-article-references__text" id="ref-CR55">Herz R, Engen T (1996) Odor memory: review and analysis. Psychon Bull Rev 3:300–313</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Odor%20memory%3A%20review%20and%20analysis&amp;journal=Psychon%20Bull%20Rev&amp;volume=3&amp;pages=300-313&amp;publication_year=1996&amp;author=Herz%2CR&amp;author=Engen%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Herz, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Herz R (1998) Are odors the best cues to memory? A cross-modal comparison of associative memory stimuli. Annal" /><p class="c-article-references__text" id="ref-CR56">Herz R (1998) Are odors the best cues to memory? A cross-modal comparison of associative memory stimuli. Annals of the New York Academy of Sciences 855; 670–674</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20odors%20the%20best%20cues%20to%20memory%3F%20A%20cross-modal%20comparison%20of%20associative%20memory%20stimuli&amp;journal=Annals%20of%20the%20New%20York%20Academy%20of%20Sciences&amp;volume=855&amp;pages=670-674&amp;publication_year=1998&amp;author=Herz%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Pausch R, Goble J, Kassell N (1994) Passive real-world interface props for neurosurgical visualiza" /><p class="c-article-references__text" id="ref-CR57">Hinckley K, Pausch R, Goble J, Kassell N (1994) Passive real-world interface props for neurosurgical visualization. ACM CHI; 452–458</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hirata Y, Sato M (1992) 3-dimensional interface device for virtual work space. Proc. 1992 IEEE/RSJ Int. Conf. " /><p class="c-article-references__text" id="ref-CR58">Hirata Y, Sato M (1992) 3-dimensional interface device for virtual work space. Proc. 1992 IEEE/RSJ Int. Conf. on IROS 2:889–896</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Ihlenfeldt, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Ihlenfeldt W (1997) Virtual Reality in Chemistry. J Mol Mod 3:386–402" /><p class="c-article-references__text" id="ref-CR59">Ihlenfeldt W (1997) Virtual Reality in Chemistry. J Mol Mod 3:386–402</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs008940050056" aria-label="View reference 59">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20Reality%20in%20Chemistry&amp;journal=J%20Mol%20Mod&amp;volume=3&amp;pages=386-402&amp;publication_year=1997&amp;author=Ihlenfeldt%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Inglese F-X, Jeong S-J, Richard P, Sato M (2005) A multi-modal virtual environment. Proc. Int. Conf. Virtual C" /><p class="c-article-references__text" id="ref-CR60">Inglese F-X, Jeong S-J, Richard P, Sato M (2005) A multi-modal virtual environment. Proc. Int. Conf. Virtual Concept’05, Biarritz, France, 8–10 November 2005</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ishii, M. Sato, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ishii M, Sato M (1994) 3D spatial interface device using tensed strings. Presence: Teleoperators and Virtual E" /><p class="c-article-references__text" id="ref-CR61">Ishii M, Sato M (1994) 3D spatial interface device using tensed strings. Presence: Teleoperators and Virtual Environments 3(1):81–86</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20spatial%20interface%20device%20using%20tensed%20strings&amp;journal=Presence%3A%20Teleoperators%20and%20Virtual%20Environments&amp;volume=3&amp;issue=1&amp;pages=81-86&amp;publication_year=1994&amp;author=Ishii%2CM&amp;author=Sato%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jansson G, Petrie H, Colwell C, Kornbrot D, Fänger J, König H, Billberger K, Hardwick A, Furner S (1999) Hapti" /><p class="c-article-references__text" id="ref-CR62">Jansson G, Petrie H, Colwell C, Kornbrot D, Fänger J, König H, Billberger K, Hardwick A, Furner S (1999) Haptic virtual environments for blind people: exploratory experiments with two devices. Int. J. Virtual Real 4 1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jones A, Scanlon E, Blake, C (1998) Reflections on a model for evaluating learning technologies, In: Oliver M." /><p class="c-article-references__text" id="ref-CR63">Jones A, Scanlon E, Blake, C (1998) Reflections on a model for evaluating learning technologies, In: Oliver M. (ed) Innovation in the evaluation of learning technology. University of North London; 25–41</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kalawsky R (1993) The science of virtual reality and virtual environments, Addison-Wesley, Pub. Co" /><p class="c-article-references__text" id="ref-CR64">Kalawsky R (1993) The science of virtual reality and virtual environments, Addison-Wesley, Pub. Co</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Karr, R. Brady, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Karr T, Brady R (2000) Virtual biology in the CAVE. Trends Genet 16:231–232" /><p class="c-article-references__text" id="ref-CR65">Karr T, Brady R (2000) Virtual biology in the CAVE. Trends Genet 16:231–232</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0168-9525%2800%2901996-X" aria-label="View reference 65">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 65 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20biology%20in%20the%20CAVE&amp;journal=Trends%20Genet&amp;volume=16&amp;pages=231-232&amp;publication_year=2000&amp;author=Karr%2CT&amp;author=Brady%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab" /><p class="c-article-references__text" id="ref-CR66">Kaye J (2001) Symbolic olfactory display. Master’s Thesis, MIT Media Lab</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Keller, R. Kouzes, L. Kangas, S. Hashem, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Keller P, Kouzes R, Kangas L, Hashem S (1995) Transmission of olfactory information for telemedicine, In: Morg" /><p class="c-article-references__text" id="ref-CR67">Keller P, Kouzes R, Kangas L, Hashem S (1995) Transmission of olfactory information for telemedicine, In: Morgan K, Satava R, Sieburg H, Matteus R, Christensen J. (eds) Interactive technology and the new paradigm for healthcare. IOS Press and Ohmsha, Amsterdam, pp 168–172</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 67 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20technology%20and%20the%20new%20paradigm%20for%20healthcare&amp;pages=168-172&amp;publication_year=1995&amp;author=Keller%2CP&amp;author=Kouzes%2CR&amp;author=Kangas%2CL&amp;author=Hashem%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim S, Hasegawa Y, Koike M, Sato M (2002) Tension based 7 DOF force feedback device: SPIDAR-G. Proc. IEEE virt" /><p class="c-article-references__text" id="ref-CR68">Kim S, Hasegawa Y, Koike M, Sato M (2002) Tension based 7 DOF force feedback device: SPIDAR-G. Proc. IEEE virtual reality conf. (VRC’02)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Köster E (2002) The specific characteristics of the sense of smell, In: Rouby C, Schaal B, Dubois D, Gervay R," /><p class="c-article-references__text" id="ref-CR69">Köster E (2002) The specific characteristics of the sense of smell, In: Rouby C, Schaal B, Dubois D, Gervay R, Holley A (eds) Olfaction, taste, and cognition. Cambridge Univ. Press 3, pp. 27–43</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Langrana, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Langrana N (1997) Human performance using virtual reality tumor palpation simulation. Comput Graph 21(4):451–4" /><p class="c-article-references__text" id="ref-CR70">Langrana N (1997) Human performance using virtual reality tumor palpation simulation. Comput Graph 21(4):451–458</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2897%2900021-6" aria-label="View reference 70">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20performance%20using%20virtual%20reality%20tumor%20palpation%20simulation&amp;journal=Comput%20Graph&amp;volume=21&amp;issue=4&amp;pages=451-458&amp;publication_year=1997&amp;author=Langrana%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lécuyer A, Coquillart S, Kheddar A, Richard P, Coiffet P (2000) Pseudo-haptic feedback: can isometric input de" /><p class="c-article-references__text" id="ref-CR71">Lécuyer A, Coquillart S, Kheddar A, Richard P, Coiffet P (2000) Pseudo-haptic feedback: can isometric input devices simulate force feedback ?, Proc. IEEE virtual reality conf. 2000 (VR’00), New Brunswick, New Jersey; 83–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lécuyer A, Burkhardt J.-M, Etienne L (2004) Feeling bumps and holes without a haptic interface: the perception" /><p class="c-article-references__text" id="ref-CR72">Lécuyer A, Burkhardt J.-M, Etienne L (2004) Feeling bumps and holes without a haptic interface: the perception of pseudo-haptic textures. Proc. CHI 2004; 239–247</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Mikropoulos, A. Chalkidis, A. Katsikis, A. Emvalotis, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Mikropoulos T, Chalkidis A, Katsikis A, Emvalotis A (1998) Student’s attitudes towards educational virtual env" /><p class="c-article-references__text" id="ref-CR73">Mikropoulos T, Chalkidis A, Katsikis A, Emvalotis A (1998) Student’s attitudes towards educational virtual environments. Educ Inf Technol 3:137–148</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1009687025419" aria-label="View reference 73">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 73 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Student%E2%80%99s%20attitudes%20towards%20educational%20virtual%20environments&amp;journal=Educ%20Inf%20Technol&amp;volume=3&amp;pages=137-148&amp;publication_year=1998&amp;author=Mikropoulos%2CT&amp;author=Chalkidis%2CA&amp;author=Katsikis%2CA&amp;author=Emvalotis%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. A. Mikropoulos, A. Katsikis, E. Nikolou, P. Tsakalis, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Mikropoulos T. A, Katsikis A, Nikolou E, Tsakalis P (2003) Virtual environments in biology teaching. J Biol Ed" /><p class="c-article-references__text" id="ref-CR74">Mikropoulos T. A, Katsikis A, Nikolou E, Tsakalis P (2003) Virtual environments in biology teaching. J Biol Educ 37(4):176–181</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environments%20in%20biology%20teaching&amp;journal=J%20Biol%20Educ&amp;volume=37&amp;issue=4&amp;pages=176-181&amp;publication_year=2003&amp;author=Mikropoulos%2CT.%20A&amp;author=Katsikis%2CA&amp;author=Nikolou%2CE&amp;author=Tsakalis%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Mochizuki, T. Amada, S. Sawa, T. Takeda, S. Motoyashiki, K. Kohyama, M. Imura, K. Chihara, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Mochizuki A, Amada T, Sawa S, Takeda T, Motoyashiki S, Kohyama K, Imura M, Chihara K (2004) An olfactory displ" /><p class="c-article-references__text" id="ref-CR75">Mochizuki A, Amada T, Sawa S, Takeda T, Motoyashiki S, Kohyama K, Imura M, Chihara K (2004) An olfactory display device linked with human gesture. Proc. SCI’04, Kyoto, Japan 48(6004):531–532</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 75 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20olfactory%20display%20device%20linked%20with%20human%20gesture&amp;journal=Proc.%20SCI%E2%80%9904%2C%20Kyoto%2C%20Japan&amp;volume=48&amp;issue=6004&amp;pages=531-532&amp;publication_year=2004&amp;author=Mochizuki%2CA&amp;author=Amada%2CT&amp;author=Sawa%2CS&amp;author=Takeda%2CT&amp;author=Motoyashiki%2CS&amp;author=Kohyama%2CK&amp;author=Imura%2CM&amp;author=Chihara%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morie J, Iyer K, Valanejad K, Sadek R, Miraglia D, Milam D, Williams J, Luigi D.-P, Leshin J (2003) Sensory de" /><p class="c-article-references__text" id="ref-CR76">Morie J, Iyer K, Valanejad K, Sadek R, Miraglia D, Milam D, Williams J, Luigi D.-P, Leshin J (2003) Sensory design for virtual environments, SIGGRAPH 2003 Sketch, San Diego, CA, July</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nikolou E, Mikropoulos T, Katsikis A (1997) Virtual realities in biology teaching, In: Bevan M (ed) Proc. Int." /><p class="c-article-references__text" id="ref-CR77">Nikolou E, Mikropoulos T, Katsikis A (1997) Virtual realities in biology teaching, In: Bevan M (ed) Proc. Int. conference virtual reality in education and training. Loughborough, UK; 59–63</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouh-Young G, Pique M, Hughes J, Srinivasan N, Brooks Jr. F (1988) Using a manipulator for force display in mol" /><p class="c-article-references__text" id="ref-CR78">Ouh-Young G, Pique M, Hughes J, Srinivasan N, Brooks Jr. F (1988) Using a manipulator for force display in molecular docking. Proc. IEEE robotics and automation conference, Philadelphia, PA; 1824–1829</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouh-Young M, Beard D, Brooks F (1989) Force display performs better than visual display in a simple 6-D dockin" /><p class="c-article-references__text" id="ref-CR79">Ouh-Young M, Beard D, Brooks F (1989) Force display performs better than visual display in a simple 6-D docking task. Proc. IEEE Int. Conf. on robotics and automation (ICRA’89)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Paljic A, Tarrin N, Coquillart S, Bouguila L, Sato M (2004) The passive stringed haptic spidar for the worlkbe" /><p class="c-article-references__text" id="ref-CR80">Paljic A, Tarrin N, Coquillart S, Bouguila L, Sato M (2004) The passive stringed haptic spidar for the worlkbench. EuroGraphics’04, Grenoble, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Papin J.-P, Bouallagui M, Ouali A, Richard P, Tijou A, Poisson P, Bartoli W (2003) DIODE: Smell-diffusion in r" /><p class="c-article-references__text" id="ref-CR81">Papin J.-P, Bouallagui M, Ouali A, Richard P, Tijou A, Poisson P, Bartoli W (2003) DIODE: Smell-diffusion in real and virtual environments. Proc. 5th Int. Conf. on virtual reality (VRIC’03). Laval, France, May; 113–117</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Richard, G. Birebent, G. Burdea, D. Gomez, N. Langrana, P. Coiffet, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Richard P, Birebent G, Burdea G, Gomez D, Langrana N, Coiffet P (1996) Effect of frame rate and force feedback" /><p class="c-article-references__text" id="ref-CR82">Richard P, Birebent G, Burdea G, Gomez D, Langrana N, Coiffet P (1996) Effect of frame rate and force feedback on virtual objects manipulation. Presence: Teleoperators and Virtual Environments 15:95–108</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 82 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effect%20of%20frame%20rate%20and%20force%20feedback%20on%20virtual%20objects%20manipulation&amp;journal=Presence%3A%20Teleoperators%20and%20Virtual%20Environments&amp;volume=15&amp;pages=95-108&amp;publication_year=1996&amp;author=Richard%2CP&amp;author=Birebent%2CG&amp;author=Burdea%2CG&amp;author=Gomez%2CD&amp;author=Langrana%2CN&amp;author=Coiffet%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richard P, Allain P, Richard E, Le Gall D (2006a) Projet EVACOG–Environnements Virtuels Appliqués aux Sciences" /><p class="c-article-references__text" id="ref-CR83">Richard P, Allain P, Richard E, Le Gall D (2006a) Projet EVACOG–Environnements Virtuels Appliqués aux Sciences Cognitives. Handicap 2006, Proc. 4th Conf. "Nouvelles Technologies au service de l'homme", Handicap 2006, Paris, France, 7–9 June 2006, pp 233–239</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richard P, Chamaret D, Inglese F-X, Lucidarme P, Ferrier J-L (2006b) Human-scale haptic virtual environment fo" /><p class="c-article-references__text" id="ref-CR124">Richard P, Chamaret D, Inglese F-X, Lucidarme P, Ferrier J-L (2006b) Human-scale haptic virtual environment for product design: effect of sensory substitution. Int J Virtual Real (in press)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richard P, Coiffet P (1995) Human perceptual issues in virtual environments : sensory substitution and informa" /><p class="c-article-references__text" id="ref-CR84">Richard P, Coiffet P (1995) Human perceptual issues in virtual environments : sensory substitution and information redundancy. Proc. of the IEEE Int. work. on robot and human communication, Tokyo, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Riganelli, O. Gervasi, A. Laganà, M. Alberti, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Riganelli A, Gervasi O, Laganà A, Alberti M (2003) A multi-scale virtual reality approach to chemical experime" /><p class="c-article-references__text" id="ref-CR85">Riganelli A, Gervasi O, Laganà A, Alberti M (2003) A multi-scale virtual reality approach to chemical experiments. LNCS 2658:324–330</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 86 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20multi-scale%20virtual%20reality%20approach%20to%20chemical%20experiments&amp;journal=LNCS&amp;volume=2658&amp;pages=324-330&amp;publication_year=2003&amp;author=Riganelli%2CA&amp;author=Gervasi%2CO&amp;author=Lagan%C3%A0%2CA&amp;author=Alberti%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rizzo A (2005) Development of a virtual reality therapy application for Iraq war veterans with PTSD, virtual r" /><p class="c-article-references__text" id="ref-CR86">Rizzo A (2005) Development of a virtual reality therapy application for Iraq war veterans with PTSD, virtual reality. Associated technologies and rehabilitation, Three-day symposium, University of Haifa, Israel, March 7–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rizzo, A. Jounghyun, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Rizzo A, Jounghyun A (2005) SWOT analysis of the field of virtual reality rehabilitation and therapy. Presence" /><p class="c-article-references__text" id="ref-CR87">Rizzo A, Jounghyun A (2005) SWOT analysis of the field of virtual reality rehabilitation and therapy. Presence: Teleoperators and Virtual Environments 14(2):119–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F1054746053967094" aria-label="View reference 88">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 88 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SWOT%20analysis%20of%20the%20field%20of%20virtual%20reality%20rehabilitation%20and%20therapy&amp;journal=Presence%3A%20Teleoperators%20and%20Virtual%20Environments&amp;volume=14&amp;issue=2&amp;pages=119-146&amp;publication_year=2005&amp;author=Rizzo%2CA&amp;author=Jounghyun%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 " /><p class="c-article-references__text" id="ref-CR88">Roussos M, Gillingham M (1998) evaluation of an immersive collaborative virtual learning environment for K-12 education, AERA Roundtable session at the American Educational Research Association annual meeting, San Diego, US, April</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Ruiz, E. Espinosa, G. Garcia, M. Gómez-Nieto, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Ruiz I, Espinosa E, Garcia G, Gómez-Nieto M (2002) Computer-assisted learning of chemical experiments through " /><p class="c-article-references__text" id="ref-CR89">Ruiz I, Espinosa E, Garcia G, Gómez-Nieto M (2002) Computer-assisted learning of chemical experiments through a 3D virtual laboratory. LNCS 2329:704–712</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1048.68896" aria-label="View reference 90 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 90 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer-assisted%20learning%20of%20chemical%20experiments%20through%20a%203D%20virtual%20laboratory&amp;journal=LNCS&amp;volume=2329&amp;pages=704-712&amp;publication_year=2002&amp;author=Ruiz%2CI&amp;author=Espinosa%2CE&amp;author=Garcia%2CG&amp;author=G%C3%B3mez-Nieto%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Salzman, C. Dede, R. Loftin, J. Chen, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Salzman M, Dede C, Loftin R, Chen J (1999) a model for understanding how virtual reality aids complex conceptu" /><p class="c-article-references__text" id="ref-CR90">Salzman M, Dede C, Loftin R, Chen J (1999) a model for understanding how virtual reality aids complex conceptual learning. Presence: Teleoperators and Virtual Environments 8(3):293–316</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474699566242" aria-label="View reference 91">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 91 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=a%20model%20for%20understanding%20how%20virtual%20reality%20aids%20complex%20conceptual%20learning&amp;journal=Presence%3A%20Teleoperators%20and%20Virtual%20Environments&amp;volume=8&amp;issue=3&amp;pages=293-316&amp;publication_year=1999&amp;author=Salzman%2CM&amp;author=Dede%2CC&amp;author=Loftin%2CR&amp;author=Chen%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural mole" /><p class="c-article-references__text" id="ref-CR91">Sankaranarayanan G, Weghorst S, Sanner M, Gillet A, Olson A (2003) Role of haptics in teaching structural molecular biology. Proc. 11th Symp. on Haptic interfaces for virtual environment and teleoperator systems. Los Angeles, CA; 365</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato M (2001) Evolution of SPIDAR. Proc. 3rd Int. virtual reality conf. (VRIC’01) Laval, May, France" /><p class="c-article-references__text" id="ref-CR92">Sato M (2001) Evolution of SPIDAR. Proc. 3rd Int. virtual reality conf. (VRIC’01) Laval, May, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Sauer, W. Hastings, A. Okamura, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Sauer C, Hastings W, Okamura A (2004) Virtual environment for exploring atomic bonding. Proc. EuroHaptics’04, " /><p class="c-article-references__text" id="ref-CR93">Sauer C, Hastings W, Okamura A (2004) Virtual environment for exploring atomic bonding. Proc. EuroHaptics’04, Munich, Germany 15:232–239</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 94 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environment%20for%20exploring%20atomic%20bonding&amp;journal=Proc.%20EuroHaptics%E2%80%9904%2C%20Munich%2C%20Germany&amp;volume=15&amp;pages=232-239&amp;publication_year=2004&amp;author=Sauer%2CC&amp;author=Hastings%2CW&amp;author=Okamura%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schiffman S, Pearce T (2002) Introduction to olfaction: perception, anatomy, physiology, and molecular biology" /><p class="c-article-references__text" id="ref-CR94">Schiffman S, Pearce T (2002) Introduction to olfaction: perception, anatomy, physiology, and molecular biology. In: Pearce T, Schiffman S, Nagle H, Gardner JW (eds) Handbook of machine olfaction: Electronic Nose Technology. Wiley-VCH</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shaffer D, Meglan D, Ferrell M, Dawson S (1999) Virtual rounds: simulation-based education in procedural medic" /><p class="c-article-references__text" id="ref-CR95">Shaffer D, Meglan D, Ferrell M, Dawson S (1999) Virtual rounds: simulation-based education in procedural medicine. Proc. 1999 SPIE Battlefield Biomedical Technologies Conf., Orlando, FL 3712:99–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sharma G, Mavroidis C, Ferreira A (2005) Virtual reality and haptics in nano- and bionanotechnology, In: Rieth" /><p class="c-article-references__text" id="ref-CR96">Sharma G, Mavroidis C, Ferreira A (2005) Virtual reality and haptics in nano- and bionanotechnology, In: Rieth M, Schommers W (eds) Handbook of theoretical and computational nanotechnology 10 40; 1–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Srinivasan, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Srinivasan M (1995) Haptic Interfaces. In: Durlach NI, Mavor AS (Eds) Virtual reality: scientific and technica" /><p class="c-article-references__text" id="ref-CR97">Srinivasan M (1995) Haptic Interfaces. In: Durlach NI, Mavor AS (Eds) Virtual reality: scientific and technical challenges. National Academic Press, Washington DC, pp 161–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 98 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20scientific%20and%20technical%20challenges&amp;pages=161-187&amp;publication_year=1995&amp;author=Srinivasan%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Srinivasan, C. Basadogan, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Srinivasan M, Basadogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. C" /><p class="c-article-references__text" id="ref-CR98">Srinivasan M, Basadogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. Comput Graph 21(4):393–404</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2897%2900030-7" aria-label="View reference 99">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 99 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptics%20in%20virtual%20environments%3A%20taxonomy%2C%20research%20status%2C%20and%20challenges&amp;journal=Comput%20Graph&amp;volume=21&amp;issue=4&amp;pages=393-404&amp;publication_year=1997&amp;author=Srinivasan%2CM&amp;author=Basadogan%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="KM. Stanney, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Stanney KM (2002) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlb" /><p class="c-article-references__text" id="ref-CR99">Stanney KM (2002) Handbook of the virtual environments: design, implementation and applications. Lawrence Erlbaum Associates, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 100 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20the%20virtual%20environments%3A%20design%2C%20implementation%20and%20applications&amp;publication_year=2002&amp;author=Stanney%2CKM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Stone, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Stone R (2001) Haptic feedback: a brief history from telepresence to virtual reality. LNCS 2058:1–16" /><p class="c-article-references__text" id="ref-CR100">Stone R (2001) Haptic feedback: a brief history from telepresence to virtual reality. LNCS 2058:1–16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0980.68697" aria-label="View reference 101 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 101 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20feedback%3A%20a%20brief%20history%20from%20telepresence%20to%20virtual%20reality&amp;journal=LNCS&amp;volume=2058&amp;pages=1-16&amp;publication_year=2001&amp;author=Stone%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stredney D, Wiet G, Yagel R, Sessanna D, Kurzion Y, Fontana M, Shareef N, Levin M, Martin K, Okamura A (1998) " /><p class="c-article-references__text" id="ref-CR101">Stredney D, Wiet G, Yagel R, Sessanna D, Kurzion Y, Fontana M, Shareef N, Levin M, Martin K, Okamura A (1998) A comparative analysis of integrating visual representations with haptic displays, In: Westwood et al. (ed) Proc. MMVR6, IOS Press, Amsterdam; 20–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sundgren H, Winquist F, Lundstrom I (1992) Artificial olfactory system based on field effect devices. Proc. in" /><p class="c-article-references__text" id="ref-CR102">Sundgren H, Winquist F, Lundstrom I (1992) Artificial olfactory system based on field effect devices. Proc. interfaces to real and virtual worlds, Montpellier, France; 463–472</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Suzuki, M. Kamiko, R. Yamamoto, Y. Tateizumi, M. Hashimoto, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Suzuki A, Kamiko M, Yamamoto R, Tateizumi Y, Hashimoto M (1999) Molecular simulations in the virtual material " /><p class="c-article-references__text" id="ref-CR103">Suzuki A, Kamiko M, Yamamoto R, Tateizumi Y, Hashimoto M (1999) Molecular simulations in the virtual material laboratory. Comput Mater Sci 14:227–231</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0927-0256%2898%2900135-9" aria-label="View reference 104">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 104 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Molecular%20simulations%20in%20the%20virtual%20material%20laboratory&amp;journal=Comput%20Mater%20Sci&amp;volume=14&amp;pages=227-231&amp;publication_year=1999&amp;author=Suzuki%2CA&amp;author=Kamiko%2CM&amp;author=Yamamoto%2CR&amp;author=Tateizumi%2CY&amp;author=Hashimoto%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tarrin N, Coquillart S, Hasegawa S, Bouguila L, Sato M. (2003) The stringed haptic workbench: a new haptic wor" /><p class="c-article-references__text" id="ref-CR104">Tarrin N, Coquillart S, Hasegawa S, Bouguila L, Sato M. (2003) The stringed haptic workbench: a new haptic workbench solution. EuroGraphics’03 22; 3</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. pro" /><p class="c-article-references__text" id="ref-CR105">Tijou A, Richard E, Richard P (2006a) Using olfactive virtual environments for learning organic molecules. proceedings series: LNCS, Vol. 3942, Pan, Z. et al, (eds), Technologies for e-learning and digital entertainment, first international conference, Edutainment 2006, Hangzhou, China; 1223–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tijou A, Richard P, Papin J. –P (2006b) Diffusion d’odeurs dans les environnements virtuels: étude préliminair" /><p class="c-article-references__text" id="ref-CR106">Tijou A, Richard P, Papin J. –P (2006b) Diffusion d’odeurs dans les environnements virtuels: étude préliminaire, IEEE Conf. Int. Francophone d’Automatique (CIFA’06), 30 mai-01 juin, Bordeaux, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trindade J, Fiolhais C, Gil V (1999) Virtual water, an application of virtual environments as an education too" /><p class="c-article-references__text" id="ref-CR107">Trindade J, Fiolhais C, Gil V (1999) Virtual water, an application of virtual environments as an education tool for physics and chemistry. In: Cumming G et al. (eds.) Advanced research in computers and communication in education. Proc. 7th Int. conf. on computers in education, ICCE’99, Chiba, Japan, IOS Press 2; 655–658</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Trindade, J. Paiva, C. Fiolhais, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Trindade J, Paiva J, Fiolhais C (2001) Visualizing atoms and molecules in on-line simulations and virtual real" /><p class="c-article-references__text" id="ref-CR108">Trindade J, Paiva J, Fiolhais C (2001) Visualizing atoms and molecules in on-line simulations and virtual reality. Europhys News 32(11):14–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1051%2Fepn%3A2001103" aria-label="View reference 109">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 109 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visualizing%20atoms%20and%20molecules%20in%20on-line%20simulations%20and%20virtual%20reality&amp;journal=Europhys%20News&amp;volume=32&amp;issue=11&amp;pages=14-15&amp;publication_year=2001&amp;author=Trindade%2CJ&amp;author=Paiva%2CJ&amp;author=Fiolhais%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Békésy, " /><meta itemprop="datePublished" content="1964" /><meta itemprop="headline" content="Von Békésy G (1964) Olfactory analog to directional hearing. J Appl Physiol 19:369–373" /><p class="c-article-references__text" id="ref-CR109">Von Békésy G (1964) Olfactory analog to directional hearing. J Appl Physiol 19:369–373</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 110 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20analog%20to%20directional%20hearing&amp;journal=J%20Appl%20Physiol&amp;volume=19&amp;pages=369-373&amp;publication_year=1964&amp;author=B%C3%A9k%C3%A9sy%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Walairacht, M. Ishii, Y. Koike, M. Sato, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Walairacht S, Ishii M, Koike Y, Sato M (2001) Two-handed multi-fingers string-based haptic interface device. T" /><p class="c-article-references__text" id="ref-CR110">Walairacht S, Ishii M, Koike Y, Sato M (2001) Two-handed multi-fingers string-based haptic interface device. The IEICE Transactions, E84-D 3:365–373</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 111 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two-handed%20multi-fingers%20string-based%20haptic%20interface%20device&amp;journal=The%20IEICE%20Transactions%2C%20E84-D&amp;volume=3&amp;pages=365-373&amp;publication_year=2001&amp;author=Walairacht%2CS&amp;author=Ishii%2CM&amp;author=Koike%2CY&amp;author=Sato%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Warm, W. Dember, R. Parasuraman, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Warm J, Dember W, Parasuraman R (1990) Effects on fragrance on vigilance, performance and stress. Perfumer and" /><p class="c-article-references__text" id="ref-CR111">Warm J, Dember W, Parasuraman R (1990) Effects on fragrance on vigilance, performance and stress. Perfumer and Flavorist 15:15–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 112 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20on%20fragrance%20on%20vigilance%2C%20performance%20and%20stress&amp;journal=Perfumer%20and%20Flavorist&amp;volume=15&amp;pages=15-18&amp;publication_year=1990&amp;author=Warm%2CJ&amp;author=Dember%2CW&amp;author=Parasuraman%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modelin" /><p class="c-article-references__text" id="ref-CR112">Washburn D, Jones L, Satya R, Bowers C, Cortes A (2003) Olfactory use in virtual environment training. Modeling and simulation magazine 2 3</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Washburn D, Jones L (2004) Could olfactory displays improve data visualization?. Computing in science and engi" /><p class="c-article-references__text" id="ref-CR113">Washburn D, Jones L (2004) Could olfactory displays improve data visualization?. Computing in science and engineering, Nov-Dec; 80–83</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Williams II R, Chen M.-Y, Seaton J (2002) Haptics-augmented high school physics tutorials. Int J Virtual Real " /><p class="c-article-references__text" id="ref-CR114">Williams II R, Chen M.-Y, Seaton J (2002) Haptics-augmented high school physics tutorials. Int J Virtual Real 5 1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Williams II R, Srivastava M, Howell J, Conatser Jr, R, Eland D, Burns J, Chila A. (2004) The virtual haptic ba" /><p class="c-article-references__text" id="ref-CR115">Williams II R, Srivastava M, Howell J, Conatser Jr, R, Eland D, Burns J, Chila A. (2004) The virtual haptic back for palpatory training. Proc. 6th Int. Conf. on multimodal interfaces, State College, PA, USA, October</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Williams II R.L (1999) Planar cable-suspended haptic interface: design for Wrench Exertion. Proc. 1999 ASME de" /><p class="c-article-references__text" id="ref-CR116">Williams II R.L (1999) Planar cable-suspended haptic interface: design for Wrench Exertion. Proc. 1999 ASME design tech. conf., 25th design automation conf., Las Vegas</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Wilson, R. Stevenson, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Wilson D, Stevenson R (2003) Olfactory perceptual learning: the critical role of memory in odor discrimination" /><p class="c-article-references__text" id="ref-CR117">Wilson D, Stevenson R (2003) Olfactory perceptual learning: the critical role of memory in odor discrimination. Neurosci Biobehav Rev 27:307–328</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0149-7634%2803%2900050-2" aria-label="View reference 118">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 118 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20perceptual%20learning%3A%20the%20critical%20role%20of%20memory%20in%20odor%20discrimination&amp;journal=Neurosci%20Biobehav%20Rev&amp;volume=27&amp;pages=307-328&amp;publication_year=2003&amp;author=Wilson%2CD&amp;author=Stevenson%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HI" /><p class="c-article-references__text" id="ref-CR118">Winn W (1993) A conceptual basis for educational applications of virtual reality, University of Washington, HITL, Seattle, WA, technical publication: R-93–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Winn, M. Windschitl, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Winn W, Windschitl M (2001) Learning science in virtual environments: the interplay of theory and experience. " /><p class="c-article-references__text" id="ref-CR119">Winn W, Windschitl M (2001) Learning science in virtual environments: the interplay of theory and experience. Themes Educ 1(4):373–389</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 120 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20science%20in%20virtual%20environments%3A%20the%20interplay%20of%20theory%20and%20experience&amp;journal=Themes%20Educ&amp;volume=1&amp;issue=4&amp;pages=373-389&amp;publication_year=2001&amp;author=Winn%2CW&amp;author=Windschitl%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu W, Basdogan C, Srinivasan M (1999) The effect of perspective on visual-haptic perception of object size and" /><p class="c-article-references__text" id="ref-CR120">Wu W, Basdogan C, Srinivasan M (1999) The effect of perspective on visual-haptic perception of object size and compliance in virtual environments. Proc. ASME Dynamic systems and control division; 67</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose trac" /><p class="c-article-references__text" id="ref-CR121">Yanagida, Y, Kawato S, Nom a H, Tomono A, Tetsutani N (2004) Projection-based olfactory display with nose tracking. Proc. IEEE virtual reality conf. (VRC’04) Chicago, March; 43–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Youngblut C, Johnson R, Nash S, Weinclaw R, Will C (1996) Review of virtual environment interface technology I" /><p class="c-article-references__text" id="ref-CR122">Youngblut C, Johnson R, Nash S, Weinclaw R, Will C (1996) Review of virtual environment interface technology IDA paper P-3186 8:209–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US" /><p class="c-article-references__text" id="ref-CR123">Youngblut C (1998) Educational use of virtual reality technology. Tech. Report. Inst. Defense Analyses, US</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0040-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We wish to acknowledge the assistance of the student Pierre Guérin, who has developed the “Haptic Atomic” user interface. </p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Laboratoire d’Ingénierie des Systèmes Automatisés, Université d’Angers – EA 4014, 62, Avenue Notre Dame du Lac, 49000, Angers, France</p><p class="c-article-author-affiliation__authors-list">E. Richard, A. Tijou, P. Richard &amp; J.-L. Ferrier</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-E_-Richard"><span class="c-article-authors-search__title u-h3 js-search-name">E. Richard</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;E.+Richard&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=E.+Richard" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22E.+Richard%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-A_-Tijou"><span class="c-article-authors-search__title u-h3 js-search-name">A. Tijou</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;A.+Tijou&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=A.+Tijou" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22A.+Tijou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-P_-Richard"><span class="c-article-authors-search__title u-h3 js-search-name">P. Richard</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;P.+Richard&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=P.+Richard" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22P.+Richard%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-J__L_-Ferrier"><span class="c-article-authors-search__title u-h3 js-search-name">J.-L. Ferrier</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J.-L.+Ferrier&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J.-L.+Ferrier" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J.-L.+Ferrier%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0040-8/email/correspondent/c1/new">P. Richard</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Multi-modal%20virtual%20environments%20for%20education%20with%20haptic%20and%20olfactory%20feedback&amp;author=E.%20Richard%20et%20al&amp;contentID=10.1007%2Fs10055-006-0040-8&amp;publication=1359-4338&amp;publicationDate=2006-10-17&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Richard, E., Tijou, A., Richard, P. <i>et al.</i> Multi-modal virtual environments for education with haptic and olfactory feedback.
                    <i>Virtual Reality</i> <b>10, </b>207–225 (2006). https://doi.org/10.1007/s10055-006-0040-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0040-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12-17">17 December 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05-22">22 May 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-10-17">17 October 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-12">December 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0040-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0040-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual Environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Olfaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multi-modal feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human scale</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Education</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0040-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=40;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

