<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Immersive manipulation of virtual objects through glove-based hand ges"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are therefore presented to provide natural..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Immersive manipulation of virtual objects through glove-based hand gesture interaction"/>

    <meta name="dc.source" content="Virtual Reality 2011 16:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-08-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are therefore presented to provide natural user interaction to enable effortless virtual object manipulation. Presented in this paper is the development and evaluation of an immersive human&#8211;computer interaction system based on stereoscopic viewing and natural hand gestures. For the development, it is based on the integration of a back-projection stereoscopic system for object and hand display, a hybrid inertial and ultrasonic tracking system to provide the absolute positions and orientations of the user&#8217;s head and hands, as well as a pair of high degrees-of-freedom data gloves to provide the relative positions and orientations of digit joints and tips on both hands. For the evaluation, it is based on a two-object scene with a virtual cube and a CT (computed tomography) volume created for demonstration of real-time immersive object manipulation. The system is shown to provide a correct user view of objects and hands in 3D with depth, as well as to enable a user to use a number of simple hand gestures to perform basic object manipulation tasks involving selection, release, translation, rotation and scaling. Also included in the evaluation are some quantitative tests of the system performance in terms of speed and latency."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-08-05"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="243"/>

    <meta name="prism.endingPage" content="252"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0195-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0195-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0195-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0195-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Immersive manipulation of virtual objects through glove-based hand gesture interaction"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2012/09"/>

    <meta name="citation_online_date" content="2011/08/05"/>

    <meta name="citation_firstpage" content="243"/>

    <meta name="citation_lastpage" content="252"/>

    <meta name="citation_article_type" content="SI: Cyberworlds09"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0195-9"/>

    <meta name="DOI" content="10.1007/s10055-011-0195-9"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0195-9"/>

    <meta name="description" content="Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are"/>

    <meta name="dc.creator" content="Gan Lu"/>

    <meta name="dc.creator" content="Lik-Kwan Shark"/>

    <meta name="dc.creator" content="Geoff Hall"/>

    <meta name="dc.creator" content="Ulrike Zeshan"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adamo-Villani N., Heisler J, Arns L (2007) Two gesture recognition systems for immersive math education for the deaf. In: ACM proceedings of IMMERSCOM 2007, Verona, Italy, pp 10&#8211;12"/>

    <meta name="citation_reference" content="citation_title=Simulating humans, computer graphics animation and control; citation_publication_date=1993; citation_id=CR2; citation_author=NI Badler; citation_author=CB Philips; citation_author=BL Webber; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. In: Proceedings of HCI international, New Orleans, USA, pp 629&#8211;633"/>

    <meta name="citation_reference" content="Corradini A, Cohen P (2002) On the relationships among speech, gestures, and object manipulation in virtual environments: initial Evidence. In: Proceedings of the international CLASS workshop on natural, intelligent and effective interaction in multimodal dialogue systems, Copenhagen, Denmark"/>

    <meta name="citation_reference" content="Corvaglia D (2004) Virtual training for manufacturing and maintenance based on Web3d technologies. In: Proceeding of LET-Web3D 2004: 1st international workshop on Web3D technologies in learning, education and training, pp 28&#8211;33"/>

    <meta name="citation_reference" content="citation_journal_title=Sens Rev; citation_title=Spatially continuous six degree of freedom position, orientation sensor; citation_author=L Danisch, K Englehart, A Trivett; citation_volume=19; citation_issue=2; citation_publication_date=1999; citation_pages=106-112; citation_doi=10.1108/02602289910266142; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Virutal Real; citation_title=Untethered gesture acquisition and recognition for virtual world manipulation; citation_author=D Demirdjian, T Ko, T Darrell; citation_volume=8; citation_publication_date=2005; citation_pages=222-230; citation_doi=10.1007/s10055-005-0155-3; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part C: Appl Rev; citation_title=A survey of glove-based systems and their applications; citation_author=L Dipietro, AM Sabatini; citation_volume=38; citation_publication_date=2008; citation_pages=461-482; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Vision-based hand pose estimation: a review; citation_author=A Erol, G Bebis, M Nicolescu, RD Boyle, X Twombly; citation_volume=108; citation_publication_date=2007; citation_pages=52-73; citation_doi=10.1016/j.cviu.2006.10.012; citation_id=CR9"/>

    <meta name="citation_reference" content="Garg P, Aggarwal N, Sofat S (2009) Vision based hand gesture recognition. In: Proceedings of world academy of science, engineering and technology, pp 972&#8211;977"/>

    <meta name="citation_reference" content="IS-900 Precision Inertial-ultrasonic Motion Tracking System, InterSense Inc. 
                    http://www.isense.com
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Sigart Bulletin, ACM Press; citation_title=Steve: an animated pedagogical agent for procedural training in virtual environments; citation_author=WL Johnson, J Rickel; citation_volume=8; citation_issue=1&#8211;4; citation_publication_date=1997; citation_pages=16-21; citation_id=CR12"/>

    <meta name="citation_reference" content="Kober C, Boerner BI, Mori S, Tellez CB, Klarh&#246;fer M, Scheffler K, Sader R, Zeilhofer HF (2007) Stereoscopic 4D-visualization of craniofacial soft tissue based on dynamic MRI and 256 row 4D-CT. Adv Med Eng Springer Proc Phys Part II, pp 175&#8211;180"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Brain-computer interfaces, virtual reality, and videogames; citation_author=A L&#233;cuyer, F Lotte, R Reilly, R Leeb, M Hirose, M Slater; citation_volume=41; citation_publication_date=2008; citation_pages=66-72; citation_doi=10.1109/MC.2008.410; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Chirurgie de la Main; citation_title=The future of robotics in hand surgery; citation_author=P Liverneaux, E Nectoux, C Taleb; citation_volume=28; citation_publication_date=2009; citation_pages=278-285; citation_doi=10.1016/j.main.2009.08.002; citation_id=CR15"/>

    <meta name="citation_reference" content="O&#8217;Hagan RG, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments, interacting with computers, vol 14, pp 231&#8211;250, April, 2002"/>

    <meta name="citation_reference" content="citation_journal_title=Radiother Oncol; citation_title=A virtual reality solution for evaluation of radiotherapy plans; citation_author=D Patel, LP Muren, A Mehus, Y Kvinnsland, DM Ulvang, KP Villanger; citation_volume=82; citation_issue=2; citation_publication_date=2007; citation_pages=218-221; citation_doi=10.1016/j.radonc.2006.11.024; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Designing cranial implants in a haptic augmented reality environment; citation_author=C Scharver, R Evenhouse, A Johnson, J Leigh; citation_volume=47; citation_publication_date=2004; citation_pages=32-38; citation_doi=10.1145/1012037.1012059; citation_id=CR18"/>

    <meta name="citation_reference" content="ShapeHand Data Glove, Measurand Inc. 
                    http://www.measurand.com
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=A survey of glove-based input; citation_author=DJ Sturman, D Zeltzer; citation_volume=14; citation_issue=1; citation_publication_date=1994; citation_pages=30-39; citation_doi=10.1109/38.250916; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=IEIC Tech Rep; citation_title=Trials on grasping of a 3D virtual object in CAVE using EEG signals; citation_author=H Toyama; citation_volume=91; citation_publication_date=2006; citation_pages=53-56; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Acad Radiol; citation_title=Stereo CT image compositing methods for lung nodule detection, characterization; citation_author=XH Wang, WF Good, CR Fuhrman, JH Sumkin, CA Britton, SK Golla; citation_volume=12; citation_issue=12; citation_publication_date=2005; citation_pages=1512-1520; citation_doi=10.1016/j.acra.2005.06.009; citation_id=CR21"/>

    <meta name="citation_reference" content="Woods AJ (2005) Compatibility of display products with stereoscopic display methods. In: Proceedings of the international display manufacturing conference, Taipei, Taiwan"/>

    <meta name="citation_reference" content="Wormell D, Foxlin E (2003) Advancements in 3D interactive devices for virtual environments. In: Proceedings of the workshop on virtual environments, Zurich, Switzerland, pp 47&#8211;56"/>

    <meta name="citation_reference" content="Zhang S, Demiralp C, Keefe D, DaSilva M, Laidlaw DH, Greenberg, BD, Basser PJ, Pierpaoli C, Chiocca EA, Deisboeck TS (2001) An immersive virtual environment for DT-MRI volume visualization applications: a case study. In: Proceedings of IEEE visualization conference, San Diego, USA, pp 437&#8211;440"/>

    <meta name="citation_author" content="Gan Lu"/>

    <meta name="citation_author_email" content="glu@uclan.ac.uk"/>

    <meta name="citation_author_institution" content="Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, UK"/>

    <meta name="citation_author" content="Lik-Kwan Shark"/>

    <meta name="citation_author_institution" content="Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, UK"/>

    <meta name="citation_author" content="Geoff Hall"/>

    <meta name="citation_author_institution" content="Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, UK"/>

    <meta name="citation_author" content="Ulrike Zeshan"/>

    <meta name="citation_author_institution" content="International Centre for Sign Languages and Deaf Studies, University of Central Lancashire, Preston, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0195-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0195-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Immersive manipulation of virtual objects through glove-based hand gesture interaction"/>
        <meta property="og:description" content="Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are therefore presented to provide natural user interaction to enable effortless virtual object manipulation. Presented in this paper is the development and evaluation of an immersive human–computer interaction system based on stereoscopic viewing and natural hand gestures. For the development, it is based on the integration of a back-projection stereoscopic system for object and hand display, a hybrid inertial and ultrasonic tracking system to provide the absolute positions and orientations of the user’s head and hands, as well as a pair of high degrees-of-freedom data gloves to provide the relative positions and orientations of digit joints and tips on both hands. For the evaluation, it is based on a two-object scene with a virtual cube and a CT (computed tomography) volume created for demonstration of real-time immersive object manipulation. The system is shown to provide a correct user view of objects and hands in 3D with depth, as well as to enable a user to use a number of simple hand gestures to perform basic object manipulation tasks involving selection, release, translation, rotation and scaling. Also included in the evaluation are some quantitative tests of the system performance in terms of speed and latency."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Immersive manipulation of virtual objects through glove-based hand gesture interaction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0195-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Hand gesture tracking and recognition, Immersive stereoscopic visualisation, Virtual object manipulation","kwrd":["Hand_gesture_tracking_and_recognition","Immersive_stereoscopic_visualisation","Virtual_object_manipulation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0195-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0195-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=195;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0195-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Immersive manipulation of virtual objects through glove-based hand gesture interaction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0195-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0195-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Cyberworlds09</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-08-05" itemprop="datePublished">05 August 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Immersive manipulation of virtual objects through glove-based hand gesture interaction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gan-Lu" data-author-popup="auth-Gan-Lu" data-corresp-id="c1">Gan Lu<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Central Lancashire" /><meta itemprop="address" content="grid.7943.9, 0000000121673843, Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, PR1 2HE, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Lik_Kwan-Shark" data-author-popup="auth-Lik_Kwan-Shark">Lik-Kwan Shark</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Central Lancashire" /><meta itemprop="address" content="grid.7943.9, 0000000121673843, Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, PR1 2HE, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Geoff-Hall" data-author-popup="auth-Geoff-Hall">Geoff Hall</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Central Lancashire" /><meta itemprop="address" content="grid.7943.9, 0000000121673843, Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, PR1 2HE, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ulrike-Zeshan" data-author-popup="auth-Ulrike-Zeshan">Ulrike Zeshan</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Central Lancashire" /><meta itemprop="address" content="grid.7943.9, 0000000121673843, International Centre for Sign Languages and Deaf Studies, University of Central Lancashire, Preston, PR1 2HE, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">243</span>–<span itemprop="pageEnd">252</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">882 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">19 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0195-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Immersive visualisation is increasingly being used for comprehensive and rapid analysis of objects in 3D and object dynamic behaviour in 4D. Challenges are therefore presented to provide natural user interaction to enable effortless virtual object manipulation. Presented in this paper is the development and evaluation of an immersive human–computer interaction system based on stereoscopic viewing and natural hand gestures. For the development, it is based on the integration of a back-projection stereoscopic system for object and hand display, a hybrid inertial and ultrasonic tracking system to provide the absolute positions and orientations of the user’s head and hands, as well as a pair of high degrees-of-freedom data gloves to provide the relative positions and orientations of digit joints and tips on both hands. For the evaluation, it is based on a two-object scene with a virtual cube and a CT (computed tomography) volume created for demonstration of real-time immersive object manipulation. The system is shown to provide a correct user view of objects and hands in 3D with depth, as well as to enable a user to use a number of simple hand gestures to perform basic object manipulation tasks involving selection, release, translation, rotation and scaling. Also included in the evaluation are some quantitative tests of the system performance in terms of speed and latency.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Stereoscopic display (Woods <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Woods AJ (2005) Compatibility of display products with stereoscopic display methods. In: Proceedings of the international display manufacturing conference, Taipei, Taiwan" href="/article/10.1007/s10055-011-0195-9#ref-CR22" id="ref-link-section-d17e331">2005</a>) enables viewing of an object as an image in 3D with depth information and the object behaviour as an image sequence in 4D with depth and time information. This is increasingly being used in comprehensive and rapid visualisation of complex data sets, such as 3D CT (computed tomography) data and 4D dynamic MR (magnetic resonance) data in medical diagnosis and treatment planning (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wang XH, Good WF, Fuhrman CR, Sumkin JH, Britton CA, Golla SK (2005) Stereo CT image compositing methods for lung nodule detection, characterization. Acad Radiol 12(12):1512–1520" href="/article/10.1007/s10055-011-0195-9#ref-CR21" id="ref-link-section-d17e334">2005</a>; Patel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Patel D, Muren LP, Mehus A, Kvinnsland Y, Ulvang DM, Villanger KP (2007) A virtual reality solution for evaluation of radiotherapy plans. Radiother Oncol 82(2):218–221" href="/article/10.1007/s10055-011-0195-9#ref-CR17" id="ref-link-section-d17e337">2007</a>; Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Zhang S, Demiralp C, Keefe D, DaSilva M, Laidlaw DH, Greenberg, BD, Basser PJ, Pierpaoli C, Chiocca EA, Deisboeck TS (2001) An immersive virtual environment for DT-MRI volume visualization applications: a case study. In: Proceedings of IEEE visualization conference, San Diego, USA, pp 437–440" href="/article/10.1007/s10055-011-0195-9#ref-CR11" id="ref-link-section-d17e340">2001</a>; Kober et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kober C, Boerner BI, Mori S, Tellez CB, Klarhöfer M, Scheffler K, Sader R, Zeilhofer HF (2007) Stereoscopic 4D-visualization of craniofacial soft tissue based on dynamic MRI and 256 row 4D-CT. Adv Med Eng Springer Proc Phys Part II, pp 175–180" href="/article/10.1007/s10055-011-0195-9#ref-CR13" id="ref-link-section-d17e343">2007</a>). A more immersive visualisation can be achieved by the use of a large wall display, as well as head tracking to allow the user to move around and to view from different perspectives based on the user’s head position and orientation. However, there is a challenge to provide an immersive interaction with the virtual object projected in stereoscopic mode without using indirect manipulation methods such as keyboard-based control, mouse-based 3D widgets, or hand-held input devices like wireless 3D wands.</p><p>Although more direct manipulation of virtual objects could be achieved by sensing and processing various natural human communication signals such as audible speech and hidden electroencephalography (EEG) (Demirdjian et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipulation. Virutal Real 8:222–230" href="/article/10.1007/s10055-011-0195-9#ref-CR7" id="ref-link-section-d17e349">2005</a>; Corradini and Cohen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Corradini A, Cohen P (2002) On the relationships among speech, gestures, and object manipulation in virtual environments: initial Evidence. In: Proceedings of the international CLASS workshop on natural, intelligent and effective interaction in multimodal dialogue systems, Copenhagen, Denmark" href="/article/10.1007/s10055-011-0195-9#ref-CR4" id="ref-link-section-d17e352">2002</a>; Lécuyer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lécuyer A, Lotte F, Reilly R, Leeb R, Hirose M, Slater M (2008) Brain-computer interfaces, virtual reality, and videogames. Computer 41:66–72" href="/article/10.1007/s10055-011-0195-9#ref-CR14" id="ref-link-section-d17e355">2008</a>; Toyama <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Toyama H (2006) Trials on grasping of a 3D virtual object in CAVE using EEG signals. IEIC Tech Rep 91:53–56" href="/article/10.1007/s10055-011-0195-9#ref-CR20" id="ref-link-section-d17e358">2006</a>), none of these human–computer interface strategies can be compared with the hand gesture–based user interface that provides a far more intuitive, natural and immersive interaction for users to manipulate 3D virtual objects based on normal human actions (Dipietro and Sabatini <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dipietro L, Sabatini AM (2008) A survey of glove-based systems and their applications. IEEE Trans Syst Man Cybern Part C: Appl Rev 38:461–482" href="/article/10.1007/s10055-011-0195-9#ref-CR8" id="ref-link-section-d17e361">2008</a>; O’Hagan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="O’Hagan RG, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments, interacting with computers, vol 14, pp 231–250, April, 2002" href="/article/10.1007/s10055-011-0195-9#ref-CR16" id="ref-link-section-d17e365">2002</a>; Garg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Garg P, Aggarwal N, Sofat S (2009) Vision based hand gesture recognition. In: Proceedings of world academy of science, engineering and technology, pp 972–977" href="/article/10.1007/s10055-011-0195-9#ref-CR10" id="ref-link-section-d17e368">2009</a>).</p><p>Among various approaches to recognise hand gestures, one is based on the use of a video camera (Erol et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Erol A, Bebis G, Nicolescu M, Boyle RD, Twombly X (2007) Vision-based hand pose estimation: a review. Comput Vis Image Underst 108:52–73" href="/article/10.1007/s10055-011-0195-9#ref-CR9" id="ref-link-section-d17e374">2007</a>), whereby the hand movements and gestures are recognised based on the dynamic hand shapes extracted from the video sequence. The difficulties associated with this approach include:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>self-occlusion resulting in capture of partial hand gestures due to the restricted camera view angle;</p>
                  </li>
                  <li>
                    <p>incorrect image segmentation of hands due to different lighting and background conditions; and</p>
                  </li>
                  <li>
                    <p>high computation cost due to the requirements to track rapid hand motion and to handle high complexity of hand with at least one degree of freedom (DOF) for each hand digit joint and 27 DOF for just one hand (Adamo-Villani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Adamo-Villani N., Heisler J, Arns L (2007) Two gesture recognition systems for immersive math education for the deaf. In: ACM proceedings of IMMERSCOM 2007, Verona, Italy, pp 10–12" href="/article/10.1007/s10055-011-0195-9#ref-CR1" id="ref-link-section-d17e393">2007</a>).</p>
                  </li>
                </ul>
              <p>These difficulties can be overcome by wearing a pair of data gloves with sensors to provide finger and thumb movement information at the expense of introducing a small inconvenience to the user (Sturman and Zeltzer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39" href="/article/10.1007/s10055-011-0195-9#ref-CR19" id="ref-link-section-d17e402">1994</a>). The simplest type of data glove is based on contact, using conductive patches in the glove (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. In: Proceedings of HCI international, New Orleans, USA, pp 629–633" href="/article/10.1007/s10055-011-0195-9#ref-CR3" id="ref-link-section-d17e405">2001</a>). With the requirement of hand digits touching each other to make electrical contact, the recognisable gestures are not necessarily natural, and the number of identifiable gestures is limited. A more sophisticated data glove providing more digit movement information is based on flexure. It uses fibre-optic, mechanical or piezoresistive sensors to measure the deformation of each hand digit. For the work described in this paper, the hand gesture recognition is based on a high DOF data glove, called ShapeHand from Measurand (ShapeHand Data Glove, Measurand Inc., <a href="http://www.measurand.com">http://www.measurand.com</a>, Danisch et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Danisch L, Englehart K, Trivett A (1999) Spatially continuous six degree of freedom position, orientation sensor. Sens Rev 19(2):106–112" href="/article/10.1007/s10055-011-0195-9#ref-CR6" id="ref-link-section-d17e415">1999</a>). It can be considered as one of the most precise and sophisticated data gloves, which uses 40 fibre-optic sensors on each glove to provide all finger joint movement information, 27 DOF for each hand.</p><p>To achieve immersive manipulation of virtual objects through glove-based hand gesture interaction, presented in this paper is a small-scale system implemented for demonstration and evaluation. The system integrates three elements:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>a pair of wireless high DOF data gloves (Measurand ShapeHand Data Glove, Measurand Inc., <a href="http://www.measurand.com">http://www.measurand.com</a>) to provide all the hand digit joint movement information;</p>
                  </li>
                  <li>
                    <p>a wireless hybrid inertial and ultrasonic tracking system (IS-900 Precision Inertial-ultrasonic Motion Tracking System, InterSense Inc., <a href="http://www.isense.com">http://www.isense.com</a>; Wormell and Foxlin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wormell D, Foxlin E (2003) Advancements in 3D interactive devices for virtual environments. In: Proceedings of the workshop on virtual environments, Zurich, Switzerland, pp 47–56" href="/article/10.1007/s10055-011-0195-9#ref-CR23" id="ref-link-section-d17e448">2003</a>) to provide the 3D positions and 3D orientations of the user’s head and two hands; and</p>
                  </li>
                  <li>
                    <p>a large screen with two stereoscopic back projections to show the virtual object being manipulated and to provide a visual feedback of two hands with respect to the virtual object in 3D with depth.</p>
                  </li>
                </ul>
              <p>The paper is organised as follows. While Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec2">2</a> presents system development, which includes a description of the hardware integration in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec3">2.1</a> and software implementation in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec4">2.2</a>. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec3">3</a> presents algorithms developed for data integration in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec6">3.1</a>, for gesture recognition in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec7">3.2</a> and for object distance computation in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec8">3.3</a>. These are followed by some system demonstration and evaluation results in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec4">4</a> to show system performance. Finally, some concluding remarks are given in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0195-9#Sec12">5</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">System development</h2><div class="c-article-section__content" id="Sec2-content"><p>This section presents hardware and software aspects associated with system development in terms of integration.</p><h3 class="c-article__sub-heading" id="Sec3">Hardware integration</h3><p>The demonstration system developed for real-time immersive object manipulation is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig1">1</a>. Driven by a desktop computer, the system is an integration of three subsystems for gesture data acquisition, position data acquisition and stereoscopic display.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>System hardware block diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For gesture data acquisition from hands, a pair of wireless ShapeHand data gloves from Measurand is used. These data gloves are based on flexible tapes embedded with multiple fibre-optic curvature sensors arranged to sense bend and twist along the length of each tape. By attaching the tapes to run along each digit with one end at the finger tip and the other end fed to a small data acquisition box at wrist, the gesture movements of digits introduce deformation to the tapes. Also, the bend and twist measured at each sensor location with respect to the wrist end of the tape enables relative positions and orientations of each digit joint to be determined. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig1">1</a>, via the ShapeHand Data Concentrator, the collected hand gesture data are transmitted to a wireless receiver/router connected to the Ethernet port of the computer.</p><p>Since absolute hand position and orientation data in 3D space are not provided by ShapeHand data gloves, an IS-900 wireless tracking system from InterSense (IS-900 Precision Inertial-ultrasonic Motion Tracking System, InterSense Inc., <a href="http://www.isense.com">http://www.isense.com</a>) is used to provide the required hand position data. The system is also used to provide the head position and orientation data in order to generate a correct view. The operation of the system is based on a combination of inertial tracking and ultrasonic tracking. Whilst the outputs from the inertial sensors, consisting of accelerometers and gyros, are used to determine the position and orientation of each sensor in 3D space, the range measurements based on time-of-flight between ultrasonic emitters and receivers are used to correct the drifting effect inherent within the inertial sensors.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig1">1</a>, IS-900 SoniStrips containing ultrasonic emitters are mounted on the ceiling, which transmit ultrasonic pulses upon receiving addressed signals from the IS-900 processor that is connected to the serial port of the computer. Three MiniTrax tracking devices containing inertial sensors and ultrasonic receivers are used with two attached to the user’s wrists and one attached to the user’s head. Each MiniTrax tracking device performs time-of-flight range measurement based on the ultrasonic pulses received, and transmit its position and orientation data to the corresponding MiniTrax receiver connected to the IS-900.</p><p>For stereoscopic display, the demonstration system uses one large screen with size of 2.74 m × 2.06 m (shown as the middle screen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig1">1</a>), and two back projectors operating in passive circular polarisation mode are connected to the computer through two dual DVI graphics card output ports. 3D objects with depth effect are seen by the user wearing a pair of light-weight polarised glasses.</p><p>The computer used in the demonstration system runs on Microsoft Windows XP and is based on an Intel Xeon 3.06 GHz CPU with 2 GB RAM and NVIDIA Quadro FX 3,000 Graphics Card with 256 MB memory.</p><h3 class="c-article__sub-heading" id="Sec4">Software implementation</h3><p>The system software is implemented as a Windows XP-based application using C++. To minimise development time, the software utilises the Microsoft Foundation Classes (MFC) to build the user interface and control units. A modified version of the standard Document View Model has been implemented to allow the input data to update the document object (containing the current user head and hand position, as well as gesture data) and the output display to be treated as an individual ‘view’ of the document object. Furthermore, the software is implemented following a multi-thread approach to minimise response time for interactive object manipulation. There are five parallel program threads with two of them performing hand gesture data acquisition and extraction from ShapeHand, and the other three performing position data acquisition from InterSense, gesture recognition and stereoscopic display, respectively.</p><p>The two program threads for hand gesture data acquisition and extraction are implemented based on the ShapeHand API (Application Programming Interface). With steps including initiation of data collection, receiving data and checking received data, the program thread for data acquisition obtains raw data from a pair of wireless ShapeHand data glove via the Ethernet port of the computer. Using the raw data obtained, the required positions and orientations of a digit joint are determined in the program thread for data extraction.</p><p>The program thread of position data acquisition is implemented based on the InterSense API. It performs the data acquisition from the three MiniTrax tracking devices attached to the head and two wrists of the user via the serial port of the computer to provide their position and orientation data. Steps in this thread include data collection and data updating if the incoming data are found to be different from the previously received data.</p><p>The program thread of gesture recognition is based on the algorithm presented in the next Section. Essentially, it involves data merging through coordinate transformations, as well as tracking and recognising a number of pre-specified hand gestures for manipulation of the displayed virtual objects.</p><p>The program thread of stereoscopic display is implemented based on OpenGL programming to provide 3D visual feedback to the user. This is done by generating two views of the virtual objects and two hands. As viewing through polarisation glasses results in each eye seeing only the view generated for it, it creates a visual immersion with depth impression. Steps in this program thread include the use of the head position data acquired to specify the viewing position and direction of the left and right eyes, configuration of the viewing frustum for each eye and stereo rendering to draw the left and right images of the 3D objects and hand models by perspective projection.</p><p>These five program threads are executed simultaneously by the computer with each thread assigned a slice of its CPU (central processing unit) time. The scheduling of the threads is done in a round-robin manner, with all threads having the same priority. In execution of the program thread of position data acquisition during its allocated time slice, no change in the received data will result in early switching to the next program thread.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Hand tracking and gesture recognition</h2><div class="c-article-section__content" id="Sec5-content"><p>This section focuses on two data processing operations, namely, coordinate transformation to fuse multiple position and orientation data sets, acquired using different referencing systems, and the dynamic hand gesture tracking and recognition methods for immersive objects manipulation.</p><h3 class="c-article__sub-heading" id="Sec6">Data integration through coordinate transformation</h3><p>With different equipments using different coordinate systems for data acquisition, processing and display, coordinate transformations are required to bring different data sets into a unified coordinate system.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig2">2</a> illustrates the spatial relationship between different coordinate systems. The world coordinate system is defined to have the same orientation as the stereoscopic display. With the <i>x</i>-axis (denoted by <i>X</i>
                  <sub>
                    <i>w</i>
                  </sub>) pointing towards the right, the <i>y</i>-axis (denoted by <i>Y</i>
                  <sub>
                    <i>w</i>
                  </sub>) pointing upwards and the <i>z</i>-axis (denoted by <i>Z</i>
                  <sub>
                    <i>w</i>
                  </sub>) pointing towards the viewer, this forms a right-handed coordinate system with a positive rotation about the axis in the anticlockwise direction. Furthermore, the origin of the world coordinate system (denoted by <i>O</i>
                  <sub>
                    <i>w</i>
                  </sub>) is located at the middle of the screen along the <i>x</i>-axis (1.37 m away from the screen right edge), 1 m above the floor along the <i>y</i>-axis and 1.9 m in front of the screen along the <i>z</i>-axis.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Coordinate system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For the InterSense system with its coordinate axes denoted by (<i>X</i>
                  <sub>
                    <i>I</i>
                  </sub>, <i>Y</i>
                  <sub>
                    <i>I</i>
                  </sub>, <i>Z</i>
                  <sub>
                    <i>I</i>
                  </sub>), the position data acquired for head and wrists are calibrated with respect to its origin denoted by <i>O</i>
                  <sub>I</sub> at (−1.8, 1.5, 0 m) in the world coordinate system as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig2">2</a>. Furthermore, two rotation operations are required to align the orientations of the InterSense coordinate system with the orientations of the world coordinate system, namely, rotation of −90° about the InterSense <i>y</i>-axis, to make the new InterSense <i>x</i>-axis parallel to the world coordinate <i>x</i>-axis, and rotation of 90° about the new InterSense <i>x</i>-axis to make the new InterSense <i>y-</i> and <i>z-</i> axes parallel to the world coordinate systems. If <b><i>i</i></b> = [<i>x</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>y</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>z</i>
                  <sub>
                    <i>i</i>
                  </sub>, 1]′ denotes the homogeneous coordinates of a position in the InterSense coordinate system, then its corresponding homogeneous coordinates in the specified world coordinate system denoted by <b><i>i</i></b>
                  <sub>
                    <i>w</i>
                  </sub> = [<i>x</i>
                  <sub>
                    <i>iw</i>
                  </sub>, <i>y</i>
                  <sub>
                    <i>iw</i>
                  </sub>, <i>z</i>
                  <sub>
                    <i>iw</i>
                  </sub>, 1]′ are given by</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ i_{w} = \user2{T}^{I \to W} i $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <b><i>T</i></b>
                  <sup><i>I</i>→<i>W</i></sup> is the matrix for geometric transformation from the InterSense coordinate system to the world coordinate system. Based on the geometric relationship between the two coordinate systems described above, <b><i>T</i></b>
                  <sup><i>I</i>→<i>W</i></sup> is given by</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \user2{T}^{I \to W} = \left[ {\begin{array}{*{20}c} 0 &amp; 1 &amp; 0 &amp; { - 1.8} \\ 0 &amp; 0 &amp; { - 1} &amp; {1.5} \\ { - 1} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ \end{array} } \right]. $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                <p>For the ShapeHand system, the position data of each digit joint are acquired using the local tape coordinate system. When the hand is fully open as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig2">2</a>, the <i>x</i>-axis (denoted by <i>X</i>
                  <sub>
                    <i>S</i>
                  </sub>) runs along the length of the narrow flat tape (along each finger towards the finger tip), the <i>y</i>-axis (denoted by <i>Y</i>
                  <sub>
                    <i>S</i>
                  </sub>) is perpendicular to the back of the narrow flat tape (perpendicular to the palm back) and the <i>z</i>-axis (denoted by <i>Z</i>
                  <sub>
                    <i>S</i>
                  </sub>) is towards the side of the narrow flat tape (in the direction across the palm). Furthermore, the origin is fixed at the bottom of the palm in the middle of the wrist. With the wrist position and orientation data provided by the InterSense system, the digit joint position data need to be transformed from their local coordinate system to the InterSense coordinate system first and to the world coordinate system subsequently.</p><p>If <b><i>s</i></b> = [<i>x</i>
                  <sub>
                    <i>s</i>
                  </sub>, <i>y</i>
                  <sub>
                    <i>s</i>
                  </sub>, <i>z</i>
                  <sub>
                    <i>s</i>
                  </sub>, 1]′ denotes the homogeneous coordinates of a position in the local ShapeHand coordinate system, then its corresponding homogeneous coordinates in the specified world coordinate system denoted by <b><i>s</i></b>
                  <sub>
                    <i>w</i>
                  </sub> = [<i>x</i>
                  <sub>
                    <i>sw</i>
                  </sub>, <i>y</i>
                  <sub>
                    <i>sw</i>
                  </sub>, <i>z</i>
                  <sub>
                    <i>sw</i>
                  </sub>, 1]′ are given by</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \user2{s}_{w} = \user2{T}^{I \to W} \user2{T}^{s \to I} \user2{s} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <b><i>T</i></b>
                  <sup><i>S</i>→<i>I</i></sup> is the matrix for geometric transformation from the ShapeHand coordinate system to the InterSense coordinate system. If the position and orientation data provided by the InterSense system are denoted by (<i>x</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>y</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>z</i>
                  <sub>
                    <i>i</i>
                  </sub>) and (<i>α</i>
                  <sub>
                    <i>ι</i>
                  </sub>, <i>β</i>
                  <sub>
                    <i>ι</i>
                  </sub>, <i>γ</i>
                  <sub>
                    <i>ι</i>
                  </sub>), then <b><i>Τ</i></b>
                  <sup><i>S</i>→<i>I</i></sup> is given by</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \user2{T}^{S \to I} = \left[ {\begin{array}{*{20}c} {c_{\alpha } s_{\beta } - s_{\alpha } c_{\beta } s_{\gamma } } &amp; {c_{\beta } c_{i} } &amp; {c_{\alpha } c_{\beta } s_{\gamma } + s_{\alpha } s_{\beta } } &amp; {x_{i} } \\ {s_{\alpha } c_{\gamma } } &amp; { - s_{\gamma } } &amp; {c_{\alpha } c_{\gamma } } &amp; {y_{i} } \\ { - s_{\alpha } s_{\beta } s_{\gamma } - c_{\alpha } c_{\beta } } &amp; {s_{\alpha } c_{\gamma } } &amp; {c_{\alpha } s_{\beta } s_{\gamma } - s_{\alpha } c_{\gamma } } &amp; {z_{i} } \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ \end{array} } \right] $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>c</i> and <i>s</i> denoting cos and sin functions, with subscripts denoting the orientation angles from the InterSense system.</p><h3 class="c-article__sub-heading" id="Sec7">Gesture recognition</h3><p>A basic sequence in immersive virtual object manipulation can be considered as consisting of object selection at the start, followed by object manipulation, which can be a combination of translation, rotation and scaling, and object release at the end. The implementation requires selection of a set of meaningful hand gestures, as well as computation of the distances between hands and objects. For natural interaction and user’s comfort, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig3">3</a> shows three hand gestures selected for implementation of the five basic object manipulation operations, where the index finger pointing gesture shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig3">3</a>a is used not only for selection of a virtual object but also for object translation and rotation based on the position of the left or right index fingertip and the hand orientation; the hand-open gesture shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig3">3</a>b is used for the release of a selected object; and the gesture of two moving hands with the ring and small fingers closed shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig3">3</a>c is for object scaling.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Hand gestures: <b>a</b> object selection, translation and rotation; <b>b</b> object release; and <b>c</b> object scaling</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In order to execute the object selection and manipulation operation using the selected gestures, the 3D position of the left and right index fingertips need to be determined and tracked. As a hinged joint, there is only one DOF for the distal interphalangeal joint in the index finger.</p><p>With the distal phalanx length known through the measurement of the user’s index finger, the index fingertip position can be determined based on the distal interphalangeal joint position and the distal phalanx bending angle with respect to the middle phalanx provided by the ShapeHand data glove, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Index finger model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig4">4</a>, if <b><i>s</i></b>
                  <sup>dip</sup> =<span class="mathjax-tex">\( \left[ {x_{s}^{\text{dip}} ,y_{s}^{\text{dip}} ,z_{s}^{\text{dip}} ,1} \right]^{\prime } \)</span> denotes the homogeneous coordinates of the distal interphalangeal joint and <span class="mathjax-tex">\( \alpha_{s}^{\text{dp}} \)</span> denotes the distal phalanx bending angle in the local ShapeHand coordinate system, then the homogeneous coordinates of the index finger tip position in the world coordinate system denoted by <span class="mathjax-tex">\( \user2{s}_{w}^{\text{tip}} = \left[ {x_{sw}^{tip} ,y_{sw}^{tip} ,z_{sw}^{tip} ,1} \right]^{\prime } \)</span> are given by</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \user2{s}_{w}^{\text{tip}} = \user2{T}^{I \to W} \user2{T}^{S \to I} \left[ {\begin{array}{*{20}c} 1 &amp; 0 &amp; 0 &amp; {\cos \alpha_{s}^{dp} L_{dp} } \\ 0 &amp; 1 &amp; 0 &amp; {\sin \alpha_{s}^{dp} L_{dp} } \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ \end{array} } \right]\user2{s}^{\text{dip}} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>L</i>
                  <sub>dp</sub> denotes the distal phalanx length.</p><p>Moreover, the three selected gestures shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig3">3</a> are seen to consist of a combination of bending down and extending thumb and fingers in each hand, which can be determined based on the flexion angle of the proximal phalanx of the thumb or finger with respect to the back of the hand. With the digit flexion angle calibrated to 0º to correspond to a fully extended position (by hand opening) and 90º to correspond to a fully bending down position (by hand closing), the selected hand gestures can be recognised by expressing them using the corresponding binary state based on a threshold of 45º. Let two hands be denoted by <i>H,</i> with its binary state set to logic 0 for the left and logic 1 for the right, and let the thumb and four fingers in each hand be denoted by <i>T</i>, <i>I</i>, <i>M</i>, <i>R</i> and <i>S,</i> with the binary state of each one set to logic 0 if its flexion angle, <span class="mathjax-tex">\( \alpha_{s}^{\text{pp}} , \)</span> measured by the ShapeHand data glove is greater than 45º, and logic 1 otherwise. The index finger pointing gesture is given by</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left( {\bar{H}\bar{T}I\bar{M}\bar{R}\bar{S}} \right) \cup \left( {H\bar{T}I\bar{M}\bar{R}\bar{S}} \right) $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>the hand-open gesture is given by</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left( {\bar{H}TIMRS} \right) \cup \left( {HTIMRS} \right) $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>and the two hand moving gesture for object scaling is give by</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left( {\bar{H}TIM\bar{R}\bar{S}} \right) \cap \left( {HTIM\bar{R}\bar{S}} \right). $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                <h3 class="c-article__sub-heading" id="Sec8">Object distance computation</h3><p>Apart from the object release operation that requires only recognition of the corresponding hand gesture, other object manipulation operations require additional information of the object with respect to the user’s hands in terms of its location, orientation and size.</p><p>Let the virtual object to be manipulated be denoted by <b><i>o</i></b> centred at (<i>o</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub>) in the world coordinate system, with orientation of (<i>o</i>
                  <sub>α</sub>, <i>o</i>
                  <sub>β</sub>, <i>o</i>
                  <sub>γ</sub>), and with its bounding box defined by lengths of (<i>L</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>L</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>L</i>
                  <sub>
                    <i>z</i>
                  </sub>). Object selection requires not only recognition of the index finger pointing gesture by using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ6">6</a>) but also the position of the index finger tip with respect to the object bounding box in 3D space. A virtual object is selected, if the index finger tip (left or right) touches anywhere in one of the side faces of the bounding box. Hence, upon recognition of the index finger pointing gesture, two more conditions need to be satisfied for a virtual object to be selected. One is based on the distances between the index finger tip and the side face centres of the bounding box, and the other is based on the distances between the index finger tip and the side face planes of the bounding box.</p><p>For the first condition, if <i>S</i>
                  <sub>
                    <i>i</i>
                  </sub> with <i>i</i> = 1, 2, …, 6, denote the six side planes of the virtual object bounding box, and <i>P</i>
                  <sub>
                    <i>i</i>
                    <b>,</b>1</sub> = [<i>x</i>
                  <sub>
                    <i>Pi</i>,1</sub>, <i>y</i>
                  <sub>
                    <i>Pi</i>,1</sub>, <i>z</i>
                  <sub>
                    <i>Pi</i>,1</sub>] the coordinates of each side plane centre, then the distance between the pointing index finger tip and each side plane centre is given by</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\text{dist}}\left( {s_{w}^{\text{tip}} ,\user2{P}_{i,1} } \right) = \sqrt {\left( {x_{sw}^{\text{tip}} - x_{Pi,1} } \right)^{2} + \left( {y_{sw}^{\text{tip}} - y_{Pi,1} } \right)^{2} + \left( {z_{sw}^{\text{tip}} - z_{Pi,1} } \right)^{2} } $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
                <p>For the second condition, three non-collinear points lying on each side of the bounding box are selected to represent each side plane. If these three points are denoted by <b><i>P</i></b>
                  <sub>
                    <i>i</i>
                    <b>,</b>
                    <i>j</i>
                  </sub> with <i>i</i> = 1, 2, …, 6 and <i>j</i> = 1, 2, 3, then the distance between the pointing index finger tip and each side plane of the bounding box is given by</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\text{dist}}\left( {\user2{s}_{sw}^{\text{tip}} ,S_{i} } \right) = \frac{{\left| {\left( {a_{i} x_{sw}^{\text{tip}} + b_{i} y_{sw}^{\text{tip}} + c_{i} z_{sw}^{\text{tip}} + d_{i} } \right)} \right|}}{{\sqrt {a_{i}^{2} + b_{i}^{2} + c_{i}^{2} } }} $$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>where <i>a</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>b</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>c</i>
                  <sub>
                    <i>i</i>
                  </sub> and <i>d</i>
                  <sub>
                    <i>i</i>
                  </sub> are the coefficients of the plane equation for <i>S</i>
                  <sub>
                    <i>i</i>
                  </sub> and are obtained by solving the following equation</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[ {\begin{array}{*{20}c} {x_{Pi,1} } &amp; {y_{Pi,1} } &amp; {z_{Pi,1} } &amp; 1 \\ {x_{Pi,2} } &amp; {y_{Pi,2} } &amp; {z_{Pi,2} } &amp; 1 \\ {x_{Pi,3} } &amp; {y_{Pi,3} } &amp; {z_{Pi,3} } &amp; 1 \\ \end{array} } \right]\left[ {\begin{array}{*{20}c} {a_{i} } \\ {b_{i} } \\ {c_{i} } \\ {d_{i} } \\ \end{array} } \right] = \left[ {\begin{array}{*{20}c} 0 \\ 0 \\ 0 \\ \end{array} } \right] $$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div>
                <p>In the implementation, the three non-collinear points selected for each side of the bounding box include the corresponding side plane centre, and a virtual object is selected when the following condition is true</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left\{ {\left( {\overline{H} \overline{T} I\overline{M} \overline{R} \overline{S} } \right) \cup \left( {H\overline{T} I\overline{M} \overline{R} \overline{S} } \right)} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw}^{\text{tip}} ,\user2{P}_{i,1} } \right) \le \min (L_{x} ,L_{y} ,L_{z} )/2} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw}^{\text{tip}} ,S_{i} } \right) = 0} \right\} $$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
                <p>In (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ12">12</a>), the first two terms come from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ6">6</a>) and are used to confirm the index finger pointing gesture that can be made by the left and/or right hand, the third and fourth terms indicate the index finger tip touching a side face of the bounding box. When (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ12">12</a>) is satisfied, the bounding box of the virtual object is highlighted to provide a visual feedback to the user.</p><p>For object translation and rotation following object selection, it was implemented by making the object centre follow the current 3D position of the index fingertip computed using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ5">5</a>) and the object 3D orientation to follow the current wrist orientation provided by InterSense. If both hands are making the index finger pointing gestures, the position of the selected object will follow the index finger with minimum distance to the object centre. Since a user may use one pointing index finger to do object selection, translation and rotation with the other hand open, the object release operation is disabled if (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ12">12</a>) is satisfied.</p><p>Object scaling requires not only recognition of the two hand gesture using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ8">8</a>) but also the positions of the left and right index fingertips with respect to the object bounding box in 3D space. Hence, the function to activate the scaling operation can be expressed by</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left\{ {\left( {\overline{H}TIM \overline{R} \overline{S}} \right) \cap \left( {HTIM \overline{R} \overline{S}} \right)} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw,l}^{\text{tip}} ,\user2{P}_{i,1} } \right) \le \min (L_{x} ,L_{y} ,L_{z} )/2} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw,l}^{\text{tip}} ,S_{i} } \right) = 0} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw,r}^{tip} ,\user2{P}_{i,1} } \right) \le \min (L_{x} ,L_{y} ,L_{z} )/2} \right\} \cap \left\{ {{\text{dist}}\left( {\user2{s}_{sw,r}^{\text{tip}} ,S_{i} } \right) = 0} \right\} $$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>where the first two terms come from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ8">8</a>) and are used to recognise the object scaling gesture, the middle two terms indicate the left index finger tip touching a side of the bounding box and the last two terms indicated the right index finger tip touching a side of the bounding box. When (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0195-9#Equ13">13</a>) is satisfied, the scaling operation is activated, and the virtual object size is enlarged or reduced uniformly in 3D by setting the length of the object bounding box equal to the distance between two index fingertips.</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ L = \max \left[ {abs\left( {x_{w,l}^{\text{tip}} - x_{w,r}^{\text{tip}} } \right),abs\left( {y_{w,l}^{\text{tip}} - y_{w,r}^{\text{tip}} } \right),abs\left( {z_{w,l}^{\text{tip}} - z_{w,r}^{\text{tip}} } \right)} \right] $$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div>
                </div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">System performance</h2><div class="c-article-section__content" id="Sec9-content"><p>Presented in this section are the evaluations performed on the developed system, which include manipulation of two virtual objects that are created to demonstrate the system usability, as well as speed and latency assessment involving the use of a high-speed camera.</p><h3 class="c-article__sub-heading" id="Sec10">Immersive virtual object manipulation</h3><p>To demonstrate the usability and evaluate the performance of the developed system, a scene with two virtual objects was created for immersive manipulation by the user wearing a pair of wireless ShapeHand data gloves, a pair of wrist-tracking devices, a head-tracking device and a pair of polarised glasses. One object is a simple six-colour cube with an initial size of 80 × 80 × 80 m<sup>3</sup>, and the other is a medical CT volume with 256 × 256 × 256 voxels.</p><p>From the visual perspective, the user is able to see the stereoscopic images of the cube and CT volume as well as his/her hands displayed through two projectors placed at the back of a large screen operating in passive circular polarisation mode. With the head-tracking device providing the position and orientation of the user’s head, the user can physically move around in front of the display screen with an impression of a 3D virtual cube and a 3D-CT volume floating in space, whereby a forward movement causes each object to appear nearer and larger, a backward movement causes each object to appear further and smaller, and a side movement with a side look via head rotation causes a different side of each object to appear.</p><p>From the interaction perspective, the user is able to see his/her hands in 3D with respect to the virtual objects, as well as the gestures made. Furthermore, when the pointing index finger tip of the user reaches a side plane of the virtual object, the object bounding box is highlighted to provide a visual feedback of the object selected. The simplicity and intuitiveness of the hand gestures were seen to enable a new user to quickly handle and manipulate individual object or both objects simultaneously, namely, pointing the index finger(s) to touch (select), drag, rotate the 3D cube or the CT volume, or both in 3D space as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig5">5</a>, passing the selected object from one hand to another hand (from one pointing index finger to another pointing index finger), sliding two hands (with the ring and small finger closed) with respect to each other to enlarge and reduce the size of the selected as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig6">6</a> and opening the hand(s) to detach from the selected object(s).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>User performing simultaneous translation and rotation of two objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>User performing scaling of CT volume</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Furthermore, the system is able to perform the required operations with certain deviation in the gestures made, such as hand digits not fully extended and closed, and highly robust recognition can be achieved by performing calibration of hand close and open gestures at the start.</p><p>As an example, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig7">7</a> shows some of the data acquired from performing a short sequence of selection, translation and release of one of the virtual object in the scene, namely, (<i>o</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub>) to show the 3D position variation of the virtual object centre using red, green and blue dotted lines, <span class="mathjax-tex">\( \left( {x_{sw,r}^{\text{tip}} ,y_{sw,r}^{\text{tip}} ,z_{sw,r}^{\text{tip}} } \right) \)</span> to show the 3D position variation of the right hand index finger tip using red, green and blue solid lines and <span class="mathjax-tex">\( \alpha_{s}^{\text{pp}} \)</span> to show the flexion angle of the proximal phalanx of the right hand middle finger in a black solid line.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>A sequence of dynamic gesture data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>With an open-hand gesture at the start of the sequence, it is seen from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig7">7</a> that <span class="mathjax-tex">\( \alpha_{s}^{\text{pp}} \)</span> is around 0º, the user’s right hand moves in the horizontal plane as indicated by the changing coordinate values of <span class="mathjax-tex">\( x_{sw,r}^{\text{tip}} \)</span> and <span class="mathjax-tex">\( z_{sw,r}^{\text{tip}} \)</span> with <span class="mathjax-tex">\( y_{sw,r}^{\text{tip}} \)</span> roughly constant, and the virtual object is stationary at the origin of the world coordinate system as indicated by (<i>o</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub>) = (0, 0, 0). Soon after the hand gesture changed into an index finger pointing gesture, as indicated by the sharp rise of <span class="mathjax-tex">\( \alpha_{s}^{\text{pp}} \)</span> from 0º to around 70º due to the middle finger closed, the index finger tip is seen to approach the virtual object with the coordinate values of <span class="mathjax-tex">\( \left( {x_{sw,r}^{\text{tip}} ,y_{sw,r}^{\text{tip}} ,z_{sw,r}^{\text{tip}} } \right) \)</span> moving towards (<i>o</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub>). When the distance is sufficiently small, the object is seen to be selected. Once selected, the virtual object is seen to follow the index finger tip with the coordinate values of <i>o</i>
                  <sub>
                    <i>x</i>
                  </sub> following <span class="mathjax-tex">\( x_{sw,r}^{\text{tip}} , \)</span>
                  <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub> following <span class="mathjax-tex">\( y_{sw,r}^{\text{tip}} , \)</span> and <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub> following <span class="mathjax-tex">\( z_{sw,r}^{\text{tip}} . \)</span> Finally, the user’s hand opens to release the object, as indicated by <span class="mathjax-tex">\( \alpha_{s}^{\text{pp}} \)</span> falling back to around 0° due to the opening of the middle finger, the virtual object is seen to stay at its final position with (<i>o</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>y</i>
                  </sub>, <i>o</i>
                  <sub>
                    <i>z</i>
                  </sub>) fixed as the user’s hand moves away.</p><h3 class="c-article__sub-heading" id="Sec11">System performance evaluation and analysis</h3><p>A number of tests were also conducted to assess the key measures of system real-time performance in terms of speed and latency.</p><p>For manipulation of a graphics-based object, speed performance evaluation was based on the virtual cube, and for manipulation of a data-based object, it was based on the CT volume. The evaluation was implemented by inserting a counter at the start of each program thread to record the number of times to run the thread per second.</p><p>For manipulation of the virtual cube, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig8">8</a> shows a typical example showing the frequency of executing each program thread over a period of one minute with continuous hand movement in 3D space. From Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig8">8</a>, the program threads of hand gesture recognition and position data acquisition based on InterSense are seen to be relatively fast with relatively large fluctuations. Whilst the former is seen to be the fastest one with an average execution frequency of 136 times per second, the largest variation between the maximum of 171 times per second and the minimum of 102 times per second, the latter is the second fastest with an average execution frequency of 118 times per second, a variation between the maximum of 159 times per second and the minimum of 99 times per second. Very similar behaviour of the execution frequencies for the ShapeHand gesture data acquisition program thread and the stereoscopic display program thread are also seen from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0195-9#Fig8">8</a>, with the former slightly faster at 62 times per second on average between the maximum of 65 times per second and the minimum of 59 times per second, and the latter at 60 times per second between the maximum of 64 times per second and the minimum at 54 times per second.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0195-9/MediaObjects/10055_2011_195_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Thread execution frequency versus time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0195-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For manipulation of the CT volume, the stereoscopic display program thread was also found to be the slowest with the worst case execution frequency of 31 times per second. Hence, the speed of the system depends on the complexity and the number of the objects to be displayed.</p><p>To confirm the adequacy of the system speed performance for dynamic gesture recognition, a test was also carried out to check the tracking speed against the maximum speed of finger movement. By wearing the ShapeHand data glove with the index finger opening and closing repeatedly at the highest possible speed, the virtual hands on the stereoscopic display was found to follow the angular movement of the index finger at a maximum speed around 14 times per second.</p><p>For performance evaluation of latency, a high-speed video camera was used to record the hand movement made by a user wearing the ShapeHand data glove as well as the movement of the virtual hands appeared on the stereoscopic screen. With the hand opening and closing repeatedly, the video was captured at 64 frames per second, and video analysis of the corresponding hand gestures showed a delay around 6 frames of the virtual hand movement with respect to the real hand movement, which is equivalent to a latency of approximately 94 ms.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Conclusions</h2><div class="c-article-section__content" id="Sec12-content"><p>The paper demonstrates an approach to achieve immersive manipulation of virtual objects using natural hand gestures. In particular, the paper describes (a) the integration of a wireless high DOF hand gesture data glove, a wireless position tracking system and a stereoscopic display; (b) algorithms developed for recognition of dynamic hand gestures and object distances; and (c) system performance evaluation conducted to assess its usability. Overall, the system is shown to provide an immersive and interactive environment, whereby a user can visualise in stereoscopic mode and interact in 3D with virtual objects using natural hand gestures. Furthermore, the simplicity and intuitiveness of the selected hand gestures, together with robustness in recognition of imprecise hand gestures, enables users to quickly master the object manipulation operations with little effort. Particularly, it has been shown that the system can operate at 54 frames per second for graphics-based objects and 31 frames per second for data-based objects, as well as a latency time of approximately 94 ms using a PC with 3 GHz CPU.</p><p>Although the virtual objects used for system demonstration are simple, with one based on a cube and the other based on a 3D-CT volume, the work is seen as an important first step towards simultaneous manipulations of multiple objects with complex shapes, deformable surface and mutual physical interaction. Further work will extend the system to recognise not only a wide range of more intricate hand gestures but also complex hand movement trajectories in 3D space, thereby enabling the use of natural hand movements and gestures to perform complex virtual object manipulation, such as assembling and disassembling tasks in equipment maintenance and repair (Corvaglia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Corvaglia D (2004) Virtual training for manufacturing and maintenance based on Web3d technologies. In: Proceeding of LET-Web3D 2004: 1st international workshop on Web3D technologies in learning, education and training, pp 28–33" href="/article/10.1007/s10055-011-0195-9#ref-CR5" id="ref-link-section-d17e2036">2004</a>; Badler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Badler NI, Philips CB, Webber BL (1993) Simulating humans, computer graphics animation and control. Oxford University Press, New York" href="/article/10.1007/s10055-011-0195-9#ref-CR2" id="ref-link-section-d17e2039">1993</a>; Johnson and Rickel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Johnson WL, Rickel J (1997) Steve: an animated pedagogical agent for procedural training in virtual environments. Sigart Bulletin, ACM Press 8(1–4): 16–21" href="/article/10.1007/s10055-011-0195-9#ref-CR12" id="ref-link-section-d17e2042">1997</a>), and complex virtual tool operation, such as control of robots in medical surgery (Liverneaux et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Liverneaux P, Nectoux E, Taleb C (2009) The future of robotics in hand surgery. Chirurgie de la Main 28:278–285" href="/article/10.1007/s10055-011-0195-9#ref-CR15" id="ref-link-section-d17e2045">2009</a>; Scharver et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Scharver C, Evenhouse R, Johnson A, Leigh J (2004) Designing cranial implants in a haptic augmented reality environment. Commun ACM 47:32–38" href="/article/10.1007/s10055-011-0195-9#ref-CR18" id="ref-link-section-d17e2048">2004</a>).</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">Refereneces</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adamo-Villani N., Heisler J, Arns L (2007) Two gesture recognition systems for immersive math education for th" /><p class="c-article-references__text" id="ref-CR1">Adamo-Villani N., Heisler J, Arns L (2007) Two gesture recognition systems for immersive math education for the deaf. In: ACM proceedings of IMMERSCOM 2007, Verona, Italy, pp 10–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="NI. Badler, CB. Philips, BL. Webber, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Badler NI, Philips CB, Webber BL (1993) Simulating humans, computer graphics animation and control. Oxford Uni" /><p class="c-article-references__text" id="ref-CR2">Badler NI, Philips CB, Webber BL (1993) Simulating humans, computer graphics animation and control. Oxford University Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulating%20humans%2C%20computer%20graphics%20animation%20and%20control&amp;publication_year=1993&amp;author=Badler%2CNI&amp;author=Philips%2CCB&amp;author=Webber%2CBL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction tec" /><p class="c-article-references__text" id="ref-CR3">Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. In: Proceedings of HCI international, New Orleans, USA, pp 629–633</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Corradini A, Cohen P (2002) On the relationships among speech, gestures, and object manipulation in virtual en" /><p class="c-article-references__text" id="ref-CR4">Corradini A, Cohen P (2002) On the relationships among speech, gestures, and object manipulation in virtual environments: initial Evidence. In: Proceedings of the international CLASS workshop on natural, intelligent and effective interaction in multimodal dialogue systems, Copenhagen, Denmark</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Corvaglia D (2004) Virtual training for manufacturing and maintenance based on Web3d technologies. In: Proceed" /><p class="c-article-references__text" id="ref-CR5">Corvaglia D (2004) Virtual training for manufacturing and maintenance based on Web3d technologies. In: Proceeding of LET-Web3D 2004: 1st international workshop on Web3D technologies in learning, education and training, pp 28–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Danisch, K. Englehart, A. Trivett, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Danisch L, Englehart K, Trivett A (1999) Spatially continuous six degree of freedom position, orientation sens" /><p class="c-article-references__text" id="ref-CR6">Danisch L, Englehart K, Trivett A (1999) Spatially continuous six degree of freedom position, orientation sensor. Sens Rev 19(2):106–112</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2F02602289910266142" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatially%20continuous%20six%20degree%20of%20freedom%20position%2C%20orientation%20sensor&amp;journal=Sens%20Rev&amp;volume=19&amp;issue=2&amp;pages=106-112&amp;publication_year=1999&amp;author=Danisch%2CL&amp;author=Englehart%2CK&amp;author=Trivett%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Demirdjian, T. Ko, T. Darrell, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipula" /><p class="c-article-references__text" id="ref-CR7">Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipulation. Virutal Real 8:222–230</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-005-0155-3" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Untethered%20gesture%20acquisition%20and%20recognition%20for%20virtual%20world%20manipulation&amp;journal=Virutal%20Real&amp;volume=8&amp;pages=222-230&amp;publication_year=2005&amp;author=Demirdjian%2CD&amp;author=Ko%2CT&amp;author=Darrell%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Dipietro, AM. Sabatini, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Dipietro L, Sabatini AM (2008) A survey of glove-based systems and their applications. IEEE Trans Syst Man Cyb" /><p class="c-article-references__text" id="ref-CR8">Dipietro L, Sabatini AM (2008) A survey of glove-based systems and their applications. IEEE Trans Syst Man Cybern Part C: Appl Rev 38:461–482</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20glove-based%20systems%20and%20their%20applications&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20C%3A%20Appl%20Rev&amp;volume=38&amp;pages=461-482&amp;publication_year=2008&amp;author=Dipietro%2CL&amp;author=Sabatini%2CAM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Erol, G. Bebis, M. Nicolescu, RD. Boyle, X. Twombly, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Erol A, Bebis G, Nicolescu M, Boyle RD, Twombly X (2007) Vision-based hand pose estimation: a review. Comput V" /><p class="c-article-references__text" id="ref-CR9">Erol A, Bebis G, Nicolescu M, Boyle RD, Twombly X (2007) Vision-based hand pose estimation: a review. Comput Vis Image Underst 108:52–73</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2006.10.012" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision-based%20hand%20pose%20estimation%3A%20a%20review&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=108&amp;pages=52-73&amp;publication_year=2007&amp;author=Erol%2CA&amp;author=Bebis%2CG&amp;author=Nicolescu%2CM&amp;author=Boyle%2CRD&amp;author=Twombly%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Garg P, Aggarwal N, Sofat S (2009) Vision based hand gesture recognition. In: Proceedings of world academy of " /><p class="c-article-references__text" id="ref-CR10">Garg P, Aggarwal N, Sofat S (2009) Vision based hand gesture recognition. In: Proceedings of world academy of science, engineering and technology, pp 972–977</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="IS-900 Precision Inertial-ultrasonic Motion Tracking System, InterSense Inc. http://www.isense.com&#xA;           " /><p class="c-article-references__text" id="ref-CR25">IS-900 Precision Inertial-ultrasonic Motion Tracking System, InterSense Inc. <a href="http://www.isense.com">http://www.isense.com</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WL. Johnson, J. Rickel, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Johnson WL, Rickel J (1997) Steve: an animated pedagogical agent for procedural training in virtual environmen" /><p class="c-article-references__text" id="ref-CR12">Johnson WL, Rickel J (1997) Steve: an animated pedagogical agent for procedural training in virtual environments. Sigart Bulletin, ACM Press 8(1–4): 16–21</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Steve%3A%20an%20animated%20pedagogical%20agent%20for%20procedural%20training%20in%20virtual%20environments&amp;journal=Sigart%20Bulletin%2C%20ACM%20Press&amp;volume=8&amp;issue=1%E2%80%934&amp;pages=16-21&amp;publication_year=1997&amp;author=Johnson%2CWL&amp;author=Rickel%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kober C, Boerner BI, Mori S, Tellez CB, Klarhöfer M, Scheffler K, Sader R, Zeilhofer HF (2007) Stereoscopic 4D" /><p class="c-article-references__text" id="ref-CR13">Kober C, Boerner BI, Mori S, Tellez CB, Klarhöfer M, Scheffler K, Sader R, Zeilhofer HF (2007) Stereoscopic 4D-visualization of craniofacial soft tissue based on dynamic MRI and 256 row 4D-CT. Adv Med Eng Springer Proc Phys Part II, pp 175–180</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Lécuyer, F. Lotte, R. Reilly, R. Leeb, M. Hirose, M. Slater, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Lécuyer A, Lotte F, Reilly R, Leeb R, Hirose M, Slater M (2008) Brain-computer interfaces, virtual reality, an" /><p class="c-article-references__text" id="ref-CR14">Lécuyer A, Lotte F, Reilly R, Leeb R, Hirose M, Slater M (2008) Brain-computer interfaces, virtual reality, and videogames. Computer 41:66–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2008.410" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain-computer%20interfaces%2C%20virtual%20reality%2C%20and%20videogames&amp;journal=Computer&amp;volume=41&amp;pages=66-72&amp;publication_year=2008&amp;author=L%C3%A9cuyer%2CA&amp;author=Lotte%2CF&amp;author=Reilly%2CR&amp;author=Leeb%2CR&amp;author=Hirose%2CM&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Liverneaux, E. Nectoux, C. Taleb, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Liverneaux P, Nectoux E, Taleb C (2009) The future of robotics in hand surgery. Chirurgie de la Main 28:278–28" /><p class="c-article-references__text" id="ref-CR15">Liverneaux P, Nectoux E, Taleb C (2009) The future of robotics in hand surgery. Chirurgie de la Main 28:278–285</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.main.2009.08.002" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20future%20of%20robotics%20in%20hand%20surgery&amp;journal=Chirurgie%20de%20la%20Main&amp;volume=28&amp;pages=278-285&amp;publication_year=2009&amp;author=Liverneaux%2CP&amp;author=Nectoux%2CE&amp;author=Taleb%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Hagan RG, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments, interacting with" /><p class="c-article-references__text" id="ref-CR16">O’Hagan RG, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments, interacting with computers, vol 14, pp 231–250, April, 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Patel, LP. Muren, A. Mehus, Y. Kvinnsland, DM. Ulvang, KP. Villanger, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Patel D, Muren LP, Mehus A, Kvinnsland Y, Ulvang DM, Villanger KP (2007) A virtual reality solution for evalua" /><p class="c-article-references__text" id="ref-CR17">Patel D, Muren LP, Mehus A, Kvinnsland Y, Ulvang DM, Villanger KP (2007) A virtual reality solution for evaluation of radiotherapy plans. Radiother Oncol 82(2):218–221</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.radonc.2006.11.024" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20virtual%20reality%20solution%20for%20evaluation%20of%20radiotherapy%20plans&amp;journal=Radiother%20Oncol&amp;volume=82&amp;issue=2&amp;pages=218-221&amp;publication_year=2007&amp;author=Patel%2CD&amp;author=Muren%2CLP&amp;author=Mehus%2CA&amp;author=Kvinnsland%2CY&amp;author=Ulvang%2CDM&amp;author=Villanger%2CKP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Scharver, R. Evenhouse, A. Johnson, J. Leigh, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Scharver C, Evenhouse R, Johnson A, Leigh J (2004) Designing cranial implants in a haptic augmented reality en" /><p class="c-article-references__text" id="ref-CR18">Scharver C, Evenhouse R, Johnson A, Leigh J (2004) Designing cranial implants in a haptic augmented reality environment. Commun ACM 47:32–38</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1012037.1012059" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Designing%20cranial%20implants%20in%20a%20haptic%20augmented%20reality%20environment&amp;journal=Commun%20ACM&amp;volume=47&amp;pages=32-38&amp;publication_year=2004&amp;author=Scharver%2CC&amp;author=Evenhouse%2CR&amp;author=Johnson%2CA&amp;author=Leigh%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ShapeHand Data Glove, Measurand Inc. http://www.measurand.com&#xA;                " /><p class="c-article-references__text" id="ref-CR24">ShapeHand Data Glove, Measurand Inc. <a href="http://www.measurand.com">http://www.measurand.com</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Sturman, D. Zeltzer, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39" /><p class="c-article-references__text" id="ref-CR19">Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.250916" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20glove-based%20input&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=14&amp;issue=1&amp;pages=30-39&amp;publication_year=1994&amp;author=Sturman%2CDJ&amp;author=Zeltzer%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Toyama, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Toyama H (2006) Trials on grasping of a 3D virtual object in CAVE using EEG signals. IEIC Tech Rep 91:53–56" /><p class="c-article-references__text" id="ref-CR20">Toyama H (2006) Trials on grasping of a 3D virtual object in CAVE using EEG signals. IEIC Tech Rep 91:53–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Trials%20on%20grasping%20of%20a%203D%20virtual%20object%20in%20CAVE%20using%20EEG%20signals&amp;journal=IEIC%20Tech%20Rep&amp;volume=91&amp;pages=53-56&amp;publication_year=2006&amp;author=Toyama%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="XH. Wang, WF. Good, CR. Fuhrman, JH. Sumkin, CA. Britton, SK. Golla, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Wang XH, Good WF, Fuhrman CR, Sumkin JH, Britton CA, Golla SK (2005) Stereo CT image compositing methods for l" /><p class="c-article-references__text" id="ref-CR21">Wang XH, Good WF, Fuhrman CR, Sumkin JH, Britton CA, Golla SK (2005) Stereo CT image compositing methods for lung nodule detection, characterization. Acad Radiol 12(12):1512–1520</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.acra.2005.06.009" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stereo%20CT%20image%20compositing%20methods%20for%20lung%20nodule%20detection%2C%20characterization&amp;journal=Acad%20Radiol&amp;volume=12&amp;issue=12&amp;pages=1512-1520&amp;publication_year=2005&amp;author=Wang%2CXH&amp;author=Good%2CWF&amp;author=Fuhrman%2CCR&amp;author=Sumkin%2CJH&amp;author=Britton%2CCA&amp;author=Golla%2CSK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Woods AJ (2005) Compatibility of display products with stereoscopic display methods. In: Proceedings of the in" /><p class="c-article-references__text" id="ref-CR22">Woods AJ (2005) Compatibility of display products with stereoscopic display methods. In: Proceedings of the international display manufacturing conference, Taipei, Taiwan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wormell D, Foxlin E (2003) Advancements in 3D interactive devices for virtual environments. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR23">Wormell D, Foxlin E (2003) Advancements in 3D interactive devices for virtual environments. In: Proceedings of the workshop on virtual environments, Zurich, Switzerland, pp 47–56</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang S, Demiralp C, Keefe D, DaSilva M, Laidlaw DH, Greenberg, BD, Basser PJ, Pierpaoli C, Chiocca EA, Deisbo" /><p class="c-article-references__text" id="ref-CR11">Zhang S, Demiralp C, Keefe D, DaSilva M, Laidlaw DH, Greenberg, BD, Basser PJ, Pierpaoli C, Chiocca EA, Deisboeck TS (2001) An immersive virtual environment for DT-MRI volume visualization applications: a case study. In: Proceedings of IEEE visualization conference, San Diego, USA, pp 437–440</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0195-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors thank Dr. X. Chen for his technical support.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Applied Digital Signal and Image Processing Research Centre, University of Central Lancashire, Preston, PR1 2HE, UK</p><p class="c-article-author-affiliation__authors-list">Gan Lu, Lik-Kwan Shark &amp; Geoff Hall</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">International Centre for Sign Languages and Deaf Studies, University of Central Lancashire, Preston, PR1 2HE, UK</p><p class="c-article-author-affiliation__authors-list">Ulrike Zeshan</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Gan-Lu"><span class="c-article-authors-search__title u-h3 js-search-name">Gan Lu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gan+Lu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gan+Lu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gan+Lu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Lik_Kwan-Shark"><span class="c-article-authors-search__title u-h3 js-search-name">Lik-Kwan Shark</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Lik-Kwan+Shark&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lik-Kwan+Shark" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lik-Kwan+Shark%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Geoff-Hall"><span class="c-article-authors-search__title u-h3 js-search-name">Geoff Hall</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Geoff+Hall&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Geoff+Hall" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Geoff+Hall%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ulrike-Zeshan"><span class="c-article-authors-search__title u-h3 js-search-name">Ulrike Zeshan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ulrike+Zeshan&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ulrike+Zeshan" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ulrike+Zeshan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0195-9/email/correspondent/c1/new">Gan Lu</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Immersive%20manipulation%20of%20virtual%20objects%20through%20glove-based%20hand%20gesture%20interaction&amp;author=Gan%20Lu%20et%20al&amp;contentID=10.1007%2Fs10055-011-0195-9&amp;publication=1359-4338&amp;publicationDate=2011-08-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lu, G., Shark, L., Hall, G. <i>et al.</i> Immersive manipulation of virtual objects through glove-based hand gesture interaction.
                    <i>Virtual Reality</i> <b>16, </b>243–252 (2012). https://doi.org/10.1007/s10055-011-0195-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0195-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-20">20 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-07-22">22 July 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-08-05">05 August 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-09">September 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0195-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0195-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Hand gesture tracking and recognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive stereoscopic visualisation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual object manipulation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0195-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=195;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

