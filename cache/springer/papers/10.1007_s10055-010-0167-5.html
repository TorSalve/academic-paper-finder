<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Piavca: a framework for heterogeneous interactions with virtual charac"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents a virtual character animation system for real- time multimodal interaction in an immersive virtual reality setting. Human to human interaction is highly multimodal, involving..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/14/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Piavca: a framework for heterogeneous interactions with virtual characters"/>

    <meta name="dc.source" content="Virtual Reality 2010 14:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-07-30"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents a virtual character animation system for real- time multimodal interaction in an immersive virtual reality setting. Human to human interaction is highly multimodal, involving features such as verbal language, tone of voice, facial expression, gestures and gaze. This multimodality means that, in order to simulate social interaction, our characters must be able to handle many different types of interaction and many different types of animation, simultaneously. Our system is based on a model of animation that represents different types of animations as instantiations of an abstract function representation. This makes it easy to combine different types of animation. It also encourages the creation of behavior out of basic building blocks, making it easy to create and configure new behaviors for novel situations. The model has been implemented in Piavca, an open source character animation system."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-07-30"/>

    <meta name="prism.volume" content="14"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="221"/>

    <meta name="prism.endingPage" content="228"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0167-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0167-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0167-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0167-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Piavca: a framework for heterogeneous interactions with virtual characters"/>

    <meta name="citation_volume" content="14"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2010/12"/>

    <meta name="citation_online_date" content="2010/07/30"/>

    <meta name="citation_firstpage" content="221"/>

    <meta name="citation_lastpage" content="228"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0167-5"/>

    <meta name="DOI" content="10.1007/s10055-010-0167-5"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0167-5"/>

    <meta name="description" content="This paper presents a virtual character animation system for real- time multimodal interaction in an immersive virtual reality setting. Human to human inte"/>

    <meta name="dc.creator" content="Marco Gillies"/>

    <meta name="dc.creator" content="Xueni Pan"/>

    <meta name="dc.creator" content="Mel Slater"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Representing animations by principal components; citation_author=M Alexa, W M&#252;ller; citation_volume=19; citation_issue=3; citation_publication_date=2000; citation_pages=411-418; citation_doi=10.1111/1467-8659.00433; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_title=Gaze and mutual gaze; citation_publication_date=1976; citation_id=CR2; citation_author=M Argyle; citation_author=M Cookv; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Interactive motion generation from examples; citation_author=O Arikan, DA Forsyth; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=483-490; citation_doi=10.1145/566654.566606; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_title=Simulating humans: computer graphics, animation and control; citation_publication_date=1993; citation_id=CR4; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="Bui TD, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of computer graphics international 2004, pp 284&#8211;290"/>

    <meta name="citation_reference" content="Cassell J, Bickmore T, Campbell L, Chang K,Vilhj&#225;lmsson H, Yan H (1999) Embodiment in conversational interfaces: Rea. In: ACM SIGCHI , ACM Press, pp 520&#8211;527"/>

    <meta name="citation_reference" content="Cassell J, Bickmore T, Campbell L, Chang K Vilhj&#225;lmsson H, Yan H (1999) Requirements for an architecture for embodied conversational characters. In: Proceedings of computer animation and simulation &#8217;99"/>

    <meta name="citation_reference" content="Cassell J, Stone M (1999) Living hand to mouth: psychological theories about speech and gesture in interactive dialogue systems. In: AAI 1999 fall symposium on psychological models of communication in collaborative systems, pp 34&#8211;42"/>

    <meta name="citation_reference" content="Egges A, Molet T, Magnenat-Thalmann N (2004) Personalised real-time idle motion synthesis. In: 12th Pacific conference on computer graphics and applications, pp 121&#8211;130"/>

    <meta name="citation_reference" content="Elliott C, Schechter G, Yeung R, Abi-Ezzi S (July 1994) Tbag: a high level framework for interactive, animated 3d graphics applications. In: Proceedings of SIGGRAPH 94, computer graphics proceedings, annual conference series, pp 421&#8211;434"/>

    <meta name="citation_reference" content="Figueroa P, Green M, Hoover HJ (2002) Intml: a description language for vr applications. In: Proceedings of Web3D &#8217;02"/>

    <meta name="citation_reference" content="Gillies M, Ballin D (2003) A model of interpersonal attitude and posture generation. In: Rist T, Aylett R, Ballin D, Rickel J (eds) Fourth workshop on intelligent virtual agents. Kloster Irsee, Germany"/>

    <meta name="citation_reference" content="citation_journal_title=J Graph Tools; citation_title=Practical parameterization of rotations using the exponential map; citation_author=FS Grassia; citation_volume=3; citation_issue=3; citation_publication_date=1998; citation_pages=29-48; citation_id=CR13"/>

    <meta name="citation_reference" content="Gratch J, Marsella S (2001) Tears and fears: modeling emotions and emotional behaviors in synthetic agents. In: AGENTS &#8217;01, Proceedings of the fifth international conference on autonomous agents, pp 278&#8211;285, ACM Press, New York"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real J; citation_title=Non-verbal communication interface for collaborative virtual environments; citation_author=A Guye-Vuill&#233;me, TK Capin, IS Pandzic, N Magnenat-Thalmann, D Thalmann; citation_volume=4; citation_publication_date=1999; citation_pages=49-59; citation_doi=10.1007/BF01434994; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Psychol; citation_title=Movement coordination in social interaction; citation_author=A Kendon; citation_volume=32; citation_publication_date=1970; citation_pages=1-25; citation_doi=10.1016/0001-6918(70)90088-0; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=KI-Knstliche Intelligenz; citation_title=Max&#8211;a multimodal assistant in virtual reality construction; citation_author=S Kopp, B Jung, N Lessmann, I Wachsmuth; citation_volume=3; citation_issue=4; citation_publication_date=2003; citation_pages=11-17; citation_id=CR17"/>

    <meta name="citation_reference" content="Kopp S, Tepper P, Cassell J (2005) Towards integrated microplanning of language and iconic gesture for multimodal output. In: International Conference on Multimodal Interfaces (ICMI&#8217;04), ACM Press, pp 97&#8211;104"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Animat Virtual Worlds; citation_title=Synthesizing multimodal utterances for conversational agents; citation_author=S Kopp, I Wachsmuth; citation_volume=15; citation_issue=1; citation_publication_date=2004; citation_pages=39-52; citation_doi=10.1002/cav.6; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Motion graphs; citation_author=L Kovar, M Gleicher, F Pighin; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=473-482; citation_doi=10.1145/566654.566605; citation_id=CR20"/>

    <meta name="citation_reference" content="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: 2nd international symposium on smart graphics, pp 107&#8211;115"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Interactive control of avatars animated with human motion data; citation_author=J Lee, J Chai, PSA Reitsma, JK Hodgins, NS Pollard; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=491-500; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Eyes alive; citation_author=SP Lee, JB Badler, NI Badler; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=637-644; citation_id=CR23"/>

    <meta name="citation_reference" content="Maatman RM, Gratch J, Marsella S (2005) Natural behavior of a listening agent. In: Panayiotopoulos T, Gratch J, Aylett R, Ballin D, Olivier P, Rist T (eds) Intelligent virtual agents. 5th international working conference, Kos"/>

    <meta name="citation_reference" content="Pan X, Gillies M, Slater M (2008) Male bodily responses during an interaction with a virtual woman. In: Prendinger H, Lester JC, Ishizuka M (eds) Intelligent virtual agents. 8th international conference, IVA 2008, Tokyo, Japan, September 1&#8211;3, 2008. Proceedings, Vol 5208 of lecture notes in computer science, Springer, pp 89&#8211;96"/>

    <meta name="citation_reference" content="Pan X, Slater M (2007) A preliminary study of shy males interacting with a virtual female. In: Presence 2007. The 10th annual international workshop on presence, pp 101&#8211;108"/>

    <meta name="citation_reference" content="Poggi I, Pelachaud C, De Rosis F, Carofiglio V, De Carolis B (2005) Greta. A believable embodied conversational agent. In: Stock O, Zancanaro M (eds) Multimodal intelligent information presentation. Text, speech and language technology, vol 27. Kluwer"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Verbs and adverbs: multidimensional motion interpolation; citation_author=C Rose, MF Cohen, B Bodenheimer; citation_volume=18; citation_issue=5; citation_publication_date=1998; citation_pages=32-40; citation_id=CR28"/>

    <meta name="citation_reference" content="Slater M, Usoh M (1994) Body centred interaction in immersive virtual environments. In: Thalmann NM, Thalmann D (eds) Artificial life and virtual reality. Wiley, pp 125&#8211;148"/>

    <meta name="citation_reference" content="Stone M, DeCarlo D, Oh I, Rodriguez C, Stere A, Lees A, Bregler C (2004) Speaking with hands: creating animated conversational characters from recordings of human performance. In: SIGGRAPH &#8217;04, ACM SIGGRAPH 2004 papers. ACM Press, New York, pp 506&#8211;513"/>

    <meta name="citation_reference" content="Th&#243;risson K (1998) Real-time decision making in multimodal face-to-face communication. In: Second ACM international conference on autonomous agents, pp 16&#8211;23"/>

    <meta name="citation_reference" content="Vilhj&#225;lmsson HH (2005) Augmenting online conversation through automated discourse tagging. In: 6th annual minitrack on persistent conversation at the 38th Hawaii international conference on system sciences"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=An eye gaze model for dyadic interaction in an immersive virtual environment: practice and experience; citation_author=V Vinayagamoorthy, M Garau, A Steed, M Slater; citation_volume=23; citation_issue=1; citation_publication_date=2004; citation_pages=1-12; citation_doi=10.1111/j.1467-8659.2004.00001.x; citation_id=CR33"/>

    <meta name="citation_reference" content="Vinayagamoorthy V, Gillies M, Steed A, Tanguy E, Pan X, Loscos C, Slater M (2006) Building expression into virtual characters. In: Eurographics conference state of the art reports"/>

    <meta name="citation_reference" content="Witkin A, Popovi&#263; Z (1995) Motion warping. In: ACM SIGGRAPH, pp 105&#8211;108"/>

    <meta name="citation_author" content="Marco Gillies"/>

    <meta name="citation_author_email" content="m.gillies@gold.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computing, Goldsmiths College, University of London, London, UK"/>

    <meta name="citation_author" content="Xueni Pan"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University College London, London, UK"/>

    <meta name="citation_author" content="Mel Slater"/>

    <meta name="citation_author_institution" content="ICREA-Universitat de Barcelona, Barcelona, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0167-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2010/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0167-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Piavca: a framework for heterogeneous interactions with virtual characters"/>
        <meta property="og:description" content="This paper presents a virtual character animation system for real- time multimodal interaction in an immersive virtual reality setting. Human to human interaction is highly multimodal, involving features such as verbal language, tone of voice, facial expression, gestures and gaze. This multimodality means that, in order to simulate social interaction, our characters must be able to handle many different types of interaction and many different types of animation, simultaneously. Our system is based on a model of animation that represents different types of animations as instantiations of an abstract function representation. This makes it easy to combine different types of animation. It also encourages the creation of behavior out of basic building blocks, making it easy to create and configure new behaviors for novel situations. The model has been implemented in Piavca, an open source character animation system."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Piavca: a framework for heterogeneous interactions with virtual characters | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0167-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Posture Shift, Skin Conductance Level, Virtual Character, Facial Animation, Head Tracker","kwrd":["Posture_Shift","Skin_Conductance_Level","Virtual_Character","Facial_Animation","Head_Tracker"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0167-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0167-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=167;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0167-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Piavca: a framework for heterogeneous interactions with virtual characters
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0167-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0167-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-07-30" itemprop="datePublished">30 July 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Piavca: a framework for heterogeneous interactions with virtual characters</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marco-Gillies" data-author-popup="auth-Marco-Gillies" data-corresp-id="c1">Marco Gillies<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of London" /><meta itemprop="address" content="grid.4464.2, 0000000121612573, Department of Computing, Goldsmiths College, University of London, London, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xueni-Pan" data-author-popup="auth-Xueni-Pan">Xueni Pan</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University College London" /><meta itemprop="address" content="grid.83440.3b, 0000000121901201, Department of Computer Science, University College London, London, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mel-Slater" data-author-popup="auth-Mel-Slater">Mel Slater</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="ICREA-Universitat de Barcelona" /><meta itemprop="address" content="grid.5841.8, 0000000419370247, ICREA-Universitat de Barcelona, Barcelona, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 14</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">221</span>–<span itemprop="pageEnd">228</span>(<span data-test="article-publication-year">2010</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">140 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0167-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents a virtual character animation system for real- time multimodal interaction in an immersive virtual reality setting. Human to human interaction is highly multimodal, involving features such as verbal language, tone of voice, facial expression, gestures and gaze. This multimodality means that, in order to simulate social interaction, our characters must be able to handle many different types of interaction and many different types of animation, simultaneously. Our system is based on a model of animation that represents different types of animations as instantiations of an abstract function representation. This makes it easy to combine different types of animation. It also encourages the creation of behavior out of basic building blocks, making it easy to create and configure new behaviors for novel situations. The model has been implemented in Piavca, an open source character animation system.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Animated virtual humans are a vital part of many virtual environments today. Of particular interest are virtual humans that we can interact with in some approximation of social interaction. However, creating characters that we can interact with believably is an extremely complex problem. Part of this difficulty is that human interaction is highly multimodal. The most obvious modality is speech, but even this can be divided into the verbal content and non-verbal aspects of speech, such as tone of voice, whose function can be very subtle and complex. When we take into account bodily modalities, we also have to deal with facial expression, gaze, gesture (both accompanying speech and giving feedback while listening), posture, body movements and touch. While non-immersive environments can ignore some of these modalities of interactions, the full power of an immersive virtual reality interaction with a character can only be achieved by modeling all (of most) of the modalities.</p><p>This multimodality implies that there will be a great variety of ways of interacting with a virtual character. A human participant can use these different ways of interacting with a character:
</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Verbal interaction, characters that can engage in conversation using a dialogue engine.</p>
                  </li>
                  <li>
                    <p>Non-verbal aspects of speech, picking up features of the participants’ voice.</p>
                  </li>
                  <li>
                    <p>What Slater and Usoh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Slater M, Usoh M (1994) Body centred interaction in immersive virtual environments. In: Thalmann NM, Thalmann D (eds) Artificial life and virtual reality. Wiley, pp 125–148" href="/article/10.1007/s10055-010-0167-5#ref-CR29" id="ref-link-section-d55577e359">1994</a>) calls “Body Centered Interaction”. The participants’ movements are tracked, and their normal movements used to interact with a character. For example, the character may track the participants’ movements with their gaze and maintain a normal conversational distance to them.</p>
                  </li>
                  <li>
                    <p>Control, the character might be an avatar that is being controlled by a participant, either through one of the above modalities or a more conventional user interface such as a mouse and keyboard or a joystick.</p>
                  </li>
                  <li>
                    <p>Watching, not all of a characters behavior will be interactive, some will simply play back and the participant can observe it.</p>
                  </li>
                </ul>
                     <p>The true complexity of the behavior of an interactive character is that most of these types of interactions are likely to be happening simultaneously. For example, a participant might be engaging in conversation with a character that is controlled by a chat-bot. The character nods in rhythm with variations in the participants speech amplitude and its gaze follow the participants position, while it walks around following the joystick movements made by another participant. Finally, its posture shifts occasionally, following a non-interactive algorithm (admittedly, this example is contrived, if the character is an avatar under the control of a participant, then its speech will almost always be controlled by that participant. However, even given this constraint, many modalities of interaction are likely to appear simultaneously.) This paper presents a method of creating characters that combine these very diverse forms of interaction.</p><p>The diverse styles of interaction also imply diverse methods of generating behavior. This paper is mostly restricted to animation, but there are still many different styles. Some animation can be played back from pre-existing data, whether it is from motion capture or hand animation. Some types of animation, such as gaze or lip synchronization are best generated on the fly, algorithmically, a process called procedural animation. Finally, most interactive animation is generated by transforming and combing clips of pre-existing animation data, to produce new animations.</p><p>Another source of diversity is the different time scales of interactivity. A character’s gaze has to respond instantly to changes in the humans position, while other body centered interactions have more variable time constraints. Speech interaction tends to be turn based, with long periods of non-interaction, interrupted by a change of speaker. Other modalities have no time constraints for interaction or are not interactive at all. This flexibility in level of interactivity is also important because of the great difference in quality that exists between behavior that is generated on the fly and pre-recorded behavior. This is particularly true with speech where it can be difficult to produce coherent speech for long periods with all but the best existing dialogue systems, and where synthesized speech falls far short of recorded speech quality. However, it is also true of animation, motion captured or hand-animated animation is generally better than what can be produced in real time, even by transforming motion capture data. However, completely pre-recorded behavior reduces interactivity. The sense that we are truly interacting with a character is likely to be a strong contributor to presence and thus is vital. However, the illusion of interactivity can be maintained even if the behavior is not entirely interactive, as long as some elements are. For this reason, we believe it is important to balance the quality of pre-recorded elements. Behavior that consists of some pre-recorded clips, of audio and animation, should be used to ensure quality of the output. However, the sense of interactivity can be maintained to a degree if other aspects of behavior such as gaze and feedback, remain highly interactive. In turn, this sense of interactivity can maintain a high level of presence.</p><p>This flexibility also implies that the characters will need to be used in different ways and that different styles of interaction will need to be combined differently in different situations. Different types of behavior need to be interacted with in different ways. Sometimes facial expressions need to be under the control of a human, at other times they must respond to the participants speech and at other times they can be random or scripted. The same factors apply to all the behavior types. Therefore, it needs to be simple to create new interactive behaviors and combine them with others in a variety of different ways.</p><p>This paper describes how to create a character capable of very heterogeneous forms of interaction. We first describe related work in this area. We then describe a functional model of animation that is designed for handling very diverse styles of animation, and its implementation with the Piavca open source character animation system. Finally, we describe how it is used to create our heterogeneous character.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>This work builds on a long tradition of research on expressive virtual characters (Vinayagamoorthy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Vinayagamoorthy V, Gillies M, Steed A, Tanguy E, Pan X, Loscos C, Slater M (2006) Building expression into virtual characters. In: Eurographics conference state of the art reports" href="/article/10.1007/s10055-010-0167-5#ref-CR34" id="ref-link-section-d55577e394">2006</a>). This work has aimed at building animated characters that can autonomously produce the type of expressive non-verbal communication that humans naturally use in day to day interaction. This generally entails both an animation system and a higher level model for determining what behavior to produce in response to stimuli. Numerous general purpose systems have been produced notably the “JACK” system by Badler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Badler N, Philips C, Webber B (eds) (1993) Simulating humans: computer graphics, animation and control. Oxford University Press, Oxford" href="/article/10.1007/s10055-010-0167-5#ref-CR4" id="ref-link-section-d55577e397">1993</a>); the “GRETA” system by Pelachaud et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Poggi I, Pelachaud C, De Rosis F, Carofiglio V, De Carolis B (2005) Greta. A believable embodied conversational agent. In: Stock O, Zancanaro M (eds) Multimodal intelligent information presentation. Text, speech and language technology, vol 27. Kluwer" href="/article/10.1007/s10055-010-0167-5#ref-CR27" id="ref-link-section-d55577e400">2005</a>); work of Cassell et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cassell J, Bickmore T, Campbell L, Chang K Vilhjálmsson H, Yan H (1999) Requirements for an architecture for embodied conversational characters. In: Proceedings of computer animation and simulation ’99" href="/article/10.1007/s10055-010-0167-5#ref-CR7" id="ref-link-section-d55577e403">1999</a>), and Guye-Vuilléme et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Guye-Vuilléme A, Capin TK, Pandzic IS, Magnenat-Thalmann N, Thalmann D (1999) Non-verbal communication interface for collaborative virtual environments. Virtual Real J 4:49–59" href="/article/10.1007/s10055-010-0167-5#ref-CR15" id="ref-link-section-d55577e406">1999</a>). Much of this work has concerned expression of emotions [for example the work of Gratch and Marsella (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Gratch J, Marsella S (2001) Tears and fears: modeling emotions and emotional behaviors in synthetic agents. In: AGENTS ’01, Proceedings of the fifth international conference on autonomous agents, pp 278–285, ACM Press, New York" href="/article/10.1007/s10055-010-0167-5#ref-CR14" id="ref-link-section-d55577e410">2001</a>)], but our work is closer to research that models use of non-verbal communication in face-to-face conversation, for example the work of Vilhjálmsson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Vilhjálmsson HH (2005) Augmenting online conversation through automated discourse tagging. In: 6th annual minitrack on persistent conversation at the 38th Hawaii international conference on system sciences" href="/article/10.1007/s10055-010-0167-5#ref-CR32" id="ref-link-section-d55577e413">2005</a>). Each individual modality of expression is highly complex and many researchers have worked to create models of single modalities. Gaze is often linked to turn taking in conversation, for example the work of Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lee SP, Badler JB, Badler NI (2002) Eyes alive. ACM Trans Graph 21(3):637–644" href="/article/10.1007/s10055-010-0167-5#ref-CR23" id="ref-link-section-d55577e416">2002</a>), further developed by Vinayagamoorthy et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Vinayagamoorthy V, Garau M, Steed A, Slater M (2004) An eye gaze model for dyadic interaction in an immersive virtual environment: practice and experience. Comput Graph Forum 23(1):1–12" href="/article/10.1007/s10055-010-0167-5#ref-CR33" id="ref-link-section-d55577e419">2004</a>). Facial expression has been extensively studied in the context of emotional expression, for example the work of Kshirsagar and Magnenat-Thalmann (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: 2nd international symposium on smart graphics, pp 107–115" href="/article/10.1007/s10055-010-0167-5#ref-CR21" id="ref-link-section-d55577e422">2002</a>) or Bui et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bui TD, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of computer graphics international 2004, pp 284–290" href="/article/10.1007/s10055-010-0167-5#ref-CR5" id="ref-link-section-d55577e425">2004</a>). Gestures are very closely related to speech (Cassell and Stone <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cassell J, Stone M (1999) Living hand to mouth: psychological theories about speech and gesture in interactive dialogue systems. In: AAI 1999 fall symposium on psychological models of communication in collaborative systems, pp 34–42" href="/article/10.1007/s10055-010-0167-5#ref-CR8" id="ref-link-section-d55577e429">1999</a>) and have been modeled in a number of ways, from the highly data driven methods of Stone et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Stone M, DeCarlo D, Oh I, Rodriguez C, Stere A, Lees A, Bregler C (2004) Speaking with hands: creating animated conversational characters from recordings of human performance. In: SIGGRAPH ’04, ACM SIGGRAPH 2004 papers. ACM Press, New York, pp 506–513" href="/article/10.1007/s10055-010-0167-5#ref-CR30" id="ref-link-section-d55577e432">2004</a>) to the totally procedural methods of Kopp et al. (Kopp and Wachsmuth <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kopp S, Wachsmuth I (2004) Synthesizing multimodal utterances for conversational agents. J Comput Animat Virtual Worlds 15(1):39–52" href="/article/10.1007/s10055-010-0167-5#ref-CR19" id="ref-link-section-d55577e435">2004</a>, Kopp et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Kopp S, Tepper P, Cassell J (2005) Towards integrated microplanning of language and iconic gesture for multimodal output. In: International Conference on Multimodal Interfaces (ICMI’04), ACM Press, pp 97–104" href="/article/10.1007/s10055-010-0167-5#ref-CR18" id="ref-link-section-d55577e438">2005</a>). Finally, posture is also an important modality of expression that has been used as a means of producing believable idling behavior (Egges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Egges A, Molet T, Magnenat-Thalmann N (2004) Personalised real-time idle motion synthesis. In: 12th Pacific conference on computer graphics and applications, pp 121–130" href="/article/10.1007/s10055-010-0167-5#ref-CR9" id="ref-link-section-d55577e441">2004</a>) or of expressing interpersonal relationships (Gillies and Ballin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Gillies M, Ballin D (2003) A model of interpersonal attitude and posture generation. In: Rist T, Aylett R, Ballin D, Rickel J (eds) Fourth workshop on intelligent virtual agents. Kloster Irsee, Germany" href="/article/10.1007/s10055-010-0167-5#ref-CR12" id="ref-link-section-d55577e444">2003</a>).</p><p>One of the first systems that allows people to interact multimodally with a character using voice and gestures tracking was Thórisson’s Gandalf system (Thórisson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Thórisson K (1998) Real-time decision making in multimodal face-to-face communication. In: Second ACM international conference on autonomous agents, pp 16–23" href="/article/10.1007/s10055-010-0167-5#ref-CR31" id="ref-link-section-d55577e450">1998</a>). This work was later developed by Cassell’s group into an interaction system with a full bodied character capable of complex non-verbal communication (Cassell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cassell J, Bickmore T, Campbell L, Chang K,Vilhjálmsson H, Yan H (1999) Embodiment in conversational interfaces: Rea. In: ACM SIGCHI , ACM Press, pp 520–527" href="/article/10.1007/s10055-010-0167-5#ref-CR6" id="ref-link-section-d55577e453">1999</a>). The Max system focus primarily on voice and gesture interaction (Kopp et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kopp S, Jung B, Lessmann N, Wachsmuth I (2003) Max–a multimodal assistant in virtual reality construction. KI-Knstliche Intelligenz 3(4):11–17" href="/article/10.1007/s10055-010-0167-5#ref-CR17" id="ref-link-section-d55577e456">2003</a>). Work by Maatman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Maatman RM, Gratch J, Marsella S (2005) Natural behavior of a listening agent. In: Panayiotopoulos T, Gratch J, Aylett R, Ballin D, Olivier P, Rist T (eds) Intelligent virtual agents. 5th international working conference, Kos" href="/article/10.1007/s10055-010-0167-5#ref-CR24" id="ref-link-section-d55577e459">2005</a>) focused on the listening behavior of a character, and like our work uses head tracking and voice input. These systems have demonstrated the power of multimodal face-to-face interaction with virtual characters. This paper shows how it is possible to rapidly create and customize such systems from basic building blocks.</p><p>Our work also uses research in the area of data-driven animation and motion editing. In particular, the Motion Warping formulation of motion editing (Witkin and Popović <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Witkin A, Popović Z (1995) Motion warping. In: ACM SIGGRAPH, pp 105–108" href="/article/10.1007/s10055-010-0167-5#ref-CR35" id="ref-link-section-d55577e465">1995</a>). Our functional model is an excellent way of combining motion editing techniques and we have implemented several within our framework. Examples include, interpolation-based animation such as the work of Rose et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Rose C, Cohen MF, Bodenheimer B (1998) Verbs and adverbs: multidimensional motion interpolation. IEEE Comput Graph Appl 18(5):32–40" href="/article/10.1007/s10055-010-0167-5#ref-CR28" id="ref-link-section-d55577e468">1998</a>), principal component analysis-based animation (Alexa and Müller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Alexa M, Müller W (2000) Representing animations by principal components. Comput Graph Forum 19(3):411–418" href="/article/10.1007/s10055-010-0167-5#ref-CR1" id="ref-link-section-d55577e471">2000</a>) and the Motion Graph data structure (Arikan and Forsyth <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Arikan O, Forsyth DA (2002) Interactive motion generation from examples. ACM Trans Graph 21(3):483-490" href="/article/10.1007/s10055-010-0167-5#ref-CR3" id="ref-link-section-d55577e474">2002</a>; Kovar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kovar L, Gleicher M, Pighin F (2002) Motion graphs. ACM Trans Graph 21(3):473–482 July 2002" href="/article/10.1007/s10055-010-0167-5#ref-CR20" id="ref-link-section-d55577e477">2002</a>; Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lee J, Chai J, Reitsma PSA, Hodgins JK, Pollard NS (2002) Interactive control of avatars animated with human motion data. ACM Trans Graph 21(3):491–500" href="/article/10.1007/s10055-010-0167-5#ref-CR22" id="ref-link-section-d55577e481">2002</a>).</p><p>The work we present is a functional abstraction of character animation and behavior. In this sense, it is similar to functional abstraction frameworks used in other domains. In particular, there are a number of interesting abstraction frameworks used in virtual reality and graphics for example Figueroa et al.’s InTML system (Figueroa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Figueroa P, Green M, Hoover HJ (2002) Intml: a description language for vr applications. In: Proceedings of Web3D ’02" href="/article/10.1007/s10055-010-0167-5#ref-CR11" id="ref-link-section-d55577e487">2002</a>) or Elliott et al.’s TBAG system (Elliott et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Elliott C, Schechter G, Yeung R, Abi-Ezzi S (July 1994) Tbag: a high level framework for interactive, animated 3d graphics applications. In: Proceedings of SIGGRAPH 94, computer graphics proceedings, annual conference series, pp 421–434" href="/article/10.1007/s10055-010-0167-5#ref-CR10" id="ref-link-section-d55577e490">1994</a>).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">A functional model of animation</h2><div class="c-article-section__content" id="Sec3-content"><p>Handling and combining many diverse methods of animation on a single character requires a single representation for all of them. At its most abstract, an animation can be viewed as a function of time to the state of a character:
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x} = f(t) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <b>x</b> is some representation of a characters state. The most common representation in body animation would be a skeletal one, in which the root of the character has a vector position <i>p</i>
                        <sub>0</sub> and quaternion orientation <i>q</i>
                        <sub>0</sub> and all of the joints have quaternion orientations (<i>q</i>
                        <sub>
                  <i>i</i>
                </sub>): {<b>p</b>
                        <sub>0</sub>, <b>q</b>
                        <sub>0</sub>, <b>q</b>
                        <sub>1</sub>, ..., <b>q</b>
                        <sub>
                  <i>n</i>
                </sub>}. However, we do not restrict animations to this representation, for example joint orientations can also be represented using Grassia’s exponential map representation (Grassia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Grassia FS (1998) Practical parameterization of rotations using the exponential map. J Graph Tools 3(3):29–48" href="/article/10.1007/s10055-010-0167-5#ref-CR13" id="ref-link-section-d55577e562">1998</a>) if it is more convenient for certain calculations. For facial animation, the state can be represented as a set of weight values for morph targets or as positions or rotations of facial bones. The state can also have more abstract representations, for example, the result of one animation function can be used as the output of another, as we shall see below.</p><p>Most animations, will in fact take other parameters making this the general form of an animation function:
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x} = f(\varvec{\theta}, t) $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>We will now show how different types of animation can easily be represented in this framework.</p>
                <h3 class="c-article__sub-heading">Keyframe or motion captured animation</h3>
                <p> Both hand animated data and motion captured animation are represented as a list of evenly or unevenly spaced keyframes. This can be represented as the following function:
</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f({\bf k}, t) = {\it interpolate}(k_{\tau(t)}, k_{\tau(t)+1}, t) $$</span></div></div><p>where <b>k</b> is the keyframe data and τ(<i>t</i>) is the keyframe time prior to <i>t</i>. <i>interpolate</i> can be any suitable interpolation function, we use cubic spline interpolation.</p>
              
                <h3 class="c-article__sub-heading">Procedural animation</h3>
                <p> Procedural animation is the most general instantiation of the functional model, and can be represented as any function of the form (2). The parameters <span class="mathjax-tex">\(\varvec{\theta}\)</span> will depend heavily on the type of motion, for example, a gaze motion will have parameters the include the target of gaze and the length of gaze.</p>
              
                <h3 class="c-article__sub-heading">Motion transforms</h3>
                <p> An animation function can be a transformation of another motion function <i>g</i>:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x} = f(g,\varvec{\theta}, t) $$</span></div></div><p>The most general form of transformation is a general motion warp (Witkin and Popović <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Witkin A, Popović Z (1995) Motion warping. In: ACM SIGGRAPH, pp 105–108" href="/article/10.1007/s10055-010-0167-5#ref-CR35" id="ref-link-section-d55577e653">1995</a>), which consists of a timewarp, α, transforming <i>t</i> and a space warp, β transforming the output of <i>g</i>:</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(g, \varvec{\theta}_{\alpha}, \varvec{\theta}_{\beta}, t) = \beta(\varvec{\theta}_{\beta}, g(\alpha(\varvec{\theta}_{\alpha}, t)) $$</span></div></div><p>Simple examples include, spatial scaling:<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>
                           <i>f</i>(<i>g</i>, <i>s</i>, <i>t</i>) = <i>sg</i>(<i>t</i>); temporal scaling: <i>f</i>(<i>g</i>, <i>s</i>, <i>t</i>) = <i>g</i>(<i>st</i>), and looping <span class="mathjax-tex">\(f(g,t)=g(t \bmod |g|)\)</span>, where |<i>g</i>| is the length of <i>g</i> (in practice a more complex looping function is used to ensure smooth transitions). A particularly useful transform is a mask, which is used to select certain joint or morph targets to which to apply an animation, based on a mask, <b>m</b>:
</p><div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(g, {\bf m}, t)_{i} = \left\{ \begin{array}{ll} g(t)_i &amp; \hbox {if} \; {\bf m}_{\rm i}=1,\\ 0 &amp; \hbox{otherwise} \end{array}\right. $$</span></div></div><p>As mentioned above, the parameters of a motion transform can themselves be other motion functions. For example, a general motion warp can be created using a motion function, <i>h</i> as the warping parameter: <i>f</i>(<i>g</i>, <i>h</i>, <i>t</i>) = <i>g</i>(<i>h</i>(<i>t</i>)).</p>
              
                <h3 class="c-article__sub-heading">
                  <i>Combining animations</i>
                </h3>
                <p> Animations can also be combined together by functions of two other animations, which have the general form:
</p><div id="Eque" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(g_{1}, g_{2}, \varvec{\theta}_{\beta}, \varvec{\theta}_{\alpha 1}, \varvec{\theta}_{\alpha 2}, t) = \beta(\varvec{\theta}_{\beta}, g_{1}(\alpha_{1}(\varvec{\theta}_{\alpha 1}, t)), g_{1}(\alpha_{2}(\varvec{\theta}_{\alpha 2}, t))) $$</span></div></div><p>Examples include addition:<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
                           <i>f</i>(<i>g</i>
                           <sub>1</sub>, <i>g</i>
                           <sub>2</sub>, <i>t</i>) = <i>g</i>
                           <sub>1</sub>(<i>t</i>) + <i>g</i>
                           <sub>2</sub>(<i>t</i>); blending between animations: <i>f</i>(<i>g</i>
                           <sub>1</sub>, <i>g</i>
                           <sub>2</sub>, λ, <i>t</i>) = λ <i>g</i>
                           <sub>1</sub>(<i>t</i>) + (1 − λ)<i>g</i>
                           <sub>2</sub>(<i>t</i>) and sequencing motions:
</p><div id="Equf" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(g_{1}, g_{2},t)=\left\{ \begin{array}{ll} g_1(t) &amp; \hbox {if} \; t &lt; |g_2|,\\ g_2(t-|g_1|) &amp; \hbox {otherwise} \end{array} \right.$$</span></div></div><p>As with looped motions, in practice we would used a smoothed version of this function. There are also functions for combining multiple animations, for example blending between several animations; finite- state machines that choose different animations based on their state; animations based on a principal component analysis of other animations (Alexa and Müller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Alexa M, Müller W (2000) Representing animations by principal components. Comput Graph Forum 19(3):411–418" href="/article/10.1007/s10055-010-0167-5#ref-CR1" id="ref-link-section-d55577e885">2000</a>), and motion graphs described below.</p>
              <p>The power of the functional model of animation comes from the fact that all types of animation have the same form. The animation functions can be curried, pre-applying all parameters apart from <i>t</i> to get a new function of the form (1). Once in this form the user does not need to know anything about the type of animation. In particular, the functions for transforming and combining animations have the same form as their inputs, making it possible to compose them. As we shall see arbitrary composition of functions for transforming and combining animations can be a powerful tool for creating complex transformations. This leads to a methodology of decomposing transformations into their most basic elements, which can then be reused by composing them in a number of different ways. This in turn makes it easy to author new behaviors and variants of existing behaviors.</p><h3 class="c-article__sub-heading" id="Sec4">Implementation</h3><p>Before discussing how the functional model is used, a quick note on how it is implemented. The model has been implemented as part of Piavca (the Platform Independent Architecture for Virtual Characters and Avatars), an open source software framework for creating virtual characters (available at <a href="http://piavca.sourceforge.net/">http://piavca.sourceforge.net/</a>). Piavca is a generic API built in C++ that can be combined with different graphics engine. The current implementation uses Cal3d (<a href="http://home.gna.org/cal3d/">http://home.gna.org/cal3d/</a>) as a low-level animation system providing functionality such as smooth skinning and morph targets while Piavca overrides the motion-blending features. The renderer is based on Cal3d’s OpenGL renderer. This system can be used with numerous graphics and virtual reality systems. Currently, it is implemented on Dive (<a href="http://www.sics.se/dive/">http://www.sics.se/dive/</a>), XVR (<a href="http://www.vrmedia.it/Xvr.htm">http://www.vrmedia.it/Xvr.htm</a>) and we are working on an implementation for OpenSG (<a href="http://www.opensg.org/">http://www.opensg.org/</a>).</p><p>Our implementation (in the C++ and Python languages) is object oriented. All of the functions are in fact function objects and so can contain state. They all inherit from a single base class, motion, which acts as an abstract representation of an animation function. Currying the functions is achieved by providing all parameters except <i>t</i> at initialization time, and making the function objects callable with <i>t</i> as a parameter.</p><p>Users can configure and combine motion functions using a Python based scripting interface to Piavca, for maximum flexibility. Alternatively, there is an XML-based behavior definition language that allows users to create character behavior models without requiring programming skills. The behavior definition language exactly mirrors the functional model with tags for each possible function.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">A heterogeneous character system</h2><div class="c-article-section__content" id="Sec5-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0167-5#Fig1">1</a> shows the type of heterogeneous interaction that is possible with our characters. The human participant’s behavior is input with typical sensors for an immersive VR system, a microphone and head tracker.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Heterogeneous interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>However, these two inputs are used in a variety of different ways by different behaviors. The head tracker is used to obtain the position of the participant in order to maintain an appropriate conversational distance (maintaining distance in conversation is called proxemics in the non-verbal communication literature). It is also used to detect when the participant shifts posture. The character’s posture shifts are then synchronized with those of the participant, which is know to build rapport (Kendon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1970" title="Kendon A (1970) Movement coordination in social interaction. Acta Psychol 32:1–25" href="/article/10.1007/s10055-010-0167-5#ref-CR16" id="ref-link-section-d55577e981">1970</a>). The position of the participant is also used by a gaze behavior to ensure the character is looking in the right place. The audio from the microphone is used to detect when the participant is talking. This is used by the gaze behavior so the character can look at the participant more when listening to him or her. The character also gives head nods and other feedback signals when the participant is speaking. All these behaviors happen automatically in response to the sensor input, using real-time algorithms.</p><p>The microphone audio is also used for speech interaction. Speech interaction is either controlled by a human controller, if the character is an avatar, or by a dialogue engine. When the character speaks a number of other behaviors are triggered. The character’s lip movements will be synchronized to the speech and the character will gesture. The gaze behavior is also altered to take account of the fact that the character is speaking (Argyle and Cookv <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1976" title="Argyle M, Cookv M (1976) Gaze and mutual gaze. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-010-0167-5#ref-CR2" id="ref-link-section-d55577e987">1976</a>). As well as triggering speech the character’s controller can also trigger certain scripted actions and gestures. Apart from lip synchronization and gaze, the character’s facial expression is independent of the participant’s behavior, consisting of occasional smiling and blinking. The character therefore has a wide range of styles of interaction, all happening simultaneously. These contain many different animation processes, both facial and bodily, that must be combined to create a single coherent animation.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0167-5#Fig2">2</a> shows how this type of character can be implemented using our functional model. The example we give is of a human-controlled character that is used in Wizard of Oz style experiments. A human operator controls certain aspects of the behavior, while others are automatic. The character’s speech is controlled by a human being (the controller) selecting speech sequences from a library of possible utterances, while the character interacts with another person (the participant). This system has 3 inputs: the position of the participant, an audio signal of the voice of the participant and input from the person controlling the character, specifying speech utterances. The position input is a 3-vector whose value is obtained every frame from a head tracker on the participant. The voice signal is obtained from an ordinary microphone. For this application we simply threshold the audio value to detect whether the participant is speaking. The controller has a user interface with a number of buttons used to trigger speech utterances.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Implementing a heterogeneous interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Some behaviors of the character are not influenced by any of the inputs. For example, the character has a simple blinking behavior. This is a loop containing a blinking animation sequenced with a zero animation. A zero animation is simply an animation function that returns zero for all joint or facial expression values and in this case it is used to model the inter-blinking period. The length of the zero motion is varied every time around the loop to ensure the timings are not too repetitive. A facial animation loop, in which the character smiles occasionally, is implemented similarly. The head tracker input is used in a number of ways. The first is for posture shifts. During conversation, people tend to synchronize their movements, particularly movement such as posture shifts (Kendon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1970" title="Kendon A (1970) Movement coordination in social interaction. Acta Psychol 32:1–25" href="/article/10.1007/s10055-010-0167-5#ref-CR16" id="ref-link-section-d55577e1018">1970</a>). This synchronization is a strong sign of rapport between individuals. In order to simulate this, we detect posture shifts by finding large changes in position. We then trigger a posture shifts. The characters posture is modeled as a finite state machine animation in which each state is a different possible posture. On a posture shift, a new state is chosen at random. A finite state machine animation performs a smooth sequence between the animations associated with each state, ensuring smooth posture shifts. The head tracker is also used by the proxemics behavior. Proxmemics is the use of space in social interaction. For our characters, this means maintaining a comfortable distance from and orientation to the participant. The relative distance and angle of the participant to the character are calculated from the tracker position. If they are too large or small, the character turns to face the participant or takes a step forward or backward. Again this behavior is modeled as a finite state machine, with a default state being the zero motion and a state for each motion direction. The final use for the position input is to control the gaze behavior. The position gives a target to look at. The audio input is used to detect when the participant is speaking and give feedback behavior. In this implementation, the feedback consists of occasional nodding to give encouragement. This is implemented as a loop in the same way as blinking.</p><p>The other major input is from the controller, who can issue commands to control the character’s speech. The behavior consists of a number of multi-modal utterances that can be triggered using a graphical user interface. Multi-modal utterances are short scripted behaviors that combined speech (in this case audio files) with animation elements. For example, the audio is accompanied by facial animation for lip synchronization and also appropriate gestures. The scripts give the creators of the character very tight control of the character’s behavior and potentially high-quality behavior can be created. This comes at the possible cost of some interactivity, however, we believe that our methodology of combining more scripted elements with real-time interaction can combine benefits of both.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0167-5#Fig3">3</a> shows some still frames from an interaction with our virtual character, the accompanying video shows the actual interaction. The type of character set up we have described is only one way of interacting. For example, the character could be total autonomous with utterances triggered from an AI “chat-bot” system, or the character’s speech could be directly taken from the controllers own voice. In the second case, many elements such as gestures and lip synchronization would have to be generated automatically to suit the speech. Our frameworks make it easy to build new styles of interaction from existing components.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0167-5/MediaObjects/10055_2010_167_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A real and virtual human interacting in an immersive virtual environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0167-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Conclusion</h2><div class="c-article-section__content" id="Sec6-content"><p>This paper has presented a software framework for creating interactive virtual characters. The many different types of behavior involved with human social interaction imply a range of different styles of animation and interaction with a character. Our framework allows us to unify and combine these diverse methods using a single abstract function representation. It makes it easy to create new character systems by combining different behavior modules in different ways. The framework has been released as part of the open source project Piavca (<a href="http://piavca.sourceforge.net/">http://piavca.sourceforge.net/</a>), we encourage readers to try out the functionality.</p><p>This framework has been used for virtual reality experiments at University College London. These have demonstrated that people respond to the characters in some way as if they are human. For example, Pan and Slater (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Pan X, Slater M (2007) A preliminary study of shy males interacting with a virtual female. In: Presence 2007. The 10th annual international workshop on presence, pp 101–108" href="/article/10.1007/s10055-010-0167-5#ref-CR26" id="ref-link-section-d55577e1065">2007</a>) conducted an experiment in which socially phobic male participants interacted with a virtual woman that engaged them in conversation that became increasingly intimate. The experimenters measured skin conductance level, which demonstrated that the intimate conversation resulted in greater arousal. Interestingly, proxemic behavior played an important role in this. At one point, the character’s proxemic distance was decreased resulting in her moving closer to the participant. This produced the highest skin conductance levels in the experiment. These quantitative results were supported by the participants subjective reports, with many reporting strong emotions such as anxiety or even guilt at “cheating” on their partner with a virtual woman. An analysis of the body movements of the participants during the scenario (Pan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pan X, Gillies M, Slater M (2008) Male bodily responses during an interaction with a virtual woman. In: Prendinger H, Lester JC, Ishizuka M (eds) Intelligent virtual agents. 8th international conference, IVA 2008, Tokyo, Japan, September 1–3, 2008. Proceedings, Vol 5208 of lecture notes in computer science, Springer, pp 89–96" href="/article/10.1007/s10055-010-0167-5#ref-CR25" id="ref-link-section-d55577e1068">2008</a>) showed that they used more social non-verbal cues such as nodding or cocking their heads during the conversation than before it.</p><p>Future work on the project will involve increasing the range of functionality and of possible applications. In a modular system such as ours, it is easy to add functionality by either added new animation functions or combining existing ones in new ways. As the system is applied to different situations and styles of interaction, new requirements will naturally emerge and therefore drive the development of new functionality. We are currently applying the system to more graphically realistic character and making greater use of motion capture, raising the level of realism that is possible. This greater realism will itself bring new requirements to our animation framework. The most important change we are currently planning is the addition of a graphical user interface for combining behavior functions. This will supplement the existing scripting interface and definition language, and provided a more accessible method of creating characters.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>For quaternion animations multiplication is replaced by scaling the rotation angle by <i>s</i>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>For quaternions, quaternion multiplication is used instead of addition.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Alexa, W. Müller, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Alexa M, Müller W (2000) Representing animations by principal components. Comput Graph Forum 19(3):411–418" /><p class="c-article-references__text" id="ref-CR1">Alexa M, Müller W (2000) Representing animations by principal components. Comput Graph Forum 19(3):411–418</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2F1467-8659.00433" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Representing%20animations%20by%20principal%20components&amp;journal=Comput%20Graph%20Forum&amp;volume=19&amp;issue=3&amp;pages=411-418&amp;publication_year=2000&amp;author=Alexa%2CM&amp;author=M%C3%BCller%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Argyle, M. Cookv, " /><meta itemprop="datePublished" content="1976" /><meta itemprop="headline" content="Argyle M, Cookv M (1976) Gaze and mutual gaze. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR2">Argyle M, Cookv M (1976) Gaze and mutual gaze. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gaze%20and%20mutual%20gaze&amp;publication_year=1976&amp;author=Argyle%2CM&amp;author=Cookv%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Arikan, DA. Forsyth, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Arikan O, Forsyth DA (2002) Interactive motion generation from examples. ACM Trans Graph 21(3):483-490" /><p class="c-article-references__text" id="ref-CR3">Arikan O, Forsyth DA (2002) Interactive motion generation from examples. ACM Trans Graph 21(3):483-490</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F566654.566606" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20motion%20generation%20from%20examples&amp;journal=ACM%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=483-490&amp;publication_year=2002&amp;author=Arikan%2CO&amp;author=Forsyth%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Badler N, Philips C, Webber B (eds) (1993) Simulating humans: computer graphics, animation and control. Oxford" /><p class="c-article-references__text" id="ref-CR4">Badler N, Philips C, Webber B (eds) (1993) Simulating humans: computer graphics, animation and control. Oxford University Press, Oxford</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulating%20humans%3A%20computer%20graphics%2C%20animation%20and%20control&amp;publication_year=1993">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bui TD, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of co" /><p class="c-article-references__text" id="ref-CR5">Bui TD, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of computer graphics international 2004, pp 284–290</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cassell J, Bickmore T, Campbell L, Chang K,Vilhjálmsson H, Yan H (1999) Embodiment in conversational interface" /><p class="c-article-references__text" id="ref-CR6">Cassell J, Bickmore T, Campbell L, Chang K,Vilhjálmsson H, Yan H (1999) Embodiment in conversational interfaces: Rea. In: ACM SIGCHI , ACM Press, pp 520–527</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cassell J, Bickmore T, Campbell L, Chang K Vilhjálmsson H, Yan H (1999) Requirements for an architecture for e" /><p class="c-article-references__text" id="ref-CR7">Cassell J, Bickmore T, Campbell L, Chang K Vilhjálmsson H, Yan H (1999) Requirements for an architecture for embodied conversational characters. In: Proceedings of computer animation and simulation ’99</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cassell J, Stone M (1999) Living hand to mouth: psychological theories about speech and gesture in interactive" /><p class="c-article-references__text" id="ref-CR8">Cassell J, Stone M (1999) Living hand to mouth: psychological theories about speech and gesture in interactive dialogue systems. In: AAI 1999 fall symposium on psychological models of communication in collaborative systems, pp 34–42</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Egges A, Molet T, Magnenat-Thalmann N (2004) Personalised real-time idle motion synthesis. In: 12th Pacific co" /><p class="c-article-references__text" id="ref-CR9">Egges A, Molet T, Magnenat-Thalmann N (2004) Personalised real-time idle motion synthesis. In: 12th Pacific conference on computer graphics and applications, pp 121–130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Elliott C, Schechter G, Yeung R, Abi-Ezzi S (July 1994) Tbag: a high level framework for interactive, animated" /><p class="c-article-references__text" id="ref-CR10">Elliott C, Schechter G, Yeung R, Abi-Ezzi S (July 1994) Tbag: a high level framework for interactive, animated 3d graphics applications. In: Proceedings of SIGGRAPH 94, computer graphics proceedings, annual conference series, pp 421–434</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Figueroa P, Green M, Hoover HJ (2002) Intml: a description language for vr applications. In: Proceedings of We" /><p class="c-article-references__text" id="ref-CR11">Figueroa P, Green M, Hoover HJ (2002) Intml: a description language for vr applications. In: Proceedings of Web3D ’02</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gillies M, Ballin D (2003) A model of interpersonal attitude and posture generation. In: Rist T, Aylett R, Bal" /><p class="c-article-references__text" id="ref-CR12">Gillies M, Ballin D (2003) A model of interpersonal attitude and posture generation. In: Rist T, Aylett R, Ballin D, Rickel J (eds) Fourth workshop on intelligent virtual agents. Kloster Irsee, Germany</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FS. Grassia, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Grassia FS (1998) Practical parameterization of rotations using the exponential map. J Graph Tools 3(3):29–48" /><p class="c-article-references__text" id="ref-CR13">Grassia FS (1998) Practical parameterization of rotations using the exponential map. J Graph Tools 3(3):29–48</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Practical%20parameterization%20of%20rotations%20using%20the%20exponential%20map&amp;journal=J%20Graph%20Tools&amp;volume=3&amp;issue=3&amp;pages=29-48&amp;publication_year=1998&amp;author=Grassia%2CFS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gratch J, Marsella S (2001) Tears and fears: modeling emotions and emotional behaviors in synthetic agents. In" /><p class="c-article-references__text" id="ref-CR14">Gratch J, Marsella S (2001) Tears and fears: modeling emotions and emotional behaviors in synthetic agents. In: AGENTS ’01, Proceedings of the fifth international conference on autonomous agents, pp 278–285, ACM Press, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Guye-Vuilléme, TK. Capin, IS. Pandzic, N. Magnenat-Thalmann, D. Thalmann, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Guye-Vuilléme A, Capin TK, Pandzic IS, Magnenat-Thalmann N, Thalmann D (1999) Non-verbal communication interfa" /><p class="c-article-references__text" id="ref-CR15">Guye-Vuilléme A, Capin TK, Pandzic IS, Magnenat-Thalmann N, Thalmann D (1999) Non-verbal communication interface for collaborative virtual environments. Virtual Real J 4:49–59</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF01434994" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-verbal%20communication%20interface%20for%20collaborative%20virtual%20environments&amp;journal=Virtual%20Real%20J&amp;volume=4&amp;pages=49-59&amp;publication_year=1999&amp;author=Guye-Vuill%C3%A9me%2CA&amp;author=Capin%2CTK&amp;author=Pandzic%2CIS&amp;author=Magnenat-Thalmann%2CN&amp;author=Thalmann%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kendon, " /><meta itemprop="datePublished" content="1970" /><meta itemprop="headline" content="Kendon A (1970) Movement coordination in social interaction. Acta Psychol 32:1–25" /><p class="c-article-references__text" id="ref-CR16">Kendon A (1970) Movement coordination in social interaction. Acta Psychol 32:1–25</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0001-6918%2870%2990088-0" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Movement%20coordination%20in%20social%20interaction&amp;journal=Acta%20Psychol&amp;volume=32&amp;pages=1-25&amp;publication_year=1970&amp;author=Kendon%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Kopp, B. Jung, N. Lessmann, I. Wachsmuth, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Kopp S, Jung B, Lessmann N, Wachsmuth I (2003) Max–a multimodal assistant in virtual reality construction. KI-" /><p class="c-article-references__text" id="ref-CR17">Kopp S, Jung B, Lessmann N, Wachsmuth I (2003) Max–a multimodal assistant in virtual reality construction. KI-Knstliche Intelligenz 3(4):11–17</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Max%E2%80%93a%20multimodal%20assistant%20in%20virtual%20reality%20construction&amp;journal=KI-Knstliche%20Intelligenz&amp;volume=3&amp;issue=4&amp;pages=11-17&amp;publication_year=2003&amp;author=Kopp%2CS&amp;author=Jung%2CB&amp;author=Lessmann%2CN&amp;author=Wachsmuth%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kopp S, Tepper P, Cassell J (2005) Towards integrated microplanning of language and iconic gesture for multimo" /><p class="c-article-references__text" id="ref-CR18">Kopp S, Tepper P, Cassell J (2005) Towards integrated microplanning of language and iconic gesture for multimodal output. In: International Conference on Multimodal Interfaces (ICMI’04), ACM Press, pp 97–104</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Kopp, I. Wachsmuth, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kopp S, Wachsmuth I (2004) Synthesizing multimodal utterances for conversational agents. J Comput Animat Virtu" /><p class="c-article-references__text" id="ref-CR19">Kopp S, Wachsmuth I (2004) Synthesizing multimodal utterances for conversational agents. J Comput Animat Virtual Worlds 15(1):39–52</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fcav.6" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthesizing%20multimodal%20utterances%20for%20conversational%20agents&amp;journal=J%20Comput%20Animat%20Virtual%20Worlds&amp;volume=15&amp;issue=1&amp;pages=39-52&amp;publication_year=2004&amp;author=Kopp%2CS&amp;author=Wachsmuth%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Kovar, M. Gleicher, F. Pighin, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Kovar L, Gleicher M, Pighin F (2002) Motion graphs. ACM Trans Graph 21(3):473–482 July 2002" /><p class="c-article-references__text" id="ref-CR20">Kovar L, Gleicher M, Pighin F (2002) Motion graphs. ACM Trans Graph 21(3):473–482 July 2002</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F566654.566605" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Motion%20graphs&amp;journal=ACM%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=473-482&amp;publication_year=2002&amp;author=Kovar%2CL&amp;author=Gleicher%2CM&amp;author=Pighin%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: 2nd international symposium on sm" /><p class="c-article-references__text" id="ref-CR21">Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: 2nd international symposium on smart graphics, pp 107–115</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Lee, J. Chai, PSA. Reitsma, JK. Hodgins, NS. Pollard, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lee J, Chai J, Reitsma PSA, Hodgins JK, Pollard NS (2002) Interactive control of avatars animated with human m" /><p class="c-article-references__text" id="ref-CR22">Lee J, Chai J, Reitsma PSA, Hodgins JK, Pollard NS (2002) Interactive control of avatars animated with human motion data. ACM Trans Graph 21(3):491–500</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20control%20of%20avatars%20animated%20with%20human%20motion%20data&amp;journal=ACM%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=491-500&amp;publication_year=2002&amp;author=Lee%2CJ&amp;author=Chai%2CJ&amp;author=Reitsma%2CPSA&amp;author=Hodgins%2CJK&amp;author=Pollard%2CNS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SP. Lee, JB. Badler, NI. Badler, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lee SP, Badler JB, Badler NI (2002) Eyes alive. ACM Trans Graph 21(3):637–644" /><p class="c-article-references__text" id="ref-CR23">Lee SP, Badler JB, Badler NI (2002) Eyes alive. ACM Trans Graph 21(3):637–644</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Eyes%20alive&amp;journal=ACM%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=637-644&amp;publication_year=2002&amp;author=Lee%2CSP&amp;author=Badler%2CJB&amp;author=Badler%2CNI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maatman RM, Gratch J, Marsella S (2005) Natural behavior of a listening agent. In: Panayiotopoulos T, Gratch J" /><p class="c-article-references__text" id="ref-CR24">Maatman RM, Gratch J, Marsella S (2005) Natural behavior of a listening agent. In: Panayiotopoulos T, Gratch J, Aylett R, Ballin D, Olivier P, Rist T (eds) Intelligent virtual agents. 5th international working conference, Kos</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan X, Gillies M, Slater M (2008) Male bodily responses during an interaction with a virtual woman. In: Prendi" /><p class="c-article-references__text" id="ref-CR25">Pan X, Gillies M, Slater M (2008) Male bodily responses during an interaction with a virtual woman. In: Prendinger H, Lester JC, Ishizuka M (eds) Intelligent virtual agents. 8th international conference, IVA 2008, Tokyo, Japan, September 1–3, 2008. Proceedings, Vol 5208 of lecture notes in computer science, Springer, pp 89–96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan X, Slater M (2007) A preliminary study of shy males interacting with a virtual female. In: Presence 2007. " /><p class="c-article-references__text" id="ref-CR26">Pan X, Slater M (2007) A preliminary study of shy males interacting with a virtual female. In: Presence 2007. The 10th annual international workshop on presence, pp 101–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poggi I, Pelachaud C, De Rosis F, Carofiglio V, De Carolis B (2005) Greta. A believable embodied conversationa" /><p class="c-article-references__text" id="ref-CR27">Poggi I, Pelachaud C, De Rosis F, Carofiglio V, De Carolis B (2005) Greta. A believable embodied conversational agent. In: Stock O, Zancanaro M (eds) Multimodal intelligent information presentation. Text, speech and language technology, vol 27. Kluwer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Rose, MF. Cohen, B. Bodenheimer, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Rose C, Cohen MF, Bodenheimer B (1998) Verbs and adverbs: multidimensional motion interpolation. IEEE Comput G" /><p class="c-article-references__text" id="ref-CR28">Rose C, Cohen MF, Bodenheimer B (1998) Verbs and adverbs: multidimensional motion interpolation. IEEE Comput Graph Appl 18(5):32–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Verbs%20and%20adverbs%3A%20multidimensional%20motion%20interpolation&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=18&amp;issue=5&amp;pages=32-40&amp;publication_year=1998&amp;author=Rose%2CC&amp;author=Cohen%2CMF&amp;author=Bodenheimer%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Slater M, Usoh M (1994) Body centred interaction in immersive virtual environments. In: Thalmann NM, Thalmann " /><p class="c-article-references__text" id="ref-CR29">Slater M, Usoh M (1994) Body centred interaction in immersive virtual environments. In: Thalmann NM, Thalmann D (eds) Artificial life and virtual reality. Wiley, pp 125–148</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stone M, DeCarlo D, Oh I, Rodriguez C, Stere A, Lees A, Bregler C (2004) Speaking with hands: creating animate" /><p class="c-article-references__text" id="ref-CR30">Stone M, DeCarlo D, Oh I, Rodriguez C, Stere A, Lees A, Bregler C (2004) Speaking with hands: creating animated conversational characters from recordings of human performance. In: SIGGRAPH ’04, ACM SIGGRAPH 2004 papers. ACM Press, New York, pp 506–513</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thórisson K (1998) Real-time decision making in multimodal face-to-face communication. In: Second ACM internat" /><p class="c-article-references__text" id="ref-CR31">Thórisson K (1998) Real-time decision making in multimodal face-to-face communication. In: Second ACM international conference on autonomous agents, pp 16–23</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vilhjálmsson HH (2005) Augmenting online conversation through automated discourse tagging. In: 6th annual mini" /><p class="c-article-references__text" id="ref-CR32">Vilhjálmsson HH (2005) Augmenting online conversation through automated discourse tagging. In: 6th annual minitrack on persistent conversation at the 38th Hawaii international conference on system sciences</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Vinayagamoorthy, M. Garau, A. Steed, M. Slater, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Vinayagamoorthy V, Garau M, Steed A, Slater M (2004) An eye gaze model for dyadic interaction in an immersive " /><p class="c-article-references__text" id="ref-CR33">Vinayagamoorthy V, Garau M, Steed A, Slater M (2004) An eye gaze model for dyadic interaction in an immersive virtual environment: practice and experience. Comput Graph Forum 23(1):1–12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2004.00001.x" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20eye%20gaze%20model%20for%20dyadic%20interaction%20in%20an%20immersive%20virtual%20environment%3A%20practice%20and%20experience&amp;journal=Comput%20Graph%20Forum&amp;volume=23&amp;issue=1&amp;pages=1-12&amp;publication_year=2004&amp;author=Vinayagamoorthy%2CV&amp;author=Garau%2CM&amp;author=Steed%2CA&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vinayagamoorthy V, Gillies M, Steed A, Tanguy E, Pan X, Loscos C, Slater M (2006) Building expression into vir" /><p class="c-article-references__text" id="ref-CR34">Vinayagamoorthy V, Gillies M, Steed A, Tanguy E, Pan X, Loscos C, Slater M (2006) Building expression into virtual characters. In: Eurographics conference state of the art reports</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Witkin A, Popović Z (1995) Motion warping. In: ACM SIGGRAPH, pp 105–108" /><p class="c-article-references__text" id="ref-CR35">Witkin A, Popović Z (1995) Motion warping. In: ACM SIGGRAPH, pp 105–108</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0167-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We would like to thank the funders of this work: BT plc, the European Union FET project PRESENCIA (contract number 27731) and the Empathic Avatars project funded by the UK Engineering and Physical Sciences Research Council. We also would like to thank the members of the University College London Department of Computer Science Virtual Environments and Graphics Group.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computing, Goldsmiths College, University of London, London, UK</p><p class="c-article-author-affiliation__authors-list">Marco Gillies</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Computer Science, University College London, London, UK</p><p class="c-article-author-affiliation__authors-list">Xueni Pan</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">ICREA-Universitat de Barcelona, Barcelona, Spain</p><p class="c-article-author-affiliation__authors-list">Mel Slater</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Marco-Gillies"><span class="c-article-authors-search__title u-h3 js-search-name">Marco Gillies</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marco+Gillies&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marco+Gillies" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marco+Gillies%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Xueni-Pan"><span class="c-article-authors-search__title u-h3 js-search-name">Xueni Pan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Xueni+Pan&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xueni+Pan" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xueni+Pan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mel-Slater"><span class="c-article-authors-search__title u-h3 js-search-name">Mel Slater</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mel+Slater&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mel+Slater" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mel+Slater%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0167-5/email/correspondent/c1/new">Marco Gillies</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Piavca%3A%20a%20framework%20for%20heterogeneous%20interactions%20with%20virtual%20characters&amp;author=Marco%20Gillies%20et%20al&amp;contentID=10.1007%2Fs10055-010-0167-5&amp;publication=1359-4338&amp;publicationDate=2010-07-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Gillies, M., Pan, X. &amp; Slater, M. Piavca: a framework for heterogeneous interactions with virtual characters.
                    <i>Virtual Reality</i> <b>14, </b>221–228 (2010). https://doi.org/10.1007/s10055-010-0167-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0167-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-06-13">13 June 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-07-10">10 July 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-07-30">30 July 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-12">December 2010</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0167-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0167-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Posture Shift</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Skin Conductance Level</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual Character</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Facial Animation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Head Tracker</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0167-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=167;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

