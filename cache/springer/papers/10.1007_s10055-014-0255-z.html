<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A seamless solution for 3D real-time interaction: design and evaluatio"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A seamless solution for 3D real-time interaction: design and evaluation"/>

    <meta name="dc.source" content="Virtual Reality 2014 19:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-11-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are not enough for interacting in 3D VE; current motion capture systems are expensive and require wearing equipment. We developed a solution to allow more natural interactions with objects and VE for navigation and manipulation tasks. We conducted an experimental study involving 20 participants. The goal was to realize object manipulation (moving, orientation, scaling) and navigation tasks in VE. We compared our solution (Microsoft Kinect-based) with data gloves and magnetic sensors (3DGloves) regarding two criteria: performance and acceptability. Results demonstrate similar performance (precision, execution time) but a better overall acceptability for our solution. Preferences of participants are mostly in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. Our solution can be considered as a real alternative to conventional systems for object manipulation in virtual reality."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-11-05"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="20"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0255-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0255-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0255-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0255-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A seamless solution for 3D real-time interaction: design and evaluation"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2015/03"/>

    <meta name="citation_online_date" content="2014/11/05"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="20"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0255-z"/>

    <meta name="DOI" content="10.1007/s10055-014-0255-z"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0255-z"/>

    <meta name="description" content="This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (V"/>

    <meta name="dc.creator" content="Franck Hernoux"/>

    <meta name="dc.creator" content="Olivier Christmann"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Special section on touching the 3rd dimension: a survey of 3D object selection techniques for virtual environments; citation_author=F Argelaguet, C Andujar; citation_volume=37; citation_issue=3; citation_publication_date=2013; citation_pages=121-136; citation_doi=10.1016/j.cag.2012.12.003; citation_id=CR1"/>

    <meta name="citation_reference" content="Beaudouin-Lafon M (2004) Designing interaction, not interfaces. In: Working conference on advanced visual interfaces, Gallipoli, Italy, pp 15&#8211;22"/>

    <meta name="citation_reference" content="Berard F, Ip J, Benovoy M, El-Shimy D, Blum JR, Cooperstock JR (2009) Did minority report get it wrong? Superiority of the mouse over 3D input devices in a 3D placement task. In: 12th IFIP TC 13 international conference on human&#8211;computer interaction: part II, Uppsala, Sweden. Springer, Berlin, pp 400&#8211;414"/>

    <meta name="citation_reference" content="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Symposium on interactive 3D graphics. ACM, pp 35&#8211;38"/>

    <meta name="citation_reference" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Virtual reality annual international symposium, 1997, IEEE 1997, 1&#8211;5 Mar 1997, pp 45&#8211;52, 215. doi:
                    10.1109/vrais.1997.583043
                    
                  
                "/>

    <meta name="citation_reference" content="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Paper presented at the proceedings of the ACM symposium on virtual reality software and technology, London, United Kingdom"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces: theory and practice; citation_publication_date=2004; citation_id=CR7; citation_author=DA Bowman; citation_author=E Kruijff; citation_author=J LaViola; citation_author=I Poupyrev; citation_publisher=Addison Wesley Longman Publishing Co., Inc."/>

    <meta name="citation_reference" content="Callahan J, Hopkins D, Weiser M, Shneiderman B (1988) An empirical comparison of pie vs. linear menus. In: SIGCHI conference on human factors in computing systems, Washington, DC, United States. ACM, pp 95&#8211;100. doi:
                    10.1145/57167.57182
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=SIGGRAPH Comput Graph; citation_title=A study in interactive 3-D rotation using 2-D control devices; citation_author=M Chen, SJ Mountford, A Sellen; citation_volume=22; citation_issue=4; citation_publication_date=1988; citation_pages=121-129; citation_doi=10.1145/378456.378497; citation_id=CR9"/>

    <meta name="citation_reference" content="Coquillart S, Fuchs P, Grosjean J, Hachet M, Bechmann D, Stenberger L (2003) Les Techniques d&#8217;Interaction pour les Primitives Comportementales Virtuelles. In: Trait&#233; de la r&#233;alit&#233; virtuelle: volume 2, L&#8217;Interfa&#231;age, l&#8217;Immersion et l&#8217;Interaction, vol 2, 3 edn. Les Presses de l&#8217;&#201;cole des Mines de Paris, p 332"/>

    <meta name="citation_reference" content="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Vancouver, British Columbia, Canada"/>

    <meta name="citation_reference" content="Dewaele G, Devernay F, Horaud R (2004) Hand motion from 3D point trajectories and a smooth surface model. In: 8th European conference on computer vision, Prague, Czech Republic. Springer, pp 495&#8211;507"/>

    <meta name="citation_reference" content="citation_title=Chasing the colour glove: visual hand tracking; citation_publication_date=1994; citation_id=CR13; citation_author=B Dorner; citation_publisher=Simon Fraser University"/>

    <meta name="citation_reference" content="Elmezain M, Al-Hamadi A, Appenrodt J, Michaelis B (2008) A hidden Markov model-based continuous gesture recognition system for hand motion trajectory. In: 19th international conference on pattern recognition, Tampa, FL, USA. IEEE, pp 1&#8211;4"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Interact Des Manuf; citation_title=Design review of CAD assemblies using bimanual natural interface; citation_author=M Fiorentino, R Radkowski, C Stritzke, A Uva, G Monno; citation_volume=7; citation_issue=4; citation_publication_date=2013; citation_pages=249-260; citation_doi=10.1007/s12008-012-0179-3; citation_id=CR15"/>

    <meta name="citation_reference" content="Fuchs P, Mathieu H (2003) Les Interfaces Sp&#233;cifiques de la Localisation Corporelle - Introduction. In: Trait&#233; de la R&#233;alit&#233; Virtuelle: volume 2, L&#8217;interfa&#231;age, l&#8217;immersion et l&#8217;interaction, vol 2, 3 edn. Les Presses de l&#8217;&#201;cole des Mines de Paris, pp 93&#8211;94"/>

    <meta name="citation_reference" content="Fuchs P, Moreau G (2003) Trait&#233; de la R&#233;alit&#233; Virtuelle, vol 2, 2 edn. Les Presses de l&#8217;Ecole des Mines de Paris"/>

    <meta name="citation_reference" content="Geebelen G, Maesen S, Cuypers T, Bekaert P (2010) Real-time hand tracking with a colored glove. In: 3D Stereo media, Luik, Belgium"/>

    <meta name="citation_reference" content="citation_journal_title=Technol Health Care; citation_title=A non-contact mouse for surgeon&#8211;computer interaction; citation_author=C Gratzel, T Fong, S Grange, C Baur; citation_volume=12; citation_issue=3; citation_publication_date=2004; citation_pages=245-257; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=J Mot Behav; citation_title=Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model; citation_author=Y Guiard; citation_volume=9; citation_issue=4; citation_publication_date=1987; citation_pages=486-517; citation_doi=10.1080/00222895.1987.10735426; citation_id=CR20"/>

    <meta name="citation_reference" content="Haik E, Barker T, Sapsford J, Trainis S (2002) Investigation into effective navigation in desktop virtual interfaces. In: Paper presented at the seventh international conference on 3D Web technology, Tempe, Arizona, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=A survey of 3D interaction techniques; citation_author=C Hand; citation_volume=16; citation_issue=5; citation_publication_date=1997; citation_pages=269-281; citation_doi=10.1111/1467-8659.00194; citation_id=CR22"/>

    <meta name="citation_reference" content="Hassanpour R, Shahbahrami A, Wong S (2008) Adaptive Gaussian mixture model for skin color segmentation. Int J Comput Inf Sci Eng 2(2):1&#8211;6"/>

    <meta name="citation_reference" content="Hayward V, Astley OR (1996) Performance measures for haptic interfaces. In: 7th International symposium on robotics research. Springer, pp 195&#8211;207"/>

    <meta name="citation_reference" content="citation_journal_title=J Nucl Med; citation_title=An image processing method for feature extraction of space-occupying lesions; citation_author=K Homma, E-I Takenaka; citation_volume=26; citation_issue=1985; citation_publication_date=1985; citation_pages=1472-1477; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=A 3D vision-based ambient user interface; citation_author=D Hong, W Woo; citation_volume=20; citation_issue=3; citation_publication_date=2006; citation_pages=271-284; citation_doi=10.1207/s15327590ijhc2003_6; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Multimed Tools Appl; citation_title=Gesture-based interaction via finger tracking for mobile augmented reality; citation_author=W H&#252;rst, C Wezel; citation_volume=62; citation_issue=1; citation_publication_date=2013; citation_pages=233-258; citation_doi=10.1007/s11042-011-0983-y; citation_id=CR27"/>

    <meta name="citation_reference" content="Jaehong L, Heon G, Hyungchan K, Jungmin K, Hyoungrae K, Hakil K (2013) Interactive manipulation of 3D objects using Kinect for visualization tools in education. In: Control, automation and systems (ICCAS), 2013 13th international conference on, 20&#8211;23 Oct 2013, pp 1220&#8211;1222. doi:
                    10.1109/iccas.2013.6704175
                    
                  
                "/>

    <meta name="citation_reference" content="Khan A, Mordatch I, Fitzmaurice G, Matejka J, Kurtenbach G (2008) ViewCube: a 3D orientation indicator and controller. In: Paper presented at the proceedings of the 2008 symposium on interactive 3D graphics and games, Redwood City, California"/>

    <meta name="citation_reference" content="Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical robots: technical and surgical skill comparison to current robotic masters. Surg Endosc 1&#8211;8. doi:
                    10.1007/s00464-013-3383-8
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=A design study of direct-touch interaction for exploratory 3D scientific visualization; citation_author=T Klein, F Gu&#233;niat, L Pastur, F Vernier, T Isenberg; citation_volume=31; citation_issue=3pt3; citation_publication_date=2012; citation_pages=1225-1234; citation_doi=10.1111/j.1467-8659.2012.03115.x; citation_id=CR31"/>

    <meta name="citation_reference" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Pauly M, Greiner G (eds) 30th annual conference of the European association for computer graphics, Munich, Germany, pp 119&#8211;134"/>

    <meta name="citation_reference" content="citation_title=3D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology; citation_publication_date=2000; citation_id=CR33; citation_author=R Lange; citation_publisher=University of Siegen"/>

    <meta name="citation_reference" content="citation_journal_title=Sens Rev; citation_title=Seeing distance&#8212;a fast time-of-flight 3D camera; citation_author=R Lange, P Seitz; citation_volume=20; citation_issue=3; citation_publication_date=2000; citation_pages=212-217; citation_doi=10.1108/02602280010372359; citation_id=CR34"/>

    <meta name="citation_reference" content="Laurel BK (1986) Interface as mimesis. User centered system design: new perspectives on human&#8211;computer interaction. Lawrence Erlbaum Associates, Hillsdale"/>

    <meta name="citation_reference" content="LeapMotion (2012). 
                    https://www.leapmotion.com
                    
                  
                "/>

    <meta name="citation_reference" content="Lempereur M (2008) Simulation du Mouvement d&#8217;Entr&#233;e dans un V&#233;hicule Automobile. Universit&#233; de Valenciennes et du Hainaut-Cambr&#233;sis"/>

    <meta name="citation_reference" content="Levesque JC, Laurendeau D, Mokhtari M (2011) Bimanual gestural interface for virtual environments. In: Virtual reality conference (VR), 2011 IEEE, 19&#8211;23 March 2011, pp 223&#8211;224. doi:
                    10.1109/vr.2011.5759479
                    
                  
                "/>

    <meta name="citation_reference" content="Lin J, Sun Q, Li G, He Y (2013) SnapBlocks: a snapping interface for assembling toy blocks with XBOX Kinect. Multimed Tools Appl 1&#8211;24. doi:
                    10.1007/s11042-013-1690-7
                    
                  
                "/>

    <meta name="citation_reference" content="Livingston MA, Sebastian J, Ai Z, Decker JW (2012) Performance measurements for the Microsoft Kinect skeleton. In: IEEE virtual reality, 2012. IEEE Computer Society, pp 119&#8211;120. doi:
                    10.1109/vr.2012.6180911
                    
                  
                "/>

    <meta name="citation_reference" content="Loup-Escande E, Burkhardt J-M, Richir S (2011) Anticiper et Evaluer l&#8217;Utilit&#233; dans la Conception Ergonomique des Technologies Emergentes: Une Revue. Le Travail Humain&#160;76:27&#8211;55"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=The &#8220;Conducting Master&#8221;: an interactive, real-time gesture monitoring system based on spatiotemporal motion templates; citation_author=P-J Maes, D Amelynck, M Lesaffre, M Leman, DK Arvind; citation_publication_date=2012; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=Vision for performance in virtual environments: the role of feedback timing; citation_author=AH Mason, BJ Bernardin; citation_volume=25; citation_issue=8; citation_publication_date=2009; citation_pages=785-805; citation_doi=10.1080/10447310903025529; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Rob Syst; citation_title=3D cameras: 3D computer vision of wide scope; citation_author=S May, K Pervoelz, H Surmann; citation_volume=4; citation_publication_date=2007; citation_pages=181-202; citation_id=CR44"/>

    <meta name="citation_reference" content="McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Paper presented at the proceedings of the 2009 symposium on interactive 3D graphics and games, Boston, Massachusetts"/>

    <meta name="citation_reference" content="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill CS Dept"/>

    <meta name="citation_reference" content="Mistry P, Maes P, Chang L (2009) WUW&#8212;Wear Ur World: a wearable gestural interface. In: 27th international conference extended abstracts on human factors in computing systems, Boston, MA, USA. ACM, pp 4111&#8211;4116. doi:
                    10.1145/1520340.1520626
                    
                  
                "/>

    <meta name="citation_reference" content="Moerman C, Marchal D, Grisoni L (2012) Drag&#8217;n go: simple and fast navigation in virtual environment. In: 2012 IEEE Symposium on 3D user interfaces (3DUI), 4&#8211;5 March 2012, pp 15&#8211;18. doi:
                    10.1109/3dui.2012.6184178
                    
                  
                "/>

    <meta name="citation_reference" content="Mohr D, Zachmann G (2009) Continuous edge gradient-based template matching for articulated object. In: International conference on computer vision theory and applications, Lisbon, Portugal, pp 519&#8211;524"/>

    <meta name="citation_reference" content="Mohr D, Zachmann G (2010a) FAST: Fast adaptive silhouette area based template matching. In: Labrosse F, Zwiggelaar R, Liu Y, Tiddeman B (eds) British machine vision conference. BMVA Press, pp 39.31&#8211;39.12"/>

    <meta name="citation_reference" content="Mohr D, Zachmann G (2010b) Silhouette area based similarity measure for template matching in constant time. In: Proceedings of the 6th international conference on articulated motion and deformable objects, Mallorca, Spain. Springer"/>

    <meta name="citation_reference" content="Movea (2009) MotionPod&#8482; Technology. 
                    http://movea.com/healthcare/motion_pod/index.html
                    
                  . Accessed 26 May 2009"/>

    <meta name="citation_reference" content="Nan X, Zhang Z, Zhang N, Guo F, He Y, Guan L (2013) vDesign: toward Image Segmentation and composition in CAVE using finger interaction. In: International conference on signal and information processing (ChinaSIP), Beijing, China, 6&#8211;10 July 2013. IEEE, pp 461&#8211;465"/>

    <meta name="citation_reference" content="Nedel LP, Dal Sasso Freitas CM, Jacob LJ, Pimenta MS (2003) Testing the use of egocentric interactive techniques in immersive virtual environments. In: Paper presented at the 9&#160;h IFIP TC13 international conference on human&#8211;computer interaction, Zurich, Switzerland"/>

    <meta name="citation_reference" content="citation_title=The psychology of everyday things; citation_publication_date=1988; citation_id=CR55; citation_author=DA Norman; citation_publisher=Basic Books"/>

    <meta name="citation_reference" content="Ouhaddi H, Horain P (1998) Conception et Ajustement d&#8217;un Mod&#232;le 3D Articul&#233; de la Main. In: 6&#232;mes Journ&#233;es de Travail du GT R&#233;alit&#233; Virtuelle, Issy-les-Moulineaux, France, 13/03/1998 1998, pp 83&#8211;90"/>

    <meta name="citation_reference" content="Pamplona VF, Fernandes LAF, Prauchner J, Nedel LP, Oliveira MM (2008) The image-based data glove. In: 10th symposium on virtual and augmented reality, Jo&#227;o Pessoa, Brazil, pp 204&#8211;211"/>

    <meta name="citation_reference" content="Pedersoli F, Benini S, Adami N, Leonardi R (2014) XKin: an open source framework for hand pose and gesture recognition using Kinect. Vis Comput 1&#8211;16. doi:
                    10.1007/s00371-014-0921-x
                    
                  
                "/>

    <meta name="citation_reference" content="Poor GM, Tomlinson BJ, Guinness D, Jaffee SD, Leventhal LM, Zimmerman G, Klopfer DS (2013) Tangible or gestural: comparing tangible vs. Kinect&#8482; interactions with an object manipulation task. In: International conference on tangible, embedded and embodied interaction, Barcelona, Spain"/>

    <meta name="citation_reference" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Paper presented at the 9th annual ACM symposium on user interface software and technology, Seattle, Washington, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques; citation_author=I Poupyrev, T Ichikawa, S Weghorst, M Billinghurst; citation_volume=17; citation_issue=3; citation_publication_date=1998; citation_pages=41-52; citation_doi=10.1111/1467-8659.00252; citation_id=CR61"/>

    <meta name="citation_reference" content="Prisacariu VA, Reid I (2011) Robust 3D hand tracking for human computer interaction. In: IEEE international conference on automatic face &amp; gesture recognition and workshops, pp 368&#8211;375"/>

    <meta name="citation_reference" content="Raheja JL, Chaudhary A, Singal K (2011) Tracking of fingertips and centers of palm using KINECT. In: Computational intelligence, modelling and simulation (CIMSiM), 2011 third international conference on, 20&#8211;22 Sept 2011, pp 248&#8211;252. doi:
                    10.1109/CIMSim.2011.51
                    
                  
                "/>

    <meta name="citation_reference" content="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Paper presented at the proceedings of the 1992 symposium on interactive 3D graphics, Cambridge, Massachusetts, USA"/>

    <meta name="citation_reference" content="Rodr&#237;guez N, Wikstr&#246;m R, Lilius J, Cu&#233;llar M, Delgado Calvo Flores M (2013) Understanding movement and interaction: an ontology for Kinect-based 3D depth sensors. In: Urzaiz G, Ochoa S, Bravo J, Chen L, Oliveira J (eds) Ubiquitous computing and ambient intelligence. Context-awareness and context-driven interaction, vol 8276. Lecture Notes in Computer Science. Springer International Publishing, pp 254&#8211;261. doi:
                    10.1007/978-3-319-03176-7_33
                    
                  
                "/>

    <meta name="citation_reference" content="Schlattmann M, Klein R (2009) Efficient bimanual symmetric 3D manipulation for markerless hand-tracking. In: Paper presented at the virtual reality international conference, Laval, France"/>

    <meta name="citation_reference" content="Schlattmann M, Na Nakorn T, Klein R (2009) 3D interaction techniques for 6 DOF markerless hand-tracking. In: International conference on computer graphics, visualization and computer vision, Plzen-Bory, Czech Republic"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=Vision-based hand interaction in augmented reality environment; citation_author=Y Shen, SK Ong, AYC Nee; citation_volume=27; citation_issue=6; citation_publication_date=2011; citation_pages=523-544; citation_doi=10.1080/10447318.2011.555297; citation_id=CR68"/>

    <meta name="citation_reference" content="Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper presented at the proceedings of the 12th ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry, Hong Kong, Hong Kong"/>

    <meta name="citation_reference" content="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA"/>

    <meta name="citation_reference" content="Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulation in CAD application. Comput Aided Des 46:239&#8211;245"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=A fast area-based stereo matching algorithm; citation_author=LD Stefano, M Marchionni, S Mattoccia; citation_volume=22; citation_issue=12; citation_publication_date=2004; citation_pages=983-1005; citation_doi=10.1016/j.imavis.2004.03.009; citation_id=CR72"/>

    <meta name="citation_reference" content="Stenger B (2006) Template-based hand pose recognition using multiple cues. In: Computer vision, Hyderabad, India, 2006. Lecture Notes in Computer Science. Springer, pp 551&#8211;560. doi:
                    10.1007/11612704_55
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Model-based hand tracking using a hierarchical bayesian filter; citation_author=B Stenger, A Thayananthan, PHS Torr, R Cipolla; citation_volume=28; citation_issue=9; citation_publication_date=2006; citation_pages=1372-1384; citation_doi=10.1109/TPAMI.2006.189; citation_id=CR74"/>

    <meta name="citation_reference" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Denver, Colorado, USA"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Pitching a baseball: tracking high-speed motion with multi-exposure images; citation_author=C Theobalt, I Albrecht, J Haber, M Magnor, H-P Seidel; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=540-547; citation_doi=10.1145/1015706.1015758; citation_id=CR76"/>

    <meta name="citation_reference" content="citation_title=3D hand tracking in video sequences; citation_publication_date=2005; citation_id=CR77; citation_author=A Tokatli; citation_publisher=Middle East Technical University"/>

    <meta name="citation_reference" content="citation_title=Visual articulated hand tracking for interactive surfaces; citation_publication_date=2006; citation_id=CR78; citation_author=M Tosas; citation_publisher=University of Nottingham"/>

    <meta name="citation_reference" content="Tosas M, Bai L (2007) Virtual touch screen: a vision-based interactive surface. In: 9th IASTED international conference on computer graphics and imaging, Innsbruck, Austria. ACTA Press, pp 81&#8211;86"/>

    <meta name="citation_reference" content="Tricot A, Pl&#233;gat-Soutjis F, Camps J-F, Amiel A, Lutz G, Morcillo A (2003) Utilit&#233;, utilisabilit&#233;, acceptabilit&#233; : interpr&#233;ter les relations entre trois dimensions de l&#8217;&#233;valuation des EIAH. In: Conf&#233;rence EIAH 2003, Strabourg, France"/>

    <meta name="citation_reference" content="citation_title=Affections Neurologiques; citation_inbook_title=Trait&#233; de Chirurgie de la Main; citation_publication_date=1991; citation_pages=56-58; citation_id=CR81; citation_author=R Tubiana; citation_author=A-I Kapandji; citation_publisher=Masson"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Industr Electron; citation_title=Hand pose estimation for vision-based human interface; citation_author=E Ueda; citation_volume=50; citation_issue=4; citation_publication_date=2003; citation_pages=676-684; citation_doi=10.1109/TIE.2003.814758; citation_id=CR82"/>

    <meta name="citation_reference" content="Ughini CS, Blanco FR, Pinto FM, Freitas CM, Nedel LP (2006) EyeScope: a 3D interaction technique for accurate object selection in immersive environments. In: SBC symposium on virtual reality 2006, pp 77&#8211;88"/>

    <meta name="citation_reference" content="Unseok L, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Asia Pacific conference on computer human interaction"/>

    <meta name="citation_reference" content="citation_title=Practical color-based motion capture; citation_publication_date=2011; citation_id=CR85; citation_author=RY Wang; citation_publisher=Massachusetts Institute of Technology"/>

    <meta name="citation_reference" content="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA"/>

    <meta name="citation_reference" content="citation_journal_title=SIGGRAPH Comput Graph; citation_title=Exploration and virtual camera control in virtual three dimensional environments; citation_author=C Ware, S Osborne; citation_volume=24; citation_issue=2; citation_publication_date=1990; citation_pages=175-183; citation_doi=10.1145/91394.91442; citation_id=CR87"/>

    <meta name="citation_reference" content="citation_title=Statistical principles in experimental design; citation_publication_date=1971; citation_id=CR88; citation_author=BJ Winer; citation_publisher=McGraw-Hill"/>

    <meta name="citation_reference" content="Winkler S, Yu H, Zhou Z (2007) Tangible reality desktop for digital media management. In: Engineering reality of virtual reality, San Jose. SPIE"/>

    <meta name="citation_reference" content="Yeo H-S, Lee B-G, Lim H (2013) Hand tracking and gesture recognition system for human&#8211;computer interaction using low-cost hardware. Multimed Tools Appl 1&#8211;29. doi:
                    10.1007/s11042-013-1501-1
                    
                  
                "/>

    <meta name="citation_reference" content="Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface. In: Paper presented at the 22nd WSCG international conference on computer graphics, visualization and computer vision, Plzen, Czech Republic, June 2&#8211;5, 2014"/>

    <meta name="citation_reference" content="citation_journal_title=Biomed Signal Process Control; citation_title=Human motion tracking for rehabilitation&#8212;a survey; citation_author=H Zhou, H Hu; citation_volume=3; citation_issue=1; citation_publication_date=2008; citation_pages=1-18; citation_doi=10.1016/j.bspc.2007.09.001; citation_id=CR92"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Multimedia; citation_title=Robust part-based hand gesture recognition using Kinect sensor; citation_author=R Zhou, Y Junsong, M Jingjing, Z Zhengyou; citation_volume=15; citation_issue=5; citation_publication_date=2013; citation_pages=1110-1120; citation_doi=10.1109/TMM.2013.2246148; citation_id=CR93"/>

    <meta name="citation_author" content="Franck Hernoux"/>

    <meta name="citation_author_email" content="hernouxfranck@yahoo.fr"/>

    <meta name="citation_author_institution" content="LAMPA - Arts &amp; M&#233;tiers ParisTech, Angers, France"/>

    <meta name="citation_author" content="Olivier Christmann"/>

    <meta name="citation_author_email" content="olivier.christmann@ensam.eu"/>

    <meta name="citation_author_institution" content="LAMPA - Arts &amp; M&#233;tiers ParisTech, Angers, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0255-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0255-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A seamless solution for 3D real-time interaction: design and evaluation"/>
        <meta property="og:description" content="This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are not enough for interacting in 3D VE; current motion capture systems are expensive and require wearing equipment. We developed a solution to allow more natural interactions with objects and VE for navigation and manipulation tasks. We conducted an experimental study involving 20 participants. The goal was to realize object manipulation (moving, orientation, scaling) and navigation tasks in VE. We compared our solution (Microsoft Kinect-based) with data gloves and magnetic sensors (3DGloves) regarding two criteria: performance and acceptability. Results demonstrate similar performance (precision, execution time) but a better overall acceptability for our solution. Preferences of participants are mostly in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. Our solution can be considered as a real alternative to conventional systems for object manipulation in virtual reality."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A seamless solution for 3D real-time interaction: design and evaluation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0255-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Seamless solution, Hand tracking, Real time, 3D interaction, 3D camera","kwrd":["Virtual_reality","Seamless_solution","Hand_tracking","Real_time","3D_interaction","3D_camera"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0255-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0255-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=255;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0255-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A seamless solution for 3D real-time interaction: design and evaluation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0255-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0255-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-11-05" itemprop="datePublished">05 November 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A seamless solution for 3D real-time interaction: design and evaluation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Franck-Hernoux" data-author-popup="auth-Franck-Hernoux" data-corresp-id="c1">Franck Hernoux<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="LAMPA - Arts &amp; Métiers ParisTech" /><meta itemprop="address" content="LAMPA - Arts &amp; Métiers ParisTech, 2, Bd du Ronceray, 49000, Angers, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Olivier-Christmann" data-author-popup="auth-Olivier-Christmann">Olivier Christmann</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="LAMPA - Arts &amp; Métiers ParisTech" /><meta itemprop="address" content="LAMPA - Arts &amp; Métiers ParisTech, 2, Bd du Ronceray, 49000, Angers, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">1</span>–<span itemprop="pageEnd">20</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1040 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">7 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0255-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are not enough for interacting in 3D VE; current motion capture systems are expensive and require wearing equipment. We developed a solution to allow more natural interactions with objects and VE for navigation and manipulation tasks. We conducted an experimental study involving 20 participants. The goal was to realize object manipulation (moving, orientation, scaling) and navigation tasks in VE. We compared our solution (Microsoft Kinect-based) with data gloves and magnetic sensors (3DGloves) regarding two criteria: performance and acceptability. Results demonstrate similar performance (precision, execution time) but a better overall acceptability for our solution. Preferences of participants are mostly in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. Our solution can be considered as a real alternative to conventional systems for object manipulation in virtual reality.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <div class="c-article-section__content"><p>The desktop metaphor, which appeared on Apple computers in 1984, was the real beginning of the Window Icon Menu Pointer paradigm (Beaudouin-Lafon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Beaudouin-Lafon M (2004) Designing interaction, not interfaces. In: Working conference on advanced visual interfaces, Gallipoli, Italy, pp 15–22" href="/article/10.1007/s10055-014-0255-z#ref-CR2" id="ref-link-section-d79199e293">2004</a>), which goes together with the use of the mouse. Since then, despite the fact that much effort has been focused on improving graphical user interfaces, interactions are still based on the keyboard–mouse duo. Nowadays, applications, either games or professional (e.g., modeling …), allow the user to navigate or to “manipulate” content in a tridimensional environment. In this case, interactions are done with six degrees of freedom (DoF) (three for position and three for orientation). Usually, these DoFs are factored into 2D subspaces that are mapped on the axis of a mouse (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e296">2011</a>), where metaphors help to facilitate understanding and interaction (e.g., manipulating an imaginary sphere for rotating an object (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Chen M, Mountford SJ, Sellen A (1988) A study in interactive 3-D rotation using 2-D control devices. SIGGRAPH Comput Graph 22(4):121–129. doi:&#xA;                    10.1145/54852.378497&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR9" id="ref-link-section-d79199e299">1988</a>). 2D devices are widespread but their performance or usability is debatable (Berard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Berard F, Ip J, Benovoy M, El-Shimy D, Blum JR, Cooperstock JR (2009) Did minority report get it wrong? Superiority of the mouse over 3D input devices in a 3D placement task. In: 12th IFIP TC 13 international conference on human–computer interaction: part II, Uppsala, Sweden. Springer, Berlin, pp 400–414" href="/article/10.1007/s10055-014-0255-z#ref-CR3" id="ref-link-section-d79199e302">2009</a>). In addition, current devices are sometimes unsuitable, for example in medicine where seamless devices are sometimes necessary (Fuchs and Mathieu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Fuchs P, Mathieu H (2003) Les Interfaces Spécifiques de la Localisation Corporelle - Introduction. In: Traité de la Réalité Virtuelle: volume 2, L’interfaçage, l’immersion et l’interaction, vol 2, 3 edn. Les Presses de l’École des Mines de Paris, pp 93–94" href="/article/10.1007/s10055-014-0255-z#ref-CR16" id="ref-link-section-d79199e305">2003</a>). In fact, no interactive device for manipulating 3D objects has been widely adopted by the general public. If advances regarding tangible user interactions seem promising (Poor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Poor GM, Tomlinson BJ, Guinness D, Jaffee SD, Leventhal LM, Zimmerman G, Klopfer DS (2013) Tangible or gestural: comparing tangible vs. Kinect™ interactions with an object manipulation task. In: International conference on tangible, embedded and embodied interaction, Barcelona, Spain" href="/article/10.1007/s10055-014-0255-z#ref-CR59" id="ref-link-section-d79199e309">2013</a>), they impose to look away from the screen to watch the manipulated object.</p></div><div class="c-article-section__content"><p>Currently, most devices follow Norman’s model (Norman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Norman DA (1988) The psychology of everyday things. Basic Books, New York" href="/article/10.1007/s10055-014-0255-z#ref-CR55" id="ref-link-section-d79199e315">1988</a>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig1">1</a>) where a user’s stimuli are sent through any device to the computer that returns visual data to the user (black arrows). This model was enriched (gray dotted arrows) by (Nedel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nedel LP, Dal Sasso Freitas CM, Jacob LJ, Pimenta MS (2003) Testing the use of egocentric interactive techniques in immersive virtual environments. In: Paper presented at the 9 h IFIP TC13 international conference on human–computer interaction, Zurich, Switzerland" href="/article/10.1007/s10055-014-0255-z#ref-CR54" id="ref-link-section-d79199e321">2003</a>) with a feedback loop from the computer to the user through the device. A behavioral interface (Fuchs and Moreau <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Fuchs P, Moreau G (2003) Traité de la Réalité Virtuelle, vol 2, 2 edn. Les Presses de l’Ecole des Mines de Paris" href="/article/10.1007/s10055-014-0255-z#ref-CR17" id="ref-link-section-d79199e324">2003</a>), which makes the interface “disappear” to allow a “natural” interaction for users, is often presented as the final outcome of direct manipulation interfaces; it could be the next step to improve Norman’s model. The feeling of immersion and presence can be enhanced if the user has no equipment to wear; the system becomes seamless to the user (Winkler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Winkler S, Yu H, Zhou Z (2007) Tangible reality desktop for digital media management. In: Engineering reality of virtual reality, San Jose. SPIE" href="/article/10.1007/s10055-014-0255-z#ref-CR89" id="ref-link-section-d79199e327">2007</a>). Allowing the user to interact directly with his or her hands in a virtual environment would be the best way to make the system seamless when manipulating content.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Model of human–computer interaction (<i>left</i>), from (Norman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Norman DA (1988) The psychology of everyday things. Basic Books, New York" href="/article/10.1007/s10055-014-0255-z#ref-CR55" id="ref-link-section-d79199e344">1988</a>). Our view of the Norman’s model for human–computer interaction (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            </div><div class="c-article-section__content"><p>The hand provides 70 % of our motor skills (Lempereur <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lempereur M (2008) Simulation du Mouvement d’Entrée dans un Véhicule Automobile. Université de Valenciennes et du Hainaut-Cambrésis" href="/article/10.1007/s10055-014-0255-z#ref-CR37" id="ref-link-section-d79199e362">2008</a>) and offers many different types of opposition-based grasps (Tubiana and Kapandji <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Tubiana R, Kapandji A-I (1991) Affections Neurologiques. Traité de Chirurgie de la Main, vol 4. Masson, Paris, pp 56–58" href="/article/10.1007/s10055-014-0255-z#ref-CR81" id="ref-link-section-d79199e365">1991</a>), which allow us to manipulate and interact with our environment. It seems simple to use this natural interaction for implementing 3D interfaces but in fact the challenge is to transpose the manual interaction (or bimanual) from the real world to the virtual one, with complete transparency to the user. A great deal of research work has been done using stereoscopic systems [e.g., in computer-aided design (CAD) area (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e368">2011</a>)], but the emergence of 3D cameras (especially Microsoft’s Kinect) sheds new light on this question. As stated by Pedersoli et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Pedersoli F, Benini S, Adami N, Leonardi R (2014) XKin: an open source framework for hand pose and gesture recognition using Kinect. Vis Comput 1–16. doi:&#xA;                    10.1007/s00371-014-0921-x&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR58" id="ref-link-section-d79199e371">2014</a>) “Kinect approach is non-intrusive; sensing is passive, silent, and permits to overcome the limitation of robustness, speed, and accuracy of typical image processing algorithms by combining color images and depth information.” If Microsoft Kinect found a great echo in the scientific community to propose new ways of interaction, most of the articles focus on the “technical” side [e.g., (Raheja et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Raheja JL, Chaudhary A, Singal K (2011) Tracking of fingertips and centers of palm using KINECT. In: Computational intelligence, modelling and simulation (CIMSiM), 2011 third international conference on, 20–22 Sept 2011, pp 248–252. doi:&#xA;                    10.1109/CIMSim.2011.51&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR63" id="ref-link-section-d79199e374">2011</a>; Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zhou R, Junsong Y, Jingjing M, Zhengyou Z (2013) Robust part-based hand gesture recognition using Kinect sensor. IEEE Trans Multimedia 15(5):1110–1120. doi:&#xA;                    10.1109/tmm.2013.2246148&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR93" id="ref-link-section-d79199e378">2013</a>)] or the interaction metaphors [e.g., (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e381">2012</a>)]. To the best of our knowledge, few scientific studies have focused on the real contribution of this emerging technology through an extensive study (with qualitative and quantitative assessment) comparing a complete system to a common hardware.</p></div><div class="c-article-section__content"><p>The objective of this paper was to report the design and evaluation of a solution to capture hand movements without sensors, making real-time 3D seamless interaction possible for navigation and manipulation tasks. We mean a solution as a combination of a technology, computer vision algorithms and appropriate interaction modalities. Indeed, the work on modalities of interaction is crucial because users are manipulating a virtual object and laws of physics do not apply in this case (Poor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Poor GM, Tomlinson BJ, Guinness D, Jaffee SD, Leventhal LM, Zimmerman G, Klopfer DS (2013) Tangible or gestural: comparing tangible vs. Kinect™ interactions with an object manipulation task. In: International conference on tangible, embedded and embodied interaction, Barcelona, Spain" href="/article/10.1007/s10055-014-0255-z#ref-CR59" id="ref-link-section-d79199e387">2013</a>). We need to provide users simple metaphors to allow them to understand the interaction means and predict the result of their gestures. Contrary to other approaches that aim to associate classic interfaces with seamless systems (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e390">2011</a>), and according to recent research works [e.g., (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulation in CAD application. Comput Aided Des 46:239–245" href="/article/10.1007/s10055-014-0255-z#ref-CR71" id="ref-link-section-d79199e393">2014</a>; Raheja et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Raheja JL, Chaudhary A, Singal K (2011) Tracking of fingertips and centers of palm using KINECT. In: Computational intelligence, modelling and simulation (CIMSiM), 2011 third international conference on, 20–22 Sept 2011, pp 248–252. doi:&#xA;                    10.1109/CIMSim.2011.51&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR63" id="ref-link-section-d79199e396">2011</a>; Pedersoli et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Pedersoli F, Benini S, Adami N, Leonardi R (2014) XKin: an open source framework for hand pose and gesture recognition using Kinect. Vis Comput 1–16. doi:&#xA;                    10.1007/s00371-014-0921-x&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR58" id="ref-link-section-d79199e399">2014</a>; Rodríguez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Rodríguez N, Wikström R, Lilius J, Cuéllar M, Delgado Calvo Flores M (2013) Understanding movement and interaction: an ontology for Kinect-based 3D depth sensors. In: Urzaiz G, Ochoa S, Bravo J, Chen L, Oliveira J (eds) Ubiquitous computing and ambient intelligence. Context-awareness and context-driven interaction, vol 8276. Lecture Notes in Computer Science. Springer International Publishing, pp 254–261. doi:&#xA;                    10.1007/978-3-319-03176-7_33&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR65" id="ref-link-section-d79199e403">2013</a>)] and more precisely those that focus on the capture of the state of the hands (open or close) [e.g., (Yeo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Yeo H-S, Lee B-G, Lim H (2013) Hand tracking and gesture recognition system for human–computer interaction using low-cost hardware. Multimed Tools Appl 1–29. doi:&#xA;                    10.1007/s11042-013-1501-1&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR90" id="ref-link-section-d79199e406">2013</a>; Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e409">2012</a>; Jaehong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Jaehong L, Heon G, Hyungchan K, Jungmin K, Hyoungrae K, Hakil K (2013) Interactive manipulation of 3D objects using Kinect for visualization tools in education. In: Control, automation and systems (ICCAS), 2013 13th international conference on, 20–23 Oct 2013, pp 1220–1222. doi:&#xA;                    10.1109/iccas.2013.6704175&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR28" id="ref-link-section-d79199e412">2013</a>; Unseok and Tanaka <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Unseok L, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Asia Pacific conference on computer human interaction" href="/article/10.1007/s10055-014-0255-z#ref-CR84" id="ref-link-section-d79199e415">2012</a>)], our objective was to propose a fully natural interaction. Our originality lies in providing a complete system and to assess its interest with a detailed experimental study. The intended use concerns desktop applications and, more specifically, large-scale immersive environments where the user can stand and benefit from stereoscopic vision. Application domains are numerous and include health care, including rehabilitation (Movea <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Movea (2009) MotionPod™ Technology. &#xA;                    http://movea.com/healthcare/motion_pod/index.html&#xA;                    &#xA;                  . Accessed 26 May 2009" href="/article/10.1007/s10055-014-0255-z#ref-CR52" id="ref-link-section-d79199e418">2009</a>; Zhou and Hu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhou H, Hu H (2008) Human motion tracking for rehabilitation—a survey. Biomed Signal Process Control 3(1):1–18" href="/article/10.1007/s10055-014-0255-z#ref-CR92" id="ref-link-section-d79199e422">2008</a>).</p></div><div class="c-article-section__content"><p>After presenting an overview of the current means of interaction, we will detail the design of our solution and the proposed interactions modalities. We have focused on three tasks that are the most common when interacting in virtual environments (Coquillart et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Coquillart S, Fuchs P, Grosjean J, Hachet M, Bechmann D, Stenberger L (2003) Les Techniques d’Interaction pour les Primitives Comportementales Virtuelles. In: Traité de la réalité virtuelle: volume 2, L’Interfaçage, l’Immersion et l’Interaction, vol 2, 3 edn. Les Presses de l’École des Mines de Paris, p 332" href="/article/10.1007/s10055-014-0255-z#ref-CR10" id="ref-link-section-d79199e428">2003</a>): object selection, object manipulation, and navigation. We will then present our experiment conducted with 20 participants, comparing our solution and data gloves paired with magnetic sensors, relatively to performances, acceptability, and preferences of participants. We will end this paper with conclusions and perspectives.</p></div><section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Related work</h2><div class="c-article-section__content" id="Sec1-content"><p>The capture of hand movements can rely on “software” or “hardware” techniques. Software techniques imply that a physical interface captures a video or a 3D data stream, but the core work is based on image processing. “Hardware” techniques can be of various kinds: electromagnetic, mechanical, optical, ultrasonic, etc. Each solution, software or hardware, seamless or not, can be classified into three major families (Hayward and Astley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Hayward V, Astley OR (1996) Performance measures for haptic interfaces. In: 7th International symposium on robotics research. Springer, pp 195–207" href="/article/10.1007/s10055-014-0255-z#ref-CR24" id="ref-link-section-d79199e439">1996</a>): low (2–3 DoF), high (4–6 DoF), or very high (more than 6 DoF) degree-of-freedom device. Hardware systems are the most commonly used, because they are widespread and allow a high accuracy and a high reliability of data, whatever the technology used: mechanical exoskeletons, date gloves (pressure sensor, optical fiber, bending sensors), optical systems (based on passive or active markers), and magnetic systems. These systems have common drawbacks: they are expensive, require a calibration phase, and need sensors or equipment on the hand(s) of the user. For example, wearing a glove can be uncomfortable during long work sessions (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e442">2011</a>). For the design of our solution, such devices do not meet the constraint of transparency so we focused our study on software systems.</p><h3 class="c-article__sub-heading" id="Sec2">Software systems</h3><p>Much research work has been focused on the recognition and tracking by one camera (monoscopic systems) or more (stereoscopic or multiview systems) by means of image processing rather than the usage of captors. This area is called computer vision and processes information in both 2D and 3D. Computer vision usually requires a combination of low-level algorithms to improve the quality of the image and high-level ones to “understand” the picture (recognize patterns and interpret them). Each technique is presented in Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0255-z#Sec31">1</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec3">Techniques and algorithms used in monoscopic vision</h4><p>To ensure accurate tracking of hand movements in three dimensions, monoscopic vision is often associated with other techniques, which can be based on the use of markers or not.</p><p>
                    <i>Approaches with markers</i> Color markers or patterns (binary images) are placed on each of the user’s fingers (Pamplona et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pamplona VF, Fernandes LAF, Prauchner J, Nedel LP, Oliveira MM (2008) The image-based data glove. In: 10th symposium on virtual and augmented reality, João Pessoa, Brazil, pp 204–211" href="/article/10.1007/s10055-014-0255-z#ref-CR57" id="ref-link-section-d79199e467">2008</a>; Hürst and Wezel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Hürst W, Wezel C (2013) Gesture-based interaction via finger tracking for mobile augmented reality. Multimed Tools Appl 62(1):233–258. doi:&#xA;                    10.1007/s11042-011-0983-y&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR27" id="ref-link-section-d79199e470">2013</a>). Colored gloves (Dorner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Dorner B (1994) Chasing the colour glove: visual hand tracking. Simon Fraser University, Burnaby" href="/article/10.1007/s10055-014-0255-z#ref-CR13" id="ref-link-section-d79199e473">1994</a>; Geebelen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Geebelen G, Maesen S, Cuypers T, Bekaert P (2010) Real-time hand tracking with a colored glove. In: 3D Stereo media, Luik, Belgium" href="/article/10.1007/s10055-014-0255-z#ref-CR18" id="ref-link-section-d79199e476">2010</a>; Tokatli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Tokatli A (2005) 3D hand tracking in video sequences. Middle East Technical University, Çankaya" href="/article/10.1007/s10055-014-0255-z#ref-CR77" id="ref-link-section-d79199e480">2005</a>; Wang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang RY (2011) Practical color-based motion capture. Massachusetts Institute of Technology, Cambridge" href="/article/10.1007/s10055-014-0255-z#ref-CR85" id="ref-link-section-d79199e483">2011</a>) and colored rings solutions (Mistry et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Mistry P, Maes P, Chang L (2009) WUW—Wear Ur World: a wearable gestural interface. In: 27th international conference extended abstracts on human factors in computing systems, Boston, MA, USA. ACM, pp 4111–4116. doi:&#xA;                    10.1145/1520340.1520626&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR47" id="ref-link-section-d79199e486">2009</a>) are based on the color, which becomes the essential information required to follow the hand or finger movements.</p><p>
                    <i>Approaches without markers</i> Some systems are based on the recognition of skin color (Shen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Shen Y, Ong SK, Nee AYC (2011) Vision-based hand interaction in augmented reality environment. Int J Hum Comput Interact 27(6):523–544. doi:&#xA;                    10.1080/10447318.2011.555297&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR68" id="ref-link-section-d79199e495">2011</a>); they only obtain good results if the brightness is constant (Hassanpour et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Hassanpour R, Shahbahrami A, Wong S (2008) Adaptive Gaussian mixture model for skin color segmentation. Int J Comput Inf Sci Eng 2(2):1–6" href="/article/10.1007/s10055-014-0255-z#ref-CR23" id="ref-link-section-d79199e498">2008</a>). The “template-matching” technique is used in many studies to detect and track hand movements and is based either on the contours of the hand (Mohr and Zachmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Mohr D, Zachmann G (2009) Continuous edge gradient-based template matching for articulated object. In: International conference on computer vision theory and applications, Lisbon, Portugal, pp 519–524" href="/article/10.1007/s10055-014-0255-z#ref-CR49" id="ref-link-section-d79199e501">2009</a>; Stenger <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Stenger B (2006) Template-based hand pose recognition using multiple cues. In: Computer vision, Hyderabad, India, 2006. Lecture Notes in Computer Science. Springer, pp 551–560. doi:&#xA;                    10.1007/11612704_55&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR73" id="ref-link-section-d79199e504">2006</a>) or on silhouettes of the hand (Mohr and Zachmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010a" title="Mohr D, Zachmann G (2010a) FAST: Fast adaptive silhouette area based template matching. In: Labrosse F, Zwiggelaar R, Liu Y, Tiddeman B (eds) British machine vision conference. BMVA Press, pp 39.31–39.12" href="/article/10.1007/s10055-014-0255-z#ref-CR50" id="ref-link-section-d79199e508">2010a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Mohr D, Zachmann G (2010b) Silhouette area based similarity measure for template matching in constant time. In: Proceedings of the 6th international conference on articulated motion and deformable objects, Mallorca, Spain. Springer" href="/article/10.1007/s10055-014-0255-z#ref-CR51" id="ref-link-section-d79199e511">b</a>) or color (Stenger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Stenger B, Thayananthan A, Torr PHS, Cipolla R (2006) Model-based hand tracking using a hierarchical bayesian filter. IEEE Trans Pattern Anal Mach Intell 28(9):1372–1384. doi:&#xA;                    10.1109/tpami.2006.189&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR74" id="ref-link-section-d79199e514">2006</a>). Contours are more characteristic for articulated objects but they can be difficult to obtain because of external constraints such as lighting or camera settings. The recognition of silhouettes uses the silhouette (Prisacariu and Reid <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Prisacariu VA, Reid I (2011) Robust 3D hand tracking for human computer interaction. In: IEEE international conference on automatic face &amp; gesture recognition and workshops, pp 368–375" href="/article/10.1007/s10055-014-0255-z#ref-CR62" id="ref-link-section-d79199e517">2011</a>; Tosas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Tosas M (2006) Visual articulated hand tracking for interactive surfaces. University of Nottingham, Nottingham" href="/article/10.1007/s10055-014-0255-z#ref-CR78" id="ref-link-section-d79199e520">2006</a>; Tosas and Bai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Tosas M, Bai L (2007) Virtual touch screen: a vision-based interactive surface. In: 9th IASTED international conference on computer graphics and imaging, Innsbruck, Austria. ACTA Press, pp 81–86" href="/article/10.1007/s10055-014-0255-z#ref-CR79" id="ref-link-section-d79199e523">2007</a>) of the user’s hand to position and best match a 3D model. This method requires many templates for a single match, which has a significant impact on the computation time and thus on the real-time aspect.</p><p>Finally, systems based on 3D models re-adjust an articulated 3D model of the hand by adapting the most probable posture corresponding to the image seen by the camera (Ouhaddi and Horain <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ouhaddi H, Horain P (1998) Conception et Ajustement d’un Modèle 3D Articulé de la Main. In: 6èmes Journées de Travail du GT Réalité Virtuelle, Issy-les-Moulineaux, France, 13/03/1998 1998, pp 83–90" href="/article/10.1007/s10055-014-0255-z#ref-CR56" id="ref-link-section-d79199e529">1998</a>). This is one of the most widely used techniques to estimate the postures of the hand from a single video stream but it does not allow the user to know the position of the model in 3D.</p><p>Monoscopic vision techniques are not enough to obtain accurate 3D information and track the movements of the hand in three dimensions. These techniques have to be coupled with other technologies (e.g., sensors positions) or adapted to stereoscopic systems to allow 3D interaction in virtual environments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec4">Main algorithms used in stereoscopic vision</h4><p>Two or more cameras can provide depth information through stereoscopy. All systems seen above can therefore be reused in stereoscopic systems [e.g., systems based on skin color (Elmezain et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Elmezain M, Al-Hamadi A, Appenrodt J, Michaelis B (2008) A hidden Markov model-based continuous gesture recognition system for hand motion trajectory. In: 19th international conference on pattern recognition, Tampa, FL, USA. IEEE, pp 1–4" href="/article/10.1007/s10055-014-0255-z#ref-CR14" id="ref-link-section-d79199e543">2008</a>) or on colored gloves (Theobalt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Theobalt C, Albrecht I, Haber J, Magnor M, Seidel H-P (2004) Pitching a baseball: tracking high-speed motion with multi-exposure images. ACM Trans Graph 23(3):540–547. doi:&#xA;                    10.1145/1015706.1015758&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR76" id="ref-link-section-d79199e546">2004</a>)]. Systems based on 3D models are also suitable but the cloud of points coming from the depth map replace former 2D images (Dewaele et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Dewaele G, Devernay F, Horaud R (2004) Hand motion from 3D point trajectories and a smooth surface model. In: 8th European conference on computer vision, Prague, Czech Republic. Springer, pp 495–507" href="/article/10.1007/s10055-014-0255-z#ref-CR12" id="ref-link-section-d79199e549">2004</a>). There are also multi-view reconstructions (Hong and Woo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hong D, Woo W (2006) A 3D vision-based ambient user interface. Int J Hum Comput Interact 20(3):271–284. doi:&#xA;                    10.1207/s15327590ijhc2003_6&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR26" id="ref-link-section-d79199e552">2006</a>) that require multiple cameras around the hand and that are based on different methods like “shape from silhouettes” (Ueda <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ueda E (2003) Hand pose estimation for vision-based human interface. IEEE Trans Industr Electron 50(4):676–684" href="/article/10.1007/s10055-014-0255-z#ref-CR82" id="ref-link-section-d79199e555">2003</a>). Finally, (Schlattmann and Klein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Schlattmann M, Klein R (2009) Efficient bimanual symmetric 3D manipulation for markerless hand-tracking. In: Paper presented at the virtual reality international conference, Laval, France" href="/article/10.1007/s10055-014-0255-z#ref-CR66" id="ref-link-section-d79199e559">2009</a>) suggest a system, based on the usage of 3 cameras and a technique called “pose estimation” that allows the user to manipulate the information in 3D.</p><p>To allow interactions with the 3D objects, the system detects the posture of the hands thanks to a coarse model obtained from the three cameras. Some systems use only 2 cameras, for example those that are for CAD applications (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e565">2011</a>) but are limited to some gestures (i.e., pinching). This system is inexpensive and allows bimanual interaction, but the user must keep a particular pose of the hand for a moment to allow the detection of the action, which is unsuitable for real-time and direct interaction/manipulation.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Conclusion</h4><p>Algorithms used in monoscopic vision do not allow real-time 3D interaction. Stereoscopic solutions are more effective (Stefano et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Stefano LD, Marchionni M, Mattoccia S (2004) A fast area-based stereo matching algorithm. Image Vis Comput 22(12):983–1005" href="/article/10.1007/s10055-014-0255-z#ref-CR72" id="ref-link-section-d79199e576">2004</a>) but they have to be improved to allow real-time processing with sufficient resolution. In addition, methods used to obtain the third dimension require a long computation time that leaves few resources available for additional treatments (i.e., capturing and tracking the movements) and make the final system too slow to be considered real time. Finally, stereoscopy brings about other complex difficulties such as parallax and lens distortion.</p><h3 class="c-article__sub-heading" id="Sec6">Emerging technologies: 3D cameras</h3><p>No hardware solution is directly suitable for all possible virtual reality applications. The choice of a system over another requires a set of criteria and constraints related to the tasks to be performed. These constraints are numerous: real-time capture, nature of the movements to track, the absence of physical connection, size of the workspace, accuracy, resolution, environment, or price. Software solutions can override most of these constraints to propose seamless low-cost systems but algorithms need to be strictly optimized to ensure real-time interaction. An interesting solution would be to release the computer from depth computation by directly moving them onto the capture system.</p><p>Such systems already exist and are known as 3D cameras (Lange <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lange R (2000) 3D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology. University of Siegen, Siegen" href="/article/10.1007/s10055-014-0255-z#ref-CR33" id="ref-link-section-d79199e590">2000</a>; Lange and Seitz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lange R, Seitz P (2000) Seeing distance—a fast time-of-flight 3D camera. Sens Rev 20(3):212–217" href="/article/10.1007/s10055-014-0255-z#ref-CR34" id="ref-link-section-d79199e593">2000</a>). An evaluation of the use of these cameras in the field of computer graphics was done by (Kolb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Pauly M, Greiner G (eds) 30th annual conference of the European association for computer graphics, Munich, Germany, pp 119–134" href="/article/10.1007/s10055-014-0255-z#ref-CR32" id="ref-link-section-d79199e596">2009</a>). Major techniques are triangulation-based cameras (May et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="May S, Pervoelz K, Surmann H (2007) 3D cameras: 3D computer vision of wide scope. Int J Adv Rob Syst 4:181–202" href="/article/10.1007/s10055-014-0255-z#ref-CR44" id="ref-link-section-d79199e599">2007</a>) and “time-of-flight cameras” (TOF), also called RGB-D cameras (RGB-Depth) or Z-cam. Microsoft’s Kinect uses the first technology. The Kinect gives 3D depth and relies on computer vision algorithms to detect objects or persons. Thanks to the OpenNI library, it is possible to determine the positions of the user’s limbs, and relative to the scope of our study, this technology allows hand tracking without additional sensors, contrary to data gloves that require electromagnetic sensors, for example. Other advantages are its relative insensitivity to ambient light (the system is based on infrared light and able to work both day and night) and to magnetic disturbances. Based only on distances (with the depth map) and not on color, it is possible to isolate the person facing the screen and to focus on his hands, removing people and objects in the background. 3D cameras have the potential to overcome most of the limitations of other devices (e.g., colors, lighting conditions, metal sensitivity, and use of sensors). Moreover, the main advantage of a Kinect-like camera is the price/quality ratio and the OpenNI library, which is freely available (Microsoft’s Software Development Kit (SDK) was not released at the moment of our developments). On the other hand, it is important to mention that the Kinect also has some drawbacks. First, its framerate (30 fps) could appear low compared to cameras used in optical tracking systems (up to 1,000 fps), even if this is enough for real-time interactions. The Kinect sensitivity to infrared light might be a problem if used with an optical tracking device or when there are some reflective or transparent objects. When infrared dots (from the light pattern) hit a reflective object, light is deflected and the Kinect cannot provide any depth information for these points. Finally, if an object is close to the camera, a considerable shadow is present, due to the distance between the infrared projector and the depth camera.</p><h3 class="c-article__sub-heading" id="Sec7">Our view of Norman’s model</h3><p>The use of a 3D camera allows us to eliminate the feedback between the user and the device, and between the computer and the device, making the system completely seamless to the user. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig1">1</a> shows our view of Norman’s model presented above: the large gray loop corresponds to what the user perceives and the black loop corresponds to what is really happening. When the user moves his hands, his gesture is visually relayed to him by the computer: the visual feedback associated with the absence of wearable sensors makes the device totally seamless, even if the device is still present. Computations must be performed in real time and with the lowest latency between the real action and the visual feedback returned by the computer.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Method</h2><div class="c-article-section__content" id="Sec8-content"><p>We took an experimental approach based on a comparative study between our solution and a common and functionally equivalent system. We focused on three common tasks when interacting in virtual environments (Coquillart et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Coquillart S, Fuchs P, Grosjean J, Hachet M, Bechmann D, Stenberger L (2003) Les Techniques d’Interaction pour les Primitives Comportementales Virtuelles. In: Traité de la réalité virtuelle: volume 2, L’Interfaçage, l’Immersion et l’Interaction, vol 2, 3 edn. Les Presses de l’École des Mines de Paris, p 332" href="/article/10.1007/s10055-014-0255-z#ref-CR10" id="ref-link-section-d79199e622">2003</a>): object selection, object manipulation (orientation, position, rescale), and navigation. The system that we compare to ours is composed of data gloves and magnetic sensors. We chose them because they are commonly used in businesses as well as in research. They are generally cheaper than optical systems and therefore closer to a low-cost system, such as the one we propose.</p><h3 class="c-article__sub-heading" id="Sec9">Design of our solution</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Implementation</h4><p>For our solution, we chose Microsoft’s Kinect 3D camera, as it offers 30 fps and a resolution of 640 × 480 pixels, far superior to other cameras (e.g., Mesa SR4000, PMD). The user sits at a table facing a screen with a Kinect on it. This way, the Kinect can retrieve the position of the user’s hands without disturbing the field of view or the workspace (150 × 150 × 100 cm) of the user (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig2">2</a>, acquisition unit). All the objects to manipulate are virtually placed between the Kinect and the user. The user’s movements in the real world correspond to the same as in the virtual one; we work on a 1:1 scale. The distances are relative to the position of the Kinect (or the antenna for the second system).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>How the solution works and what it can do</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>To determine the 3D position of the user’s hands in the workspace and the status (whether they are opened or closed) in real time, we used the OpenNI SDK (Primesense), in order to get the 3D video stream (actually 2, 5D), to apply a skeleton to the user and to recover the positions of his hands. We were also able to differentiate the left hand from the right in order to take into account the lateralization of the user when handling objects. However, the SDK gives no information about the orientation or state of the hands. We thus developed our own algorithms in C++ to determine the status of the user’s hands, using the OpenCV library. At the time of development, the OpenNI SDK only allowed the user to know the exact position of the wrist in 3D, but it was not possible to get the exact positions of the center of the palms nor to know whether the hands were currently opened or closed. In order to interact accurately in 3D with virtual objects, it was then necessary to have more information and to develop new algorithms.</p><p>The complete processing is described as follows (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig3">3</a>). Once the depth image is acquired by the Kinect (1), it is possible to determine how many users are present in the scene (2). For each of these users, a skeleton is retrieved (3) and it is also possible to get the position of the wrist (4). Steps 1–4 are available directly through OpenNI functions (gray rectangles in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig3">3</a>); all the following steps compose the algorithm we developed to improve the accuracy of the tracking and to determine the state of the hands. When the skeleton of the current user is acquired, the distance and the size of the user are recorded to get a ratio (5) which will be used latter. The distance is obtained by simply retrieving the position (x, y, z) of the belly of the user. As the reference (0, 0, 0) is in the center of the Kinect, the distance of the person is obtained directly. To get the size of the person, we calculate the distance between the highest point of the head and the ground. Then, for each hands of the user, using the position of the wrist determined in step 4, we create a virtual sphere that encloses the hand (6). The radius of this sphere depends on the ratio calculated before. This fictitious sphere allows to only keep the pixels belonging to the hand and that must be taken into account (7) for the following steps, all the others pixels are not used. To know whether a pixel belongs to the sphere, it is necessary to compute for each pixel the Euclidian distance between this pixel and the wrist. If the distance is less than the radius of the sphere, the point belongs to it and it is stored in a structure. This allows to reduce the number of computations in the following steps and to improve the global speed of the algorithm. The following steps are done on 2D images representing the projection of the 3D point cloud onto the plane of the camera (8). A binarization is done in order to have a black and white image (9), white pixels represent the hand of the user and black pixels are considered as background. The binarization process was used in order to reduce the number of computations as in this case there only is one channel to compute instead of three (red, blue, green). A first filter—a simple erosion operation done through a convolution mask—is applied on the image in order to delete the noise around the hands (10). Then, morphological “closing” is applied on the image (11). These steps are necessary because sometimes some holes can appear in the hand, resulting from the binarization, the erosion, or from reflective object like jewelry or rings on the fingers. Once these steps are done, the image is improved and the hand is clean from noise or holes, the contours and the length of the hand are then retrieved (12). It is also possible to calculate the center of gravity of the shape by calculating the mean position of all the points of the hand, this mean position represents the center of the palm (13). The 3D coordinates of this point are then retrieved from the depth image. The position of the hand is thus accurately determined (14). To detect whether the hand is opened or closed, the method of (Homma and Takenaka <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Homma K, Takenaka E-I (1985) An image processing method for feature extraction of space-occupying lesions. J Nucl Med 26(1985):1472–1477" href="/article/10.1007/s10055-014-0255-z#ref-CR25" id="ref-link-section-d79199e671">1985</a>) was used. The convex hull of the contour of the hand is computed, and then the convexity defects are determined thanks to an OpenCV function (15). If the defect is longer than 2 cm, the finger is considered as raised. It is then possible to count how many fingers are down, if two or more fingers are raised, the hand is considered as opened, otherwise it is considered as closed (16). To have an idea of the orientation of the hand, points all around the center of the palm are taken. The 3D coordinates of these points give a circle from which the orientation of the hand can be determined (17).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Process diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>These different steps are done for each hand of each user present in the scene. Once the accurate position and the state of the hand are computed, interaction with virtual objects can be done. Steps 5–17 are improvements that OpenNI does not offer yet (white rectangles in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig3">3</a>). All these steps require 23 ms from the acquisition of the depth image to the determination of the position and state of each hand.</p><p>Our algorithm (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig3">3</a>) is thus able to recognize the open or closed status of each users’ hand in front of the Kinect (up to 3 simultaneous users reliably). Although entirely possible, this study is not concerned with this aspect.</p><p>Based on the work of Kim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical robots: technical and surgical skill comparison to current robotic masters. Surg Endosc 1–8. doi:&#xA;                    10.1007/s00464-013-3383-8&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR30" id="ref-link-section-d79199e708">2014</a>), we measure the latency of our system: the latency was measured by moving the hand and then abruptly stopping, while a video of the hand and the application was recorded using a high-speed camera (120 fps). Analyzing the video, frame by frame, gives us a mean latency of 287 ms for ten attempts (SD 22 ms). This value corresponds to the global latency of the system (from the data acquirement to the visual feedback on the computer screen). In video games (running at 30 fps), the mean latency is about 132 ms. The latency induced just by the data acquisition of the Kinect is about 90 ms, and Livingston et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Livingston MA, Sebastian J, Ai Z, Decker JW (2012) Performance measurements for the Microsoft Kinect skeleton. In: IEEE virtual reality, 2012. IEEE Computer Society, pp 119–120. doi:&#xA;                    10.1109/vr.2012.6180911&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR40" id="ref-link-section-d79199e711">2012</a>) measured the latency of the skeleton tracking, which is about 146 ms (for one skeleton in front of the Kinect). Our results are consistent with other works, like for example with the work of Kim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical robots: technical and surgical skill comparison to current robotic masters. Surg Endosc 1–8. doi:&#xA;                    10.1007/s00464-013-3383-8&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR30" id="ref-link-section-d79199e714">2014</a>) where Kinect tracking adds between, 200 and 400 ms compared to a robot master. Even not negligible, the latency is enough low to not disrupt the user experience.</p><p>The virtual environment (VE) has been modeled in 3DS Max, and interactions have been developed in Virtools 5.0. Interactions we developed are selection/deselection, manipulation (moving, rotation, and resizing) of objects, navigation in the VE, and control of the application. For this, we have developed new ways of interaction that we wanted to be intuitive and close to our real actions so that anyone can use our solution without special knowledge in computers or virtual reality.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Interaction modalities</h4><p>Besides simple and intuitive ways of interaction, we added visual feedbacks to indicate the user’s hands position (2 small spheres) as well as their status (green for opened/red for closed) as it is suggested in (Mason and Bernardin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Mason AH, Bernardin BJ (2009) Vision for performance in virtual environments: the role of feedback timing. Int J Hum Comput Interact 25(8):785–805. doi:&#xA;                    10.1080/10447310903025529&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR43" id="ref-link-section-d79199e727">2009</a>). Several actions only require the use of the dominant hand (initialized for each user): selecting/unselecting an object, displaying the menu, moving objects, and navigating in the virtual environment. We considered the lateralization of the user in order to make our solution easier and to enable better accuracy as the predominant hand is capable of producing fine-grained gestures (Guiard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Mot Behav 9(4):486–517" href="/article/10.1007/s10055-014-0255-z#ref-CR20" id="ref-link-section-d79199e730">1987</a>). More complex tasks like resizing and rotation require the use of both hands.</p><p>Interaction techniques have aroused great interest in the scientific community (Beaudouin-Lafon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Beaudouin-Lafon M (2004) Designing interaction, not interfaces. In: Working conference on advanced visual interfaces, Gallipoli, Italy, pp 15–22" href="/article/10.1007/s10055-014-0255-z#ref-CR2" id="ref-link-section-d79199e736">2004</a>; Klein et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Klein T, Guéniat F, Pastur L, Vernier F, Isenberg T (2012) A design study of direct-touch interaction for exploratory 3D scientific visualization. Comput Graph Forum 31(3pt3):1225–1234" href="/article/10.1007/s10055-014-0255-z#ref-CR31" id="ref-link-section-d79199e739">2012</a>) but they are mainly dedicated to 2D applications. Today, manipulation of objects in 3D space becomes accessible to the general public mainly through video games devices (e.g., Nintendo Wiimote, Sony PS Move). Like (Hand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hand C (1997) A survey of 3D interaction techniques. Comput Graph Forum 16(5):269–281" href="/article/10.1007/s10055-014-0255-z#ref-CR22" id="ref-link-section-d79199e742">1997</a>; Laurel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Laurel BK (1986) Interface as mimesis. User centered system design: new perspectives on human–computer interaction. Lawrence Erlbaum Associates, Hillsdale" href="/article/10.1007/s10055-014-0255-z#ref-CR35" id="ref-link-section-d79199e745">1986</a>), we wanted to develop interaction modalities that are as close as possible to real actions. Many works deal with 3D interaction techniques for seamless devices (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulation in CAD application. Comput Aided Des 46:239–245" href="/article/10.1007/s10055-014-0255-z#ref-CR71" id="ref-link-section-d79199e748">2014</a>; Lin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Lin J, Sun Q, Li G, He Y (2013) SnapBlocks: a snapping interface for assembling toy blocks with XBOX Kinect. Multimed Tools Appl 1–24. doi:&#xA;                    10.1007/s11042-013-1690-7&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR39" id="ref-link-section-d79199e752">2013</a>; Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e755">2012</a>) but there is still no universal solution adopted by everybody. We based our interaction techniques on different works like the ones of Fiorentino et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Fiorentino M, Radkowski R, Stritzke C, Uva A, Monno G (2013) Design review of CAD assemblies using bimanual natural interface. Int J Interact Des Manuf 7(4):249–260. doi:&#xA;                    10.1007/s12008-012-0179-3&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR15" id="ref-link-section-d79199e758">2013</a>) for the scaling modality for example. Concerning the rotation, different modalities have been proposed like the one of (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulation in CAD application. Comput Aided Des 46:239–245" href="/article/10.1007/s10055-014-0255-z#ref-CR71" id="ref-link-section-d79199e761">2014</a>) where the rotation is intuitive but need the use of an eye-tracker to select the center of rotation. Other works like (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e764">2012</a>) or (Soh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper presented at the proceedings of the 12th ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry, Hong Kong, Hong Kong" href="/article/10.1007/s10055-014-0255-z#ref-CR69" id="ref-link-section-d79199e767">2013</a>) deal with the rotation modality but these techniques are not intuitive, we nevertheless used the idea of (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e771">2012</a>) concerning the possibility of doing the movement in several times, they called this technique the “pedaling” motion.</p><p>As (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface. In: Paper presented at the 22nd WSCG international conference on computer graphics, visualization and computer vision, Plzen, Czech Republic, June 2–5, 2014" href="/article/10.1007/s10055-014-0255-z#ref-CR91" id="ref-link-section-d79199e777">2014</a>), we can distinguish two kinds of gestures: gestures which trigger a control (called <i>offline</i>) and those which are interpreted and processed in real time, like object manipulation (called <i>online</i>). Regarding our system, we could make the distinction between movements that are used to validate an action (open hand or closed hand) and those directly related to the manipulation and the navigation in virtual environment. This is consistent with (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Symposium on interactive 3D graphics. ACM, pp 35–38" href="/article/10.1007/s10055-014-0255-z#ref-CR4" id="ref-link-section-d79199e786">1997</a>) by considering two steps in object manipulation: grabbing and manipulation interaction (orientation and position of the object). Consistent with the recommendations of (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Symposium on interactive 3D graphics. ACM, pp 35–38" href="/article/10.1007/s10055-014-0255-z#ref-CR4" id="ref-link-section-d79199e789">1997</a>) to separate grabbing and manipulation to ensure good overall usability, we implemented these two steps separately. We compared the most common techniques presented in the literature to our constraints namely that the objects to select/manipulate directly in the workspace of the user, that there is no occlusion between the objects, and finally, that the navigation is done between two scenes along a path. These tasks were chosen deliberately simple to validate our system.</p><p>Poupyrev et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Poupyrev I, Ichikawa T, Weghorst S, Billinghurst M (1998) Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques. Comput Graph Forum 17(3):41–52. doi:&#xA;                    10.1111/1467-8659.00252&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR61" id="ref-link-section-d79199e795">1998</a>) proposed to categorize selection techniques among exocentric and egocentric metaphors. In the latter, they distinguish two metaphors, “virtual hand” (such as arm-extension techniques) and “virtual pointer” (mainly ray-casting techniques). If ray-casting techniques (Mine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill CS Dept" href="/article/10.1007/s10055-014-0255-z#ref-CR46" id="ref-link-section-d79199e798">1995</a>) are convenient to reach an object, they make the manipulation complex due to the use of a global coordinate system (especially for rotations). Moreover, the selection of small or distant objects through virtual pointing remains to be a difficult task (Argelaguet and Andujar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Argelaguet F, Andujar C (2013) Special section on touching the 3rd dimension: a survey of 3D object selection techniques for virtual environments. Comput Graph 37(3):121–136. doi:&#xA;                    10.1016/j.cag.2012.12.003&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR1" id="ref-link-section-d79199e801">2013</a>). On the other hand, arm-extension techniques are convenient to manipulate objects directly in their own coordinate system. That is why we have privileged direct manipulation techniques for interacting with objects. Indeed, until the arrival of Kinect, data gloves were naturally used for “virtual hand”-based interaction (Ughini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ughini CS, Blanco FR, Pinto FM, Freitas CM, Nedel LP (2006) EyeScope: a 3D interaction technique for accurate object selection in immersive environments. In: SBC symposium on virtual reality 2006, pp 77–88" href="/article/10.1007/s10055-014-0255-z#ref-CR83" id="ref-link-section-d79199e804">2006</a>); our system is proposing to replace the data gloves; we focused on that kind of metaphors which allow a better immersion as the user can see a representation of his hands. For selection, as all items are available in the workspace of the user, we have not had to implement methods like go–go (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Paper presented at the 9th annual ACM symposium on user interface software and technology, Seattle, Washington, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR60" id="ref-link-section-d79199e807">1996</a>) for example. The ray-casting is advantageously replaced by the selection method which is called 3D paint-to-select by (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface. In: Paper presented at the 22nd WSCG international conference on computer graphics, visualization and computer vision, Plzen, Czech Republic, June 2–5, 2014" href="/article/10.1007/s10055-014-0255-z#ref-CR91" id="ref-link-section-d79199e811">2014</a>). The constraints (no occlusion, limited workspace, and limited precision) of our system allowed us to avoid the use of interaction metaphors such as world in miniature (Stoakley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Denver, Colorado, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR75" id="ref-link-section-d79199e814">1995</a>). Our approach is therefore closer to hybrid techniques such HOMER (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Symposium on interactive 3D graphics. ACM, pp 35–38" href="/article/10.1007/s10055-014-0255-z#ref-CR4" id="ref-link-section-d79199e817">1997</a>), as we separate selection and manipulation. But we used a simpler technique than ray-casting for selection and we implemented direct manipulation rather than arm-extension techniques. In this manner, users can grab and manipulate objects simply using natural motions (Robinett and Holloway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Paper presented at the proceedings of the 1992 symposium on interactive 3D graphics, Cambridge, Massachusetts, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR64" id="ref-link-section-d79199e820">1992</a>).</p><p>Finally, new paradigms tend to adapt multi-touch gestures used in today’s smartphones to manipulate 3D objects in virtual environments. For example, (Nan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Nan X, Zhang Z, Zhang N, Guo F, He Y, Guan L (2013) vDesign: toward Image Segmentation and composition in CAVE using finger interaction. In: International conference on signal and information processing (ChinaSIP), Beijing, China, 6–10 July 2013. IEEE, pp 461–465" href="/article/10.1007/s10055-014-0255-z#ref-CR53" id="ref-link-section-d79199e827">2013</a>) proposed a system dedicated to image segmentation and composition in CAVE using fingers interaction. The bimanual interaction is close to our system, and resizing is implemented in a similar manner (the size of the object is linearly indexed to the distance between the hands). For cons, the implementation of the moving is more complex, because the reference of the movement is the midpoint between the two, imposing to maintain a constant distance between hands during translation, while not allowing a validation action. The rotation is not necessarily intuitive for the chosen axis. One improvement proposed by (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface. In: Paper presented at the 22nd WSCG international conference on computer graphics, visualization and computer vision, Plzen, Czech Republic, June 2–5, 2014" href="/article/10.1007/s10055-014-0255-z#ref-CR91" id="ref-link-section-d79199e830">2014</a>) allows for 3 handling tasks simultaneously, but requires the use of a wand-type device (a button allows to switch between interaction mode and pause mode).</p><p>
                    <i>Simple tasks</i> For object selection, the user must move his dominant hand on the object and close his hand on it for 2 s. When the object is selected, its color turns blue. To unselect, the dominant hand must be held opened on the object for 2 s, when it is deselected the object goes back to its original color. To move a selected object, the user has to put his dominant hand (closed) on the object. The object is thus attached to the hand and then follows its movements while it remains closed. When the user opens his hand, the object is released. Our implementation of the selection follows the guidelines of (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Paper presented at the proceedings of the ACM symposium on virtual reality software and technology, London, United Kingdom" href="/article/10.1007/s10055-014-0255-z#ref-CR6" id="ref-link-section-d79199e839">1999</a>) as the user can see the collision between the sphere representing his hand and the object to select (object indication), close his hand to select an object (confirmation selection) and see the changing of the color of the object when selected (visual feedback).</p><p>For the navigation, we chose the metaphor of a joystick. Navigation has been described as consisting of 2 components: travel (the task of moving from one location to another) and wayfinding (the task of acquiring and using spatial knowledge) (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bowman DA, Kruijff E, LaViola J, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison Wesley Longman Publishing Co., Inc., Boston" href="/article/10.1007/s10055-014-0255-z#ref-CR7" id="ref-link-section-d79199e845">2004</a>). Interaction techniques for travel [e.g., (Mine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill CS Dept" href="/article/10.1007/s10055-014-0255-z#ref-CR46" id="ref-link-section-d79199e848">1995</a>; Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Virtual reality annual international symposium, 1997, IEEE 1997, 1–5 Mar 1997, pp 45–52, 215. doi:&#xA;                    10.1109/vrais.1997.583043&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR5" id="ref-link-section-d79199e851">1997</a>)] and wayfinding aids [e.g., (Darken and Sibert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Vancouver, British Columbia, Canada" href="/article/10.1007/s10055-014-0255-z#ref-CR11" id="ref-link-section-d79199e854">1996</a>)] have been proposed in the literature. Among three general interaction paradigms for 3D virtual environments proposed by (Ware and Osborne <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Ware C, Osborne S (1990) Exploration and virtual camera control in virtual three dimensional environments. SIGGRAPH Comput Graph 24(2):175–183. doi:&#xA;                    10.1145/91394.91442&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR87" id="ref-link-section-d79199e857">1990</a>) (eyeball in hand, scene in hand, and flying vehicle control), we chose an egocentric technique: flying vehicle because it is more consistent for our system where the user moves from one selection to another scene. Here, there was no way to inspect an item as it can be possible with techniques such as ViewCube (Khan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Khan A, Mordatch I, Fitzmaurice G, Matejka J, Kurtenbach G (2008) ViewCube: a 3D orientation indicator and controller. In: Paper presented at the proceedings of the 2008 symposium on interactive 3D graphics and games, Redwood City, California" href="/article/10.1007/s10055-014-0255-z#ref-CR29" id="ref-link-section-d79199e861">2008</a>). To get as close as possible to free navigation, and given the simplicity of the navigation task we implemented, we excluded “point-of-interest” (POI) techniques, although they may have interests such as quick navigation (Haik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Haik E, Barker T, Sapsford J, Trainis S (2002) Investigation into effective navigation in desktop virtual interfaces. In: Paper presented at the seventh international conference on 3D Web technology, Tempe, Arizona, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR21" id="ref-link-section-d79199e864">2002</a>). Moreover, in the context of the discovery of an environment, it is not relevant to guide the user. Recent POI techniques as “Drag’n go” (Moerman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Moerman C, Marchal D, Grisoni L (2012) Drag’n go: simple and fast navigation in virtual environment. In: 2012 IEEE Symposium on 3D user interfaces (3DUI), 4–5 March 2012, pp 15–18. doi:&#xA;                    10.1109/3dui.2012.6184178&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR48" id="ref-link-section-d79199e867">2012</a>) are often more suited to the use of a device (as the user must “validate” the target he wants to reach), and ask an heavier cognitive load. Finally, our environment was not specific and did not require a variable level of details, which is why we have not adopted any multiscale techniques like the one proposed by (McCrae et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Paper presented at the proceedings of the 2009 symposium on interactive 3D graphics and games, Boston, Massachusetts" href="/article/10.1007/s10055-014-0255-z#ref-CR45" id="ref-link-section-d79199e870">2009</a>). Concerning the wayfinding, many arrows are placed in the environment to indicate the direction to follow. Regarding the travel, it appeared that we needed a starting point for the user and a navigation direction. We wanted to have a visual reference for the user to know at all times how he moves in space relative to his starting point. The solution of a virtual joystick was chosen because the rest position of the stick of the joystick is used as a reference and the user can easily see in which direction he moves. The metaphor of the joystick allows the user to finely control the speed of movement at every moment, navigating, as speed of motion is important to effectively navigate in 3D environments (McCrae et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Paper presented at the proceedings of the 2009 symposium on interactive 3D graphics and games, Boston, Massachusetts" href="/article/10.1007/s10055-014-0255-z#ref-CR45" id="ref-link-section-d79199e873">2009</a>). For people who never used a joystick, it remains intuitive because they just have to move the hand in the direction they want to go from a starting position. To navigate, the user closes his hand on the virtual joystick for 2 s to select it and moves his hand in the direction he wants to go. When the joystick is selected, a frame and arrows appear to show the user the correct direction as well as the final location.</p><p>
                    <i>Complex tasks</i> The following tasks are more complex because they require the coordination of both hands. By using the analogy of a spring that can be compressed or stretched to change its size, users have to spread or bring their hands closer to enlarge and reduce the object. Resizing is homogeneous in all three axes. This solution is close to solutions proposed recently [e.g., (Fiorentino et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Fiorentino M, Radkowski R, Stritzke C, Uva A, Monno G (2013) Design review of CAD assemblies using bimanual natural interface. Int J Interact Des Manuf 7(4):249–260. doi:&#xA;                    10.1007/s12008-012-0179-3&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR15" id="ref-link-section-d79199e882">2013</a>; Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e885">2012</a>)].</p><p>For the rotation, the user must put his dominant hand (closed) on the object. The representation of the hand disappears and is automatically positioned at the center of the object. By moving the secondary hand (closed) along the <i>X</i> axis (or <i>Y</i> or <i>Z</i>), the object rotates in the same direction. Contrary to others approaches like the use of sheet of paper (Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR70" id="ref-link-section-d79199e900">2012</a>) with impose to use a tangible object, the rotation with two hands (Soh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper presented at the proceedings of the 12th ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry, Hong Kong, Hong Kong" href="/article/10.1007/s10055-014-0255-z#ref-CR69" id="ref-link-section-d79199e903">2013</a>) which is not very intuitive, and unlike (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e907">2011</a>), we chose to simplify this interaction, even though by doing this it becomes less realistic.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Application control: a pie menu</h4><p>To allow the user to switch from one action to another, we set up a menu. We excluded gestural language like in (Soh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper presented at the proceedings of the 12th ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry, Hong Kong, Hong Kong" href="/article/10.1007/s10055-014-0255-z#ref-CR69" id="ref-link-section-d79199e918">2013</a>) which we considered to be too complex and which could lead to misinterpreted gestures; moreover, they oblige the user to memorize the gesture for each action he wants to do. We also excluded voice commands because we did not want to introduce new variables related to multimodality. Levesque et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Levesque JC, Laurendeau D, Mokhtari M (2011) Bimanual gestural interface for virtual environments. In: Virtual reality conference (VR), 2011 IEEE, 19–23 March 2011, pp 223–224. doi:&#xA;                    10.1109/vr.2011.5759479&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR38" id="ref-link-section-d79199e921">2011</a>) also proposed a 3D bimanual gestural interface using data gloves for 3D interaction; the left hand can select interaction modes while the right hand is for the interaction itself, like the rotation of an object. This solution was not chosen because we wanted the user can use his two hands to perform the different actions. We chose a pie menu (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig4">4</a>) as previous work demonstrated the importance of these menus over linear menus (Callahan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Callahan J, Hopkins D, Weiser M, Shneiderman B (1988) An empirical comparison of pie vs. linear menus. In: SIGCHI conference on human factors in computing systems, Washington, DC, United States. ACM, pp 95–100. doi:&#xA;                    10.1145/57167.57182&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR8" id="ref-link-section-d79199e927">1988</a>). They allow increased speed while minimizing errors of selection. In addition, the distance between the point of activation and all the different items is the same, due to the circular organization.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Pie menu with four choices</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>In order to bring up the menu, the user closes his hand, opens it, and keeps it open for 2 s. The menu appears at the exact position where the user closed his hand. To choose an action, the user must stay on the item selected for 2 s. The method we have chosen (i.e., a waiting period of 2 s) for the validation of the selection (Gratzel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Gratzel C, Fong T, Grange S, Baur C (2004) A non-contact mouse for surgeon–computer interaction. Technol Health Care 12(3):245–257" href="/article/10.1007/s10055-014-0255-z#ref-CR19" id="ref-link-section-d79199e951">2004</a>) causes a lag in the interaction which is a disadvantage of our solution. But it was the only solution compatible with our goals of reliability, stability, and simplicity, unlike other methods, such as those outlined by (Schlattmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Schlattmann M, Na Nakorn T, Klein R (2009) 3D interaction techniques for 6 DOF markerless hand-tracking. In: International conference on computer graphics, visualization and computer vision, Plzen-Bory, Czech Republic" href="/article/10.1007/s10055-014-0255-z#ref-CR67" id="ref-link-section-d79199e954">2009</a>): a physical button (not seamless), head movements (unnatural), speech recognition (multimodal). Moreover, selecting the closest object (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA" href="/article/10.1007/s10055-014-0255-z#ref-CR86" id="ref-link-section-d79199e957">2011</a>) is not viable in complex virtual worlds. The criteria for reliability, stability, and simplicity correspond to criteria that any system should meet to be both useful and usable (Loup-Escande et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Loup-Escande E, Burkhardt J-M, Richir S (2011) Anticiper et Evaluer l’Utilité dans la Conception Ergonomique des Technologies Emergentes: Une Revue. Le Travail Humain 76:27–55" href="/article/10.1007/s10055-014-0255-z#ref-CR41" id="ref-link-section-d79199e960">2011</a>). The reliability indicates here that the proposed system allows the user to make the proposed tasks (i.e., selection, navigation, and rotation). Stability is related to the use of capture device: visual feedback of hands avatars should faithfully follow the gestures of the user. Finally, the simplicity corresponds to the simplicity in the handling of the system (before the interaction) and then in the understanding of the different features, more specifically the interaction metaphors. Even if new devices such as Leap Motion (LeapMotion <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="LeapMotion (2012). &#xA;                    https://www.leapmotion.com&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR36" id="ref-link-section-d79199e963">2012</a>) still have some limitations and need to be improved in tracking fingers, it will certainly allow the user to more accurately know the position of the fingers and thus recognize more complex gestures in real time, a feature which is not possible with a Kinect.</p><h3 class="c-article__sub-heading" id="Sec13">Tasks, participants, and procedure</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Virtual environments tasks: design</h4><p>Manipulation tasks are divided into three stages of gradual difficulty. The three objects of the first scene are only associated with a single interaction, whereas those of the second scene are associated with two basic types of actions (e.g., moving and rotating). The last scene contains only one object to be moved, rotated and resized. To move from one scene to another, the participant must “navigate.” The set of subtasks is summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab1">1</a>. The tasks consist of matching objects with a transparent model. The game is displayed in perspective, allowing the user to better evaluate the distances between and the orientation of objects. Items are in the same conditions (position, orientation, scale) in both devices in order to obtain comparable data from both systems. Learning effects are controlled by mechanisms that ensure internal validity of our experiment (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0255-z#Sec18">2.2.5</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig5">5</a> shows three objects in a scene before and after manipulation.</p><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Tab1_HTML.gif" alt="" />
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Sequence of tasks (<i>M</i> moving, <i>S</i> scaling, <i>R</i> rotation, Nav. = navigation)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Scene 2 with 3 objects (<i>globe</i>, <i>clock</i>, and <i>computer</i>) before and after manipulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Participants</h4><p>The experiment involved 20 volunteer participants, 8 women and 12 men, aged between 21 and 34 (average = 26; SD = 3.7). All were experienced users of computers and had a college BA or BS. The majority were graduate students in virtual reality, or members of a Virtual Reality Laboratory (PhDs, Research Engineers). The experiment was presented as a comparison between two VR systems without any mention of “our” solution. Participants were “naïve” as the experiment took part at the beginning of the academic year: they knew neither our work nor the person performing the experiment (who had no link with them). As the number of participants was relatively small, we were careful to restrict the age range to limit inter-individual variability with respect to performance and subjective preferences. The average age of our relatively small group of participants is explained by the fact that we wanted to first validate qualitatively and quantitatively the interest of our solution with an audience who has a certain ease with the technology and better acceptance of change.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Material, procedure, and instructions</h4><p>We used a computer with an Intel Core ™ i7 930, 6 GB of RAM, an NVidia GeForce GTX480 graphics card, and a 22″ 120 Hz screen. For the “3DCam” solution, the Kinect is placed above the screen (16″ from the table) and interferes neither with the vision of the user nor his workspace. In the case of “3DGloves,” a magnetic sensor (a Polhemus Patriot) was attached to each glove (5DT glove) and all were connected by wires to a computer (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Participant with the “3DGloves” system (<i>left</i>) and the “3DCam” system (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The objective of each task was the same: matching precisely the object and its model. We did not ask participants to accord more importance to precision than to the time necessary to perform the 9 tasks.</p><p>The experiment was divided into different stages: after completing an identification questionnaire, the participant received instructions explaining the experiment, the tasks, and the proposed interactions. These instructions were also given in a written form and freely available during the experimentation. The participant began the experiment by a learning process (unlimited time). Once the user was ready, he carried out the 9 tasks for which data were recorded. After that, he filled in a final questionnaire concerning his or her feelings, comments, and subjective judgments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Working hypotheses and measures</h4><p>We formulated four hypotheses concerning our comparative study: a 3DCam solution provides greater accuracy in manipulation tasks due to the absence of equipment to wear (H1), better execution times (H2), a higher level of overall acceptability due to a better comfort, a better efficiency and a better effectiveness (H3), and obtains participants’ preferences due to a higher feeling of immersion as well as a greater ease of use (H4) compared to the 3DGloves system. These hypotheses, if confirmed, will allow us to come to the conclusion that the 3DCam is superior to the 3DGloves when it comes to manipulation tasks in VE associated with our interaction modalities. To study the participants’ performances, specific metrics were taken with regard to each system:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Total execution and manipulation time (seconds) of the 9 tasks and of each task: manipulation time = execution time − choices in the menu or “inactivity” periods</p>
                      </li>
                      <li>
                        <p>Accuracy error, averaged over the three axes (for general errors), or calculated for each axis (when comparing rotation and moving errors along each axis).</p><ul class="u-list-style-bullet">
                            <li>
                              <p>The rescaling error is the percentage of difference from the reference scale;</p>
                            </li>
                            <li>
                              <p>The rotation error is a percentage calculated from the angular shift between manipulated and reference objects (error of 100 % = angular shift of 180°);</p>
                            </li>
                            <li>
                              <p>The error of movement is a percentage calculated (independently for each axis) relative to the reference position, standardized according the object size. A null accuracy corresponds to a shift at least the size of the object.</p>
                            </li>
                          </ul>
                        
                      </li>
                    </ul>
                  <p>The study of subjective preferences and participants’ comments is divided into two parts: judgments about the interaction modalities and an evaluation of each system separately and then the two compared to each other. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab2">2</a> reviews the variables studied and the method of measurement. Responses to Likert scales are encoded and treated as numeric variables (1 = worst, 5 = best). Participants could also provide feedback through open-ended questions for each modality.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Variables studied and measures</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>We decided to study the different criteria of the acceptability because they are clearly explained by numerous reference papers in the field of ergonomics and interaction with complex systems (Tricot et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Tricot A, Plégat-Soutjis F, Camps J-F, Amiel A, Lutz G, Morcillo A (2003) Utilité, utilisabilité, acceptabilité : interpréter les relations entre trois dimensions de l’évaluation des EIAH. In: Conférence EIAH 2003, Strabourg, France" href="/article/10.1007/s10055-014-0255-z#ref-CR80" id="ref-link-section-d79199e1381">2003</a>). This explains our choice not to use standardized questionnaires such as the NASA TLX for example.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Internal and external validity</h4><p>To ensure the internal validity of this experimentation, we restricted the origin and the age of the participants. We chose to rely on a population with prior knowledge in VR to avoid the novelty effect induced by the wearing of unfamiliar material which might result in a possible bias regarding the acceptability of the system. The identification questionnaire showed that participants had no particular experience of natural user interfaces (UI). However, we must mention that restricting the profile of our participants implies limiting the generalization of the possible results. We carefully prepared the experimental protocol to avoid bias on the perception of “expectations” of the experiment. It was presented as a “simple” comparison of two systems, without any references on the work done on our solution.</p><p>We also attempted to control extraneous variables from the real environment (lighting, noise, and temperature): the experiment took place in a single room, with air conditioning and no windows. We counterbalanced the presentation in order to compensate the potential learning effect which could have led to a potential variability of the results during the experimentation (improvement or deterioration). Participants were randomly divided into two groups (G1 and G2), each one with 4 women and 6 men.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Statistical analysis</h4><p>To check the normality of all the distributions of values, we performed Shapiro–Wilk test since it is more powerful in detecting normality for samples sizes up to 2,000. For the few variables which did not follow a normal distribution, we chose nonparametric tests, with the exception of the study of the interaction of several factors for which we performed an ANOVA, given its robustness for type 1 errors (Winer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Winer BJ (1971) Statistical principles in experimental design, 2nd edn. McGraw-Hill, New York City" href="/article/10.1007/s10055-014-0255-z#ref-CR88" id="ref-link-section-d79199e1402">1971</a>).</p><p>To analyze the influence of the system on participants’ performances, we used the Student’s <i>t</i> test for paired samples or the Wilcoxon test (depending on the distributions). Analysis of the effects of gender is based on a mixed ANOVA with one within-subject variable (the system) and a between-subject variable (gender). The study of simple effects was based on a Student’s <i>t</i> test for independent variables (two conditions) or a simple ANOVA (three conditions). Post hoc LSD test of Fisher was used to study the possible main effects. Results were considered significant when <i>p</i> ≤ 0.05 and as a trend when 0.05 &lt; <i>p</i> ≤ 0.1.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Results</h2><div class="c-article-section__content" id="Sec20-content"><h3 class="c-article__sub-heading" id="Sec21">Participant’s performance</h3><p>We compared global results of the participants for each system (for all nine tasks) and assessed the possible influence of gender on them. Then, we focused on each task separately. We concluded the presentation of the results by a further study, comparing the accuracy along the axes for rotation and moving.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Global results</h4><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab3">3</a> shows the comparison of the experimentation time (ExpT) and the total manipulation time (TMT) between the two systems. We did not observe any significant difference between the systems for ExpT and TMT. A mixed ANOVA (gender x system) on the ExpT did not show any effect of the system [<i>F</i>(1, 18) = 0.772, <i>p</i> = 0.391] or the gender [<i>F</i>(1, 18) = 1.337, <i>p</i> = 0.263] and no interaction between these two factors [<i>F</i>(1, 18) = 0.157, <i>p</i> = 0.696]. Regarding the TMT, we observed no effect of the system [<i>F</i>(1, 18) = 0.05, <i>p</i> = 0.945] or the gender [<i>F</i>(1, 18 = 1.551, <i>p</i> = 0.229] and no interaction between these two factors [<i>F</i>(1, 18) = 0.261, <i>p</i> = 0.615].</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Time of experimentation and the total manipulation time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>We had no significant results for the overall results or effects of sex. Total time during which participants manipulated or navigated was not significantly lower with our system, which neither allowed the validation nor the rejection of the hypothesis H2. First tests are partial because the variables are macroscopic and do not allow to highlight potential differences in execution time between the interaction modalities. These results are presented in the following section.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Detailed results</h4><p>For each task and each system, we studied the following variables:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Execution time (ET) in seconds;</p>
                      </li>
                      <li>
                        <p>Manipulation time (MT) in seconds; it is differentiated according to the moving (MTM), rotation (MTR) and scaling (MTS) as appropriate;</p>
                      </li>
                      <li>
                        <p>The error of accuracy, given as four separate variables: moving (Err_M), rotation (Err_R), navigation (Err_N), and scaling (Err_S);</p>
                      </li>
                    </ul>
                  <p>For five out of the nine tasks, we observed no significant difference between 3DGloves and 3DCam: task 1 (moving), task 3 (rotation), task 4 (navigation), task 5 (moving + scaling), task 8 (navigation). For the four remaining tasks, relevant data are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab4">4</a>. For the simple task of scaling (task 2, horse), ET and MT are lower with 3DGloves. For the task requiring moving and rotation (task 6, clock), we observed a lower moving time with 3DGloves and a lower error for rotation with this system. For the task requiring scaling and rotation (task 7, computer), scaling time was lower with 3DGloves but rotation time was lower with 3DCam. Finally, the task mixing the three basic actions (task 9, boat), the moving error was lower with 3DGloves.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Significant differences between 3DGloves (G) and 3DCam (C)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>3DGloves seem to be able to minimize the ET for the scaling tasks. This task requires less precision than the other, and the best time obtained with 3DGloves can be explained by a better recognition of the state of the hand and because the hands (i.e., magnetic sensors) do not move away from the antenna, which results in stable values.</p><p>These results refuted hypothesis H1 and did not confirm hypothesis H2. We expected a superiority of 3DCam on precision due to the absence of weight. Finally, the accuracy appears equivalent between the two systems. We can therefore say, considering current developments, that the 3DCam does not outperform a current virtual reality device (i.e., 3DGloves) in terms of performance (time, accuracy).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Additional study (for 3DCam only): precision along the 3 axes</h4><p>The results for moving tasks (1: well, 5: globe, 6: clock, and 9: boat) are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab5">5</a>. For tasks 1 and 5, the error is significantly higher on the <i>Z</i> axis than on the <i>X</i> and <i>Y</i> axes. These findings are reversed for task 6, with a lower error on the <i>Z</i> axis. For the task 9, the error is significantly greater with the <i>Y</i> axis than the <i>Z</i> axis and the <i>X</i> axis. The results for rotation tasks (3: house, 6: clock, 7: computer, and 9: boat) are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0255-z#Tab6">6</a>. For the task 7, the orientation error is significantly lower with the <i>Y</i> and <i>Z</i> axes than with the <i>X</i> axis. For the task 9, the orientation error is significantly lower with the <i>X</i> axis than with the axis <i>Y</i>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Decomposition of the positioning error along each axis for 3DCam</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Decomposition of the orientation error along each axis for 3DCam</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>
                    <i>Synthesis</i> For the first tasks (well and globe), movements are significantly less accurate with the <i>Z</i> axis. For the well, this can be explained by the amplitude of the required movement associated with a problem of the recognition of the state of the hands as well as the possibility of being out of the Kinect’s range. It is the same for the globe and the clock but we also observed that a perspective effect could bring about an inaccurate sensation of correct positioning. This problem was less present due to the orientation of the boat and the clock. We also noticed that results on the <i>X</i> axis are poorer when the movement is associated with a rotation. The results are less conclusive for rotation given the variability of the data (e.g., in the task 7, the rotation along the <i>X</i> axis (hand’s movement in depth) is the most imprecise as it appears better for task 9). However, we thought that the rotation along the <i>Z</i> axis (vertical hand’s movement) could also be less precise because this axis is the one for which we have the lowest amplitude, and it is difficult to lower the arm without moving it forward (which produces a second rotation simultaneously).</p><p>The differences are thus mainly related to the interaction and the task. For moving tasks, we can recommend finding a technique to limit the workspace and prevent the participant from being outside the scope of the Kinect. For rotations, the difficulty of separating the axes may be a justification of the variability of results (which must be confirmed by the analysis of participants’ preferences and feedbacks).</p><h3 class="c-article__sub-heading" id="Sec25">Participants’ preferences</h3><p>We studied the participants’ subjective preferences based on their answers to the final questionnaire, regarding acceptability and global preferences. We then presented the results for interaction modalities and suggestions for improvement made by the participants. We performed nonparametric Wilcoxon tests rather than Student’s <i>t</i> tests as our data did not follow a normal distribution (see “Statistical analysis”).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Acceptability</h4><p>Mean scores to Likert scale for each criterion are represented on Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig7">7</a>. The surrounded criteria indicate a significant difference (in black) or a trend (gray) between 3DGloves and 3DCam.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mean scores for each criterion. The surrounded criteria indicate a significant difference (in <i>black</i>) or a trend (<i>gray</i>) between 3DGloves and 3DCam</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>There is no significant difference between the two systems concerning the tiredness (<i>Z</i> = 0.302), the precision (<i>Z</i> = 1.218), the stability (<i>Z</i> = 1.510), the effectiveness (<i>Z</i> = 0.286), and the ease of use (<i>Z</i> = 0.707).</p><p>Comfort (<i>Z</i> = 2.448; <i>p</i> = 0.014), handiness (<i>Z</i> = 2.673; <i>p</i> = 0.008) were significantly better with 3DCam; we observed a trend in favor of this solution for freedom of movement (<i>Z</i> = 1.654; <i>p</i> = 0.098). Reliability is significantly better with the system 3DGloves (<i>Z</i> = 3.038; <i>p</i> = 0.002), due to problems linked to the detection of the state of the hand by the 3DCam which was identified by several participants. Freedom of movement was considered superior to the 3DCam because participants were not wearing equipment with this solution while the 3DGloves system necessitates being connected by four cables to the computer (2 for gloves, 2 for magnetic tracking system). The handiness was judged inferior for the 3DGloves system which can be explained by the single size of the data gloves, which are not suitable for all hands. The feeling of tiredness was judged equivalent between the two systems because the weight of the gloves and magnetic sensors is negligible compared to the tiredness caused by the arm raising without support.</p><p>Acceptability appears to be better with 3DCam, which is justified by the comments and subjective judgments of the participants. The negative point, illustrated with reliability, is the bad recognition of the states of the hand that may occur sometimes.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Global preferences</h4><p>We compared preferences according to five criteria: general preference (GP), relevance to the task (RT), feeling of immersion (FoI), precision (P), and ease of use (EoU). Participants could choose 3DGloves, 3DCam or “equal.” Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig8">8</a> shows the results.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Stacked histogram of the distribution of participants according to each criterion</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Participants were less favorable to the 3DGloves system than to the 3DCam solution for the five criteria: for each of the five criteria, more participants preferred the 3DCam. For three out of five criteria (GP, RT, and P), more than half of the participants gave their preference to the 3DCam.</p><p>With regard to the EoU, the 3DCam seems to provide a significant advantage compared to the 3DGloves, in line with the above results (handiness and freedom of movement). Clearly, although it should be qualified with regard to the number of participants, the 3DCam brings an equal or even greater FoI compared to 3DGloves (8 against 2, 10 without preference). We can hypothesize that the absence of equipment (and thus weight on the hands) and the freedom of movement are likely to strengthen the FoI as the interaction is the same for both systems. The short range of the magnetic tracking system (which trembles when it is too far from the antenna) was also likely to weaken the involvement of the participant in the task and thus to reduce the FoI.</p><p>Results reflect a weakness for the 3DGloves precision, which is confirmed by the participants’ comments. General preferences are in favor of the 3DCam, since 12 out of the 20 would choose the 3DCam, as opposed to only 3 for 3DGloves (and 5 “equal”).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Assessment of the interaction modalities</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig9">9</a> summarizes the participants’ answers to Likert scale questions. It appears that rotation is significantly lower than the three other interaction modalities, for each criterion and moving is considered more intuitive than navigation (<i>Z</i> = 1.897; <i>p</i> = 0.058) and scaling (<i>Z</i> = 2.310; <i>p</i> = 0.021). Only one participant found rotation modality intuitive; for the others, two main reasons justify the weakness of rotation:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Average scores for each criterion each interaction modality</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <ul class="u-list-style-bullet">
                      <li>
                        <p>The unusual form of the action was difficult in initial learning; this reason was mentioned by 7 participants.</p>
                      </li>
                      <li>
                        <p>The difficulty to associate a particular movement to a specific rotation; this reason was given by 5 participants.</p>
                      </li>
                    </ul>
                  <p>Moving was considered effective, simple, and intuitive by all the participants. Negative comments were more general and concerned the stability and the quality of the recognition of the hand’s state. Similarly, scaling was evaluated positively by 16 participants, considered as being intuitive, fast, and accurate. Only one participant mentioned that it was difficult to know how to position both hands before rescaling. Finally, navigation was also considered very intuitive and very fast by 12 participants and as a fun experience by four participants. However, 2 participants noticed that this interaction was too sensitive and not very accurate over short distances and 5 participants mentioned excessive speed.</p><p>Moving was probably considered more intuitive than the other interaction modalities because it is the only one which is directly adapted from the real environment without an interaction metaphor (joystick for navigation, spring for scaling) or an adaptation (x, y, z decomposition for rotation).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec29">Improvement suggestions</h4><p>Suggestions for improvement concern comments and criticisms in general terms or specific to an interaction modality. Several solutions were proposed by participants to avoid the instability, especially with 3DGloves, which makes tasks requiring precision difficult, such as moving: modulating the speed of movement as a function of distance, or using the second hand to validate the end of a movement. These solutions could also be applied to scaling and rotation.</p><p>As far as rotation is concerned, opinions are divided. 6 participants enjoyed the simple access to the 3 axes without an additional menu but 12 participants experienced difficulties, mainly concerning the “initial learning.” Seven participants suggested giving visual feedback to indicate which movement corresponds to which axis in order to help. Another participant suggested limiting the choice to a single axis and to lock the two others, which seems to go against the interest of an immediate access to the three axes. Finally, one participant mentioned rotation by a turning of the hand. Scaling was considered positively by all participants. But some also mentioned difficulties when, for example, they started the decrease movement with hands held too close together. Since they then ended up with their hands touching each other, they were obliged to “disengage” and reposition their hands to start over. A first solution is that one hand manages the scale in one axis while the other acts like an on/off switch for the action. The second is an exponential scale when hands are close: the size of the object continues to decrease avoiding the need to release. Navigation was mainly evaluated as quick and intuitive by the participants.</p><p>These various comments and suggestions will be the basis for improving our solution.</p></div></div></section><section aria-labelledby="Sec30"><div class="c-article-section" id="Sec30-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec30">Conclusions and perspectives</h2><div class="c-article-section__content" id="Sec30-content"><p>The work reported here is based on several premises. First, a kind of technology which aims to make the interface “disappear” to allow a “natural” user interaction is often presented as the final outcome of direct manipulation interfaces (Fuchs and Moreau <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Fuchs P, Moreau G (2003) Traité de la Réalité Virtuelle, vol 2, 2 edn. Les Presses de l’Ecole des Mines de Paris" href="/article/10.1007/s10055-014-0255-z#ref-CR17" id="ref-link-section-d79199e3874">2003</a>). These authors also suggest that motor responses must be ideally transmitted without link between man and machine. Finally, according to (Winkler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Winkler S, Yu H, Zhou Z (2007) Tangible reality desktop for digital media management. In: Engineering reality of virtual reality, San Jose. SPIE" href="/article/10.1007/s10055-014-0255-z#ref-CR89" id="ref-link-section-d79199e3877">2007</a>), having no hardware to wear and making the system seamless to the user is likely to improve the sense of immersion and presence. The solution we developed tries to provide, in part, answers to these questions that are crucial in the VR field.</p><p>In this paper, we presented the design of a seamless solution dedicated to capturing hand movements for real-time interaction in 3D environments. Specifically, we were interested in the selection, manipulation, and navigation tasks. We propose a real-time solution concerning the interactions (navigation, moving, scaling, and rotation of the objects); however, calling the menu and selecting the action to perform, selecting, and unselecting objects are not done in real time because it requires poses of 2 s to validate the choices. The solution is understood here as a complete system composed of the device (Kinect), the processing and the detection algorithms as well as the developed interaction modalities. The objective was to demonstrate the value of such a seamless solution. The underlying and secondary objective was to evaluate the interaction modalities in order to propose an effective, efficient, and comfortable solution.</p><p>To accomplish this, we carried out an experiment with 20 participants and we contrasted our solution to an existing and functionally equivalent commercial system. In order to obtain comparable results, we used the same interaction modalities in the two systems. Based on our objectives, we formulated four hypotheses related to performance and subjective preferences. We expected, for our solution, a better precision (H1), better execution times (H2), a better acceptability (H3), and better subjective preferences (H4). We measured the participants’ performance for nine different tasks according to global criteria or relative to each task. In addition, we assessed the subjective preferences of the participants and collected their comments and suggestions for improvements through a questionnaire.</p><p>Hypotheses H1 and H3 are linked since the absence of wearing equipment (and wired connection between the devices and the computer) should impact performances and preferences. The absence of weight and “hindrance” could help to minimize both fatigue and maximize accuracy and allow participants to experience better comfort and efficiency.</p><p>Concerning accuracy, no device performs better than the other which does not validate H1. We thought that the absence of equipment (i.e., weight) was sufficient to minimize fatigue and therefore the imprecision. Two explanations are possible: firstly, the experimentation time could be insufficient to induce fatigue and thus to affect the results; secondly, the fatigue caused by wearing the gloves and sensors could be negligible compared to the tiredness of raising an arm without support. From this viewpoint, it could be interesting to perform a longitudinal study to provide a large number of results and data related to a real use of a seamless system dedicated to interaction in VE. Longer experiment times could allow us to assess the relative importance of different criteria studied (e.g., comfort, freedom of movement…) on participants’ global preferences. Contrary to hypothesis H2, execution times were no better for one device than the other. Comments from participants shed more light on these explanations since they evoked a problem of reliability in the recognition of the state of the hand with the 3DCam and a problem of trembling of the 3DGloves data when the hands were too far from the antenna. These two negative points, inherent to technical solutions, could have smoothed out the performance results. Thus, if the detection of the movements with the 3DCam and 3DGloves were the same with regard to accuracy, the reliability of the recognition of the state of the hand remains lower for the 3DCam. This problem could be overcome by adding cameras, to ensure that the hand is always correctly oriented. With a robust detection, our algorithm would be more effective. Perspectives of improvements are important: thanks to a system like the LeapMotion we probably could in the short-term consider the position of each finger with more precision, making possible a richer interaction with a much more diverse interaction language. Participant’s preferences are in majority in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. The feeling of immersion is equivalent between the two systems for half of the participants, but is perceived as higher for the 3DCam for 8 of the 10 others. Similarly, perceived ease of use is superior to the 3DCam for nine participants, when only five evoke 3DGloves for this criterion. These results tend to validate the hypotheses H3 and H4.</p><p>Currently, our solution is able to provide an alternative to a conventional system for the main tasks usually performed in VR applications, regarding the profile of our participants. The major advantage is the cost, much cheaper than common VR equipment. The other major advantage is the potential of 3D cameras, which go far beyond the tasks we developed. Our solution can replace classic equipment while providing a better “use-value” (Loup-Escande et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Loup-Escande E, Burkhardt J-M, Richir S (2011) Anticiper et Evaluer l’Utilité dans la Conception Ergonomique des Technologies Emergentes: Une Revue. Le Travail Humain 76:27–55" href="/article/10.1007/s10055-014-0255-z#ref-CR41" id="ref-link-section-d79199e3892">2011</a>). The improvements of the OpenNI or Microsoft’s libraries will also enable our solution to gradually extend its potential, even if the proposed interactions can today cover the most common needs in VE.</p><p>Concerning the latency of our system, even if the value may seem significant in absolute terms, in the questionnaires filled out by the participants, no participant has been disturbed or noted the latency between their movement and the action performed on the screen. Maybe participants did not noticed this latency also because none of the interactions required to make movements needs to be fast. As moves are made at normal speed, the difference between the actual movement and the movement in the 3D environment is not really noticeable. The optimization of the code using GPGPU has the potential to bring back the latency to an effective not perceptive level. New devices like Microsoft Kinect 2 are hoped to provide shorter latencies, greater resolution, and closer range (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical robots: technical and surgical skill comparison to current robotic masters. Surg Endosc 1–8. doi:&#xA;                    10.1007/s00464-013-3383-8&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR30" id="ref-link-section-d79199e3898">2014</a>).</p><p>The potential applications are numerous and go far beyond only the VR field. We can imagine using gestures to control computers and navigate through applications. Uses may also be extended to health care and particularly the rehabilitation of upper limbs or in areas such as home automation where it would be possible to control different devices only with gestures (lower or raise lights, electric shutters, etc.). In art and music, it might be interesting to record the fingers movements of a virtuoso (Maes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Maes P-J, Amelynck D, Lesaffre M, Leman M, Arvind DK (2012) The “Conducting Master”: an interactive, real-time gesture monitoring system based on spatiotemporal motion templates. Int J Hum Comput Interact. doi:&#xA;                    10.1080/10447318.2012.720197&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0255-z#ref-CR42" id="ref-link-section-d79199e3904">2012</a>). These are just a few examples among many, but the range of possible applications is vast.</p><p>Ultimately, we believe that these new ways of interacting with the environment (real or virtual) will be part of our lives. It remains to improve motion capture algorithms, 3D cameras (higher resolution, higher refresh rate, improved accuracy, as promised in the Kinect2) and also improve the interaction modalities. These perspectives lead us to continue our efforts and our work in this exciting promising direction.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Argelaguet, C. Andujar, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Argelaguet F, Andujar C (2013) Special section on touching the 3rd dimension: a survey of 3D object selection " /><p class="c-article-references__text" id="ref-CR1">Argelaguet F, Andujar C (2013) Special section on touching the 3rd dimension: a survey of 3D object selection techniques for virtual environments. Comput Graph 37(3):121–136. doi:<a href="https://doi.org/10.1016/j.cag.2012.12.003">10.1016/j.cag.2012.12.003</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2012.12.003" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Special%20section%20on%20touching%20the%203rd%20dimension%3A%20a%20survey%20of%203D%20object%20selection%20techniques%20for%20virtual%20environments&amp;journal=Comput%20Graph&amp;doi=10.1016%2Fj.cag.2012.12.003&amp;volume=37&amp;issue=3&amp;pages=121-136&amp;publication_year=2013&amp;author=Argelaguet%2CF&amp;author=Andujar%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Beaudouin-Lafon M (2004) Designing interaction, not interfaces. In: Working conference on advanced visual inte" /><p class="c-article-references__text" id="ref-CR2">Beaudouin-Lafon M (2004) Designing interaction, not interfaces. In: Working conference on advanced visual interfaces, Gallipoli, Italy, pp 15–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berard F, Ip J, Benovoy M, El-Shimy D, Blum JR, Cooperstock JR (2009) Did minority report get it wrong? Superi" /><p class="c-article-references__text" id="ref-CR3">Berard F, Ip J, Benovoy M, El-Shimy D, Blum JR, Cooperstock JR (2009) Did minority report get it wrong? Superiority of the mouse over 3D input devices in a 3D placement task. In: 12th IFIP TC 13 international conference on human–computer interaction: part II, Uppsala, Sweden. Springer, Berlin, pp 400–414</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immers" /><p class="c-article-references__text" id="ref-CR4">Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Symposium on interactive 3D graphics. ACM, pp 35–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint mot" /><p class="c-article-references__text" id="ref-CR5">Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Virtual reality annual international symposium, 1997, IEEE 1997, 1–5 Mar 1997, pp 45–52, 215. doi:<a href="https://doi.org/10.1109/vrais.1997.583043">10.1109/vrais.1997.583043</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: " /><p class="c-article-references__text" id="ref-CR6">Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Paper presented at the proceedings of the ACM symposium on virtual reality software and technology, London, United Kingdom</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Bowman, E. Kruijff, J. LaViola, I. Poupyrev, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Bowman DA, Kruijff E, LaViola J, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison Wesley Lon" /><p class="c-article-references__text" id="ref-CR7">Bowman DA, Kruijff E, LaViola J, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison Wesley Longman Publishing Co., Inc., Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20theory%20and%20practice&amp;publication_year=2004&amp;author=Bowman%2CDA&amp;author=Kruijff%2CE&amp;author=LaViola%2CJ&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Callahan J, Hopkins D, Weiser M, Shneiderman B (1988) An empirical comparison of pie vs. linear menus. In: SIG" /><p class="c-article-references__text" id="ref-CR8">Callahan J, Hopkins D, Weiser M, Shneiderman B (1988) An empirical comparison of pie vs. linear menus. In: SIGCHI conference on human factors in computing systems, Washington, DC, United States. ACM, pp 95–100. doi:<a href="https://doi.org/10.1145/57167.57182">10.1145/57167.57182</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Chen, SJ. Mountford, A. Sellen, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Chen M, Mountford SJ, Sellen A (1988) A study in interactive 3-D rotation using 2-D control devices. SIGGRAPH " /><p class="c-article-references__text" id="ref-CR9">Chen M, Mountford SJ, Sellen A (1988) A study in interactive 3-D rotation using 2-D control devices. SIGGRAPH Comput Graph 22(4):121–129. doi:<a href="https://doi.org/10.1145/54852.378497">10.1145/54852.378497</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F378456.378497" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20study%20in%20interactive%203-D%20rotation%20using%202-D%20control%20devices&amp;journal=SIGGRAPH%20Comput%20Graph&amp;doi=10.1145%2F54852.378497&amp;volume=22&amp;issue=4&amp;pages=121-129&amp;publication_year=1988&amp;author=Chen%2CM&amp;author=Mountford%2CSJ&amp;author=Sellen%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coquillart S, Fuchs P, Grosjean J, Hachet M, Bechmann D, Stenberger L (2003) Les Techniques d’Interaction pour" /><p class="c-article-references__text" id="ref-CR10">Coquillart S, Fuchs P, Grosjean J, Hachet M, Bechmann D, Stenberger L (2003) Les Techniques d’Interaction pour les Primitives Comportementales Virtuelles. In: Traité de la réalité virtuelle: volume 2, L’Interfaçage, l’Immersion et l’Interaction, vol 2, 3 edn. Les Presses de l’École des Mines de Paris, p 332</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Paper presented a" /><p class="c-article-references__text" id="ref-CR11">Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Vancouver, British Columbia, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dewaele G, Devernay F, Horaud R (2004) Hand motion from 3D point trajectories and a smooth surface model. In: " /><p class="c-article-references__text" id="ref-CR12">Dewaele G, Devernay F, Horaud R (2004) Hand motion from 3D point trajectories and a smooth surface model. In: 8th European conference on computer vision, Prague, Czech Republic. Springer, pp 495–507</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Dorner, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Dorner B (1994) Chasing the colour glove: visual hand tracking. Simon Fraser University, Burnaby" /><p class="c-article-references__text" id="ref-CR13">Dorner B (1994) Chasing the colour glove: visual hand tracking. Simon Fraser University, Burnaby</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Chasing%20the%20colour%20glove%3A%20visual%20hand%20tracking&amp;publication_year=1994&amp;author=Dorner%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Elmezain M, Al-Hamadi A, Appenrodt J, Michaelis B (2008) A hidden Markov model-based continuous gesture recogn" /><p class="c-article-references__text" id="ref-CR14">Elmezain M, Al-Hamadi A, Appenrodt J, Michaelis B (2008) A hidden Markov model-based continuous gesture recognition system for hand motion trajectory. In: 19th international conference on pattern recognition, Tampa, FL, USA. IEEE, pp 1–4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Fiorentino, R. Radkowski, C. Stritzke, A. Uva, G. Monno, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Fiorentino M, Radkowski R, Stritzke C, Uva A, Monno G (2013) Design review of CAD assemblies using bimanual na" /><p class="c-article-references__text" id="ref-CR15">Fiorentino M, Radkowski R, Stritzke C, Uva A, Monno G (2013) Design review of CAD assemblies using bimanual natural interface. Int J Interact Des Manuf 7(4):249–260. doi:<a href="https://doi.org/10.1007/s12008-012-0179-3">10.1007/s12008-012-0179-3</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs12008-012-0179-3" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20review%20of%20CAD%20assemblies%20using%20bimanual%20natural%20interface&amp;journal=Int%20J%20Interact%20Des%20Manuf&amp;doi=10.1007%2Fs12008-012-0179-3&amp;volume=7&amp;issue=4&amp;pages=249-260&amp;publication_year=2013&amp;author=Fiorentino%2CM&amp;author=Radkowski%2CR&amp;author=Stritzke%2CC&amp;author=Uva%2CA&amp;author=Monno%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs P, Mathieu H (2003) Les Interfaces Spécifiques de la Localisation Corporelle - Introduction. In: Traité " /><p class="c-article-references__text" id="ref-CR16">Fuchs P, Mathieu H (2003) Les Interfaces Spécifiques de la Localisation Corporelle - Introduction. In: Traité de la Réalité Virtuelle: volume 2, L’interfaçage, l’immersion et l’interaction, vol 2, 3 edn. Les Presses de l’École des Mines de Paris, pp 93–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs P, Moreau G (2003) Traité de la Réalité Virtuelle, vol 2, 2 edn. Les Presses de l’Ecole des Mines de Par" /><p class="c-article-references__text" id="ref-CR17">Fuchs P, Moreau G (2003) Traité de la Réalité Virtuelle, vol 2, 2 edn. Les Presses de l’Ecole des Mines de Paris</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geebelen G, Maesen S, Cuypers T, Bekaert P (2010) Real-time hand tracking with a colored glove. In: 3D Stereo " /><p class="c-article-references__text" id="ref-CR18">Geebelen G, Maesen S, Cuypers T, Bekaert P (2010) Real-time hand tracking with a colored glove. In: 3D Stereo media, Luik, Belgium</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Gratzel, T. Fong, S. Grange, C. Baur, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Gratzel C, Fong T, Grange S, Baur C (2004) A non-contact mouse for surgeon–computer interaction. Technol Healt" /><p class="c-article-references__text" id="ref-CR19">Gratzel C, Fong T, Grange S, Baur C (2004) A non-contact mouse for surgeon–computer interaction. Technol Health Care 12(3):245–257</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20non-contact%20mouse%20for%20surgeon%E2%80%93computer%20interaction&amp;journal=Technol%20Health%20Care&amp;volume=12&amp;issue=3&amp;pages=245-257&amp;publication_year=2004&amp;author=Gratzel%2CC&amp;author=Fong%2CT&amp;author=Grange%2CS&amp;author=Baur%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Guiard, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model." /><p class="c-article-references__text" id="ref-CR20">Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Mot Behav 9(4):486–517</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F00222895.1987.10735426" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asymmetric%20division%20of%20labor%20in%20human%20skilled%20bimanual%20action%3A%20the%20kinematic%20chain%20as%20a%20model&amp;journal=J%20Mot%20Behav&amp;volume=9&amp;issue=4&amp;pages=486-517&amp;publication_year=1987&amp;author=Guiard%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haik E, Barker T, Sapsford J, Trainis S (2002) Investigation into effective navigation in desktop virtual inte" /><p class="c-article-references__text" id="ref-CR21">Haik E, Barker T, Sapsford J, Trainis S (2002) Investigation into effective navigation in desktop virtual interfaces. In: Paper presented at the seventh international conference on 3D Web technology, Tempe, Arizona, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Hand, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hand C (1997) A survey of 3D interaction techniques. Comput Graph Forum 16(5):269–281" /><p class="c-article-references__text" id="ref-CR22">Hand C (1997) A survey of 3D interaction techniques. Comput Graph Forum 16(5):269–281</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2F1467-8659.00194" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%203D%20interaction%20techniques&amp;journal=Comput%20Graph%20Forum&amp;volume=16&amp;issue=5&amp;pages=269-281&amp;publication_year=1997&amp;author=Hand%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hassanpour R, Shahbahrami A, Wong S (2008) Adaptive Gaussian mixture model for skin color segmentation. Int J " /><p class="c-article-references__text" id="ref-CR23">Hassanpour R, Shahbahrami A, Wong S (2008) Adaptive Gaussian mixture model for skin color segmentation. Int J Comput Inf Sci Eng 2(2):1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hayward V, Astley OR (1996) Performance measures for haptic interfaces. In: 7th International symposium on rob" /><p class="c-article-references__text" id="ref-CR24">Hayward V, Astley OR (1996) Performance measures for haptic interfaces. In: 7th International symposium on robotics research. Springer, pp 195–207</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Homma, E-I. Takenaka, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Homma K, Takenaka E-I (1985) An image processing method for feature extraction of space-occupying lesions. J N" /><p class="c-article-references__text" id="ref-CR25">Homma K, Takenaka E-I (1985) An image processing method for feature extraction of space-occupying lesions. J Nucl Med 26(1985):1472–1477</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image%20processing%20method%20for%20feature%20extraction%20of%20space-occupying%20lesions&amp;journal=J%20Nucl%20Med&amp;volume=26&amp;issue=1985&amp;pages=1472-1477&amp;publication_year=1985&amp;author=Homma%2CK&amp;author=Takenaka%2CE-I">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Hong, W. Woo, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Hong D, Woo W (2006) A 3D vision-based ambient user interface. Int J Hum Comput Interact 20(3):271–284. doi:10" /><p class="c-article-references__text" id="ref-CR26">Hong D, Woo W (2006) A 3D vision-based ambient user interface. Int J Hum Comput Interact 20(3):271–284. doi:<a href="https://doi.org/10.1207/s15327590ijhc2003_6">10.1207/s15327590ijhc2003_6</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327590ijhc2003_6" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%203D%20vision-based%20ambient%20user%20interface&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1207%2Fs15327590ijhc2003_6&amp;volume=20&amp;issue=3&amp;pages=271-284&amp;publication_year=2006&amp;author=Hong%2CD&amp;author=Woo%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Hürst, C. Wezel, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Hürst W, Wezel C (2013) Gesture-based interaction via finger tracking for mobile augmented reality. Multimed T" /><p class="c-article-references__text" id="ref-CR27">Hürst W, Wezel C (2013) Gesture-based interaction via finger tracking for mobile augmented reality. Multimed Tools Appl 62(1):233–258. doi:<a href="https://doi.org/10.1007/s11042-011-0983-y">10.1007/s11042-011-0983-y</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11042-011-0983-y" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gesture-based%20interaction%20via%20finger%20tracking%20for%20mobile%20augmented%20reality&amp;journal=Multimed%20Tools%20Appl&amp;doi=10.1007%2Fs11042-011-0983-y&amp;volume=62&amp;issue=1&amp;pages=233-258&amp;publication_year=2013&amp;author=H%C3%BCrst%2CW&amp;author=Wezel%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jaehong L, Heon G, Hyungchan K, Jungmin K, Hyoungrae K, Hakil K (2013) Interactive manipulation of 3D objects " /><p class="c-article-references__text" id="ref-CR28">Jaehong L, Heon G, Hyungchan K, Jungmin K, Hyoungrae K, Hakil K (2013) Interactive manipulation of 3D objects using Kinect for visualization tools in education. In: Control, automation and systems (ICCAS), 2013 13th international conference on, 20–23 Oct 2013, pp 1220–1222. doi:<a href="https://doi.org/10.1109/iccas.2013.6704175">10.1109/iccas.2013.6704175</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Khan A, Mordatch I, Fitzmaurice G, Matejka J, Kurtenbach G (2008) ViewCube: a 3D orientation indicator and con" /><p class="c-article-references__text" id="ref-CR29">Khan A, Mordatch I, Fitzmaurice G, Matejka J, Kurtenbach G (2008) ViewCube: a 3D orientation indicator and controller. In: Paper presented at the proceedings of the 2008 symposium on interactive 3D graphics and games, Redwood City, California</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical" /><p class="c-article-references__text" id="ref-CR30">Kim Y, Leonard S, Shademan A, Krieger A, Kim PW (2014) Kinect technology for hand tracking control of surgical robots: technical and surgical skill comparison to current robotic masters. Surg Endosc 1–8. doi:<a href="https://doi.org/10.1007/s00464-013-3383-8">10.1007/s00464-013-3383-8</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Klein, F. Guéniat, L. Pastur, F. Vernier, T. Isenberg, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Klein T, Guéniat F, Pastur L, Vernier F, Isenberg T (2012) A design study of direct-touch interaction for expl" /><p class="c-article-references__text" id="ref-CR31">Klein T, Guéniat F, Pastur L, Vernier F, Isenberg T (2012) A design study of direct-touch interaction for exploratory 3D scientific visualization. Comput Graph Forum 31(3pt3):1225–1234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2012.03115.x" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20design%20study%20of%20direct-touch%20interaction%20for%20exploratory%203D%20scientific%20visualization&amp;journal=Comput%20Graph%20Forum&amp;volume=31&amp;issue=3pt3&amp;pages=1225-1234&amp;publication_year=2012&amp;author=Klein%2CT&amp;author=Gu%C3%A9niat%2CF&amp;author=Pastur%2CL&amp;author=Vernier%2CF&amp;author=Isenberg%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Pauly M, Greiner G (" /><p class="c-article-references__text" id="ref-CR32">Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Pauly M, Greiner G (eds) 30th annual conference of the European association for computer graphics, Munich, Germany, pp 119–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Lange, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Lange R (2000) 3D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-techno" /><p class="c-article-references__text" id="ref-CR33">Lange R (2000) 3D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology. University of Siegen, Siegen</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20time-of-flight%20distance%20measurement%20with%20custom%20solid-state%20image%20sensors%20in%20CMOS%2FCCD-technology&amp;publication_year=2000&amp;author=Lange%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Lange, P. Seitz, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Lange R, Seitz P (2000) Seeing distance—a fast time-of-flight 3D camera. Sens Rev 20(3):212–217" /><p class="c-article-references__text" id="ref-CR34">Lange R, Seitz P (2000) Seeing distance—a fast time-of-flight 3D camera. Sens Rev 20(3):212–217</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2F02602280010372359" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Seeing%20distance%E2%80%94a%20fast%20time-of-flight%203D%20camera&amp;journal=Sens%20Rev&amp;volume=20&amp;issue=3&amp;pages=212-217&amp;publication_year=2000&amp;author=Lange%2CR&amp;author=Seitz%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Laurel BK (1986) Interface as mimesis. User centered system design: new perspectives on human–computer interac" /><p class="c-article-references__text" id="ref-CR35">Laurel BK (1986) Interface as mimesis. User centered system design: new perspectives on human–computer interaction. Lawrence Erlbaum Associates, Hillsdale</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LeapMotion (2012). https://www.leapmotion.com&#xA;                " /><p class="c-article-references__text" id="ref-CR36">LeapMotion (2012). <a href="https://www.leapmotion.com">https://www.leapmotion.com</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lempereur M (2008) Simulation du Mouvement d’Entrée dans un Véhicule Automobile. Université de Valenciennes et" /><p class="c-article-references__text" id="ref-CR37">Lempereur M (2008) Simulation du Mouvement d’Entrée dans un Véhicule Automobile. Université de Valenciennes et du Hainaut-Cambrésis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levesque JC, Laurendeau D, Mokhtari M (2011) Bimanual gestural interface for virtual environments. In: Virtual" /><p class="c-article-references__text" id="ref-CR38">Levesque JC, Laurendeau D, Mokhtari M (2011) Bimanual gestural interface for virtual environments. In: Virtual reality conference (VR), 2011 IEEE, 19–23 March 2011, pp 223–224. doi:<a href="https://doi.org/10.1109/vr.2011.5759479">10.1109/vr.2011.5759479</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lin J, Sun Q, Li G, He Y (2013) SnapBlocks: a snapping interface for assembling toy blocks with XBOX Kinect. M" /><p class="c-article-references__text" id="ref-CR39">Lin J, Sun Q, Li G, He Y (2013) SnapBlocks: a snapping interface for assembling toy blocks with XBOX Kinect. Multimed Tools Appl 1–24. doi:<a href="https://doi.org/10.1007/s11042-013-1690-7">10.1007/s11042-013-1690-7</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Livingston MA, Sebastian J, Ai Z, Decker JW (2012) Performance measurements for the Microsoft Kinect skeleton." /><p class="c-article-references__text" id="ref-CR40">Livingston MA, Sebastian J, Ai Z, Decker JW (2012) Performance measurements for the Microsoft Kinect skeleton. In: IEEE virtual reality, 2012. IEEE Computer Society, pp 119–120. doi:<a href="https://doi.org/10.1109/vr.2012.6180911">10.1109/vr.2012.6180911</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Loup-Escande E, Burkhardt J-M, Richir S (2011) Anticiper et Evaluer l’Utilité dans la Conception Ergonomique d" /><p class="c-article-references__text" id="ref-CR41">Loup-Escande E, Burkhardt J-M, Richir S (2011) Anticiper et Evaluer l’Utilité dans la Conception Ergonomique des Technologies Emergentes: Une Revue. Le Travail Humain 76:27–55</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P-J. Maes, D. Amelynck, M. Lesaffre, M. Leman, DK. Arvind, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Maes P-J, Amelynck D, Lesaffre M, Leman M, Arvind DK (2012) The “Conducting Master”: an interactive, real-time" /><p class="c-article-references__text" id="ref-CR42">Maes P-J, Amelynck D, Lesaffre M, Leman M, Arvind DK (2012) The “Conducting Master”: an interactive, real-time gesture monitoring system based on spatiotemporal motion templates. Int J Hum Comput Interact. doi:<a href="https://doi.org/10.1080/10447318.2012.720197">10.1080/10447318.2012.720197</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20%E2%80%9CConducting%20Master%E2%80%9D%3A%20an%20interactive%2C%20real-time%20gesture%20monitoring%20system%20based%20on%20spatiotemporal%20motion%20templates&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1080%2F10447318.2012.720197&amp;publication_year=2012&amp;author=Maes%2CP-J&amp;author=Amelynck%2CD&amp;author=Lesaffre%2CM&amp;author=Leman%2CM&amp;author=Arvind%2CDK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AH. Mason, BJ. Bernardin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Mason AH, Bernardin BJ (2009) Vision for performance in virtual environments: the role of feedback timing. Int" /><p class="c-article-references__text" id="ref-CR43">Mason AH, Bernardin BJ (2009) Vision for performance in virtual environments: the role of feedback timing. Int J Hum Comput Interact 25(8):785–805. doi:<a href="https://doi.org/10.1080/10447310903025529">10.1080/10447310903025529</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10447310903025529" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision%20for%20performance%20in%20virtual%20environments%3A%20the%20role%20of%20feedback%20timing&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1080%2F10447310903025529&amp;volume=25&amp;issue=8&amp;pages=785-805&amp;publication_year=2009&amp;author=Mason%2CAH&amp;author=Bernardin%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. May, K. Pervoelz, H. Surmann, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="May S, Pervoelz K, Surmann H (2007) 3D cameras: 3D computer vision of wide scope. Int J Adv Rob Syst 4:181–202" /><p class="c-article-references__text" id="ref-CR44">May S, Pervoelz K, Surmann H (2007) 3D cameras: 3D computer vision of wide scope. Int J Adv Rob Syst 4:181–202</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20cameras%3A%203D%20computer%20vision%20of%20wide%20scope&amp;journal=Int%20J%20Adv%20Rob%20Syst&amp;volume=4&amp;pages=181-202&amp;publication_year=2007&amp;author=May%2CS&amp;author=Pervoelz%2CK&amp;author=Surmann%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Paper presented at the proceedings" /><p class="c-article-references__text" id="ref-CR45">McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Paper presented at the proceedings of the 2009 symposium on interactive 3D graphics and games, Boston, Massachusetts</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill CS Dept" /><p class="c-article-references__text" id="ref-CR46">Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill CS Dept</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mistry P, Maes P, Chang L (2009) WUW—Wear Ur World: a wearable gestural interface. In: 27th international conf" /><p class="c-article-references__text" id="ref-CR47">Mistry P, Maes P, Chang L (2009) WUW—Wear Ur World: a wearable gestural interface. In: 27th international conference extended abstracts on human factors in computing systems, Boston, MA, USA. ACM, pp 4111–4116. doi:<a href="https://doi.org/10.1145/1520340.1520626">10.1145/1520340.1520626</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moerman C, Marchal D, Grisoni L (2012) Drag’n go: simple and fast navigation in virtual environment. In: 2012 " /><p class="c-article-references__text" id="ref-CR48">Moerman C, Marchal D, Grisoni L (2012) Drag’n go: simple and fast navigation in virtual environment. In: 2012 IEEE Symposium on 3D user interfaces (3DUI), 4–5 March 2012, pp 15–18. doi:<a href="https://doi.org/10.1109/3dui.2012.6184178">10.1109/3dui.2012.6184178</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mohr D, Zachmann G (2009) Continuous edge gradient-based template matching for articulated object. In: Interna" /><p class="c-article-references__text" id="ref-CR49">Mohr D, Zachmann G (2009) Continuous edge gradient-based template matching for articulated object. In: International conference on computer vision theory and applications, Lisbon, Portugal, pp 519–524</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mohr D, Zachmann G (2010a) FAST: Fast adaptive silhouette area based template matching. In: Labrosse F, Zwigge" /><p class="c-article-references__text" id="ref-CR50">Mohr D, Zachmann G (2010a) FAST: Fast adaptive silhouette area based template matching. In: Labrosse F, Zwiggelaar R, Liu Y, Tiddeman B (eds) British machine vision conference. BMVA Press, pp 39.31–39.12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mohr D, Zachmann G (2010b) Silhouette area based similarity measure for template matching in constant time. In" /><p class="c-article-references__text" id="ref-CR51">Mohr D, Zachmann G (2010b) Silhouette area based similarity measure for template matching in constant time. In: Proceedings of the 6th international conference on articulated motion and deformable objects, Mallorca, Spain. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Movea (2009) MotionPod™ Technology. http://movea.com/healthcare/motion_pod/index.html. Accessed 26 May 2009" /><p class="c-article-references__text" id="ref-CR52">Movea (2009) MotionPod™ Technology. <a href="http://movea.com/healthcare/motion_pod/index.html">http://movea.com/healthcare/motion_pod/index.html</a>. Accessed 26 May 2009</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nan X, Zhang Z, Zhang N, Guo F, He Y, Guan L (2013) vDesign: toward Image Segmentation and composition in CAVE" /><p class="c-article-references__text" id="ref-CR53">Nan X, Zhang Z, Zhang N, Guo F, He Y, Guan L (2013) vDesign: toward Image Segmentation and composition in CAVE using finger interaction. In: International conference on signal and information processing (ChinaSIP), Beijing, China, 6–10 July 2013. IEEE, pp 461–465</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nedel LP, Dal Sasso Freitas CM, Jacob LJ, Pimenta MS (2003) Testing the use of egocentric interactive techniqu" /><p class="c-article-references__text" id="ref-CR54">Nedel LP, Dal Sasso Freitas CM, Jacob LJ, Pimenta MS (2003) Testing the use of egocentric interactive techniques in immersive virtual environments. In: Paper presented at the 9 h IFIP TC13 international conference on human–computer interaction, Zurich, Switzerland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Norman, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Norman DA (1988) The psychology of everyday things. Basic Books, New York" /><p class="c-article-references__text" id="ref-CR55">Norman DA (1988) The psychology of everyday things. Basic Books, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20psychology%20of%20everyday%20things&amp;publication_year=1988&amp;author=Norman%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouhaddi H, Horain P (1998) Conception et Ajustement d’un Modèle 3D Articulé de la Main. In: 6èmes Journées de " /><p class="c-article-references__text" id="ref-CR56">Ouhaddi H, Horain P (1998) Conception et Ajustement d’un Modèle 3D Articulé de la Main. In: 6èmes Journées de Travail du GT Réalité Virtuelle, Issy-les-Moulineaux, France, 13/03/1998 1998, pp 83–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pamplona VF, Fernandes LAF, Prauchner J, Nedel LP, Oliveira MM (2008) The image-based data glove. In: 10th sym" /><p class="c-article-references__text" id="ref-CR57">Pamplona VF, Fernandes LAF, Prauchner J, Nedel LP, Oliveira MM (2008) The image-based data glove. In: 10th symposium on virtual and augmented reality, João Pessoa, Brazil, pp 204–211</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pedersoli F, Benini S, Adami N, Leonardi R (2014) XKin: an open source framework for hand pose and gesture rec" /><p class="c-article-references__text" id="ref-CR58">Pedersoli F, Benini S, Adami N, Leonardi R (2014) XKin: an open source framework for hand pose and gesture recognition using Kinect. Vis Comput 1–16. doi:<a href="https://doi.org/10.1007/s00371-014-0921-x">10.1007/s00371-014-0921-x</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poor GM, Tomlinson BJ, Guinness D, Jaffee SD, Leventhal LM, Zimmerman G, Klopfer DS (2013) Tangible or gestura" /><p class="c-article-references__text" id="ref-CR59">Poor GM, Tomlinson BJ, Guinness D, Jaffee SD, Leventhal LM, Zimmerman G, Klopfer DS (2013) Tangible or gestural: comparing tangible vs. Kinect™ interactions with an object manipulation task. In: International conference on tangible, embedded and embodied interaction, Barcelona, Spain</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping " /><p class="c-article-references__text" id="ref-CR60">Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Paper presented at the 9th annual ACM symposium on user interface software and technology, Seattle, Washington, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, T. Ichikawa, S. Weghorst, M. Billinghurst, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Poupyrev I, Ichikawa T, Weghorst S, Billinghurst M (1998) Egocentric object manipulation in virtual environmen" /><p class="c-article-references__text" id="ref-CR61">Poupyrev I, Ichikawa T, Weghorst S, Billinghurst M (1998) Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques. Comput Graph Forum 17(3):41–52. doi:<a href="https://doi.org/10.1111/1467-8659.00252">10.1111/1467-8659.00252</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2F1467-8659.00252" aria-label="View reference 61">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Egocentric%20object%20manipulation%20in%20virtual%20environments%3A%20empirical%20evaluation%20of%20interaction%20techniques&amp;journal=Comput%20Graph%20Forum&amp;doi=10.1111%2F1467-8659.00252&amp;volume=17&amp;issue=3&amp;pages=41-52&amp;publication_year=1998&amp;author=Poupyrev%2CI&amp;author=Ichikawa%2CT&amp;author=Weghorst%2CS&amp;author=Billinghurst%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Prisacariu VA, Reid I (2011) Robust 3D hand tracking for human computer interaction. In: IEEE international co" /><p class="c-article-references__text" id="ref-CR62">Prisacariu VA, Reid I (2011) Robust 3D hand tracking for human computer interaction. In: IEEE international conference on automatic face &amp; gesture recognition and workshops, pp 368–375</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raheja JL, Chaudhary A, Singal K (2011) Tracking of fingertips and centers of palm using KINECT. In: Computati" /><p class="c-article-references__text" id="ref-CR63">Raheja JL, Chaudhary A, Singal K (2011) Tracking of fingertips and centers of palm using KINECT. In: Computational intelligence, modelling and simulation (CIMSiM), 2011 third international conference on, 20–22 Sept 2011, pp 248–252. doi:<a href="https://doi.org/10.1109/CIMSim.2011.51">10.1109/CIMSim.2011.51</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Paper pres" /><p class="c-article-references__text" id="ref-CR64">Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Paper presented at the proceedings of the 1992 symposium on interactive 3D graphics, Cambridge, Massachusetts, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rodríguez N, Wikström R, Lilius J, Cuéllar M, Delgado Calvo Flores M (2013) Understanding movement and interac" /><p class="c-article-references__text" id="ref-CR65">Rodríguez N, Wikström R, Lilius J, Cuéllar M, Delgado Calvo Flores M (2013) Understanding movement and interaction: an ontology for Kinect-based 3D depth sensors. In: Urzaiz G, Ochoa S, Bravo J, Chen L, Oliveira J (eds) Ubiquitous computing and ambient intelligence. Context-awareness and context-driven interaction, vol 8276. Lecture Notes in Computer Science. Springer International Publishing, pp 254–261. doi:<a href="https://doi.org/10.1007/978-3-319-03176-7_33">10.1007/978-3-319-03176-7_33</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schlattmann M, Klein R (2009) Efficient bimanual symmetric 3D manipulation for markerless hand-tracking. In: P" /><p class="c-article-references__text" id="ref-CR66">Schlattmann M, Klein R (2009) Efficient bimanual symmetric 3D manipulation for markerless hand-tracking. In: Paper presented at the virtual reality international conference, Laval, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schlattmann M, Na Nakorn T, Klein R (2009) 3D interaction techniques for 6 DOF markerless hand-tracking. In: I" /><p class="c-article-references__text" id="ref-CR67">Schlattmann M, Na Nakorn T, Klein R (2009) 3D interaction techniques for 6 DOF markerless hand-tracking. In: International conference on computer graphics, visualization and computer vision, Plzen-Bory, Czech Republic</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Shen, SK. Ong, AYC. Nee, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Shen Y, Ong SK, Nee AYC (2011) Vision-based hand interaction in augmented reality environment. Int J Hum Compu" /><p class="c-article-references__text" id="ref-CR68">Shen Y, Ong SK, Nee AYC (2011) Vision-based hand interaction in augmented reality environment. Int J Hum Comput Interact 27(6):523–544. doi:<a href="https://doi.org/10.1080/10447318.2011.555297">10.1080/10447318.2011.555297</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10447318.2011.555297" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision-based%20hand%20interaction%20in%20augmented%20reality%20environment&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1080%2F10447318.2011.555297&amp;volume=27&amp;issue=6&amp;pages=523-544&amp;publication_year=2011&amp;author=Shen%2CY&amp;author=Ong%2CSK&amp;author=Nee%2CAYC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper pre" /><p class="c-article-references__text" id="ref-CR69">Soh J, Choi Y, Park Y, Yang HS (2013) User-friendly 3D object manipulation gesture using Kinect. In: Paper presented at the proceedings of the 12th ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry, Hong Kong, Hong Kong</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-" /><p class="c-article-references__text" id="ref-CR70">Song P, Goh WB, Hutama W, Fu C-W, Liu X (2012) A handle bar metaphor for virtual object manipulation with mid-air interaction. In: Paper presented at the SIGCHI conference on human factors in computing systems, Austin, Texas, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulat" /><p class="c-article-references__text" id="ref-CR71">Song J, Cho S, Baek S-Y, Lee K, Bang H (2014) GaFinC: gaze and finger control interface for 3D model manipulation in CAD application. Comput Aided Des 46:239–245</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LD. Stefano, M. Marchionni, S. Mattoccia, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Stefano LD, Marchionni M, Mattoccia S (2004) A fast area-based stereo matching algorithm. Image Vis Comput 22(" /><p class="c-article-references__text" id="ref-CR72">Stefano LD, Marchionni M, Mattoccia S (2004) A fast area-based stereo matching algorithm. Image Vis Comput 22(12):983–1005</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2004.03.009" aria-label="View reference 72">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20fast%20area-based%20stereo%20matching%20algorithm&amp;journal=Image%20Vis%20Comput&amp;volume=22&amp;issue=12&amp;pages=983-1005&amp;publication_year=2004&amp;author=Stefano%2CLD&amp;author=Marchionni%2CM&amp;author=Mattoccia%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stenger B (2006) Template-based hand pose recognition using multiple cues. In: Computer vision, Hyderabad, Ind" /><p class="c-article-references__text" id="ref-CR73">Stenger B (2006) Template-based hand pose recognition using multiple cues. In: Computer vision, Hyderabad, India, 2006. Lecture Notes in Computer Science. Springer, pp 551–560. doi:<a href="https://doi.org/10.1007/11612704_55">10.1007/11612704_55</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Stenger, A. Thayananthan, PHS. Torr, R. Cipolla, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Stenger B, Thayananthan A, Torr PHS, Cipolla R (2006) Model-based hand tracking using a hierarchical bayesian " /><p class="c-article-references__text" id="ref-CR74">Stenger B, Thayananthan A, Torr PHS, Cipolla R (2006) Model-based hand tracking using a hierarchical bayesian filter. IEEE Trans Pattern Anal Mach Intell 28(9):1372–1384. doi:<a href="https://doi.org/10.1109/tpami.2006.189">10.1109/tpami.2006.189</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2006.189" aria-label="View reference 74">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Model-based%20hand%20tracking%20using%20a%20hierarchical%20bayesian%20filter&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;doi=10.1109%2Ftpami.2006.189&amp;volume=28&amp;issue=9&amp;pages=1372-1384&amp;publication_year=2006&amp;author=Stenger%2CB&amp;author=Thayananthan%2CA&amp;author=Torr%2CPHS&amp;author=Cipolla%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Paper pr" /><p class="c-article-references__text" id="ref-CR75">Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Paper presented at the proceedings of the SIGCHI conference on human factors in computing systems, Denver, Colorado, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Theobalt, I. Albrecht, J. Haber, M. Magnor, H-P. Seidel, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Theobalt C, Albrecht I, Haber J, Magnor M, Seidel H-P (2004) Pitching a baseball: tracking high-speed motion w" /><p class="c-article-references__text" id="ref-CR76">Theobalt C, Albrecht I, Haber J, Magnor M, Seidel H-P (2004) Pitching a baseball: tracking high-speed motion with multi-exposure images. ACM Trans Graph 23(3):540–547. doi:<a href="https://doi.org/10.1145/1015706.1015758">10.1145/1015706.1015758</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1015706.1015758" aria-label="View reference 76">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 76 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pitching%20a%20baseball%3A%20tracking%20high-speed%20motion%20with%20multi-exposure%20images&amp;journal=ACM%20Trans%20Graph&amp;doi=10.1145%2F1015706.1015758&amp;volume=23&amp;issue=3&amp;pages=540-547&amp;publication_year=2004&amp;author=Theobalt%2CC&amp;author=Albrecht%2CI&amp;author=Haber%2CJ&amp;author=Magnor%2CM&amp;author=Seidel%2CH-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Tokatli, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Tokatli A (2005) 3D hand tracking in video sequences. Middle East Technical University, Çankaya" /><p class="c-article-references__text" id="ref-CR77">Tokatli A (2005) 3D hand tracking in video sequences. Middle East Technical University, Çankaya</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 77 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20hand%20tracking%20in%20video%20sequences&amp;publication_year=2005&amp;author=Tokatli%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Tosas, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Tosas M (2006) Visual articulated hand tracking for interactive surfaces. University of Nottingham, Nottingham" /><p class="c-article-references__text" id="ref-CR78">Tosas M (2006) Visual articulated hand tracking for interactive surfaces. University of Nottingham, Nottingham</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 78 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20articulated%20hand%20tracking%20for%20interactive%20surfaces&amp;publication_year=2006&amp;author=Tosas%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tosas M, Bai L (2007) Virtual touch screen: a vision-based interactive surface. In: 9th IASTED international c" /><p class="c-article-references__text" id="ref-CR79">Tosas M, Bai L (2007) Virtual touch screen: a vision-based interactive surface. In: 9th IASTED international conference on computer graphics and imaging, Innsbruck, Austria. ACTA Press, pp 81–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tricot A, Plégat-Soutjis F, Camps J-F, Amiel A, Lutz G, Morcillo A (2003) Utilité, utilisabilité, acceptabilit" /><p class="c-article-references__text" id="ref-CR80">Tricot A, Plégat-Soutjis F, Camps J-F, Amiel A, Lutz G, Morcillo A (2003) Utilité, utilisabilité, acceptabilité : interpréter les relations entre trois dimensions de l’évaluation des EIAH. In: Conférence EIAH 2003, Strabourg, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Tubiana, A-I. Kapandji, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Tubiana R, Kapandji A-I (1991) Affections Neurologiques. Traité de Chirurgie de la Main, vol 4. Masson, Paris," /><p class="c-article-references__text" id="ref-CR81">Tubiana R, Kapandji A-I (1991) Affections Neurologiques. Traité de Chirurgie de la Main, vol 4. Masson, Paris, pp 56–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 81 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Trait%C3%A9%20de%20Chirurgie%20de%20la%20Main&amp;pages=56-58&amp;publication_year=1991&amp;author=Tubiana%2CR&amp;author=Kapandji%2CA-I">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Ueda, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ueda E (2003) Hand pose estimation for vision-based human interface. IEEE Trans Industr Electron 50(4):676–684" /><p class="c-article-references__text" id="ref-CR82">Ueda E (2003) Hand pose estimation for vision-based human interface. IEEE Trans Industr Electron 50(4):676–684</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIE.2003.814758" aria-label="View reference 82">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 82 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20pose%20estimation%20for%20vision-based%20human%20interface&amp;journal=IEEE%20Trans%20Industr%20Electron&amp;volume=50&amp;issue=4&amp;pages=676-684&amp;publication_year=2003&amp;author=Ueda%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ughini CS, Blanco FR, Pinto FM, Freitas CM, Nedel LP (2006) EyeScope: a 3D interaction technique for accurate " /><p class="c-article-references__text" id="ref-CR83">Ughini CS, Blanco FR, Pinto FM, Freitas CM, Nedel LP (2006) EyeScope: a 3D interaction technique for accurate object selection in immersive environments. In: SBC symposium on virtual reality 2006, pp 77–88</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Unseok L, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking wit" /><p class="c-article-references__text" id="ref-CR84">Unseok L, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Asia Pacific conference on computer human interaction</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="RY. Wang, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Wang RY (2011) Practical color-based motion capture. Massachusetts Institute of Technology, Cambridge" /><p class="c-article-references__text" id="ref-CR85">Wang RY (2011) Practical color-based motion capture. Massachusetts Institute of Technology, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 85 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Practical%20color-based%20motion%20capture&amp;publication_year=2011&amp;author=Wang%2CRY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper pres" /><p class="c-article-references__text" id="ref-CR86">Wang R, Paris S, Popovic J (2011) 6D hands: Markerless hand-tracking for computer aided design. In: Paper presented at the proceedings of the 24th annual ACM symposium on user interface software and technology, Santa Barbara, California, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Ware, S. Osborne, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Ware C, Osborne S (1990) Exploration and virtual camera control in virtual three dimensional environments. SIG" /><p class="c-article-references__text" id="ref-CR87">Ware C, Osborne S (1990) Exploration and virtual camera control in virtual three dimensional environments. SIGGRAPH Comput Graph 24(2):175–183. doi:<a href="https://doi.org/10.1145/91394.91442">10.1145/91394.91442</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F91394.91442" aria-label="View reference 87">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploration%20and%20virtual%20camera%20control%20in%20virtual%20three%20dimensional%20environments&amp;journal=SIGGRAPH%20Comput%20Graph&amp;doi=10.1145%2F91394.91442&amp;volume=24&amp;issue=2&amp;pages=175-183&amp;publication_year=1990&amp;author=Ware%2CC&amp;author=Osborne%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="BJ. Winer, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="Winer BJ (1971) Statistical principles in experimental design, 2nd edn. McGraw-Hill, New York City" /><p class="c-article-references__text" id="ref-CR88">Winer BJ (1971) Statistical principles in experimental design, 2nd edn. McGraw-Hill, New York City</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 88 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20principles%20in%20experimental%20design&amp;publication_year=1971&amp;author=Winer%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Winkler S, Yu H, Zhou Z (2007) Tangible reality desktop for digital media management. In: Engineering reality " /><p class="c-article-references__text" id="ref-CR89">Winkler S, Yu H, Zhou Z (2007) Tangible reality desktop for digital media management. In: Engineering reality of virtual reality, San Jose. SPIE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yeo H-S, Lee B-G, Lim H (2013) Hand tracking and gesture recognition system for human–computer interaction usi" /><p class="c-article-references__text" id="ref-CR90">Yeo H-S, Lee B-G, Lim H (2013) Hand tracking and gesture recognition system for human–computer interaction using low-cost hardware. Multimed Tools Appl 1–29. doi:<a href="https://doi.org/10.1007/s11042-013-1501-1">10.1007/s11042-013-1501-1</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface" /><p class="c-article-references__text" id="ref-CR91">Zhang Z, McInerney T, Zhang N, Guan L (2014) A cave based 3D immersive interactive city with gesture interface. In: Paper presented at the 22nd WSCG international conference on computer graphics, visualization and computer vision, Plzen, Czech Republic, June 2–5, 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Zhou, H. Hu, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Zhou H, Hu H (2008) Human motion tracking for rehabilitation—a survey. Biomed Signal Process Control 3(1):1–18" /><p class="c-article-references__text" id="ref-CR92">Zhou H, Hu H (2008) Human motion tracking for rehabilitation—a survey. Biomed Signal Process Control 3(1):1–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.bspc.2007.09.001" aria-label="View reference 92">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 92 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20motion%20tracking%20for%20rehabilitation%E2%80%94a%20survey&amp;journal=Biomed%20Signal%20Process%20Control&amp;volume=3&amp;issue=1&amp;pages=1-18&amp;publication_year=2008&amp;author=Zhou%2CH&amp;author=Hu%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Zhou, Y. Junsong, M. Jingjing, Z. Zhengyou, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Zhou R, Junsong Y, Jingjing M, Zhengyou Z (2013) Robust part-based hand gesture recognition using Kinect senso" /><p class="c-article-references__text" id="ref-CR93">Zhou R, Junsong Y, Jingjing M, Zhengyou Z (2013) Robust part-based hand gesture recognition using Kinect sensor. IEEE Trans Multimedia 15(5):1110–1120. doi:<a href="https://doi.org/10.1109/tmm.2013.2246148">10.1109/tmm.2013.2246148</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMM.2013.2246148" aria-label="View reference 93">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1279.39024" aria-label="View reference 93 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 93 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20part-based%20hand%20gesture%20recognition%20using%20Kinect%20sensor&amp;journal=IEEE%20Trans%20Multimedia&amp;doi=10.1109%2Ftmm.2013.2246148&amp;volume=15&amp;issue=5&amp;pages=1110-1120&amp;publication_year=2013&amp;author=Zhou%2CR&amp;author=Junsong%2CY&amp;author=Jingjing%2CM&amp;author=Zhengyou%2CZ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0255-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LAMPA - Arts &amp; Métiers ParisTech, 2, Bd du Ronceray, 49000, Angers, France</p><p class="c-article-author-affiliation__authors-list">Franck Hernoux &amp; Olivier Christmann</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Franck-Hernoux"><span class="c-article-authors-search__title u-h3 js-search-name">Franck Hernoux</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Franck+Hernoux&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Franck+Hernoux" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Franck+Hernoux%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Olivier-Christmann"><span class="c-article-authors-search__title u-h3 js-search-name">Olivier Christmann</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Olivier+Christmann&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Olivier+Christmann" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Olivier+Christmann%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0255-z/email/correspondent/c1/new">Franck Hernoux</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix</h3><p>See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0255-z#Fig10">10</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0255-z/MediaObjects/10055_2014_255_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Software solutions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0255-z/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20seamless%20solution%20for%203D%20real-time%20interaction%3A%20design%20and%20evaluation&amp;author=Franck%20Hernoux%20et%20al&amp;contentID=10.1007%2Fs10055-014-0255-z&amp;publication=1359-4338&amp;publicationDate=2014-11-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0255-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0255-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Hernoux, F., Christmann, O. A seamless solution for 3D real-time interaction: design and evaluation.
                    <i>Virtual Reality</i> <b>19, </b>1–20 (2015). https://doi.org/10.1007/s10055-014-0255-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0255-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-05-06">06 May 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-10-30">30 October 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11-05">05 November 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-03">March 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0255-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0255-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Seamless solution</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Real time</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D camera</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0255-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=255;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

