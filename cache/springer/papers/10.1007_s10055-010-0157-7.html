<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Immersive interactive reality: Internet-based on-demand VR for cultura"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents an Internet-based virtual reality technology, called panoramic broadcasting (PanoCAST) where multiple viewers share an experience yet each having full control of what they see..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Immersive interactive reality: Internet-based on-demand VR for cultural presentation"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-03-10"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents an Internet-based virtual reality technology, called panoramic broadcasting (PanoCAST) where multiple viewers share an experience yet each having full control of what they see independent from other viewers. Our solution was developed for telepresence-based cultural presentation and entertainment services. The core architecture involves a compact spherical vision system that compresses and transmits data from multiple digital video sources to a central host computer, which in turn distributes the recorded information among multiple render- and streaming servers for personalized viewing over the Internet or mobile devices. In addition, using advanced computer vision, tracking and animation features, the PanoCAST architecture introduces the notion of Clickable Content Management (CCM), where each visual element in the image becomes a source for providing further information, educational content and cultural detail. Key contributions of our application to advance the state-of-the-art include bringing streaming panoramic video onto mobile platforms, an advanced tracking interface to turn visual elements into sources of interaction, physical simulation to combine the benefits of panoramic video with that of 3D models and animated, photo-realistic faces to help users express their emotions in shared online virtual cultural experiences as well as a feedback mechanism in such environments. Therefore, we argue that the PanoCAST system offers a low-cost and economical solution for personalized content distribution and as such it can serve as a unified basis for novel applications many of which are demonstrated in this paper."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-03-10"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="267"/>

    <meta name="prism.endingPage" content="278"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0157-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0157-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0157-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0157-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Immersive interactive reality: Internet-based on-demand VR for cultural presentation"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2011/11"/>

    <meta name="citation_online_date" content="2010/03/10"/>

    <meta name="citation_firstpage" content="267"/>

    <meta name="citation_lastpage" content="278"/>

    <meta name="citation_article_type" content="SI: Cultural Technology"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0157-7"/>

    <meta name="DOI" content="10.1007/s10055-010-0157-7"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0157-7"/>

    <meta name="description" content="This paper presents an Internet-based virtual reality technology, called panoramic broadcasting (PanoCAST) where multiple viewers share an experience yet e"/>

    <meta name="dc.creator" content="Barnab&#225;s Tak&#225;cs"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adobe Flash player (2009) 
                    http://www.get.adobe.com/flashplayer/
                    
                  
                        "/>

    <meta name="citation_reference" content="Baldwin J, Basu A, Zhang H (1999) Panoramic video with predictive windows for telepresence. In: Proceedings of IEEE international conference on robotics"/>

    <meta name="citation_reference" content="Digital Elite Inc. (2009) 
                    http://www.digitalElite.US.com
                    
                  
                        "/>

    <meta name="citation_reference" content="Fraunhofer HHI, Omnicam (2009) 
                    http://www.hhi.fraunhofer.de/
                    
                  
                        "/>

    <meta name="citation_reference" content="FunIcons (2009) 
                    http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Google Street View (2009) 
                    http://www.maps.google.com/intl/en_us/help/maps/streetview/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Blue-c: a spatially immersive display and 3D video portal for telepresence; citation_author=M Gross, S W&#252;rmlin; citation_volume=22; citation_issue=3; citation_publication_date=2003; citation_pages=819-827; citation_doi=10.1145/882262.882350; citation_id=CR9"/>

    <meta name="citation_reference" content="Immersive Interactive Film (2009) &#8220;Metamorphosis&#8221; film adaptation of Franz Kafka&#8217;s short story. 
                    http://www.digitalelite.net/Metamorphosis.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Immersive Media, Dodeca Camera (2009) 
                    http://www.immersive-video.eu/en
                    
                  
                        "/>

    <meta name="citation_reference" content="Interactive Panoramic Cinema (2005) 
                    http://www.turbulence.org/blog/archives/000562.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Kaidan, Go Pano (2009) 
                    http://www.kaidan.com/
                    
                  
                        "/>

    <meta name="citation_reference" content="Kimber D, Foote J (2001) FlyAbout: spatially indexed panoramic video. In: Proceedings of ACM multimedia, Ottawa"/>

    <meta name="citation_reference" content="Majumder A, Gopi M, Seales B, Fuchs H (1999) Immersive teleconferencing: a new algorithm to generate seamless panoramic video imagery. In: ACM multimedia, pp 169&#8211;178"/>

    <meta name="citation_reference" content="Neumann U, Pintaric T, Rizzo AA (2000) Immersive panoramic video. In: Proceedings of ACM international conference on multimedia, pp. 493&#8211;494"/>

    <meta name="citation_reference" content="Ng K-T, Chan S-C, Shum H-Y (2001) The compression issues of panoramic video. In: Proceedings of international symposium on intelligent multimedia, video &amp; speech processing"/>

    <meta name="citation_reference" content="Ouglov A, Hjelsvold R (2004) Panoramic video in video-mediated education, in storage and retrieval methods and applications for multimedia 2005. In: Lienhart RW, Babaguchi N, Chang EY (eds) Proceedings of the SPIE, vol 5682, pp 326&#8211;336"/>

    <meta name="citation_reference" content="PanoCAST Inc. (2009) 
                    http://www.PanoCAST.com
                    
                  , Interactive Demonstrations, 
                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html
                    
                  , Supported I/O Devices 
                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php
                    
                  
                        "/>

    <meta name="citation_reference" content="Patil R, Rybski PE, Kanade T, Veloso MM (2004) People detection and tracking in high resolution panoramic video mosaic. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems, Sendai, Japan"/>

    <meta name="citation_reference" content="Point Grey Research, LadyBug2 &amp; LadyBug3 Cameras (2009) 
                    http://www.ptgrey.com/products/ladybug2/
                    
                  
                        "/>

    <meta name="citation_reference" content="Pryor L, Rizzo AS (2000) User directed news, 
                    http://www.imsc.usc.edu/research/project/udn/udn_nsf.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="Qin N, Song D (2007) On-demand sharing of a high-resolution panorama video from networked robotic cameras. In: Proceedings of IEEE international conference on intelligent robots and systems, San Diego"/>

    <meta name="citation_reference" content="QuickTime, Apple (2009) 
                    http://www.apple.com/quicktime/
                    
                  
                        "/>

    <meta name="citation_reference" content="Remote Reality (2009) 
                    http://www.remotereality.com/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Low-cost telepresence for collaborative virtual environments; citation_author=SM Rhee, R Ziegler; citation_volume=13; citation_issue=1; citation_publication_date=2007; citation_pages=156-166; citation_doi=10.1109/TVCG.2007.17; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_title=Issues for application development using immersive HMD 360 degree panoramic video environments; citation_inbook_title=Usability evaluation and interface design; citation_publication_date=2001; citation_pages=792-796; citation_id=CR26; citation_author=AA Rizzo; citation_author=U Neumann; citation_author=T Pintaric; citation_author=M Norden; citation_publisher=L.A. Erlbaum"/>

    <meta name="citation_reference" content="citation_title=Immersive 360-degree panoramic video environments: research on creating useful and usable applications; citation_inbook_title=Human&#8211;computer interaction: theory and practice; citation_publication_date=2003; citation_pages=1233-1237; citation_id=CR27; citation_author=AA Rizzo; citation_author=K Ghahremani; citation_author=L Pryor; citation_author=S Gardner; citation_publisher=L.A. Erlbaum"/>

    <meta name="citation_reference" content="Rizzo AA, Pryor L, Matheis R, Schultheis M, Ghahremani1&#160;K, Sey A (2004) Memory assessment using graphics-based and panoramic video virtual environments. In: Proceedings of 5th international conference on disability, virtual reality &amp; association technology, Oxford, UK"/>

    <meta name="citation_reference" content="Sun X, Kimber D, Foote J, Manjunath B (2002) Detecting path intersections in panoramic video. In: IEEE international conference on multimedia"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Special education and rehabilitation: teaching and healing with interactive graphics; citation_author=B Tak&#225;cs; citation_volume=25; citation_issue=5; citation_publication_date=2005; citation_pages=40-48; citation_doi=10.1109/MCG.2005.113; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=Cognitive, mental and physical rehabilitation using a configurable virtual reality system; citation_author=B Tak&#225;cs; citation_volume=5; citation_issue=4; citation_publication_date=2006; citation_pages=1-12; citation_id=CR31"/>

    <meta name="citation_reference" content="Tak&#225;cs B (2007) PanoMOBI: a panoramic mobile entertainment system. In: Proceedings of ICEC07, Shanghai, China"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=How and why affordable virtual reality shapes the future education; citation_author=B Tak&#225;cs; citation_volume=7; citation_issue=1; citation_publication_date=2008; citation_pages=53-66; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Virtual human interface: a photo-realistic digital human; citation_author=B Tak&#225;cs, B Kiss; citation_volume=23; citation_issue=5; citation_publication_date=2003; citation_pages=38-45; citation_doi=10.1109/MCG.2003.1231176; citation_id=CR34"/>

    <meta name="citation_reference" content="Tak&#225;cs B, Fromherz T, Tice S, Metaxas D (1999) Digital clones and virtual celebrities: facial tracking, gesture recognition and animation for the movie industry. In: Proceedings of ICCV&#8217;99, Corfu, Greece"/>

    <meta name="citation_reference" content="Transparent Telepresence Research Group (2007) 
                    http://www.telepresence.strath.ac.uk/telepresence.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="VirMED (2009) Virtual reality medicine, 
                    http://www.virmed.net
                    
                  
                        "/>

    <meta name="citation_reference" content="Virtual Museum Volos, Greece (2009) 
                    http://www.digitalelite.us.com/Pages/PanoCAST/VirtualMuseum.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Zheng J, Zhang Y, Ni G (2007) A fast global motion estimation method for panoramic video coding. In: Advances in multimedia information processing&#8212;PCM"/>

    <meta name="citation_author" content="Barnab&#225;s Tak&#225;cs"/>

    <meta name="citation_author_email" content="BTakacs@digitalCustom.com"/>

    <meta name="citation_author_institution" content="Digital Elite Inc., Los Angeles, USA"/>

    <meta name="citation_author_institution" content="Technical University of Budapest (BME), Budapest, Hungary"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0157-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0157-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Immersive interactive reality: Internet-based on-demand VR for cultural presentation"/>
        <meta property="og:description" content="This paper presents an Internet-based virtual reality technology, called panoramic broadcasting (PanoCAST) where multiple viewers share an experience yet each having full control of what they see independent from other viewers. Our solution was developed for telepresence-based cultural presentation and entertainment services. The core architecture involves a compact spherical vision system that compresses and transmits data from multiple digital video sources to a central host computer, which in turn distributes the recorded information among multiple render- and streaming servers for personalized viewing over the Internet or mobile devices. In addition, using advanced computer vision, tracking and animation features, the PanoCAST architecture introduces the notion of Clickable Content Management (CCM), where each visual element in the image becomes a source for providing further information, educational content and cultural detail. Key contributions of our application to advance the state-of-the-art include bringing streaming panoramic video onto mobile platforms, an advanced tracking interface to turn visual elements into sources of interaction, physical simulation to combine the benefits of panoramic video with that of 3D models and animated, photo-realistic faces to help users express their emotions in shared online virtual cultural experiences as well as a feedback mechanism in such environments. Therefore, we argue that the PanoCAST system offers a low-cost and economical solution for personalized content distribution and as such it can serve as a unified basis for novel applications many of which are demonstrated in this paper."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Immersive interactive reality: Internet-based on-demand VR for cultural presentation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0157-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Immersive interactive reality, Cultural presentation, Virtual reality on-demand, Virtual human interface, Panoramic broadcasting (PanoCAST)","kwrd":["Immersive_interactive_reality","Cultural_presentation","Virtual_reality_on-demand","Virtual_human_interface","Panoramic_broadcasting_(PanoCAST)"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0157-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0157-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=157;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0157-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Immersive interactive reality: Internet-based on-demand VR for cultural presentation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0157-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0157-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Cultural Technology</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-03-10" itemprop="datePublished">10 March 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Immersive interactive reality: Internet-based on-demand VR for cultural presentation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Barnab_s-Tak_cs" data-author-popup="auth-Barnab_s-Tak_cs" data-corresp-id="c1">Barnabás Takács<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Digital Elite Inc." /><meta itemprop="address" content="Digital Elite Inc., Los Angeles, CA, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technical University of Budapest (BME)" /><meta itemprop="address" content="grid.6759.d, 0000000121800451, Technical University of Budapest (BME), Budapest, Hungary" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">267</span>–<span itemprop="pageEnd">278</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">750 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0157-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents an Internet-based virtual reality technology, called panoramic broadcasting (PanoCAST) where multiple viewers share an experience yet each having full control of what they see independent from other viewers. Our solution was developed for telepresence-based cultural presentation and entertainment services. The core architecture involves a compact spherical vision system that compresses and transmits data from multiple digital video sources to a central host computer, which in turn distributes the recorded information among multiple render- and streaming servers for personalized viewing over the Internet or mobile devices. In addition, using advanced computer vision, tracking and animation features, the PanoCAST architecture introduces the notion of Clickable Content Management (CCM), where each visual element in the image becomes a source for providing further information, educational content and cultural detail. Key contributions of our application to advance the state-of-the-art include bringing streaming panoramic video onto mobile platforms, an advanced tracking interface to turn visual elements into sources of interaction, physical simulation to combine the benefits of panoramic video with that of 3D models and animated, photo-realistic faces to help users express their emotions in shared online virtual cultural experiences as well as a feedback mechanism in such environments. Therefore, we argue that the PanoCAST system offers a low-cost and economical solution for personalized content distribution and as such it can serve as a unified basis for novel applications many of which are demonstrated in this paper.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Virtual reality (VR) is making a come back and is becoming a main stream application, a new form of cultural presentation, education and entertainment. With the advent of low-cost computer accessories and peripheral devices (Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Takács B (2008) How and why affordable virtual reality shapes the future education. Int J Virtual Real 7(1):53–66" href="/article/10.1007/s10055-010-0157-7#ref-CR33" id="ref-link-section-d81376e297">2008</a>) that have been inspired by and deployed in the game industry, today’s computer users may already have access to VR glasses with built in trackers that cost below $500 and these prices are expected to drop even further as the use of VR offers more intuitive interfaces and applications areas (Takács and Kiss <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Takács B, Kiss B (2003) Virtual human interface: a photo-realistic digital human. IEEE Comput Graph Appl 23(5):38–45" href="/article/10.1007/s10055-010-0157-7#ref-CR34" id="ref-link-section-d81376e300">2003</a>; Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Comput Graph Appl 25(5):40–48" href="/article/10.1007/s10055-010-0157-7#ref-CR30" id="ref-link-section-d81376e303">2005</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Takács B (2006) Cognitive, mental and physical rehabilitation using a configurable virtual reality system. Int J Virtual Real 5(4):1–12" href="/article/10.1007/s10055-010-0157-7#ref-CR31" id="ref-link-section-d81376e306">2006</a>).</p><p>In this paper, we argue that the communication infrastructure that can form the basis of on-demand virtual reality over the Internet and mobile platforms already exists. Specifically, our panoramic broadcasting system combines three key elements which, when combined with personalized streaming capabilities, offer a new form of interactive media that advances the state-of-the-art and radically changes the way we “consume” cultural information. Specifically, we extend the boundaries of video-on-demand to <i>Immersive Interactive Reality</i> (<i>IIR</i>) where not only the content, but also the personalized point of view of the user may be changed interactively. More specifically, we move away from 3D models and traditional VR and use (1) <i>spherical video</i> in combination with (2) <i>tracking technology</i> to create <i>clickable content</i> and (3) <i>physical simulation</i> for advanced interaction. This, however, requires a new approach to content delivery as users no longer just “tap into” one of a few media streams being broadcasted simultaneously, but rather an independent view is generated for each of them allowing a level of customization and shared experience never before possible.</p><p>The remainder of this paper is structured as follows: Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0157-7#Sec2">2</a> briefly reviews the state-of-the-art and different uses of panoramic video, highlighting our proposed contribution to the field. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0157-7#Sec3">3</a> describes in detail the system architecture and operation of our solution, while Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0157-7#Sec4">4</a> further clarifies how visual elements are turned into sources of interaction (clickable content) with the help of tracking. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0157-7#Sec5">5</a> brings another novel aspect to our approach by arguing how animated facial feedback is implemented and used as a means to help the learning process. It also describes a case study for cultural presentation using a real-life example of a new kind of virtual museum. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0157-7#Sec6">6</a> contains our conclusion.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">From panoramic video to telepresence and immersive interactive reality</h2><div class="c-article-section__content" id="Sec2-content"><p>Our technological approach, <i>called Immersive Interactive Reality</i>, relates to two active fields of research, namely <i>Telepresence and Panoramic Video</i>. We use these technologies in a novel manner and turn real-life scenes into interactive experiences that <i>motivate cultural visitors</i> to explore and experience virtual sites in a playful and exploratory manner. <i>Telepresence</i> is defined as “the experience of or impression of being present at a location remote from one’s own immediate environment” (Rhee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Rhee SM, Ziegler R et al (2007) Low-cost telepresence for collaborative virtual environments. IEEE Trans Vis Comput Graph 13(1):156–166" href="/article/10.1007/s10055-010-0157-7#ref-CR25" id="ref-link-section-d81376e369">2007</a>; Transparent Telepresence Research Group <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Transparent Telepresence Research Group (2007) &#xA;                    http://www.telepresence.strath.ac.uk/telepresence.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR36" id="ref-link-section-d81376e373">2007</a>). To achieve this high level of immersion, a number of sensory stimuli, such as visual, auditory, tactile and perhaps olfactory, need to be captured, encoded, transmitted and subsequently presented or rendered to the user in a real-time and fully transparent manner. The ultimate goal of such technical solutions, however, is to provide their users with the most up-to-date information and control over a remote environment. For the purposes of our current work, we focused on presenting visual and auditory stimuli to multiple users at a time with the help of panoramic video recorded on location in a museum, for example. <i>Panoramic Video</i> has its roots in a similar technology (i.e., panoramic photography) such as QuickTime VR (QuickTime and Apple <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="QuickTime, Apple (2009) &#xA;                    http://www.apple.com/quicktime/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR23" id="ref-link-section-d81376e379">2009</a>), which offers the possibility of looking around in a static, but photo-realistic environment. Recently, video-based solutions that employ multiple camera heads to create a seamlessly stitched video scene have become the focus of active research. Early versions of these panoramic recording systems used a single sensor with special optics or mirrors (Kaidan and Go Pano <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kaidan, Go Pano (2009) &#xA;                    http://www.kaidan.com/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR13" id="ref-link-section-d81376e382">2009</a>; Remote Reality <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Remote Reality (2009) &#xA;                    http://www.remotereality.com/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR24" id="ref-link-section-d81376e385">2009</a>) and thus offer rather low resolution, often not enough for detailed view of a scene needed for cultural experiences. Much research focused on technical aspects of such scenery, including compression, coding and streaming (Ng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Ng K-T, Chan S-C, Shum H-Y (2001) The compression issues of panoramic video. In: Proceedings of international symposium on intelligent multimedia, video &amp; speech processing" href="/article/10.1007/s10055-010-0157-7#ref-CR17" id="ref-link-section-d81376e388">2001</a>; Zheng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Zheng J, Zhang Y, Ni G (2007) A fast global motion estimation method for panoramic video coding. In: Advances in multimedia information processing—PCM" href="/article/10.1007/s10055-010-0157-7#ref-CR39" id="ref-link-section-d81376e392">2007</a>) or people detection (Patil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Patil R, Rybski PE, Kanade T, Veloso MM (2004) People detection and tracking in high resolution panoramic video mosaic. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems, Sendai, Japan" href="/article/10.1007/s10055-010-0157-7#ref-CR19" id="ref-link-section-d81376e395">2004</a>) and early tele-immersion applications (Baldwin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Baldwin J, Basu A, Zhang H (1999) Panoramic video with predictive windows for telepresence. In: Proceedings of IEEE international conference on robotics" href="/article/10.1007/s10055-010-0157-7#ref-CR5" id="ref-link-section-d81376e398">1999</a>; Majumder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Majumder A, Gopi M, Seales B, Fuchs H (1999) Immersive teleconferencing: a new algorithm to generate seamless panoramic video imagery. In: ACM multimedia, pp 169–178" href="/article/10.1007/s10055-010-0157-7#ref-CR15" id="ref-link-section-d81376e401">1999</a>). For cultural and educational applications, however, high-resolution capture hardware is required to allow visitors to freely zoom in on details and explore these spaces with the help of virtual reality glasses.</p><p>High-resolution recording systems employ multiple-head camera hardware and record data to a set of digital tape recorders or store digitally on a hard drive from which surround images are stitched together without depth information or direct point-to-point correspondence. Early versions of these worked typically offline, and they were somewhat bulky and difficult to use (Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Gross M, Würmlin S et al (2003) Blue-c: a spatially immersive display and 3D video portal for telepresence. ACM Trans Graph 22(3):819–827" href="/article/10.1007/s10055-010-0157-7#ref-CR9" id="ref-link-section-d81376e407">2003</a>; Pryor and Rizzo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Pryor L, Rizzo AS (2000) User directed news, &#xA;                    http://www.imsc.usc.edu/research/project/udn/udn_nsf.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR21" id="ref-link-section-d81376e410">2000</a>). Frequently, they also did not provide full spherical video (only cylindrical), a feature required, though, by many new applications for full immersion. More recently, new advances in CCD resolution and compression technology have created the opportunity to design and build cameras that can capture and process high fidelity views in real time (Fraunhofer and Omnicam <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Fraunhofer HHI, Omnicam (2009) &#xA;                    http://www.hhi.fraunhofer.de/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR6" id="ref-link-section-d81376e413">2009</a>) and eventually almost complete spherical video images with the help of compact designs (Immersive Media and Dodeca Camera <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Immersive Media, Dodeca Camera (2009) &#xA;                    http://www.immersive-video.eu/en&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR11" id="ref-link-section-d81376e416">2009</a>; Point Grey Research, LadyBug2 &amp; LadyBug3 Cameras <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Point Grey Research, LadyBug2 &amp; LadyBug3 Cameras (2009) &#xA;                    http://www.ptgrey.com/products/ladybug2/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR20" id="ref-link-section-d81376e419">2009</a>), one of which we used in our experiments as well. There are several problems, however, with these solutions. First, because transmitting and sharing high-resolution panoramic images still remain a bottleneck (Qin and Song <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Qin N, Song D (2007) On-demand sharing of a high-resolution panorama video from networked robotic cameras. In: Proceedings of IEEE international conference on intelligent robots and systems, San Diego" href="/article/10.1007/s10055-010-0157-7#ref-CR22" id="ref-link-section-d81376e423">2007</a>), and second because interacting with these videos is limited to the experience of looking around (Rizzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Rizzo AA, Neumann U, Pintaric T, Norden M (2001) Issues for application development using immersive HMD 360 degree panoramic video environments. In: Smith MJ, Salvendy G, Harris D, Koubek RJ (eds) Usability evaluation and interface design, vol 1. L.A. Erlbaum, New York, pp 792–796" href="/article/10.1007/s10055-010-0157-7#ref-CR26" id="ref-link-section-d81376e426">2001</a>). While some companies offer dedicated hardware for streaming (Immersive Media and Dodeca Camera <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Immersive Media, Dodeca Camera (2009) &#xA;                    http://www.immersive-video.eu/en&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR11" id="ref-link-section-d81376e429">2009</a>), price/performance ratio still limits these solutions from entering the general public space and does not offer low-cost solutions to multiple viewers sharing scenes simultaneously.</p><p>Applications in the domain of panoramic video along with the ability to look around while recording a complete view first focused on <i>surveillance and security</i> (see references above). Subsequently, the technology to <i>spatially address</i> (and/or geo-reference) indexing large data sets of panoramic video was developed (Kimber and Foote <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kimber D, Foote J (2001) FlyAbout: spatially indexed panoramic video. In: Proceedings of ACM multimedia, Ottawa" href="/article/10.1007/s10055-010-0157-7#ref-CR14" id="ref-link-section-d81376e441">2001</a>; Sun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Sun X, Kimber D, Foote J, Manjunath B (2002) Detecting path intersections in panoramic video. In: IEEE international conference on multimedia" href="/article/10.1007/s10055-010-0157-7#ref-CR29" id="ref-link-section-d81376e444">2002</a>) and leads to services such as Goggle’s StreetView (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Google Street View (2009) &#xA;                    http://www.maps.google.com/intl/en_us/help/maps/streetview/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR8" id="ref-link-section-d81376e447">2009</a>). The advent of high visual fidelity cameras combined with dedicated communication channels has lead to a new form of <i>telepresence and teleconferencing</i> (Baldwin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Baldwin J, Basu A, Zhang H (1999) Panoramic video with predictive windows for telepresence. In: Proceedings of IEEE international conference on robotics" href="/article/10.1007/s10055-010-0157-7#ref-CR5" id="ref-link-section-d81376e454">1999</a>; Majumder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Majumder A, Gopi M, Seales B, Fuchs H (1999) Immersive teleconferencing: a new algorithm to generate seamless panoramic video imagery. In: ACM multimedia, pp 169–178" href="/article/10.1007/s10055-010-0157-7#ref-CR15" id="ref-link-section-d81376e457">1999</a>) and eventually high-resolution surround imagery also gave rise to novel entertainment possibilities including <i>immersive media</i> and <i>interactive cinema</i> (Interactive Panoramic Cinema <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Interactive Panoramic Cinema (2005) &#xA;                    http://www.turbulence.org/blog/archives/000562.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR12" id="ref-link-section-d81376e466">2005</a>; Immersive Interactive Film <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Immersive Interactive Film (2009) “Metamorphosis” film adaptation of Franz Kafka’s short story. &#xA;                    http://www.digitalelite.net/Metamorphosis.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR10" id="ref-link-section-d81376e470">2009</a>; Neumann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Neumann U, Pintaric T, Rizzo AA (2000) Immersive panoramic video. In: Proceedings of ACM international conference on multimedia, pp. 493–494" href="/article/10.1007/s10055-010-0157-7#ref-CR16" id="ref-link-section-d81376e473">2000</a>; Rizzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Rizzo AA, Ghahremani K, Pryor L, Gardner S (2003) Immersive 360-degree panoramic video environments: research on creating useful and usable applications. In: Jacko J, Stephanidis C (eds) Human–computer interaction: theory and practice, vol 1. L.A. Erlbaum, New York, pp 1233–1237" href="/article/10.1007/s10055-010-0157-7#ref-CR27" id="ref-link-section-d81376e476">2003</a>). Not surprisingly, <i>health care</i>, more specifically rehabilitation, followed, where traditional 3D worlds of VR have been in some cases replaced with surround video images for phobia treatment and physical rehabilitation with the underlying goal to increase the level of immersion for patients, lowering production costs and the ability to personalize treatments (Rizzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rizzo AA, Pryor L, Matheis R, Schultheis M, Ghahremani1 K, Sey A (2004) Memory assessment using graphics-based and panoramic video virtual environments. In: Proceedings of 5th international conference on disability, virtual reality &amp; association technology, Oxford, UK" href="/article/10.1007/s10055-010-0157-7#ref-CR28" id="ref-link-section-d81376e482">2004</a>; Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Comput Graph Appl 25(5):40–48" href="/article/10.1007/s10055-010-0157-7#ref-CR30" id="ref-link-section-d81376e485">2005</a>; VirMED <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="VirMED (2009) Virtual reality medicine, &#xA;                    http://www.virmed.net&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR37" id="ref-link-section-d81376e489">2009</a>). Last but not least <i>educational use</i> of panoramic videos in comparison with video-mediated approaches using regular video has argued that as the former has a wider field of view, the information density available to the students is increased helping the learning experience (Ouglov and Hjelsvold <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ouglov A, Hjelsvold R (2004) Panoramic video in video-mediated education, in storage and retrieval methods and applications for multimedia 2005. In: Lienhart RW, Babaguchi N, Chang EY (eds) Proceedings of the SPIE, vol 5682, pp 326–336" href="/article/10.1007/s10055-010-0157-7#ref-CR18" id="ref-link-section-d81376e495">2004</a>). <i>Cultural presentation</i> as well as gaming that is based upon panoramic video, interaction, immersive media and virtual reality has been—to our knowledge—largely underrepresented in the literature. Therefore, in this paper, we aim to bridge that gap and describe a system we have developed as a novel delivery platform of such content, called <i>Immersive Interactive Reality</i> (<i>IIR</i>), and show how it may extend the use of virtual reality as we know it today. To address the above requirements, we have designed and architecture that can redistribute panoramic (or spherical) video to multiple independent viewers each having control over their own respective point of view. To achieve this goal, we built a novel streaming architecture, called <i>PanoCAST</i> (panoramic broadcasting), on top of an advanced virtual reality environment (<i>Virtual Human Interface</i>; Digital Elite Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Digital Elite Inc. (2009) &#xA;                    http://www.digitalElite.US.com&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR1" id="ref-link-section-d81376e514">2009</a>; Takács and Kiss <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Takács B, Kiss B (2003) Virtual human interface: a photo-realistic digital human. IEEE Comput Graph Appl 23(5):38–45" href="/article/10.1007/s10055-010-0157-7#ref-CR34" id="ref-link-section-d81376e517">2003</a>; Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Comput Graph Appl 25(5):40–48" href="/article/10.1007/s10055-010-0157-7#ref-CR30" id="ref-link-section-d81376e520">2005</a>) and used this system to address many real-life applications many of which will be discussed later in this paper. Our approach <i>advances the state</i>-<i>off</i>-<i>the</i>-<i>art</i> in many aspects as described as follows:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>The PanoCAST architecture brings <i>panoramic video streaming onto mobile platforms and hand</i>-<i>held</i> devices, thereby extending the boundaries and accessibility of cultural and educational applications of this technology.</p>
                  </li>
                  <li>
                    <p>It offers a <i>low</i>-<i>cost distribution methodology</i> that uses a web browser, with a Flash player (Adobe Flash player <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Adobe Flash player (2009) &#xA;                    http://www.get.adobe.com/flashplayer/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR4" id="ref-link-section-d81376e560">2009</a>) to deliver virtual reality on-demand services. Once users download a simple interface program (Device Manager) that supports a range of interactive devices, including multiple VR headsets, they can experience and interact with fully immersive VR environments (3D models and/or panoramic video scenes) from the comfort of their own home. They may simply look around and share the experience with other viewers, navigate by virtually walking through the scenes with the help of low-cost sensors (PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e563">2009</a>) or interact with 3D overlaid elements via the <i>Clickable Content Manager.</i>
                              </p>
                  </li>
                  <li>
                    <p><i>Clickable Content Management</i> (<i>CCM</i>) is a technique that turns a large number of elements of a panoramic video into active links that subsequently offer advanced interaction possibilities. The simplest form of interaction, of course, is clicking on them to access further information about a specific detail (e.g., a painting on the wall). A more elaborate way employs <i>gesture recognition</i> to create a playful/game-like interface for exploring cultural knowledge. Finally, physical properties may be assigned to each of these elements, thus allowing the IIR system to seamlessly combine panoramic video with true 3D virtual models. To produce such content, we have developed <i>advanced tracking</i> capabilities. The output of our tracking system produces <i>sets of panels</i> that move along the scenery and follow objects as they appear or disappear. These panels possess <i>physical properties</i> and serve as input to a physics simulator.</p>
                  </li>
                  <li>
                    <p><i>Physical Simulation</i> for motivation and gaming turns each tracked panel into life-like elements with physical properties, such as weight, material structure and distance. Using this feature turns video elements into breakable surfaces (e.g., when touching or exploring a painting and what is beneath it in a game-like cultural environment), or in a more complex form may trigger the motion of true 3D elements, such as a ball that may be kicked around the corridor of a virtual museum or a stadium for that matter, or flocking birds made up of particle systems.</p>
                  </li>
                  <li>
                    <p><i>Animated Photo</i>-<i>realistic Faces</i> acting as agents maybe incorporated into the panoramic scenes in order to provide non-verbal feedback with the goal of motivating users, providing cues as well as helping the learning experience.</p>
                  </li>
                </ul>
                     </div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">PanoCAST system architecture for shared virtual experiences</h2><div class="c-article-section__content" id="Sec3-content"><p><i>PanoCAST</i> = Panoramic Broadcasting (PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e624">2009</a>). It is a technology originally designed to deliver telepresence-based entertainment services over mobile networks and the Internet and thereby allow viewers to experience the very feeling of being somewhere else from their physical location (Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Takács B (2007) PanoMOBI: a panoramic mobile entertainment system. In: Proceedings of ICEC07, Shanghai, China" href="/article/10.1007/s10055-010-0157-7#ref-CR32" id="ref-link-section-d81376e627">2007</a>). The basic concept of the architecture is as follows: To record and stream high fidelity spherical video, we employ a special camera system with six lenses packed into a tiny head unit. The images captured by the camera head are compressed and sent to our server computer in real-time delivering up to 30 frames per second, where they are mapped onto a corresponding sphere for visualization. The basic server architecture then employs a number of virtual cameras and assigns them to each user who connects from a mobile phone, thereby creating their own, personal view of the events the camera is seeing or has recorded. The motion of the virtual cameras is controllable via TCP/IP with the help of a script interface that assigns camera motion and pre-programmed actions to key codes on the mobile device. The host computer then generates the virtual views each users see and stream this information back to their location using RTSP protocol. This is demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig1">1</a>/Left. The spherical camera head (left) is placed at the remote site in an event where the user wishes to participate. The camera head captures the entire spherical surroundings of the camera with resolutions up to 3 K by 1.5 K pixels and adjustable frame rates of maximum 30 fps. These images are compressed in real time and transmitted to a remote computer over G-bit Ethernet connection or using the Internet, which decompresses the data stream and re-maps the spherical imagery onto the surface of a sphere locally. Finally, the personalized rendering engine of the viewer creates TV-like imagery and sends it to a <i>mobile device</i> with the help of a <i>virtual camera,</i> the motion of which is directly controlled by the keyboard actions of the user. The key idea behind our <i>PanoCAST</i> solution is based on distributing each user only what they currently should see instead of the entire scene they may be experiencing. While this reduces the computational needs on the receiver side (essentially needing only to decode streamed video and audio data) and send tracking information and camera control back in return, it places designers of the server architecture in a difficult position. To overcome these limitations, we devised an architecture shown as a box diagram in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig1">1</a>/Right. The <i>panoramic camera head</i> is connected via an optical cable to a <i>JPG compression</i> module, which transmits compressed image frames at video rates to a distribution server using IEEE fire wire standard. The role of the <i>distribution server</i> is to multiply the video data and prepare it for broadcast via a server farm. To maximize bus capacity and minimize synchronization problems, the distribution server broadcasts its imagery via <i>UDP protocol</i> to a number of <i>virtual camera servers</i>, each being responsible for a number of individually controlled cameras. Video data are then first encoded in MPEG format and subsequently distributed among a number of <i>streaming servers</i> using RTSP (Real-time Streaming Protocol) before sent out to individual clients over 3G mobile networks (or the Internet for Wifi service).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Functional overview of the PanoCAST interactive mobile entertainment (<i>Left</i>). Server-park and data flow architecture for independently controlled viewer experience (<i>Right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Assuming 3 Gbit/s connection, a streaming server is capable of servicing up to 100 simultaneous clients at any given moment. Again, the number of streaming servers can be scaled according to the need of the broadcast. Finally, independent image streams are synchronized with audio and arrive at the user site ready to be decoded and displayed. In the <i>PanoCAST</i> system, interaction occurs by key action on the mobile device whereas the users control the orientation and field of view of the camera to observe the remote event taking place. On the client side, this occurs using mobile player programmed in <i>Symbian</i> for maximum speed and performance or a simpler Java-based client for applications where synchronized sound and high-speed image transmission are not as critical. In addition, the <i>PanoCAST</i> streams may also be viewed seamlessly from a <i>web</i>-<i>based interface</i> (via Flash or Skype) thereby offering solutions that make this personalized content available to even broader range of audiences. An example of the entire content delivery architecture is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig2">2</a>. In this example, the camera was placed on stage to provide an unusual perspective of the band playing. The six raw images from the panoramic recording head are shown in the upper left corner. From these streams, a full spherical view of the scene was created and a number of virtual cameras image the scene each controlled independently by a different user. This is shown in the upper right-hand corner with six independent views stacked up serving as examples. The streaming server then encodes and distributes these streams to individual viewers in different formats, such as RTSP on 3G mobiles (shown lower left) or web-based clients (lower right). In the former case, the rotation and field of view of the camera may be controlled from the keypad of the mobile phone, while in the latter case, HTML-embedded keys and Java script commands serve the same purpose. Finally, in both situations, the user may click on the screen the result of which the <i>PanoCAST</i> system computes which object was selected and sends back information and web-links for later use. This provides added value for services based on this technology not only for entertainment, but e-commerce, education and many other fields of application. In the next section, we briefly discuss how the functionality of <i>CCM</i> was achieved using resources of the graphics card (GPU) and our advanced real-time image processing pipeline.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Panoramic imagery is passed from the camera head to individual users who view and interactively control their viewpoint as well as content using mobile clients or web-based players</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p><i>Shared Virtual Experiences</i> is another important feature of the <i>PanoCAST</i> architecture whereas it offers the possibility to small groups of users to jointly control a single virtual camera. Specifically, upon login, the mobile client can prompt the user whether he or she wants an individualized view (i.e., a virtual camera only controlled by themselves) or join a group view where up to 10 people jointly decide the direction the virtual camera is looking at. In this mode, each member of the group is depicted with a small icon (also clickable) as well as their action, e.g., turning the camera left or right, etc. If their mobile device is equipped with a camera, the <i>PanoCAST</i> system can accept these images and place them on the icons assigned to that user in the session. Needles to say that these images may be filtered to avoid unwanted content. In our case, a face detector algorithm ensures that only pictures that contain faces may be shown to the rest of the group. Finally, when viewing the same interactive panoramic content over the Internet instead of a mobile phone, a <i>Chat</i> interface is also provided for group members jointly controlling a camera allowing them to discuss their special interests. The key advantage of our solution is that it enables playing and streaming panoramic videos onto mobile devices, most notably 3G capable portable phones. In the classical distribution scenario, such content is delivered with the help of media servers transmitting the entire panoramic image even when the user is not looking at some of its portions. For enjoyable quality panoramic video, this requires 2 Mbps data rate from the camera to the distribution server and the same from the streaming servers to each viewer. Clearly, such data load would not be plausible on today’s architecture. On the other hand, our approach sends over the portion of the panoramic video image the user is looking at in any specific moment. Since each user has direct control over their own virtual cameras, they may choose to zoom in on details without loosing image on image fidelity or increasing bandwidth. This significantly reduces the data rate demand on the streaming servers, in fact requiring not more than what is generally needed for point-to-point video conferencing on mobile phones. Therefore, our solution fits into the current infrastructure while offering the possibility to viewers to look around in a scene and enjoy interactive cultural presentations in a new form.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4"><i>Clickable content management</i> and tracking</h2><div class="c-article-section__content" id="Sec4-content"><p>To enhance the functionality and the utilization of the basic immersive interaction experience, we have incorporated a set of real-time image processing algorithms that allow for tracking different elements as they move in the scene and help to correct artefacts, produce special effects on-demand. This advanced image processing architecture allows the <i>PanoCAST</i> system to implement what we call <i>CCM</i>. In simple terms, <i>CCM</i> means that whenever the user clicks on the scene, the rendering engine “fires a search ray” from the viewing camera and orders each visual elements as a function of their distance along this ray to find the closest one. Then, the algorithm returns the object’s name, the exact polygon it is intersected at and the texture coordinates of the intersection point itself. This information allows the system to assign high-level actions, such as displaying an image and/or text message when clicking on an object of interest or branching to a website where that object may be purchased or further information can be found. For easier handling, individual texture coordinates can be grouped by regions, and tracking algorithms—running in real-time or as a pre-processor for playback—are used to annotate each element of the scene during a broadcast. The final output of such algorithms is a set of tracked regions with actions, visual and text information and web pages assigned on each of which <i>PanoCAST</i> viewers can click on during a show and learn more about the event or simply educate themselves. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig3">3</a> shows the main modules of this real-time processing pipeline. The high-resolution raw image that contains six independent camera views) enters the stream and first passed through a real-time image processing module. These processes run on the CPU (i.e., the processor of the computer) and are used for noise filtering (not only to improve overall image quality, but also to allow the better compression of the outputted image stream). Due to the very high resolution of the raw images, the average performance rate (for decoding the video frames and processing them) drops render performance to ~15–24 fps. As a function of lighting conditions and scene complexity, which is still sufficient to stream images live to the mobile devices at a rate of 15 fps. These CPU-based algorithms are also useful for extracting the information needed for <i>CCM</i>. As an example, automated object recognition algorithms (e.g., face detection) are used to identify and track the apparent motion of elements of interest. The processed images are mapped onto surfaces (a sphere in our case) using a dynamic texture mapping algorithm that was optimized for minimizing the bottleneck between the CPU and the GPU. Finally, once the textures are uploaded onto the graphics card, the parallel image processing algorithms may be constructed to further enhance the image itself using the technology of pixel shaders. In our system, we use a single pass methodology to correct color, find edges and/or filter out noise using kernels. The main advantage of processing images at this stage comes from the distributed nature of the architecture. According to our experiments, the GPU-based image enhancement algorithms we implemented caused practically no slow down in the overall rendering performance. In conclusion, the choice to distribute image processing functions between the CPU and the GPU resulted in well-balanced dual processing architecture offers high-performance image processing and takes advantage of the increased computational speed of the resulting pipeline.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Basic modules of the real-time image processing and tracking pipeline</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig4">4</a> shows some examples using <i>CCM</i> for interactive panoramic media to highlight the sites of a city. When used over the Internet, the built in Flash player allows visitors to enter the scene and control the point of view using their VR glasses. When used from a mobile device, this <i>linked content</i> appears in the form of pop-up images or <i>CallOuts</i>,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> text and Internet links where more information may be accessed at will. As a result of the tracking and production process, virtually all elements in the scene become active and information appears upon clicking on any part of the scene.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Examples of the Clickable Content Management interface allowing visitors to walk across a bridge in downtown Budapest. The spherical view of the bridge (<i>left</i>) is augmented with clickable element thus allowing visitors to explore the exotic city</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p><i>Tracking</i> was implemented using a variety of algorithms including region-based tracking algorithms, such as normalized correlation, texture-based trackers and optical flow-based estimators. The automatic solution employs a modified version of our hierarchical tracker originally developed for the film industry (see Takács et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Takács B, Fromherz T, Tice S, Metaxas D (1999) Digital clones and virtual celebrities: facial tracking, gesture recognition and animation for the movie industry. In: Proceedings of ICCV’99, Corfu, Greece" href="/article/10.1007/s10055-010-0157-7#ref-CR35" id="ref-link-section-d81376e848">1999</a> for more details). Due to the lack of stable image context such as quickly changing illumination e.g., in a concert scene, low-level trackers alone would exhibit serious limitations in a real-world environment; therefore, the use of higher level implicit structural models always requires for accurate registration. In our case, what makes this process possible is the large tolerance to errors, as our goal is not to track the tiny details of each visual element, but rather to create panels (generalized rectangles) that cover large areas and move along with them. So to overcome these problems, a hybrid-tracking algorithm is used to combine the advantages of each low-level method while minimizing the overall sensitivity to noise and variations in imaging conditions. We start with a <i>Point Tracker and</i> adaptively select the best method for each tracking point in a given frame in the region selected by an operator. The image is then processed multiple times and the results are integrated, minimizing the error and creating a smooth transition. As stated above, there is not much emphasis placed on achieving 100% accuracy in the first iteration. As time goes by and the tracking process progresses, more and more evidence emerges supporting a consistent interpretation of the underlying data. Higher level 2D/3D trackers (driven partly by the known motion of the camera) use these raw inputs and impose a simple geometric model on the tracking process and subsequently correct for inaccuracies from lower layers. This geometric correction layer operates on top of region-based, <i>Gabor Jets</i>, and <i>optical flow</i> tracking algorithms. The second layer in this architecture implements a set of s<i>tructural trackers</i> that, instead of following individual points in the image plane, take advantage of an underlying model geometry to which these (individual) points belong to. Thus, one can track point groups, 2D structures and/or splines as well as true 3D objects imported in the form of a geometry description file. One advantage of tracking in a panoramic video is the system’s ability to keep objects of interest always in the center of the camera. In other words, when following a single point in the scene, the context and window size in which the tracking algorithm operates are kept constant by moving the virtual camera’s target proportionally to the displacement measured on that feature. In other words, when the raw tracking algorithm outputs a displacement in pixel space, that information is mapped onto the 3D geometry of the spherical video and its relation to the virtual cameras relative geometric arrangement and the target of the camera is displaced such that the feature being track remains is brought back to the center of the image. As the process continues, this allows our solution to employ larger than usual tracking windows without explicitly loosing context around the edge of an image. The output of this process is a set of “invisible” panels the viewer may click on to get access to more information. Tracked panels are <i>named</i> and as such they may also be selected from a list and used to follow any given person or object with the help of the virtual camera. When selecting this mode, viewers may enjoy looking always at the same visual element (e.g., a painting on the wall, a musician on stage or actress in an interactive film) irrespective of the original camera track.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Animated facial feedback as a means to modulate the learning process</h2><div class="c-article-section__content" id="Sec5-content"><p>In many cultural presentation and educational applications of the PanoCAST system that aims to help online visitors to explore a virtual space and learn about its content a guide offering positive facial feedback would represent a major step forward. To support this functionality, we incorporated <i>animated faces</i> (photo-real or virtual characters FunIcons <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="FunIcons (2009) &#xA;                    http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR7" id="ref-link-section-d81376e878">2009</a>), which when used in this context provide visual, non-verbal feedback and confirmation. More specifically, we use <i>photo</i>-<i>realistic animated faces</i> to display emotions and various facial expressions, as non-verbal feedback most reliably recognizable by any person. These digital faces are controlled by the output parameters of physical interaction or activity data derived from the user. Each of us possess the ability to readily recognize emotions shown on another face and understand it without words, text or other further explanation. It is a language based on <i>non</i>-<i>verbal communication</i> and it provides an efficient and universal means to communicate joy, danger, fear and other aspects that help catalyze the learning process, much like a living person would. In the brain, these signals are processed at early cortical stages often without our conscious intervention and as such <i>they work across different cultures</i>, <i>races</i>, <i>age groups and individual variations.</i> Furthermore, this capability diminishes only minimally while our mental state and consciousness declines often rapidly with age making such labels ideal for the elderly. Our abilities to discern facial signals are very sensitive, people can read the slightest changes in facial expressions very easily. This distributed architecture in the brain gives rise to the very possibility of using these facial displays to help users in the learning process in general and cultural explorations in particular. Within the PanoCAST system, these animated faces appear as <i>overlays</i> embedded into the panoramic scene. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig5">5</a> shows examples of this in the context of interactive games. For more demonstrations how these faces are used, the interested reader is referred to (FunIcons <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="FunIcons (2009) &#xA;                    http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR7" id="ref-link-section-d81376e910">2009</a>; PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e913">2009</a>). In the following section, we will discuss the use of these virtual characters further as part of our effort to evaluate the system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Examples of a digitally animated faces displaying continuous facial emotions in reactions to the user’s actions in the PanoCAST environment (see FunIcons <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="FunIcons (2009) &#xA;                    http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR7" id="ref-link-section-d81376e926">2009</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p><i>Edutainment</i> is a form to education designed not only to help students to learn new material, but to entertain as well as to amuse. Edutainment typically seeks to instruct or socialize its audience by embedding lessons in some familiar form of entertaining content. Interactive graphics offer many new possibilities to aid students learn faster. We have previously developed such systems for special education and novel means of teaching geography (Takács <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Comput Graph Appl 25(5):40–48" href="/article/10.1007/s10055-010-0157-7#ref-CR30" id="ref-link-section-d81376e943">2005</a>). The <i>PanoCAST</i> architecture offers an opportunity to take educational software one step further, by offering features that allow students to use them to explore remote cities or visiting historic places or stroll an exhibition. Thus, teaching art or history becomes a game of searching for cues hidden in pictures while competing with other students. As an example, in the case of teaching art, the clickable content interface allows educators to develop classroom material where students are required to “roam” exhibitions virtually, find and collect the hidden clues by zooming on image details and clicking on details and enhance teamwork by sharing their experience in real time with others. <i>All from any place and anytime.</i> To create a web-version of our virtual museum (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig6">6</a>/Left) site, we started by creating a <i>directed graph</i> description of possible paths (edges) and junctions (nodes) where we would want visitors to be able to explore the place. The virtual visitor follows a general flow of direction based on the intention of the curators of the museums. Certain sections must be visited first, while later these limitations are removed to allow users to go back to places they have been before. At the center of the main window, a spherical video is shown that may be turned using arrow keys, the mouse or any connected VR devices, such as a <i>Head Mounted Display</i> (HMD). The left column shows a selection of two rooms, each may be visited by clicking on the icons. The right column shows the respective locations (nodes) in the scene should people wish to directly jump to one of them. Above the play controls <i>subtitle tracks in multiple languages</i> allow visitors to learn more, read about the background and the historic period, while <i>location</i>-<i>triggered audio tracks</i> play music or instructions. Finally, a <i>chat room</i> (below) is assigned to each section so that online visitors may further discuss and share their questions or experiences.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Main screen web-based Immersive Interactive Reality site created for a virtual museum visit (PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e984">2009</a>; Virtual Museum Volos and Greece <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Virtual Museum Volos, Greece (2009) &#xA;                    http://www.digitalelite.us.com/Pages/PanoCAST/VirtualMuseum.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR38" id="ref-link-section-d81376e987">2009</a>; <i>Left</i>)/Remote-controlled robot camera used to record the scenes (<i>Right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Because panoramic video records and maps the entire scene in real time, production crew, lighting and stuff needs to remain hidden entirely. To achieve maximal coverage, we devised a remote-controlled robot that was directed to cover the entire floor in a matter of a few hours. This is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig6">6</a>/Right. Once the imagery of all possible paths was recorded and converted to manageable/streamable video content in our PanoCAST Flash player, the next step was to create a <i>navigation interface</i> that seems natural and intuitive. To achieve this goal, we decided on augmenting the scene with arrows lying on the floor and showing the name of the section (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig7">7</a>/Left). These arrows appear automatically at the end of each section (or node). Because of their visual placement, sometimes they fall outside the field of view of the virtual camera if the user decides to look around. In such cases, tiny arrows (left side in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig7">7</a>/Left) on the side indicate the option of existing choices. When the mouse is moved over one of these arrows, the system displays a short titles of the section it leads to (“Portraits”).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Example of one of the possible navigation interfaces we developed that attempts to offer a seamless and easy-to-understand interface (<i>Left</i>)/“Debug” display of the virtual museum site showing panels created by the tracking process to support clickable content (<i>Right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To augment the sheer experience of simply being in the museum, we tracked each picture hanged and visible on the walls through the entire set of panoramic videos thus viewers may click on them (even the remotest ones in the corner) should they want to learn more. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig7">7</a>/Right shows the “debug” version of our system with panels assigned to each picture. Due to the tracking process, these panels move along with the paintings while the video progresses and offer constant access to information right at the fingertip of the visitors, yet without disturbing them. Finally, the list of panels visible at any moment and place from the virtual museum scene may also be accessed directly from the user interface as a list. Clicking on the requested label, the system will turn the virtual camera to the specific direction and indicate the exact location of the frame by placing a red marker on it (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig8">8</a>/Left). Naturally, instead of freely looking around while moving along the path, visitors may opt to follow predefined camera moves that highlight the main attractions of the exhibition by clicking on the “camera” icon. As briefly mentioned above, when a painting has annotation assigned to it, a tiny hand appears over its area, and a <i>description along with a zoomable image</i> appears when pressed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig8">8</a>/Right). Should somebody be interested in exploring the details of a picture, a tiled zoom viewer was also added, however, to make zooming more interesting, clues or tags may be placed on specific parts of the image that become visible only at certain zoom levels.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Locating a painting of interest (<i>Left</i>). Display of image title, description, painter and icon readily accessed by mouse clicks (<i>Right</i> see demos for more detail PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e1075">2009</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The entire operation of the site is controlled via a <i>simple XML syntax</i> interpreted by the PanoCAST Flash player and encoding tracking information, metadata for each painting and how tracks behave or follow one another. In addition to “classical” museum visit, using the more advanced features of the PanoCAST system, we have created a number of cultural games that motivate visitors to explore images. As an example, in one room, visitors may <i>kick a ball</i> with the aim to break the glass on some of the painting and thus reveal further clues to proceed. Similarly, the gesture interface was used to find an image below a painting, but visitors must be careful not to break the glass while examining it (find more interactive demos by visiting PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e1097">2009</a>). In this paper, we argue that such “games” will help bringing the virtual experience closer to every day users of the Internet and provide a new way to expand the horizon of virtual reality. We conclude this section with our vision of future museum visits and other cultural presentations over the Internet where visitors may login and experience “being there” using their virtual reality glasses. The PanoCAST system delivers this functionality (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig9">9</a>) via a simple <i>Flash player</i> and streamed immersive interactive media placed in full screen mode and thereby occupying the entire visual space in the HMD. The motion of the virtual camera is controlled by a tiny program, called the <i>Device Manager</i> (Digital Elite Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Digital Elite Inc. (2009) &#xA;                    http://www.digitalElite.US.com&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR1" id="ref-link-section-d81376e1110">2009</a>; PanoCAST Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="PanoCAST Inc. (2009) &#xA;                    http://www.PanoCAST.com&#xA;                    &#xA;                  , Interactive Demonstrations, &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html&#xA;                    &#xA;                  , Supported I/O Devices &#xA;                    http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0157-7#ref-CR3" id="ref-link-section-d81376e1113">2009</a>) that captures raw data from various supported devices and passes it along to the viewer for control. Finally, to further enhance the experience, a <i>physics simulator</i> takes its input from the attached devices and triggers animated interaction with 3D model elements that may augment the scene.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>User experiencing full VR, on-demand immersive interactive reality programming over the Internet using our PanoCAST player &amp; Device Manager (Flash-based) with Nvidia’s PhysX</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To evaluate the performance of the PanoCAST system in a more formal manner, we have collected interactive data and information on user behavior over a period of 15 months. Our primary interest focused on the use of our virtual faces as guides and means to express emotions when interacting with others online, in a shared environment. Users may have chatted with each other and use IP-based video telephony tools such as Skype, Yahoo IM or Microsoft Messenger to express their momentary emotions by controlling one of the animated faces they chose to use and show them in planar view or embedded in a panoramic scene. They could also enable their own web cameras and bring their image (as an overlay, similar to animated faces) within the same panoramic space and subsequently share that video with others. Each user acted anonymously but was identified with a unique user id and stored in our database of interaction. A brief summary of our results is as follows:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>We had recorded <i>1</i>,<i>258</i>,<i>204</i> animations from <i>93</i>,<i>153</i> different online sessions and <i>4</i>,<i>212</i> registered users.</p>
                  </li>
                  <li>
                    <p>Users have had a choice of 9 character groups (Smiley, Baby, Cartoon, Monkeys, Politicians, Characters, Statues, Strange) and a total of 32 different expression sets.</p>
                  </li>
                  <li>
                    <p>Most people overwhelmingly used the smiley face 46% (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig10">10</a>), with photo-real faces and monkeys (both 9%) placing second.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>User choice of virtual faces to express emotions while using our online system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              
                  </li>
                  <li>
                    <p>Early analysis of the emotion spaces of each character defined in the form of density plots (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0157-7#Fig11">11</a>) over the respective animation spaces revealed a clear bias toward positive and “funny” emotions in an attempt to make others laugh.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0157-7/MediaObjects/10055_2010_157_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Density plots corresponding to the expression space of different virtual faces</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0157-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              
                  </li>
                  <li>
                    <p>While further and more in depth statistical analysis of the data we collected is still undergoing, we believe that these results may be used as early indicators of how the capabilities of our system may change the way people interact with each other in shared VR environments.</p>
                  </li>
                </ul>
                     </div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Conclusion</h2><div class="c-article-section__content" id="Sec6-content"><p>The powerful graphics capabilities of portable devices in combination with and high-bandwidth connectivity over the Internet and mobile networks portable devices provide a novel opportunity for virtual reality in general, and cultural presentation in particular, thereby bringing VR into the realm of mature technologies. In this paper, we described a novel content distribution architecture for IIR, called <i>PanoCAST</i>, that utilizes the streaming capability of new generation networks and provides services using panoramic video, tracking and physical simulation. Additional features of the architecture include clickable content, whereas every element of a panoramic video scene becomes a link for further information, and a method for shared virtual experience to allow groups of people participate in the same experience. We discussed the details of this architecture and demonstrated how it is used for personalized on-demand entertainment over mobile phones and ultimately head-mounted displays. To further demonstrate the flexibility of our architecture, we briefly described a number of real-life applications including cultural presentation projects, education, film and new media. We also described in detail a case study of a virtual museum that is capable of delivering virtual reality on-demand services to a broad range of audiences and showing the full-featured capabilities of our technological solution. Based on our results and the experience of many cultural projects, we argue that a virtual reality (weather based on 3D models and 3D worlds or augmented panoramic video scenes) represents a crucial cornerstone of future culture and will help shape our societies.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p><i>CallOuts</i> only appear for a limited time when the user clicks on a specific region of the image in order not to obscure the scene on a small output device, such as the display of a mobile phone.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adobe Flash player (2009) http://www.get.adobe.com/flashplayer/&#xA;                        " /><p class="c-article-references__text" id="ref-CR4">Adobe Flash player (2009) <a href="http://www.get.adobe.com/flashplayer/">http://www.get.adobe.com/flashplayer/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baldwin J, Basu A, Zhang H (1999) Panoramic video with predictive windows for telepresence. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR5">Baldwin J, Basu A, Zhang H (1999) Panoramic video with predictive windows for telepresence. In: Proceedings of IEEE international conference on robotics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Digital Elite Inc. (2009) http://www.digitalElite.US.com&#xA;                        " /><p class="c-article-references__text" id="ref-CR1">Digital Elite Inc. (2009) <a href="http://www.digitalElite.US.com">http://www.digitalElite.US.com</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fraunhofer HHI, Omnicam (2009) http://www.hhi.fraunhofer.de/&#xA;                        " /><p class="c-article-references__text" id="ref-CR6">Fraunhofer HHI, Omnicam (2009) <a href="http://www.hhi.fraunhofer.de/">http://www.hhi.fraunhofer.de/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="FunIcons (2009) http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html&#xA;                        " /><p class="c-article-references__text" id="ref-CR7">FunIcons (2009) <a href="http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html">http://www.digitalelite.us.com/Pages/DigitalElite/FunIcons.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Google Street View (2009) http://www.maps.google.com/intl/en_us/help/maps/streetview/&#xA;                        " /><p class="c-article-references__text" id="ref-CR8">Google Street View (2009) <a href="http://www.maps.google.com/intl/en_us/help/maps/streetview/">http://www.maps.google.com/intl/en_us/help/maps/streetview/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Gross, S. Würmlin, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Gross M, Würmlin S et al (2003) Blue-c: a spatially immersive display and 3D video portal for telepresence. AC" /><p class="c-article-references__text" id="ref-CR9">Gross M, Würmlin S et al (2003) Blue-c: a spatially immersive display and 3D video portal for telepresence. ACM Trans Graph 22(3):819–827</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F882262.882350" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Blue-c%3A%20a%20spatially%20immersive%20display%20and%203D%20video%20portal%20for%20telepresence&amp;journal=ACM%20Trans%20Graph&amp;volume=22&amp;issue=3&amp;pages=819-827&amp;publication_year=2003&amp;author=Gross%2CM&amp;author=W%C3%BCrmlin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Immersive Interactive Film (2009) “Metamorphosis” film adaptation of Franz Kafka’s short story. http://www.dig" /><p class="c-article-references__text" id="ref-CR10">Immersive Interactive Film (2009) “Metamorphosis” film adaptation of Franz Kafka’s short story. <a href="http://www.digitalelite.net/Metamorphosis.html">http://www.digitalelite.net/Metamorphosis.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Immersive Media, Dodeca Camera (2009) http://www.immersive-video.eu/en&#xA;                        " /><p class="c-article-references__text" id="ref-CR11">Immersive Media, Dodeca Camera (2009) <a href="http://www.immersive-video.eu/en">http://www.immersive-video.eu/en</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Interactive Panoramic Cinema (2005) http://www.turbulence.org/blog/archives/000562.html&#xA;                      " /><p class="c-article-references__text" id="ref-CR12">Interactive Panoramic Cinema (2005) <a href="http://www.turbulence.org/blog/archives/000562.html">http://www.turbulence.org/blog/archives/000562.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaidan, Go Pano (2009) http://www.kaidan.com/&#xA;                        " /><p class="c-article-references__text" id="ref-CR13">Kaidan, Go Pano (2009) <a href="http://www.kaidan.com/">http://www.kaidan.com/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kimber D, Foote J (2001) FlyAbout: spatially indexed panoramic video. In: Proceedings of ACM multimedia, Ottaw" /><p class="c-article-references__text" id="ref-CR14">Kimber D, Foote J (2001) FlyAbout: spatially indexed panoramic video. In: Proceedings of ACM multimedia, Ottawa</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Majumder A, Gopi M, Seales B, Fuchs H (1999) Immersive teleconferencing: a new algorithm to generate seamless " /><p class="c-article-references__text" id="ref-CR15">Majumder A, Gopi M, Seales B, Fuchs H (1999) Immersive teleconferencing: a new algorithm to generate seamless panoramic video imagery. In: ACM multimedia, pp 169–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Neumann U, Pintaric T, Rizzo AA (2000) Immersive panoramic video. In: Proceedings of ACM international confere" /><p class="c-article-references__text" id="ref-CR16">Neumann U, Pintaric T, Rizzo AA (2000) Immersive panoramic video. In: Proceedings of ACM international conference on multimedia, pp. 493–494</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ng K-T, Chan S-C, Shum H-Y (2001) The compression issues of panoramic video. In: Proceedings of international " /><p class="c-article-references__text" id="ref-CR17">Ng K-T, Chan S-C, Shum H-Y (2001) The compression issues of panoramic video. In: Proceedings of international symposium on intelligent multimedia, video &amp; speech processing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouglov A, Hjelsvold R (2004) Panoramic video in video-mediated education, in storage and retrieval methods and" /><p class="c-article-references__text" id="ref-CR18">Ouglov A, Hjelsvold R (2004) Panoramic video in video-mediated education, in storage and retrieval methods and applications for multimedia 2005. In: Lienhart RW, Babaguchi N, Chang EY (eds) Proceedings of the SPIE, vol 5682, pp 326–336</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="PanoCAST Inc. (2009) http://www.PanoCAST.com, Interactive Demonstrations, http://www.digitalelite.net/Pages/Pa" /><p class="c-article-references__text" id="ref-CR3">PanoCAST Inc. (2009) <a href="http://www.PanoCAST.com">http://www.PanoCAST.com</a>, Interactive Demonstrations, <a href="http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html">http://www.digitalelite.net/Pages/PanoCAST/Interactive_Demos.html</a>, Supported I/O Devices <a href="http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php">http://www.digitalelite.net/Pages/PanoCAST/Customers_eng.php</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Patil R, Rybski PE, Kanade T, Veloso MM (2004) People detection and tracking in high resolution panoramic vide" /><p class="c-article-references__text" id="ref-CR19">Patil R, Rybski PE, Kanade T, Veloso MM (2004) People detection and tracking in high resolution panoramic video mosaic. In: Proceedings of IEEE/RSJ international conference on intelligent robots and systems, Sendai, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Point Grey Research, LadyBug2 &amp; LadyBug3 Cameras (2009) http://www.ptgrey.com/products/ladybug2/&#xA;             " /><p class="c-article-references__text" id="ref-CR20">Point Grey Research, LadyBug2 &amp; LadyBug3 Cameras (2009) <a href="http://www.ptgrey.com/products/ladybug2/">http://www.ptgrey.com/products/ladybug2/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pryor L, Rizzo AS (2000) User directed news, http://www.imsc.usc.edu/research/project/udn/udn_nsf.pdf&#xA;        " /><p class="c-article-references__text" id="ref-CR21">Pryor L, Rizzo AS (2000) User directed news, <a href="http://www.imsc.usc.edu/research/project/udn/udn_nsf.pdf">http://www.imsc.usc.edu/research/project/udn/udn_nsf.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qin N, Song D (2007) On-demand sharing of a high-resolution panorama video from networked robotic cameras. In:" /><p class="c-article-references__text" id="ref-CR22">Qin N, Song D (2007) On-demand sharing of a high-resolution panorama video from networked robotic cameras. In: Proceedings of IEEE international conference on intelligent robots and systems, San Diego</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="QuickTime, Apple (2009) http://www.apple.com/quicktime/&#xA;                        " /><p class="c-article-references__text" id="ref-CR23">QuickTime, Apple (2009) <a href="http://www.apple.com/quicktime/">http://www.apple.com/quicktime/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Remote Reality (2009) http://www.remotereality.com/&#xA;                        " /><p class="c-article-references__text" id="ref-CR24">Remote Reality (2009) <a href="http://www.remotereality.com/">http://www.remotereality.com/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SM. Rhee, R. Ziegler, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Rhee SM, Ziegler R et al (2007) Low-cost telepresence for collaborative virtual environments. IEEE Trans Vis C" /><p class="c-article-references__text" id="ref-CR25">Rhee SM, Ziegler R et al (2007) Low-cost telepresence for collaborative virtual environments. IEEE Trans Vis Comput Graph 13(1):156–166</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2007.17" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Low-cost%20telepresence%20for%20collaborative%20virtual%20environments&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=13&amp;issue=1&amp;pages=156-166&amp;publication_year=2007&amp;author=Rhee%2CSM&amp;author=Ziegler%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AA. Rizzo, U. Neumann, T. Pintaric, M. Norden, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Rizzo AA, Neumann U, Pintaric T, Norden M (2001) Issues for application development using immersive HMD 360 de" /><p class="c-article-references__text" id="ref-CR26">Rizzo AA, Neumann U, Pintaric T, Norden M (2001) Issues for application development using immersive HMD 360 degree panoramic video environments. In: Smith MJ, Salvendy G, Harris D, Koubek RJ (eds) Usability evaluation and interface design, vol 1. L.A. Erlbaum, New York, pp 792–796</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Usability%20evaluation%20and%20interface%20design&amp;pages=792-796&amp;publication_year=2001&amp;author=Rizzo%2CAA&amp;author=Neumann%2CU&amp;author=Pintaric%2CT&amp;author=Norden%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AA. Rizzo, K. Ghahremani, L. Pryor, S. Gardner, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Rizzo AA, Ghahremani K, Pryor L, Gardner S (2003) Immersive 360-degree panoramic video environments: research " /><p class="c-article-references__text" id="ref-CR27">Rizzo AA, Ghahremani K, Pryor L, Gardner S (2003) Immersive 360-degree panoramic video environments: research on creating useful and usable applications. In: Jacko J, Stephanidis C (eds) Human–computer interaction: theory and practice, vol 1. L.A. Erlbaum, New York, pp 1233–1237</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%E2%80%93computer%20interaction%3A%20theory%20and%20practice&amp;pages=1233-1237&amp;publication_year=2003&amp;author=Rizzo%2CAA&amp;author=Ghahremani%2CK&amp;author=Pryor%2CL&amp;author=Gardner%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rizzo AA, Pryor L, Matheis R, Schultheis M, Ghahremani1 K, Sey A (2004) Memory assessment using graphics-based" /><p class="c-article-references__text" id="ref-CR28">Rizzo AA, Pryor L, Matheis R, Schultheis M, Ghahremani1 K, Sey A (2004) Memory assessment using graphics-based and panoramic video virtual environments. In: Proceedings of 5th international conference on disability, virtual reality &amp; association technology, Oxford, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sun X, Kimber D, Foote J, Manjunath B (2002) Detecting path intersections in panoramic video. In: IEEE interna" /><p class="c-article-references__text" id="ref-CR29">Sun X, Kimber D, Foote J, Manjunath B (2002) Detecting path intersections in panoramic video. In: IEEE international conference on multimedia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Takács, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Com" /><p class="c-article-references__text" id="ref-CR30">Takács B (2005) Special education and rehabilitation: teaching and healing with interactive graphics. IEEE Comput Graph Appl 25(5):40–48</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2005.113" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Special%20education%20and%20rehabilitation%3A%20teaching%20and%20healing%20with%20interactive%20graphics&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=25&amp;issue=5&amp;pages=40-48&amp;publication_year=2005&amp;author=Tak%C3%A1cs%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Takács, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Takács B (2006) Cognitive, mental and physical rehabilitation using a configurable virtual reality system. Int" /><p class="c-article-references__text" id="ref-CR31">Takács B (2006) Cognitive, mental and physical rehabilitation using a configurable virtual reality system. Int J Virtual Real 5(4):1–12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%2C%20mental%20and%20physical%20rehabilitation%20using%20a%20configurable%20virtual%20reality%20system&amp;journal=Int%20J%20Virtual%20Real&amp;volume=5&amp;issue=4&amp;pages=1-12&amp;publication_year=2006&amp;author=Tak%C3%A1cs%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Takács B (2007) PanoMOBI: a panoramic mobile entertainment system. In: Proceedings of ICEC07, Shanghai, China" /><p class="c-article-references__text" id="ref-CR32">Takács B (2007) PanoMOBI: a panoramic mobile entertainment system. In: Proceedings of ICEC07, Shanghai, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Takács, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Takács B (2008) How and why affordable virtual reality shapes the future education. Int J Virtual Real 7(1):53" /><p class="c-article-references__text" id="ref-CR33">Takács B (2008) How and why affordable virtual reality shapes the future education. Int J Virtual Real 7(1):53–66</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20and%20why%20affordable%20virtual%20reality%20shapes%20the%20future%20education&amp;journal=Int%20J%20Virtual%20Real&amp;volume=7&amp;issue=1&amp;pages=53-66&amp;publication_year=2008&amp;author=Tak%C3%A1cs%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Takács, B. Kiss, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Takács B, Kiss B (2003) Virtual human interface: a photo-realistic digital human. IEEE Comput Graph Appl 23(5)" /><p class="c-article-references__text" id="ref-CR34">Takács B, Kiss B (2003) Virtual human interface: a photo-realistic digital human. IEEE Comput Graph Appl 23(5):38–45</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2003.1231176" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20human%20interface%3A%20a%20photo-realistic%20digital%20human&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=23&amp;issue=5&amp;pages=38-45&amp;publication_year=2003&amp;author=Tak%C3%A1cs%2CB&amp;author=Kiss%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Takács B, Fromherz T, Tice S, Metaxas D (1999) Digital clones and virtual celebrities: facial tracking, gestur" /><p class="c-article-references__text" id="ref-CR35">Takács B, Fromherz T, Tice S, Metaxas D (1999) Digital clones and virtual celebrities: facial tracking, gesture recognition and animation for the movie industry. In: Proceedings of ICCV’99, Corfu, Greece</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Transparent Telepresence Research Group (2007) http://www.telepresence.strath.ac.uk/telepresence.htm&#xA;         " /><p class="c-article-references__text" id="ref-CR36">Transparent Telepresence Research Group (2007) <a href="http://www.telepresence.strath.ac.uk/telepresence.htm">http://www.telepresence.strath.ac.uk/telepresence.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="VirMED (2009) Virtual reality medicine, http://www.virmed.net&#xA;                        " /><p class="c-article-references__text" id="ref-CR37">VirMED (2009) Virtual reality medicine, <a href="http://www.virmed.net">http://www.virmed.net</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Virtual Museum Volos, Greece (2009) http://www.digitalelite.us.com/Pages/PanoCAST/VirtualMuseum.html&#xA;         " /><p class="c-article-references__text" id="ref-CR38">Virtual Museum Volos, Greece (2009) <a href="http://www.digitalelite.us.com/Pages/PanoCAST/VirtualMuseum.html">http://www.digitalelite.us.com/Pages/PanoCAST/VirtualMuseum.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zheng J, Zhang Y, Ni G (2007) A fast global motion estimation method for panoramic video coding. In: Advances " /><p class="c-article-references__text" id="ref-CR39">Zheng J, Zhang Y, Ni G (2007) A fast global motion estimation method for panoramic video coding. In: Advances in multimedia information processing—PCM</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0157-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Digital Elite Inc., Los Angeles, CA, USA</p><p class="c-article-author-affiliation__authors-list">Barnabás Takács</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Technical University of Budapest (BME), Budapest, Hungary</p><p class="c-article-author-affiliation__authors-list">Barnabás Takács</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Barnab_s-Tak_cs"><span class="c-article-authors-search__title u-h3 js-search-name">Barnabás Takács</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Barnab%C3%A1s+Tak%C3%A1cs&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Barnab%C3%A1s+Tak%C3%A1cs" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Barnab%C3%A1s+Tak%C3%A1cs%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0157-7/email/correspondent/c1/new">Barnabás Takács</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Immersive%20interactive%20reality%3A%20Internet-based%20on-demand%20VR%20for%20cultural%20presentation&amp;author=Barnab%C3%A1s%20Tak%C3%A1cs&amp;contentID=10.1007%2Fs10055-010-0157-7&amp;publication=1359-4338&amp;publicationDate=2010-03-10&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Takács, B. Immersive interactive reality: Internet-based on-demand VR for cultural presentation.
                    <i>Virtual Reality</i> <b>15, </b>267–278 (2011). https://doi.org/10.1007/s10055-010-0157-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0157-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-04-17">17 April 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-02-21">21 February 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-03-10">10 March 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11">November 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0157-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0157-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Immersive interactive reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cultural presentation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality on-demand</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual human interface</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Panoramic broadcasting (PanoCAST)</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0157-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=157;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

