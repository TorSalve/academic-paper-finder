<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Virtual Bounds: a teleoperated mixed reality"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them within one fused world. All interactions are..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Virtual Bounds: a teleoperated mixed reality"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-04-26"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them within one fused world. All interactions are captured, monitored, modeled and represented with pseudo-real world physics. The objective of the presented research is to create a novel system in which the virtual and physical world would have a symbiotic relationship. In this type of system, virtual objects can impose forces on the physical world and physical world objects can impose forces on the virtual world. Virtual Bounds is an exploratory study allowing a physical probe to navigate a virtual world while observing constraints, forces, and interactions from both worlds. This scenario provides the user with the ability to create a virtual environment and to learn to operate real-life probes through its virtual terrain."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-04-26"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="41"/>

    <meta name="prism.endingPage" content="47"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0030-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0030-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0030-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0030-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Virtual Bounds: a teleoperated mixed reality"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2006/05"/>

    <meta name="citation_online_date" content="2006/04/26"/>

    <meta name="citation_firstpage" content="41"/>

    <meta name="citation_lastpage" content="47"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0030-x"/>

    <meta name="DOI" content="10.1007/s10055-006-0030-x"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0030-x"/>

    <meta name="description" content="This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them wi"/>

    <meta name="dc.creator" content="Kevin Ponto"/>

    <meta name="dc.creator" content="Falko Kuester"/>

    <meta name="dc.creator" content="Robert Nideffer"/>

    <meta name="dc.creator" content="Simon Penny"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ohta Y, Tamura H (1999) &quot;Mixed reality merging physical and virtual worlds.&quot; Ohm-sha and Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Taylor R, Robinett W, Chi V, Brooks F, Wright W, Williams R, Snyder E (1993) &#8220;The nanomanipulator: a virtual-reality interface for a scanning tunneling microscope.&#8221; In:proceedings of SIGGRAPH&#8217;93, pp. 127&#8211;134"/>

    <meta name="citation_reference" content="Brady A, MacDonald B, Oakley I, Hughes SO, Modhrain S. (2002) &#8220;RELAY: a futuristic interface for remote driving.&#8221; In:proceedings of EuroHaptics 2002, Edinburgh, pp. 8&#8211;10"/>

    <meta name="citation_reference" content="citation_journal_title=Multimedia Systems; citation_title=An Internet accessible telepresence; citation_author=A Kaplan, S Keshav, N Schryer, J Venutolo; citation_volume=5; citation_issue=1; citation_publication_date=1997; citation_pages=140-144; citation_doi=10.1007/s005300050049; citation_id=CR4"/>

    <meta name="citation_reference" content="Lawson S, Pretlove J, Wheeler A (2002) &#8220;Augmented reality as a tool to aid the telerobotic exploration and characterization of remote environments,&#8221; presence: teleoperators and virtual environments, vol 11(4). MIT, New York, pp. 352&#8211;367"/>

    <meta name="citation_reference" content="Krueger M (1983) Artificial reality. Addison-Wesley, Reading"/>

    <meta name="citation_reference" content="Crowley J, Hall D, de Verdi V (2000) Object recognition using coloured receptive fields. In:proceedings of the sixth European conference on computer vision. Lecture notes in computer science, vol 1842. Springer, Berlin Heidelberg New York 2000, pp. 164&#8211;177"/>

    <meta name="citation_reference" content="Hiroshi I, Ulmer B (1997). &#8220;Tangible bits: towards seamless interfaces between people, bits and atoms.&#8221; In: Pemberton S (ed) Proceedings of the ACM Conference on human factors in computing systems (CHI &#8216;97, Atlanta, GA, Mar. 22&#8211;27), ACM Press, New York, pp. 234&#8211;241"/>

    <meta name="citation_reference" content="Chun B, Ishii H, Orbanes J, Pardiso J, Wisneski C (1999) &#8220;PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play,&#8221; In:proceedings of computer human interaction (CHI&#8217;99), pp. 394&#8211;401"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) &#8220;Virtual object manipulation on a table-top AR environment.&#8221; In:proceedings of international symposium on augmented reality (ISAR&#8217;00), Munich, 111&#8211;119, Oct. 2000"/>

    <meta name="citation_reference" content="Billinghurst M (2002) &quot;Augmented reality in education, new horizons for learning.&quot; Internet as of March 22, 2006, 
                    http://www.newhorzons.org/strategies/technology/billinghurst.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="Billinghurst M, Kato H, Poupyrev I, Imamoto K, Tachibana K (2001) &#8220;The magicbook: a transitional AR interface.&#8221; Computers Graphics, November 2001, pp 745&#8211;753"/>

    <meta name="citation_reference" content="Sugimoto M, Kojima M, Nakamura A, Kagotani G, Nii H, Inami M (2005) &quot;Augmented coliseum: display-based computing for augmented reality inspiration computing robot.&quot; In:proceedings of horizontal interactive human&#8211;computer systems, pp 3&#8211;8"/>

    <meta name="citation_reference" content="Metaxas G, Metin B, Schneider J, Shapiro G, Zhou W, Markopoulos P (2005) &quot;SCORPIODROME: an exploration in mixed reality social gaming for children.&quot; In:proceedings of ACM conference on advances in computer entertainment"/>

    <meta name="citation_author" content="Kevin Ponto"/>

    <meta name="citation_author_email" content="kponto@uci.edu"/>

    <meta name="citation_author_institution" content="Arts Computation Engineering, Care of School of the Arts, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_author_institution" content="Calit2 Center of Gravity, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_author_institution" content="Laboratory for Game Culture and Technology, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_author" content="Falko Kuester"/>

    <meta name="citation_author_institution" content="Calit2 Center of Gravity, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_author" content="Robert Nideffer"/>

    <meta name="citation_author_institution" content="Laboratory for Game Culture and Technology, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_author" content="Simon Penny"/>

    <meta name="citation_author_institution" content="Arts Computation Engineering, Care of School of the Arts, The University of California Irvine, Irvine, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0030-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/05/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0030-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Virtual Bounds: a teleoperated mixed reality"/>
        <meta property="og:description" content="This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them within one fused world. All interactions are captured, monitored, modeled and represented with pseudo-real world physics. The objective of the presented research is to create a novel system in which the virtual and physical world would have a symbiotic relationship. In this type of system, virtual objects can impose forces on the physical world and physical world objects can impose forces on the virtual world. Virtual Bounds is an exploratory study allowing a physical probe to navigate a virtual world while observing constraints, forces, and interactions from both worlds. This scenario provides the user with the ability to create a virtual environment and to learn to operate real-life probes through its virtual terrain."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Virtual Bounds: a teleoperated mixed reality | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0030-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Mixed reality, Tangible bits, Analog gaming, Remote control","kwrd":["Augmented_reality","Mixed_reality","Tangible_bits","Analog_gaming","Remote_control"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0030-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0030-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=30;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0030-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Virtual Bounds: a teleoperated mixed reality
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0030-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0030-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-04-26" itemprop="datePublished">26 April 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Virtual Bounds: a teleoperated mixed reality</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kevin-Ponto" data-author-popup="auth-Kevin-Ponto" data-corresp-id="c1">Kevin Ponto<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Arts Computation Engineering, Care of School of the Arts, The University of California Irvine, 200 MAB, Irvine, CA, 92697-2775, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Calit2 Center of Gravity, The University of California Irvine, Irvine, CA, 92697, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Laboratory for Game Culture and Technology, The University of California Irvine, Irvine, CA, 92697, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Falko-Kuester" data-author-popup="auth-Falko-Kuester">Falko Kuester</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Calit2 Center of Gravity, The University of California Irvine, Irvine, CA, 92697, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Robert-Nideffer" data-author-popup="auth-Robert-Nideffer">Robert Nideffer</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Laboratory for Game Culture and Technology, The University of California Irvine, Irvine, CA, 92697, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Simon-Penny" data-author-popup="auth-Simon-Penny">Simon Penny</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of California Irvine" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Arts Computation Engineering, Care of School of the Arts, The University of California Irvine, 200 MAB, Irvine, CA, 92697-2775, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">41</span>–<span itemprop="pageEnd">47</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">118 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0030-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them within one fused world. All interactions are captured, monitored, modeled and represented with pseudo-real world physics. The objective of the presented research is to create a novel system in which the virtual and physical world would have a symbiotic relationship. In this type of system, virtual objects can impose forces on the physical world and physical world objects can impose forces on the virtual world. <i>Virtual Bounds</i> is an exploratory study allowing a physical probe to navigate a virtual world while observing constraints, forces, and interactions from both worlds. This scenario provides the user with the ability to create a virtual environment and to learn to operate real-life probes through its virtual terrain.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Over the past 20 years, the lines between the virtual and physical worlds have become increasingly blurred. Computer graphics techniques can now generate images that are nearly photo-realistic, and artificial intelligence has taken great strides in conquering the Turing test. Yet, physical and virtual realities stay juxtaposed when it comes to interaction paradigms. Human–computer interaction (HCI) is generally a one-way communication in which the physical world tells the virtual world what to do. The virtual world, on the other hand, has no control over the physical world, and may, at best, provide limited force-feedback. Haptics and force-feedback devices have been developed as a means to address this. Consumer-based versions of these devices usually provide the user with a tactile feedback, most commonly in the form of a rumbling controller. While these systems are evolving, most of the existing feedback devices are intended to simply augment physical world systems, but are not powerful enough to modify them.</p><p>One field in which haptic devices have proven to be effective is in teleoperations. Teleoperation refers to actions that are performed remotely with the operator removed from the actions either due to differences in location or scale. Teleoperated vehicles (probes) are often used in situations in which safety, working environment, accuracy, or practicality mandate the use of machines.</p><p>For example, teleoperated planes can be built smaller and faster than planes that require a pilot and can operate at higher <i>g</i>-force levels. Teleoperated robots can be sent into dangerous situations to disarm explosives, perform deep-sea rescues, and to do reconnaissance when a human life would be subjected to extreme danger. In the medical field, teleoperated robots are used to perform surgeries remotely when a physician is unable to be present. Similarly, miniature teleoperated robots may be used to perform surgeries that would be too invasive if performed with traditional surgery techniques. Another field in which haptics has proven useful is that of mixed reality. Mixed reality systems are composed of elements that are partially virtual and are partially physical. The advantage of a mixed reality system is that it can combine the tangible aspects of physical objects with the freedom and control available in the virtual world (Ohta and Tamura <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Ohta Y, Tamura H (1999) &#34;Mixed reality merging physical and virtual worlds.&#34; Ohm-sha and Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-006-0030-x#ref-CR1" id="ref-link-section-d5265e380">1999</a>)</p><p>This paper introduces a mixed reality workspace that allows users to combine physical and computer-generated artifacts, and to control and simulate them in one fused world. Interactions are captured, monitored, modeled, and represented with quasi-real world physics (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig1">1</a>). The objective of the presented research is to create a multimodal interface in which the virtual world and physical world have a symbiotic relationship. In this type of system, virtual and physical objects can impose forces, constraints and actions on each other. This type of system is used for a variety of applications requiring the virtual and physical world to exist on a more balanced playing field. In this way, <i>Virtual Bounds</i> is an enactive interface that makes use of non-symbolic forms of knowledge (i.e., actions) to create systems intuitively accessible to average users. A proof of concept system for a teleoperated system is presented that is amenable for teleoperations training.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A user steering a physical probe through the digital world</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec2">Related work</h3><p>Mixed reality research draws heavily from fields such as teleoperation and augmented reality with particular focus on machine vision and digital image processing, tangible interfaces and design, physics modeling and simulation, to name a few.</p><p>In the early 1990s, Taylor et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Taylor R, Robinett W, Chi V, Brooks F, Wright W, Williams R, Snyder E (1993) “The nanomanipulator: a virtual-reality interface for a scanning tunneling microscope.” In:proceedings of SIGGRAPH’93, pp. 127–134" href="/article/10.1007/s10055-006-0030-x#ref-CR2" id="ref-link-section-d5265e420">1993</a>) developed a virtual reality interface for a scanning tunneling microscope allowing users to “touch and feel” a surface at a microscopic level. In addition, users were allowed to make controlled modifications, such as sculpting, at an atomic scale. In another example, researchers use haptics to emulate the feel of piloting a teleoperated vehicle directly. Brady et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Brady A, MacDonald B, Oakley I, Hughes SO, Modhrain S. (2002) “RELAY: a futuristic interface for remote driving.” In:proceedings of EuroHaptics 2002, Edinburgh, pp. 8–10" href="/article/10.1007/s10055-006-0030-x#ref-CR3" id="ref-link-section-d5265e423">2002</a>) developed a system to relay forces and torques experienced by a teleoperated vehicle back on the user’s controller. This allowed those piloting the vehicle to experience the same sensations that they would be exposed to inside the vehicle.</p><p>Remote controlled devices are used as an interface for a wide range of applications. Kaplan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Kaplan A, Keshav S, Schryer N, Venutolo J (1997) &#34;An Internet accessible telepresence.&#34; Multimedia Systems 5(1):140–144" href="/article/10.1007/s10055-006-0030-x#ref-CR4" id="ref-link-section-d5265e429">1997</a>) present Internet accessible telepresence systems for a remote controlled vehicle. A camera was mounted on the front of the RC car, giving the user a first-person perspective of the surroundings. Lawson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lawson S, Pretlove J, Wheeler A (2002) “Augmented reality as a tool to aid the telerobotic exploration and characterization of remote environments,” presence: teleoperators and virtual environments, vol 11(4). MIT, New York, pp. 352–367" href="/article/10.1007/s10055-006-0030-x#ref-CR5" id="ref-link-section-d5265e432">2002</a>) combined the small RC car perspective with a virtual environment, creating an augmented teleoperated system. These systems combine the advantages of viewpoint and remote control, while superimposing additional information, such as dimensions, to enhance the users’ situation awareness.</p><p>Image processing and object recognition are other vital components of teleoperations systems. Krueger (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1983" title="Krueger M (1983) Artificial reality. Addison-Wesley, Reading" href="/article/10.1007/s10055-006-0030-x#ref-CR6" id="ref-link-section-d5265e438">1983</a>) was an early pioneer in the field of virtual reality and human computer interaction systems in the 1960s and 1970s. His work focused on systems in which user actions could be interpreted directly without the user communicating through a physical interface. His most recognized work, <i>Videoplace</i>, used innovated techniques to gather user information. The user was placed in front of a screen with backlighting, allowing a computer vision system to acquire user actions. From this interface, Krueger defined over 50 different types of interaction paradigms for the user to explore.</p><p>Since the 1970s, machine vision has progressed steadily towards real-time processing aided by increased computer performance. Crowley et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Crowley J, Hall D, de Verdi V (2000) Object recognition using coloured receptive fields. In:proceedings of the sixth European conference on computer vision. Lecture notes in computer science, vol 1842. Springer, Berlin Heidelberg New York 2000, pp. 164–177" href="/article/10.1007/s10055-006-0030-x#ref-CR7" id="ref-link-section-d5265e448">2000</a>) tracked individual colors in order to follow the movement of objects on a desk space.</p><p>Objects tracked in the real world can also be used as an interface to the virtual world. Hiroshi and Ulmer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hiroshi I, Ulmer B (1997). “Tangible bits: towards seamless interfaces between people, bits and atoms.” In: Pemberton S (ed) Proceedings of the ACM Conference on human factors in computing systems (CHI ‘97, Atlanta, GA, Mar. 22–27), ACM Press, New York, pp. 234–241" href="/article/10.1007/s10055-006-0030-x#ref-CR8" id="ref-link-section-d5265e454">1997</a>) of the MIT Media Laboratory coined the term “tangible bits”, to describe these types of systems. As stated by Ishii and Ulmer, “‘tangible bits’ is an attempt to bridge the gap between cyberspace and the physical environment by making digital information (bits) tangible. We are developing ways to make bits accessible through the physical environment.” (Hiroshi and Ulmer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hiroshi I, Ulmer B (1997). “Tangible bits: towards seamless interfaces between people, bits and atoms.” In: Pemberton S (ed) Proceedings of the ACM Conference on human factors in computing systems (CHI ‘97, Atlanta, GA, Mar. 22–27), ACM Press, New York, pp. 234–241" href="/article/10.1007/s10055-006-0030-x#ref-CR8" id="ref-link-section-d5265e457">1997</a>).</p><p>Using this concept, Chun et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Chun B, Ishii H, Orbanes J, Pardiso J, Wisneski C (1999) “PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play,” In:proceedings of computer human interaction (CHI’99), pp. 394–401" href="/article/10.1007/s10055-006-0030-x#ref-CR9" id="ref-link-section-d5265e463">1999</a>) created an interface for athletic-tangible computer supported cooperative play, termed <i>PingPongPlus</i>. They utilized an overhead camera to track a ping-pong ball during a normal ping-pong match. In this system, an overhead projector projects images onto the table to augment the real world game play. Several different modes were created, in which different goals and images were displayed to enable different types of game play. Chun et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Chun B, Ishii H, Orbanes J, Pardiso J, Wisneski C (1999) “PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play,” In:proceedings of computer human interaction (CHI’99), pp. 394–401" href="/article/10.1007/s10055-006-0030-x#ref-CR9" id="ref-link-section-d5265e469">1999</a>) found that their system did not only augment reality, but translated reality by creating new game interaction paradigms for participants to interact with.</p><p>Other researchers, such as Mark Billinghurst, have used machine vision to create augmented reality interfaces (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) “Virtual object manipulation on a table-top AR environment.” In:proceedings of international symposium on augmented reality (ISAR’00), Munich, 111–119, Oct. 2000" href="/article/10.1007/s10055-006-0030-x#ref-CR10" id="ref-link-section-d5265e475">2000</a>; Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Billinghurst M (2002) &#34;Augmented reality in education, new horizons for learning.&#34; Internet as of March 22, 2006, &#xA;                    http://www.newhorzons.org/strategies/technology/billinghurst.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0030-x#ref-CR11" id="ref-link-section-d5265e478">2002</a>; Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I, Imamoto K, Tachibana K (2001) “The magicbook: a transitional AR interface.” Computers Graphics, November 2001, pp 745–753" href="/article/10.1007/s10055-006-0030-x#ref-CR12" id="ref-link-section-d5265e481">2001</a>). By determining the placement and orientation of real world objects (feducials), virtual objects can be overlaid corresponding to the users perspective. Users of these systems are commonly required to wear a clear head-mounted-display in order to achieve this hybrid viewpoint. The main advantage of this augmented reality technique is that a 3D view of both the virtual and physical world is available.</p><p>Other mixed reality projects have used remote control vehicles for the purposes of gaming (Sugimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sugimoto M, Kojima M, Nakamura A, Kagotani G, Nii H, Inami M (2005) &#34;Augmented coliseum: display-based computing for augmented reality inspiration computing robot.&#34; In:proceedings of horizontal interactive human–computer systems, pp 3–8" href="/article/10.1007/s10055-006-0030-x#ref-CR13" id="ref-link-section-d5265e487">2005</a>; Metaxas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Metaxas G, Metin B, Schneider J, Shapiro G, Zhou W, Markopoulos P (2005) &#34;SCORPIODROME: an exploration in mixed reality social gaming for children.&#34; In:proceedings of ACM conference on advances in computer entertainment" href="/article/10.1007/s10055-006-0030-x#ref-CR14" id="ref-link-section-d5265e490">2005</a>) These systems have a control structure similar to that of <i>Virtual Bounds</i>, consisting of overhead projectors, tracking devices, and user control units. However, these systems aspired to augment shooter-style video games, while the goal of <i>Virtual Bounds</i> was to create an enactive teleoperation environment.</p><h3 class="c-article__sub-heading" id="Sec3">Technical approach</h3><p>The mixed reality system described in this paper consists of five main components (as shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig3">3</a>). The first component is the physical probe controller, a haptic joystick that allows the user to interact with the system. This joystick sends messages through an interface computer, which processes user input, converts it into control sequences for the remotely operated probe and then forwards these sequences to the transmitter box. The transmitter box subsequently sends the appropriate radio signals to control a small RC vehicle, which acts, in turn, as the probe into the virtual world.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>System diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Data and processing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The machine vision system consists of a camera connected to a computer node tasked with determining position and orientation of the probe. This computer acts as a tracking server and continuously sends the reference coordinates target object as an UDP stream.</p><p>The physics engine acts as an UDP client, and receives these packets. The physics engine uses this data to translate the physical world position into a corresponding virtual world location used to determine if any virtual world forces should be enacted back on the probe. Simultaneously, the physics engine provides the forces that the probe exerts inside the virtual world.</p><p>Once this process is finished, the virtual computer-generated world can be implemented to reflect the current state of the mixed reality world. The fourth component of the system allows all virtual objects to be constructed, animated and displayed. Virtual events are also sonified and outputted through series of speakers.</p><p>The display system is the fifth component in the system and consists of an overhead projector projecting onto a thin custom build, floor mounted screen. A frame was built around the screen for aesthetic, as well as practical reasons. The frame acts as a fail safe to the probe moving beyond its restricted physical world bounds and provided stability to the screen material.</p><p>To begin interaction with this system, the user is required to first charge the probe. Once charged, the probe can be placed on top of the computer-generated environment. At this point, the vision system recognizes the presence of the probe and begins to track its movement. The user can now interact with the system through a haptic controller. The physical world movement of the probe is constrained by virtual objects, their corresponding physical properties and the physics engine.</p><h3 class="c-article__sub-heading" id="Sec4">Physical probe controller</h3><p>The physical control of the system is divided into three separate modules, the joystick, the interface client, and the transmitter box. The user interface into the system is through a haptic joystick, which communicates with the interface client. A Microsoft Sidewinder 2 joystick is used for the proof of concept system. The interface client for position changes (up, down, left, right) monitors the joystick input. Intuitively, pushing the joystick up moves the probe forward. Pulling it back moves the probe in reverse. Moving the joystick to the left turns the probe left, while moving the joystick right turns the probe right. For the presented case study, the trigger button causes the probe to emit virtual projectiles and the throttle is used to control whether the probe fires projectiles that will build or destroy virtual terrain.</p><p>The monitoring interface client sends commands to the transmitter box based on the state of the haptic controller and the virtual world. The transmitter box is built using two external inputs for power and data. A 9V DC input is necessary to run the internal electronics, and a USB input is needed to communicate with the interface client. The box internally uses a Keyspan USB to Serial Converter, to send serial signals to a PIC microprocessor. The serial signal runs at 19,200 baud with eight data bits and one stop bit. The PIC microcontroller then interfaces with a modified 27 MHz RF transmitter to send the appropriate commands to the RC probe. The transmitter box subsequently sends the appropriate RF signal to the probe. The probe receives these transmitter signals and moves accordingly.</p><p>The RC probe is made from an ECOMAN Remote Control Mini Tank. This product operates on the 27 MHz frequency and has a 96:1 gear ratio allowing it to move slowly, with enhanced torque. These features are especially useful since many small RC products move at very high rates of speed. The ECOMAN Remote Control Mini Tank utilizes two treads. Each tread can either move forward or backward allowing the probe to either move forward, backward, or spin. The tank itself was modified with an overlay mask, colored yellow on the body and orange on the turret. This was done to remove the militaristic association with the RC probe and to aid in the color tracking routine. Florescent variations of the colors yellow and orange were chosen to be minimally variable under varying lighting conditions.</p><h3 class="c-article__sub-heading" id="Sec5">Machine vision system</h3><p>The machine vision system is tasked with obtaining the physical world position and orientation of the RC probe. In order to achieve this, a camera (Apple iSight <sup>®</sup>) is mounted over the projection area and image data passed to the machine vision system for processing. This small compact Firewire<sup>®</sup> camera captures 24-bit images at 640 × 480 pixel resolution at 30 frames/s. The camera is mounted 10 ft above the surface and covers an area of 6 ft × 4.5 ft, resulting in an image resolution of 0.1811 × 0.1614 inches/pixel.</p><p>The data is transmitted through a Firewire® cable to the vision server. The vision server selected is a Macintosh Dual 1.25 GHz G4 and the image-processing pipeline developed uses cv.jit (cvjit) in combination with MAX/MSP Jitter (cycling 74). This configuration is selected for its ease of use and rapid prototyping support. Intel’s OpenCV library in combination with custom algorithms are currently being used for more complicated or time critical tracking procedures.</p><p>The system reliably captures the probe using its color markings and provides the probe position and orientation within the physical world coordinates. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig4">4</a> shows a system diagram. First camera data is acquired, and presented to the operator. The operator chooses a magnified image around the probe to analyze further. From this, the operator chooses the hue and saturation values that will correspond to the front and rear of the probe. The image-processing pipeline analyzes the camera data to determine the centroids of the color values. These values are then transmitted via the UDP network protocol to the physics engine and the virtual computer-generated world.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Machine vision flowchart</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>If the probe is not in the camera space, a coordinate value of −1 is passed to the physics engine to alert the system that the probe is no longer on-screen. The machine vision system also thresholds the necessary amount of change of the probe as seen by the camera before passing the values on to the physics engine. This prevents hysteresis of the probe’s position, which is extremely noticeable in the first person perspective.</p><h3 class="c-article__sub-heading" id="Sec6">Physics engine</h3><p>The physics engine is given the task to enact virtual forces back on the RC probe. To determine what forces should be applied to the RC probe, its physical position needs to be correlated with its virtual world position. To do this, the physics engine collects the position data of the RC probes front and rear from the UDP stream generated by the machine vision system. Since these coordinates are in camera space, transformation into the virtual world reference coordinate system is necessary.</p><p>A simple calibration step is provided, allowing the user to identify a set of reference points (the corners of the physical screen space) to correlate the coordinates to the virtual space. The same calibration points also establish the reference frame for the machine vision system. The calibration step is only required during the initial system setup.</p><p>Once the physics engine has knowledge of the RC probe boundaries, it can determine the location of the probe from a fixed point. This is accomplished using inverse bilinear interpolation. The advantage of this technique is that the camera can be oriented in any direction, and does not need to be aligned with the projection system.</p><p>Once the referenced virtual world location of the probe is determined, a simple lookup can be performed to check if the RC Probe has collided with an obstacle. When a collision is detected, a virtual force can be applied back on the probe. The physics engine is also required to determine if the actions of the RC probe will enact any forces onto the virtual world. Once all of these forces are reconciled, the final virtual world is created.</p><h3 class="c-article__sub-heading" id="Sec7">Virtual world</h3><p>The virtual world is designed to provide an alternate reality for the RC probe to traverse and created in C++. OpenGL was used to represent the 2D and 3D virtual world. In addition, the OpenAL framework is used for audio output. OpenAL provides a simple method for importing and playing sound files with sound characteristics based on virtual world locations. This is useful for generating sound effects that provide audio feedback for virtual/physical world interactions.</p><p>A mixed reality study in the form of a computer game was developed to allow the exploration and analysis of the muti-modal interaction techniques and interfaces. The game allows users to build obstacles for the RC probe as well as to destroy them. Users are able to visualize the symbiosis between the virtual and physical world and to test the capabilities of the system.</p><p>Virtual characters in the system are confined to the same boundaries as the RC probe. Some virtual characters, specifically, the assets, move according to very simple flocking behavior with their goal to reach the collection box. The collection box bounces around the virtual world like a ping-pong ball, reflecting off virtual obstacles as well as the RC probe. The game itself is described in more detail below.</p><p>Once the virtual world is constructed, it can be visually represented in different forms. The most powerful representation is the projection onto the mixed reality surface. However, since the virtual model is readily available, a remote operator can select between a birds-eye-view of the scene, a first person view of the probe, or a combination of the two.</p><h3 class="c-article__sub-heading" id="Sec8">Visualization system</h3><p>The visualization system is responsible for creating representations of the virtual world in the physical world. The system is composed of three components, the projector, the frame, and the screen. The projector is mounted 15 ft above of the floor and generates an image that is 4 ft × 3 ft. A Dell 2100 MP projector is used to produce an 800 × 600 image at 110 MHz refresh rate.</p><p>The screen is placed on the floor below the projector and is made out of a specialized 9 ft × 9 ft sheet of white, reflective plastic. The sheet provides excellent reflective properties at a cost of approximately two dollars per square foot.</p><p>A frame is built around the projection area out of painted foam-core. The frame was built to designate where the projection area ended as well as to provide a failsafe boundary for the RC probe. Speakers were also placed around the user to provide spacialized sound.</p><h3 class="c-article__sub-heading" id="Sec9">Mixed reality case study</h3><p>The pervasive nature of computer games and interface technology is well suited as a mixed reality test bed. Exposure (previous experience) of users allows for a natural and intuitive transition of skills and their application in teleoperations. In this case, a simple game scenario was selected, in which users are asked to guide assets (small blue key-like shapes) into the collection box (bouncing square). To do this, the RC probe can shoot virtual projectiles that either build or destroy virtual barriers. Effectively, the RC probe acts like a shepherd herding sheep. The game is very simple, giving points for the number of assets collected in a given period of time. High scores are collected and listed on the upper corner. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig5">5</a> shows the system in operation.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The system in use with the probe in the upper left corner</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The barriers that the RC probe creates/destroys constrain the movement of the virtual characters and at the same time constrain the movement of the physical RC probe. This creates a symbiosis between the virtual and the physical world, as the virtual world creates virtual boundaries on the physical world. For many users, making and destroying these boundaries proved to be as amusing as the game itself.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Results</h2><div class="c-article-section__content" id="Sec10-content"><p>The mixed reality system achieved an effective fusion between the virtual and physical worlds. The virtual world forces seem to be intuitive and “make sense” in the minds of the tested users.</p><p>As for individual components, the vision system can track the RC probe at approximately 20 frames/s. System delay between the acquisition of position from the vision server, to the response on the client server is negligible. At this acquisition speed, the system accurately monitors the RC probe while moving forward and backward, but latency is apparent when virtual projectiles are emitted and the user spins the RC probe for extended periods of time. While this delay is noticeable, it was not considered to be disorienting for those testing the system.</p><p>The response time between movement of the haptic joystick and movement of the probe is less than 20 ms. Once instructed as to how the controls worked, users had few complaints about the accuracy of their intended movements and the resulting probe response. Overall the system preformed well during extended testing. The most significant problems during a 1 week exhibition were RC probe reliability and battery life. Higher-grade robotic probes can eliminate these problems in the future. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0030-x#Fig6">6</a> shows the manual placement and removal of a probe from the operating surface during the exhibition.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0030-x/MediaObjects/10055_2006_30_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Manual placement and removal of the RC probe during testing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0030-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec11">User performance appraisal</h3><p>Qualitative and quantitative measures were employed to assess the performance of this mixed reality system. A more simple and straightforward setup compared to that of the game environment described in the case study above, was used to survey the system. Users were tasked with operating the probe in order to find marked random mixed-world locations by touching them with either the probe or a projectile. Quantitative performance measures assessed the response times (goal acquisition time minus start time) when operating the system from the real-world perspective (viewing the probe on top of the overlayed projection) and solely from the teleoperations perspective (only using the probe’s point of view). These qualitative user assessments commented positively on the intuitive nature of the interface, believability of the mixed reality scenario, and the enjoyability of the operation. Negative comments focused on the need for fine-tuning the interface with respect to subtle movements and joystick responsivity. Five subjects participated in nine measures (trials) of response time for each mode. Data were analyzed using repeated-measures ANOVA (Statview 5.0.1). All subjects were able to successfully manipulate the probe for all modes and in all trials. Response times did not vary significantly by trial (<i>P</i> = 0.14, i.e., no apparent learning effect) but did vary by mode (<i>P</i> = 0.02). The teleoperation mode required approximately twice as long to manipulate the probe in the specified sequence of tasks (3.47 ± 2.65 s versus 5.55 ± 3.84 s, difference = 2.08 s). Although statistically significantly longer, this manipulation time was not unreasonable for the required task.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Conclusions</h2><div class="c-article-section__content" id="Sec12-content"><p>This paper presents a new mixed reality approach that combines research in computer graphics, visualization, and virtual reality with machine vision, haptics, and teleoperations. The resulting system provides a multimodal interface allowing for manipulation of teleoperated probes in virtual environments.</p><p>
                        <i>Virtual Bounds</i> provides a technique to train users of teleoperated systems. By allowing users to create virtual environments that produce physical world feedback, users can quickly explore and operate in a mixed reality space. This allows for greater flexibility compared to creating a full real-life model of the simulation environment.</p><p>This system also has potential utility in the field of teleoperated surgery. Robotic arms could be tracked in the same manner as RC probes. This would allow surgeons to train on virtual patients. Currently, teleoperated surgical systems train on physical models, whereas, the current mixed reality system would allow surgeons to easily swap one virtual patient with another, recreating difficult surgical situations and providing easy analysis of the surgeon’s teleoperation performance.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ohta Y, Tamura H (1999) &#34;Mixed reality merging physical and virtual worlds.&#34; Ohm-sha and Springer, Berlin Heid" /><p class="c-article-references__text" id="ref-CR1">Ohta Y, Tamura H (1999) "Mixed reality merging physical and virtual worlds." Ohm-sha and Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor R, Robinett W, Chi V, Brooks F, Wright W, Williams R, Snyder E (1993) “The nanomanipulator: a virtual-r" /><p class="c-article-references__text" id="ref-CR2">Taylor R, Robinett W, Chi V, Brooks F, Wright W, Williams R, Snyder E (1993) “The nanomanipulator: a virtual-reality interface for a scanning tunneling microscope.” In:proceedings of SIGGRAPH’93, pp. 127–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brady A, MacDonald B, Oakley I, Hughes SO, Modhrain S. (2002) “RELAY: a futuristic interface for remote drivin" /><p class="c-article-references__text" id="ref-CR3">Brady A, MacDonald B, Oakley I, Hughes SO, Modhrain S. (2002) “RELAY: a futuristic interface for remote driving.” In:proceedings of EuroHaptics 2002, Edinburgh, pp. 8–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kaplan, S. Keshav, N. Schryer, J. Venutolo, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Kaplan A, Keshav S, Schryer N, Venutolo J (1997) &#34;An Internet accessible telepresence.&#34; Multimedia Systems 5(1" /><p class="c-article-references__text" id="ref-CR4">Kaplan A, Keshav S, Schryer N, Venutolo J (1997) "An Internet accessible telepresence." Multimedia Systems 5(1):140–144</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs005300050049" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20Internet%20accessible%20telepresence&amp;journal=Multimedia%20Systems&amp;volume=5&amp;issue=1&amp;pages=140-144&amp;publication_year=1997&amp;author=Kaplan%2CA&amp;author=Keshav%2CS&amp;author=Schryer%2CN&amp;author=Venutolo%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lawson S, Pretlove J, Wheeler A (2002) “Augmented reality as a tool to aid the telerobotic exploration and cha" /><p class="c-article-references__text" id="ref-CR5">Lawson S, Pretlove J, Wheeler A (2002) “Augmented reality as a tool to aid the telerobotic exploration and characterization of remote environments,” presence: teleoperators and virtual environments, vol 11(4). MIT, New York, pp. 352–367</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krueger M (1983) Artificial reality. Addison-Wesley, Reading" /><p class="c-article-references__text" id="ref-CR6">Krueger M (1983) Artificial reality. Addison-Wesley, Reading</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Crowley J, Hall D, de Verdi V (2000) Object recognition using coloured receptive fields. In:proceedings of the" /><p class="c-article-references__text" id="ref-CR7">Crowley J, Hall D, de Verdi V (2000) Object recognition using coloured receptive fields. In:proceedings of the sixth European conference on computer vision. Lecture notes in computer science, vol 1842. Springer, Berlin Heidelberg New York 2000, pp. 164–177</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hiroshi I, Ulmer B (1997). “Tangible bits: towards seamless interfaces between people, bits and atoms.” In: Pe" /><p class="c-article-references__text" id="ref-CR8">Hiroshi I, Ulmer B (1997). “Tangible bits: towards seamless interfaces between people, bits and atoms.” In: Pemberton S (ed) Proceedings of the ACM Conference on human factors in computing systems (CHI ‘97, Atlanta, GA, Mar. 22–27), ACM Press, New York, pp. 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chun B, Ishii H, Orbanes J, Pardiso J, Wisneski C (1999) “PingPongPlus: design of an athletic-tangible interfa" /><p class="c-article-references__text" id="ref-CR9">Chun B, Ishii H, Orbanes J, Pardiso J, Wisneski C (1999) “PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play,” In:proceedings of computer human interaction (CHI’99), pp. 394–401</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) “Virtual object manipulation on a table-top " /><p class="c-article-references__text" id="ref-CR10">Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) “Virtual object manipulation on a table-top AR environment.” In:proceedings of international symposium on augmented reality (ISAR’00), Munich, 111–119, Oct. 2000</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M (2002) &#34;Augmented reality in education, new horizons for learning.&#34; Internet as of March 22, 20" /><p class="c-article-references__text" id="ref-CR11">Billinghurst M (2002) "Augmented reality in education, new horizons for learning." Internet as of March 22, 2006, <a href="http://www.newhorzons.org/strategies/technology/billinghurst.htm">http://www.newhorzons.org/strategies/technology/billinghurst.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I, Imamoto K, Tachibana K (2001) “The magicbook: a transitional AR interface." /><p class="c-article-references__text" id="ref-CR12">Billinghurst M, Kato H, Poupyrev I, Imamoto K, Tachibana K (2001) “The magicbook: a transitional AR interface.” Computers Graphics, November 2001, pp 745–753</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sugimoto M, Kojima M, Nakamura A, Kagotani G, Nii H, Inami M (2005) &#34;Augmented coliseum: display-based computi" /><p class="c-article-references__text" id="ref-CR13">Sugimoto M, Kojima M, Nakamura A, Kagotani G, Nii H, Inami M (2005) "Augmented coliseum: display-based computing for augmented reality inspiration computing robot." In:proceedings of horizontal interactive human–computer systems, pp 3–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Metaxas G, Metin B, Schneider J, Shapiro G, Zhou W, Markopoulos P (2005) &#34;SCORPIODROME: an exploration in mixe" /><p class="c-article-references__text" id="ref-CR14">Metaxas G, Metin B, Schneider J, Shapiro G, Zhou W, Markopoulos P (2005) "SCORPIODROME: an exploration in mixed reality social gaming for children." In:proceedings of ACM conference on advances in computer entertainment</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0030-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported in part by the Beall Center for Art, the California Institute for Telecommunications and Technology (Calit2) and the Arts Computation and Engineering (ACE) program at the University of California, Irvine. We also thank Cina Hazegh, Eric Kabisch, and Colbin Erdahl for fruitful discussions and technical assistance. </p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Arts Computation Engineering, Care of School of the Arts, The University of California Irvine, 200 MAB, Irvine, CA, 92697-2775, USA</p><p class="c-article-author-affiliation__authors-list">Kevin Ponto &amp; Simon Penny</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Calit2 Center of Gravity, The University of California Irvine, Irvine, CA, 92697, USA</p><p class="c-article-author-affiliation__authors-list">Kevin Ponto &amp; Falko Kuester</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Laboratory for Game Culture and Technology, The University of California Irvine, Irvine, CA, 92697, USA</p><p class="c-article-author-affiliation__authors-list">Kevin Ponto &amp; Robert Nideffer</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Kevin-Ponto"><span class="c-article-authors-search__title u-h3 js-search-name">Kevin Ponto</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kevin+Ponto&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kevin+Ponto" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kevin+Ponto%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Falko-Kuester"><span class="c-article-authors-search__title u-h3 js-search-name">Falko Kuester</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Falko+Kuester&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Falko+Kuester" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Falko+Kuester%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Robert-Nideffer"><span class="c-article-authors-search__title u-h3 js-search-name">Robert Nideffer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Robert+Nideffer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robert+Nideffer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robert+Nideffer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Simon-Penny"><span class="c-article-authors-search__title u-h3 js-search-name">Simon Penny</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Simon+Penny&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Simon+Penny" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Simon+Penny%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0030-x/email/correspondent/c1/new">Kevin Ponto</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Virtual%20Bounds%3A%20a%20teleoperated%20mixed%20reality&amp;author=Kevin%20Ponto%20et%20al&amp;contentID=10.1007%2Fs10055-006-0030-x&amp;publication=1359-4338&amp;publicationDate=2006-04-26&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ponto, K., Kuester, F., Nideffer, R. <i>et al.</i> Virtual Bounds: a teleoperated mixed reality.
                    <i>Virtual Reality</i> <b>10, </b>41–47 (2006). https://doi.org/10.1007/s10055-006-0030-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0030-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12-22">22 December 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04-03">03 April 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04-26">26 April 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05">May 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0030-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0030-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tangible bits</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Analog gaming</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Remote control</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0030-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=30;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

