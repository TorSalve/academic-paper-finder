<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Phenomenal regression to the real object in physical and virtual world"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In this paper, we investigate a new approach to comparing physical and virtual size and depth percepts that captures the involuntary responses of participants to different stimuli in their field of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Phenomenal regression to the real object in physical and virtual worlds"/>

    <meta name="dc.source" content="Virtual Reality 2014 19:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-12-06"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In this paper, we investigate a new approach to comparing physical and virtual size and depth percepts that captures the involuntary responses of participants to different stimuli in their field of view, rather than relying on their skill at judging size, reaching or directed walking. We show, via an effect first observed in the 1930s, that participants asked to equate the perspective projections of disc objects at different distances make a systematic error that is both individual in its extent and comparable in the particular physical and virtual setting we have tested. Prior work has shown that this systematic error is difficult to correct, even when participants are knowledgeable of its likelihood of occurring. In fact, in the real world, the error only reduces as the available cues to depth are artificially reduced. This makes the effect we describe a potentially powerful, intrinsic measure of VE quality that ultimately may contribute to our understanding of VE depth compression phenomena."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-12-06"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="21"/>

    <meta name="prism.endingPage" content="31"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0257-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0257-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0257-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0257-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Phenomenal regression to the real object in physical and virtual worlds"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2015/03"/>

    <meta name="citation_online_date" content="2014/12/06"/>

    <meta name="citation_firstpage" content="21"/>

    <meta name="citation_lastpage" content="31"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0257-x"/>

    <meta name="DOI" content="10.1007/s10055-014-0257-x"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0257-x"/>

    <meta name="description" content="In this paper, we investigate a new approach to comparing physical and virtual size and depth percepts that captures the involuntary responses of participa"/>

    <meta name="dc.creator" content="Kevin W. Elner"/>

    <meta name="dc.creator" content="Helen Wright"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=The influence of restricted viewing conditions on egocentric distance perception: implications for real and virtual environments; citation_author=SH Creem-Regehr, P Willemsen, AA Gooch, WB Thompson; citation_volume=34; citation_publication_date=2005; citation_pages=191-204; citation_doi=10.1068/p5144; citation_id=CR1"/>

    <meta name="citation_reference" content="Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth. In: Handbook of perception and cognition; perception of space and motion, vol 5. Academic Press, San Diego, pp 69&#8211;117"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Localization of virtual objects in the near visual field; citation_author=SR Ellis, BM Menges; citation_volume=40; citation_publication_date=1998; citation_pages=415-431; citation_doi=10.1518/001872098779591278; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Am J Psychol; citation_title=The effect of attitude upon the perception of size; citation_author=AS Gilinsky; citation_volume=68; citation_publication_date=1955; citation_pages=173-192; citation_doi=10.2307/1418890; citation_id=CR4"/>

    <meta name="citation_reference" content="Interrante V, Ries B, Anderson L (2006) Distance perception in immersive virtual environments, revisited. In: Proceedings of the IEEE conference on Virtual Reality, VR &#8217;06. IEEE Computer Society, Washington, DC, pp 3&#8211;10"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Elucidating factors that can facilitate veridical spatial perception in immersive virtual environments; citation_author=V Interrante, B Ries, J Lindquist, M Kaeding, L Anderson; citation_volume=17; citation_publication_date=2008; citation_pages=176-198; citation_doi=10.1162/pres.17.2.176; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Limited field of view of head-mounted displays is not the cause of distance underestimation in virtual environments; citation_author=JM Knapp, JM Loomis; citation_volume=13; citation_publication_date=2004; citation_pages=572-577; citation_doi=10.1162/1054746042545238; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=HMD calibration and its effects on distance judgments; citation_author=SA Kuhl, WB Thompson, SH Creem-Regehr; citation_volume=6; citation_publication_date=2009; citation_pages=19:1-19:20; citation_doi=10.1145/1577755.1577762; citation_id=CR8"/>

    <meta name="citation_reference" content="Loomis JM, Knapp JM (2003) Visual perception of egocentric distance in real and virtual environments. In: Virtual and adaptive environments applications, implications, and human performance issues. CRC Press, Boca Raton, pp 21&#8211;46"/>

    <meta name="citation_reference" content="Mohler BJ, B&#252;lthoff HH, Thompson WB, Creem-Regehr SH (2008) A full-body avatar improves egocentric distance judgments in an immersive virtual environment. In: Proceedings of the 5th symposium on applied perception in graphics and visualization, APGV &#8217;08. ACM, New York, NY, p 194"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The effect of viewing a self-avatar on distance judgments in an HMD-based virtual environment; citation_author=BJ Mohler, SH Creem-Regehr, WB Thompson, HH B&#252;lthoff; citation_volume=19; citation_publication_date=2010; citation_pages=230-242; citation_doi=10.1162/pres.19.3.230; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=Estimation of distances in virtual environments using size constancy; citation_author=A Murgia, PM Sharkey; citation_volume=8; citation_publication_date=2009; citation_pages=67-74; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=Near-field distance perception in real and virtual environments using both verbal and action responses; citation_author=PE Napieralski, BM Altenhoff, JW Bertrand, LO Long, SV Babu, CC Pagano, J Kern, TA Davis; citation_volume=8; citation_publication_date=2011; citation_pages=18:1-18:19; citation_doi=10.1145/2010325.2010328; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Displays; citation_title=Egocentric distance perception in large screen immersive displays; citation_author=IV Piryankova, S Rosa, U Kloos, HH Bulthoff, BJ Mohler; citation_volume=34; citation_issue=2; citation_publication_date=2013; citation_pages=153-164; citation_doi=10.1016/j.displa.2013.01.001; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Visual Comput Graph; citation_title=Perceptual calibration for immersive display environments; citation_author=K Ponto, M Gleicher, RG Radwin, HJ Shin; citation_volume=19; citation_publication_date=2013; citation_pages=691-700; citation_doi=10.1109/TVCG.2013.36; citation_id=CR15"/>

    <meta name="citation_reference" content="Press WH, Teukolsky SA, Vetterling WT, Flannery BP, eds. (1992) Numerical Recipes in C: The Art of Scientific Computing, chap. Modelling of Data, 666&#8211;668. Cambridge University Press, 2nd edn"/>

    <meta name="citation_reference" content="Ries B, Interrante V, Kaeding M, Anderson L (2008) The effect of self-embodiment on distance perception in immersive virtual environments. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST &#8217;08. ACM, New York, NY, pp 167&#8211;170"/>

    <meta name="citation_reference" content="Sedgwick HA (1986) Space perception. In: Handbook of perception and human performance, vol 1. Wiley, New York, pp 129&#8211;158"/>

    <meta name="citation_reference" content="Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field augmented reality. In: Proceedings of the 7th symposium on applied perception in graphics and visualization, APGV &#8217;10. ACM, New York, NY, pp 149&#8211;156"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Does the quality of the computer graphics matter when judging distances in visually immersive environments; citation_author=WB Thompson, P Willemsen, AA Gooch, SH Creem-Regehr, JM Loomis, AC Beall; citation_volume=13; citation_publication_date=2004; citation_pages=560-571; citation_doi=10.1162/1054746042545292; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Br J Psychol Gen Sect; citation_title=Phenomenal regression to the real object. I; citation_author=RH Thouless; citation_volume=21; citation_publication_date=1931; citation_pages=339-359; citation_doi=10.1111/j.2044-8295.1931.tb00597.x; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Br J Psychol Gen Sect; citation_title=Phenomenal regression to the real object. II; citation_author=RH Thouless; citation_volume=22; citation_publication_date=1931; citation_pages=1-30; citation_doi=10.1111/j.2044-8295.1931.tb00609.x; citation_id=CR22"/>

    <meta name="citation_reference" content="Willemsen P, Gooch AA (2002) An experimental comparison of perceived egocentric distance in real, image-based, and traditional virtual environment using direct walking tasks. Tech. rep., University of Utah Computer Science"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Effects of stereo viewing conditions on distance perception in virtual environments; citation_author=P Willemsen, AA Gooch, WB Thompson, SH Creem-Regehr; citation_volume=17; citation_publication_date=2008; citation_pages=91-101; citation_doi=10.1162/pres.17.1.91; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept; citation_title=The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments; citation_author=P Willemsen, MB Colton, SH Creem-Regehr, WB Thompson; citation_volume=6; citation_publication_date=2009; citation_pages=8:1-8:14; citation_doi=10.1145/1498700.1498702; citation_id=CR25"/>

    <meta name="citation_author" content="Kevin W. Elner"/>

    <meta name="citation_author_email" content="k.elner@hull.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Hull, Hull, UK"/>

    <meta name="citation_author" content="Helen Wright"/>

    <meta name="citation_author_email" content="h.wright@hull.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Hull, Hull, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0257-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0257-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Phenomenal regression to the real object in physical and virtual worlds"/>
        <meta property="og:description" content="In this paper, we investigate a new approach to comparing physical and virtual size and depth percepts that captures the involuntary responses of participants to different stimuli in their field of view, rather than relying on their skill at judging size, reaching or directed walking. We show, via an effect first observed in the 1930s, that participants asked to equate the perspective projections of disc objects at different distances make a systematic error that is both individual in its extent and comparable in the particular physical and virtual setting we have tested. Prior work has shown that this systematic error is difficult to correct, even when participants are knowledgeable of its likelihood of occurring. In fact, in the real world, the error only reduces as the available cues to depth are artificially reduced. This makes the effect we describe a potentially powerful, intrinsic measure of VE quality that ultimately may contribute to our understanding of VE depth compression phenomena."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Phenomenal regression to the real object in physical and virtual worlds | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0257-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual environments, Depth and size perception, Depth compression, Index of phenomenal regression, Thouless ratio, Brunswik ratio","kwrd":["Virtual_environments","Depth_and_size_perception","Depth_compression","Index_of_phenomenal_regression","Thouless_ratio","Brunswik_ratio"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0257-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0257-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=257;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0257-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Phenomenal regression to the real object in physical and virtual worlds
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0257-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0257-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-12-06" itemprop="datePublished">06 December 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Phenomenal regression to the real object in physical and virtual worlds</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kevin_W_-Elner" data-author-popup="auth-Kevin_W_-Elner" data-corresp-id="c1">Kevin W. Elner<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Hull" /><meta itemprop="address" content="grid.9481.4, 0000000404128669, Department of Computer Science, University of Hull, Hull, HU6 7RX, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Helen-Wright" data-author-popup="auth-Helen-Wright">Helen Wright</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Hull" /><meta itemprop="address" content="grid.9481.4, 0000000404128669, Department of Computer Science, University of Hull, Hull, HU6 7RX, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">21</span>–<span itemprop="pageEnd">31</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">467 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0257-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In this paper, we investigate a new approach to comparing physical and virtual size and depth percepts that captures the involuntary responses of participants to different stimuli in their field of view, rather than relying on their skill at judging size, reaching or directed walking. We show, via an effect first observed in the 1930s, that participants asked to equate the perspective projections of disc objects at different distances make a systematic error that is both individual in its extent and comparable in the particular physical and virtual setting we have tested. Prior work has shown that this systematic error is difficult to correct, even when participants are knowledgeable of its likelihood of occurring. In fact, in the real world, the error only reduces as the available cues to depth are artificially reduced. This makes the effect we describe a potentially powerful, intrinsic measure of VE quality that ultimately may contribute to our understanding of VE depth compression phenomena.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The virtual environment (VE) is an established component of modern training systems. Some scenarios train for situational awareness, but many train specific skills involving judgements of size and distance. Clearly for these, it is preferable to have a veridical representation or at least to know whether the accuracy of spatial perception falls short. Perception of space relies jointly on the perception of distances and sizes of objects, since retinal size is an ambiguous indicator of objective size until distance is understood, and vice versa. Ability to judge distance in VEs has therefore been extensively studied, and generally, it is found to be underestimated by participants. Judgement of size has been less intensively investigated, but recent findings also show underestimation, possibly due to distance underestimation.</p><p>However, in spite of many studies, ensuring the spatial veracity of VEs is an ongoing challenge. Our contribution to this field has been to investigate the potential value in VEs of ‘phenomenal regression’, a systematic perceptual error made by test subjects when matching standard and response stimuli under certain experimental conditions. The effect was first reported by Thouless in the 1930s (Thouless <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e311">1931a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e314">b</a>). He found that when a subject is asked to match the perspective sizes of two discs at different distances, they consistently overestimate the diameter of the nearer disc. The effect is perhaps most easily understood by comparing a subject’s actual field of view (FOV) with how they perceive this FOV. Consider the classic ‘trick shot’ in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig1">1</a>: when asked to open thumb and forefinger just wide enough to ‘pick up’ some feature of the scene, a person will make a larger estimate when observing with both eyes in person than is required when seen through the camera’s lens. The effect is symmetrical—in the experimental setting, adjusting the nearer disc results in overestimation, whereas adjusting the further disc causes underestimation. Moreover, it occurs not only for the sizes of objects but also for their brightness and shapes. Thouless called this tendency ‘phenomenal regression to the real object’ (Thouless <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e320">1931a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e323">b</a>):<dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>
                      <b>phenomenal</b>
                    :</dfn></dt><dd class="c-abbreviation_list__description">
                      <p>what his subjects perceived</p>
                    </dd><dt class="c-abbreviation_list__term"><dfn>
                      <b>regression</b>
                    :</dfn></dt><dd class="c-abbreviation_list__description">
                      <p>the tendency to report a modified value for the response variable, compared with expectation</p>
                    </dd><dt class="c-abbreviation_list__term"><dfn>
                      <b>to the real object</b>
                    :</dfn></dt><dd class="c-abbreviation_list__description">
                      <p>this modified value is always towards the standard, fixed stimulus.</p>
                    </dd></dl>
                </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Matching perspective size is remarkably easy in principle but very hard to accomplish in practice. The distant window appears to occupy a larger portion of the FOV when viewed with the subject’s two eyes in person than it does through the camera’s lens. Setting up the shot by alternately focussing on the two objects results in a wider-than-needed grasp. In other words, the hand’s adjustment regresses towards the actually larger, but more distant, window frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Regression occurs innately, to different extents in different subjects under different conditions of depth cue reduction. This latter property is key to its potential to indicate the quality of a VE implementation, since it offers the opportunity to compare physical and virtual spatial percepts directly.</p><p>In this paper, we first summarise in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec2">2</a> the considerable body of prior work in VEs investigating distance underestimation, including its possible origins in the quality of the VE graphics and viewing conditions, hardware influences, user self-embodiment and cognition. The extent of regression for any individual is quantified using the Thouless ratio (TR; Thouless <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e399">1931a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e402">b</a>), so Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec3">3</a> next explains the derivation of TR in the size–distance formulation that we use, and contrasts it with traditional methods of distance estimation in VEs. Our work in this field concentrates on the depth cue approximations necessary when implementing VEs in large-screen immersive displays (LSIDs), and their potential effect on spatial perception. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec4">4</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec7">5</a> describe our experiments and findings when comparing regression in the physical realm and virtual worlds of this type, including the precautions necessary for evoking a consistent response in participants. Finally, in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec8">6</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec9">7</a>, we discuss the contribution of our research to establishing a direct measure of VE visual fidelity, independent of participants’ learnt skill in distance and size estimation tasks.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Depth compression in VEs</h2><div class="c-article-section__content" id="Sec2-content"><p>A large body of work on depth perception in VEs has investigated medium-field distances (~2 m to ~30 m), i.e. what Cutting and Vishton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth. In: Handbook of perception and cognition; perception of space and motion, vol 5. Academic Press, San Diego, pp 69–117" href="/article/10.1007/s10055-014-0257-x#ref-CR2" id="ref-link-section-d91675e429">1995</a>) categorised as action space. Its focus has been to understand the phenomenon that revealed itself in early VE studies (see review in Loomis and Knapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Loomis JM, Knapp JM (2003) Visual perception of egocentric distance in real and virtual environments. In: Virtual and adaptive environments applications, implications, and human performance issues. CRC Press, Boca Raton, pp 21–46" href="/article/10.1007/s10055-014-0257-x#ref-CR9" id="ref-link-section-d91675e432">2003</a>), whereby users with adequate real-world estimation skills nonetheless judge egocentric distances to be shorter than intended in VE scenes.</p><p>Studies that made improvements to the quality of the graphics found that depth compression was present whether rendering panoramic photographs or computer-generated polygons (Willemsen and Gooch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Willemsen P, Gooch AA (2002) An experimental comparison of perceived egocentric distance in real, image-based, and traditional virtual environment using direct walking tasks. Tech. rep., University of Utah Computer Science" href="/article/10.1007/s10055-014-0257-x#ref-CR23" id="ref-link-section-d91675e438">2002</a>). Compression was evident even when employing realistic high-resolution photographic textures (Thompson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Thompson WB, Willemsen P, Gooch AA, Creem-Regehr SH, Loomis JM, Beall AC (2004) Does the quality of the computer graphics matter when judging distances in visually immersive environments. Presence Teleoper Virtual Environ 13:560–571" href="/article/10.1007/s10055-014-0257-x#ref-CR20" id="ref-link-section-d91675e441">2004</a>). Willemsen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Willemsen P, Gooch AA, Thompson WB, Creem-Regehr SH (2008) Effects of stereo viewing conditions on distance perception in virtual environments. Presence Teleoper Virtual Environ 17:91–101" href="/article/10.1007/s10055-014-0257-x#ref-CR24" id="ref-link-section-d91675e444">2008</a>) investigated inaccuracies in stereoscopic viewing conditions—inter-pupil distance, binocular and monocular viewing—but concluded they were not likely causes of depth compression.</p><p>Other work has focused on aspects of head-mounted display (HMD) hardware. Willemsen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Willemsen P, Colton MB, Creem-Regehr SH, Thompson WB (2009) The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments. ACM Trans Appl Percept 6:8:1–8:14" href="/article/10.1007/s10055-014-0257-x#ref-CR25" id="ref-link-section-d91675e450">2009</a>) investigated mechanical aspects of HMDs. By comparing results between the real world, mock and real HMDs, they concluded that mass, inertia and FOV of a HMD contributed some but not all of the depth compression, with restriction of the FOV being the largest identifiable source. However, Knapp and Loomis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Knapp JM, Loomis JM (2004) Limited field of view of head-mounted displays is not the cause of distance underestimation in virtual environments. Presence Teleoper Virtual Environ 13:572–577" href="/article/10.1007/s10055-014-0257-x#ref-CR7" id="ref-link-section-d91675e453">2004</a>) found that limiting FOV in the real world without the additional mass and inertia typical of a HMD did not result in depth compression. Creem-Regehr et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Creem-Regehr SH, Willemsen P, Gooch AA, Thompson WB (2005) The influence of restricted viewing conditions on egocentric distance perception: implications for real and virtual environments. Perception 34:191–204" href="/article/10.1007/s10055-014-0257-x#ref-CR1" id="ref-link-section-d91675e456">2005</a>), in a similar study, found that limiting FOV did result in depth compression but only if there was also restricted head movement. Yet other work has concentrated on the optics of HMDs: Kuhl et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kuhl SA, Thompson WB, Creem-Regehr SH (2009) HMD calibration and its effects on distance judgments. ACM Trans Appl Percept 6:19:1–19:20" href="/article/10.1007/s10055-014-0257-x#ref-CR8" id="ref-link-section-d91675e459">2009</a>) demonstrated that minification and magnification in HMDs significantly affected distance judgements, but that other miscalibrations, notably pitching of the VE and pincushion distortion, did not.</p><p>In contrast to studies that have shown depth compression in synthetic VE spaces, Interrante et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Interrante V, Ries B, Anderson L (2006) Distance perception in immersive virtual environments, revisited. In: Proceedings of the IEEE conference on Virtual Reality, VR ’06. IEEE Computer Society, Washington, DC, pp 3–10" href="/article/10.1007/s10055-014-0257-x#ref-CR5" id="ref-link-section-d91675e465">2006</a>) did not observe significant compression when the VE space shown to participants corresponded with the physical space hosting their experiment. They concluded that the cause of depth compression in VEs may not be inherent in the technology itself, i.e. the HMD, but a higher-level cognitive issue with interpreting virtual stimuli. Further work by Interrante et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Interrante V, Ries B, Lindquist J, Kaeding M, Anderson L (2008) Elucidating factors that can facilitate veridical spatial perception in immersive virtual environments. Presence Teleoper Virtual Environ 17:176–198" href="/article/10.1007/s10055-014-0257-x#ref-CR6" id="ref-link-section-d91675e468">2008</a>) investigated whether exposure to the real physical space provides users with a metric for calibrating size and distance judgements in a synthetic VE replication of that physical space; however, the results did not support this hypothesis.</p><p>
Mohler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Mohler BJ, Bülthoff HH, Thompson WB, Creem-Regehr SH (2008) A full-body avatar improves egocentric distance judgments in an immersive virtual environment. In: Proceedings of the 5th symposium on applied perception in graphics and visualization, APGV ’08. ACM, New York, NY, p 194" href="/article/10.1007/s10055-014-0257-x#ref-CR10" id="ref-link-section-d91675e475">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Mohler BJ, Creem-Regehr SH, Thompson WB, Bülthoff HH (2010) The effect of viewing a self-avatar on distance judgments in an HMD-based virtual environment. Presence Teleoper Virtual Environ 19:230–242" href="/article/10.1007/s10055-014-0257-x#ref-CR11" id="ref-link-section-d91675e478">2010</a>) found that the presence of a self-avatar or displaced avatar in the VE reduced the depth compression, especially if its animation corresponded to the user’s own body movement. They theorised that an awareness of one’s body (self-embodiment) in a VE may serve as the reference frame required for scaling synthetic space correctly. A similar study by Ries et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ries B, Interrante V, Kaeding M, Anderson L (2008) The effect of self-embodiment on distance perception in immersive virtual environments. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST ’08. ACM, New York, NY, pp 167–170" href="/article/10.1007/s10055-014-0257-x#ref-CR17" id="ref-link-section-d91675e481">2008</a>) also found a reduction in depth compression and concluded that self-embodiment may facilitate a stronger sense of presence in a VE, suggesting that compression in a VE is a result of cognitive dissonance.</p><p>Much of the depth compression work in VEs has been conducted in medium-field distances and has employed the use of HMDs; however, studies have observed underestimation of distances in LSIDs. Murgia and Sharkey (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Murgia A, Sharkey PM (2009) Estimation of distances in virtual environments using size constancy. Int J Virtual Real 8:67–74" href="/article/10.1007/s10055-014-0257-x#ref-CR12" id="ref-link-section-d91675e487">2009</a>) found distance underestimation in rich cue conditions (textured background surfaces) and even greater underestimation in poor cue conditions (limited background cues), though not to the extent found in other studies. They concluded that their experimental methodology, which provided information on the relative sizes of objects in the VE, may have increased users’ accuracy in estimating egocentric distance. Piryankova et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Piryankova IV, de la Rosa S, Kloos U, Bulthoff HH, Mohler BJ (2013) Egocentric distance perception in large screen immersive displays. Displays 34(2):153–164" href="/article/10.1007/s10055-014-0257-x#ref-CR14" id="ref-link-section-d91675e490">2013</a>) investigated three different types of LSID VE and found that distance underestimation was present in all. They concluded that inclusion of stereoscopic and motion parallax cues did not mitigate compression for medium-field distances.</p><p>Far less attention has been given to VE depth perception at near-field distances (within ~2 m), i.e. what Cutting and Vishton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth. In: Handbook of perception and cognition; perception of space and motion, vol 5. Academic Press, San Diego, pp 69–117" href="/article/10.1007/s10055-014-0257-x#ref-CR2" id="ref-link-section-d91675e496">1995</a>) categorised as personal space. Napieralski et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Napieralski PE, Altenhoff BM, Bertrand JW, Long LO, Babu SV, Pagano CC, Kern J, Davis TA (2011) Near-field distance perception in real and virtual environments using both verbal and action responses. ACM Trans Appl Percept 8:18:1–18:19" href="/article/10.1007/s10055-014-0257-x#ref-CR13" id="ref-link-section-d91675e499">2011</a>) found significant underestimation in the perception of near-field distances in a HMD VE compared with a real-world condition. When using LSIDs, Piryankova et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Piryankova IV, de la Rosa S, Kloos U, Bulthoff HH, Mohler BJ (2013) Egocentric distance perception in large screen immersive displays. Displays 34(2):153–164" href="/article/10.1007/s10055-014-0257-x#ref-CR14" id="ref-link-section-d91675e502">2013</a>) found that, although distances are still underestimated, stereoscopic depth cues mitigate some of the depth compression effect in the near field.</p><p>
Ellis and Menges (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431" href="/article/10.1007/s10055-014-0257-x#ref-CR3" id="ref-link-section-d91675e508">1998</a>) examined users’ perception of distance to augmented reality (AR) objects in the near field. It was found that as depth cues degrade there is an overestimation as virtual objects are judged to be at the distance of the background surface whereupon they are superimposed. Older participants particularly struggled to localise virtual objects placed at shorter distances from them. They concluded that this was due to their inability to accommodate to these focal lengths and, relying instead on the disparity cue, they converge to the surface behind the object. In contrast, it was found that distances to AR objects are underestimated in the presence of a real occluding surface that is nearer to the viewer.</p><p>Replicating the apparatus of Ellis and Menges (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431" href="/article/10.1007/s10055-014-0257-x#ref-CR3" id="ref-link-section-d91675e514">1998</a>), Singh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field augmented reality. In: Proceedings of the 7th symposium on applied perception in graphics and visualization, APGV ’10. ACM, New York, NY, pp 149–156" href="/article/10.1007/s10055-014-0257-x#ref-CR19" id="ref-link-section-d91675e517">2010</a>) found depth judgements to AR objects in the near field to be underestimated. The presence of a salient occluding surface had a complex effect on depth judgements, with the virtual object appearing nearly at the depth of this surface if it could feasibly be associated with it.</p><p>The experiment described in the present paper investigates near-field perception in a LSID VE, neither of which has hitherto been the subject of much attention. Moreover, we take a completely different approach to that taken so far. Whereas others in the field have used the accuracy of distance estimation as an indicator of VE spatial quality, we propose to use the regression effect, introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec1">1</a>. The next section derives the TR equation needed to quantify the effect, preparatory to using it to compare some real and virtual stimuli in our target VE.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Derivation of the Thouless ratio in size–distance experiments</h2><div class="c-article-section__content" id="Sec3-content"><p>We begin the derivation of TR for size–distance, not with different diameter circles at different distances as our subsequent experiments in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec4">4</a> will actually use, but with inclined circles. The aim is to aid understanding the concepts and equation we will eventually use in the size–distance experiments. In brief, when investigating the perception of shape, Thouless asked participants to look at circles inclined by different amounts to their line of sight. When asked to report the perspective shapes of these circles, either by drawing them or responding to different ellipses held up by the experimenter, the regression effect described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec1">1</a> caused participants to reproduce or choose a shape that was intermediate between the actual perspective shape and the objective, i.e. circular, shape. In this variant of phenomenal regression, ‘phenomenal’ refers to the shape as it appeared to his subject, whilst the ‘real’ object was the circular shape without inclination. Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e540">1931a</a>, p. 341) remarks that the real, objective character is known due to observing it ‘with both eyes fully open and focussed’. In his description, Thouless also used the term ‘stimulus character’ that in this experiment means the perspective shape of the inclined object. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig2">2</a> shows these real, phenomenal and stimulus characters (<i>R</i>, <i>P</i> and <i>S</i>), as a participant in this experiment might have experienced them.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>
                        <i>R</i> (<i>solid line</i>) shows the real, objective shape of the presented disc which, due to its inclination to the participant’s line of sight, has a stimulus shape <i>S</i> (<i>grey shading</i>). The phenomenal or apparent character <i>P</i> (<i>dotted line</i>) was found always to lie somewhere between <i>R</i> and <i>S</i>. Redrawn from Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e591">1931a</a>, Fig. 2, p. 342)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Thouless noted individuals’ different degrees of regression and developed a numerical measure, the index of phenomenal regression (IPR), linking <i>R</i>, <i>P</i> and <i>S</i>
                </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\rm IPR} = \frac{\log P - \log S}{\log R - \log S} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>In essence, the denominator of this equation measures the total distance separating the stimulus character from the real, objective character. Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0257-x#Equ1">1</a> therefore shows the degree to which the perceptual, or phenomenal, character differs from the stimulus, normalised to unity. When a subject makes a perfect perspective match, the index is zero, and conversely, a perfect objective match yields an index of unity. IPR later became known as the Thouless ratio.</p><p>In the shape experiment, <i>R</i>, <i>P</i> and <i>S</i> are the ratios of the minor and major axes of the ellipses in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig2">2</a>; clearly, in this case, <span class="mathjax-tex">\(R=1\)</span>. In size-matching experiments, circular discs of different diameters <span class="mathjax-tex">\(D\)</span> and <span class="mathjax-tex">\(d\)</span> are presented without inclination at respective distances <span class="mathjax-tex">\(L\)</span> and <span class="mathjax-tex">\(l\)</span>, and the participant adjusts the size of one to phenomenal equality with the other. If, as in our experiment, the participant adjusts the further disc <span class="mathjax-tex">\(D\)</span>, the comparable stimulus character is the size that <span class="mathjax-tex">\(d\)</span> would have to assume, when placed at distance <span class="mathjax-tex">\(L\)</span>, in order to appear as it does at <span class="mathjax-tex">\(l\)</span>, i.e. <span class="mathjax-tex">\(dL/l\)</span>. This gives IPR, or now TR, as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{\log D - \log \frac{dL}{l}}{\log d - \log \frac{dL}{l}} \end{aligned}$$</span></div></div><p>A simple rearrangement is to add and subtract <span class="mathjax-tex">\(\log D\)</span> from both the numerator and denominator and collect terms, giving</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{- \log \frac{dL}{Dl}}{\log \frac{d}{D} - \log \frac{dL}{Dl}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>In this formulation, we can see parallels with the shape experiment that used the axes’ ratios of the stimulus and response shapes. Whereas in the shape experiment <span class="mathjax-tex">\(R=1\)</span> because the object was circular, in the size–distance experiment the real character is the ratio of the discs’ diameters <span class="mathjax-tex">\(d/D\)</span>. The stimulus character is the ratio of the discs’ perspective sizes <span class="mathjax-tex">\(dL/Dl\)</span>.</p><p>According to Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0257-x#Equ2">2</a>, when TR = 0 in the size–distance experiment, participants are matching the perspective sizes of objects <span class="mathjax-tex">\((d/l = D/L)\)</span>; conversely, when TR = 1, they are estimating objective equivalence <span class="mathjax-tex">\((d=D)\)</span>. Since the experiment by design involves objects at different distances, values of TR near to unity therefore demonstrate size constancy, or the ability to estimate sizes correctly at different depths in a scene. Possibly for this reason, Thouless’ work has frequently been cited in later studies of size constancy, but in the original Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e1269">1931a</a>) quite clearly is interested principally in participants’ ability to judge perspective size. Although Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e1272">1931a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e1276">b</a>, p. 353, p. 1) is consistent in describing his findings only as ‘a <b>tendency</b> [our emphasis] to constancy’, this in itself could have contributed to subsequent confusion regarding the aims of his experiments.</p><p>The formulation in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0257-x#Equ2">2</a> neatly emphasises two points. Firstly, <span class="mathjax-tex">\((D,L)\)</span> and <span class="mathjax-tex">\((d,l)\)</span> are interchangeable without affecting TR (Thouless <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e1353">1931a</a>, p. 353 footnote). This is not the case for TR’s close relative the Brunswik ratio <span class="mathjax-tex">\((P - S)/(R - S)\)</span> (Sedgwick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Sedgwick HA (1986) Space perception. In: Handbook of perception and human performance, vol 1. Wiley, New York, pp 129–158" href="/article/10.1007/s10055-014-0257-x#ref-CR18" id="ref-link-section-d91675e1405">1986</a>, pp. 21–25). A practical consequence is that a Brunswik ratio from experiments adjusting the further disc would require scaling to compare with values from experiments adjusting the nearer disc. The logarithmic formulation of TR requires no such conversion. Secondly, by comparison with Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0257-x#Equ1">1</a>, we see that <span class="mathjax-tex">\(\log P = 0\)</span> hence <span class="mathjax-tex">\(P=1\)</span>, termed ‘phenomenal equality’. This condition is captured visually in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig3">3</a>—participants perceive the discs as subtending the same angle (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig3">3</a>a), but their actual, calculated projections differ (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig3">3</a>b). The greater the difference, the larger is that participant’s TR value.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>When participants report the apparent, or phenomenal, equality of near and far discs <span class="mathjax-tex">\(d\)</span> and <span class="mathjax-tex">\(D\)</span> (<b>a</b>), their typical perspective sizes are actually as at (<b>b</b>). The discs are perceived as if subtending the same angle at the eye but in fact their perspective projections differ</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Whilst it would be expected that estimating objective size is a skill acquired through practice that might therefore deviate from perfection, the finding that participants cannot match <i>perspective</i> sizes exactly under normal viewing conditions is somewhat unexpected. It seems the perception of the retinal image itself results from some form of processing involving depth. Thouless describes the response of participants that leads to values of TR &gt; 0 as an involuntary reaction that cannot be defeated even when pointed out to them, with TR approaching zero only when various cues of depth are systematically removed (Thouless <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e1544">1931b</a>).</p><p>We can contrast the proposed TR measure with the methods for VE distance estimation used in the studies summarised in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec2">2</a>. By far the commonest is ‘blind walking’ (also called direct or directed walking) whereby participants observe a target object and then walk to its perceived position blindfolded. Examples of this technique in use occur in the work of Willemsen and Gooch (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Willemsen P, Gooch AA (2002) An experimental comparison of perceived egocentric distance in real, image-based, and traditional virtual environment using direct walking tasks. Tech. rep., University of Utah Computer Science" href="/article/10.1007/s10055-014-0257-x#ref-CR23" id="ref-link-section-d91675e1553">2002</a>), Knapp and Loomis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Knapp JM, Loomis JM (2004) Limited field of view of head-mounted displays is not the cause of distance underestimation in virtual environments. Presence Teleoper Virtual Environ 13:572–577" href="/article/10.1007/s10055-014-0257-x#ref-CR7" id="ref-link-section-d91675e1556">2004</a>), Creem-Regehr et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Creem-Regehr SH, Willemsen P, Gooch AA, Thompson WB (2005) The influence of restricted viewing conditions on egocentric distance perception: implications for real and virtual environments. Perception 34:191–204" href="/article/10.1007/s10055-014-0257-x#ref-CR1" id="ref-link-section-d91675e1559">2005</a>), Interrante et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Interrante V, Ries B, Anderson L (2006) Distance perception in immersive virtual environments, revisited. In: Proceedings of the IEEE conference on Virtual Reality, VR ’06. IEEE Computer Society, Washington, DC, pp 3–10" href="/article/10.1007/s10055-014-0257-x#ref-CR5" id="ref-link-section-d91675e1562">2006</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Interrante V, Ries B, Lindquist J, Kaeding M, Anderson L (2008) Elucidating factors that can facilitate veridical spatial perception in immersive virtual environments. Presence Teleoper Virtual Environ 17:176–198" href="/article/10.1007/s10055-014-0257-x#ref-CR6" id="ref-link-section-d91675e1566">2008</a>), Mohler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Mohler BJ, Bülthoff HH, Thompson WB, Creem-Regehr SH (2008) A full-body avatar improves egocentric distance judgments in an immersive virtual environment. In: Proceedings of the 5th symposium on applied perception in graphics and visualization, APGV ’08. ACM, New York, NY, p 194" href="/article/10.1007/s10055-014-0257-x#ref-CR10" id="ref-link-section-d91675e1569">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Mohler BJ, Creem-Regehr SH, Thompson WB, Bülthoff HH (2010) The effect of viewing a self-avatar on distance judgments in an HMD-based virtual environment. Presence Teleoper Virtual Environ 19:230–242" href="/article/10.1007/s10055-014-0257-x#ref-CR11" id="ref-link-section-d91675e1572">2010</a>), Ries et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ries B, Interrante V, Kaeding M, Anderson L (2008) The effect of self-embodiment on distance perception in immersive virtual environments. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST ’08. ACM, New York, NY, pp 167–170" href="/article/10.1007/s10055-014-0257-x#ref-CR17" id="ref-link-section-d91675e1575">2008</a>), Kuhl et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kuhl SA, Thompson WB, Creem-Regehr SH (2009) HMD calibration and its effects on distance judgments. ACM Trans Appl Percept 6:19:1–19:20" href="/article/10.1007/s10055-014-0257-x#ref-CR8" id="ref-link-section-d91675e1578">2009</a>) and Willemsen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Willemsen P, Colton MB, Creem-Regehr SH, Thompson WB (2009) The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments. ACM Trans Appl Percept 6:8:1–8:14" href="/article/10.1007/s10055-014-0257-x#ref-CR25" id="ref-link-section-d91675e1581">2009</a>). Direct walking requires locomotion to the actual target position and where this is impractical due to space or other considerations, other variants have developed, notably triangulated walking used for example by Thompson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Thompson WB, Willemsen P, Gooch AA, Creem-Regehr SH, Loomis JM, Beall AC (2004) Does the quality of the computer graphics matter when judging distances in visually immersive environments. Presence Teleoper Virtual Environ 13:560–571" href="/article/10.1007/s10055-014-0257-x#ref-CR20" id="ref-link-section-d91675e1585">2004</a>) and Willemsen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Willemsen P, Gooch AA, Thompson WB, Creem-Regehr SH (2008) Effects of stereo viewing conditions on distance perception in virtual environments. Presence Teleoper Virtual Environ 17:91–101" href="/article/10.1007/s10055-014-0257-x#ref-CR24" id="ref-link-section-d91675e1588">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Willemsen P, Colton MB, Creem-Regehr SH, Thompson WB (2009) The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments. ACM Trans Appl Percept 6:8:1–8:14" href="/article/10.1007/s10055-014-0257-x#ref-CR25" id="ref-link-section-d91675e1591">2009</a>). Yet others have used verbal reporting of distance (Knapp and Loomis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Knapp JM, Loomis JM (2004) Limited field of view of head-mounted displays is not the cause of distance underestimation in virtual environments. Presence Teleoper Virtual Environ 13:572–577" href="/article/10.1007/s10055-014-0257-x#ref-CR7" id="ref-link-section-d91675e1594">2004</a>; Napieralski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Napieralski PE, Altenhoff BM, Bertrand JW, Long LO, Babu SV, Pagano CC, Kern J, Davis TA (2011) Near-field distance perception in real and virtual environments using both verbal and action responses. ACM Trans Appl Percept 8:18:1–18:19" href="/article/10.1007/s10055-014-0257-x#ref-CR13" id="ref-link-section-d91675e1597">2011</a>; Piryankova et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Piryankova IV, de la Rosa S, Kloos U, Bulthoff HH, Mohler BJ (2013) Egocentric distance perception in large screen immersive displays. Displays 34(2):153–164" href="/article/10.1007/s10055-014-0257-x#ref-CR14" id="ref-link-section-d91675e1600">2013</a>), visually guided reaching (Singh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field augmented reality. In: Proceedings of the 7th symposium on applied perception in graphics and visualization, APGV ’10. ACM, New York, NY, pp 149–156" href="/article/10.1007/s10055-014-0257-x#ref-CR19" id="ref-link-section-d91675e1604">2010</a>; Napieralski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Napieralski PE, Altenhoff BM, Bertrand JW, Long LO, Babu SV, Pagano CC, Kern J, Davis TA (2011) Near-field distance perception in real and virtual environments using both verbal and action responses. ACM Trans Appl Percept 8:18:1–18:19" href="/article/10.1007/s10055-014-0257-x#ref-CR13" id="ref-link-section-d91675e1607">2011</a>) and blind reaching (Ellis and Menges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431" href="/article/10.1007/s10055-014-0257-x#ref-CR3" id="ref-link-section-d91675e1610">1998</a>). Less frequently, studies of depth compression invoke size estimation skills. For example, Murgia and Sharkey (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Murgia A, Sharkey PM (2009) Estimation of distances in virtual environments using size constancy. Int J Virtual Real 8:67–74" href="/article/10.1007/s10055-014-0257-x#ref-CR12" id="ref-link-section-d91675e1613">2009</a>) asked participants to estimate distance supported by information on the relative sizes of real and virtual objects, whereas Loomis and Knapp (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Loomis JM, Knapp JM (2003) Visual perception of egocentric distance in real and virtual environments. In: Virtual and adaptive environments applications, implications, and human performance issues. CRC Press, Boca Raton, pp 21–46" href="/article/10.1007/s10055-014-0257-x#ref-CR9" id="ref-link-section-d91675e1616">2003</a>) used affordance to pass through an aperture.</p><p>Distance estimation accuracy is commonly used to infer the spatial quality of a VE, but it could also conceivably be used to achieve correct perception by artificially inflating geometry to offset the compression effect. Perceptual calibration is also the aim of Ponto et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ponto K, Gleicher M, Radwin RG, Shin HJ (2013) Perceptual calibration for immersive display environments. IEEE Trans Visual Comput Graph 19:691–700" href="/article/10.1007/s10055-014-0257-x#ref-CR15" id="ref-link-section-d91675e1622">2013</a>), but their approach is conversely to allow users to adjust the VE’s projection parameters by making objects look level, square and stationary. Phenomenal regression falls between these two, being both potentially an indicator of VE visual quality and a target for calibration approaches. Our justification for this study is TR’s sheer simplicity compared with traditional methods; as captured in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig3">3</a>, it depends only on participants attempting to match, all the while remaining seated, the perspective projections of two objects in their FOV.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Comparing TR in physical and virtual environments</h2><div class="c-article-section__content" id="Sec4-content"><p>An experiment was designed to compare individual participants’ Thouless ratios utilising physical and virtual stimuli. In brief, participants were asked to adjust the size of a physical far disc to match the perspective size of a near disc, the latter presented in both physical and virtual forms. The distances to the discs are nominally 1.22 and 2.72 m, so the experiment can be classified as taking place in personal space and action space (Cutting and Vishton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth. In: Handbook of perception and cognition; perception of space and motion, vol 5. Academic Press, San Diego, pp 69–117" href="/article/10.1007/s10055-014-0257-x#ref-CR2" id="ref-link-section-d91675e1636">1995</a>), respectively, the distances within and slightly beyond an arm’s length, and beyond this but within 30 m. The following subsections provide more detail.</p><h3 class="c-article__sub-heading" id="Sec5">Apparatus</h3><p>The Hull immersive visualisation environment (HIVE) is a rear-projected, single-surface display, 5.33 m wide by 2.44 m high. A raised stage area in front is 5.33 m wide by 3.22 m deep. The display is driven by two active stereo projectors arranged horizontally with hardware edge-blending. LCD shutter glasses are tracked optically, and the resulting eye positions are used to calculate asymmetric parallel axis stereoscopic viewing frusta. The user’s head orientation is also taken into account, in total generating perspective, motion parallax and binocular disparity cues to depth (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig4">4</a>). The depth at which an object is perceived is given by the vergence point as the stereo pairs are fused (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig4">4</a>b, e). In the real world, vergence works in conjunction with accommodation of the eye’s lens, to focus the light rays coming from the object onto the retina. However, in a virtual world implemented on a projection screen, the light rays always emanate from the display surface. This creates an accommodation–vergence mismatch for the user which worsens progressively as objects’ perceived distances from the screen increase.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Depth cues in our LSID VE. <b>a</b> Shows the truncated pyramidal form of the viewing frustum that implements a perspective projection onto the image plane (<i>bold</i>) of objects at different distances. In <b>b</b>, computing separate left- (<i>dotted</i>) and right-eye images renders near objects with negative parallax and far objects with positive parallax (<b>c</b>). When fused, these objects, respectively, appear to be out of and into the screen. <b>d</b> Shows how the frustum is recalculated as the user moves, in this case towards the right (<i>dashed lines</i>; only the frustum portion beyond the fixed image plane is shown for clarity), thereby generating motion parallax. As well as tracking position information, the user’s head orientation is also used to foreshorten the stereo separation during rotation around the vertical axis (<b>e</b>). Rotation around the horizontal axis perpendicular to the screen causes the images to separate vertically (<b>f</b>), so the stereo pairs remain fusible throughout a wide range of head motions. Taken together, the illusion of depth from these various measures is compelling, but correct focus cues cannot be simulated by this type of environment (see text)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Plan view of apparatus on HIVE stage</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Photograph of apparatus. Note that the scene is illuminated, so the brightness and contrast of the displays are not representative of the actual experimental conditions experienced by a participant. Furthermore, the stereo separations seen on the left-hand, virtual disc are not evident whilst wearing the shutter glasses and the virtual disc appears above the physical tripod on the left when these separations are fused</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Three discs are projected on three displays: the HIVE screen used as a large, physical monitor, a standard LED monitor and a virtual monitor implemented using the artificial depth cues in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig4">4</a>. The displays are positioned so they do not overlap in the participant’s FOV and at such an angle that they face the participant (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig5">5</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig6">6</a>).</p><p>The centre disc displayed in the plane of the HIVE screen has zero parallax and therefore no stereo separation. The focus and vergence cues are correct here, so the disc has the perceptual properties of being physical. When the participant faces this disc, it is 2.72 m from them. This is the disc that participants adjust, and displaying it on the large HIVE screen eliminates extraneous size cues that would be available from a monitor surround.</p><p>The right-hand disc on the LED monitor is positioned 1.22 m from the participant and supported on a tripod. Like the centre disc, a disc on the LED monitor has the perceptual properties of being physical because the focus and vergence cues are correct. The physical LED monitor is tracked to ensure that the virtual monitor is reflected across the centre line of the stage. The monitor’s full physical dimensions are 56.8 cm by 33 cm; however, the screen is masked by black felt to reveal only an area of 29 cm by 29 cm. This corresponds to the maximum size of virtual monitor that is reproducible without clipping by the HIVE screen edge.</p><p>The left-hand disc is projected on the virtual monitor, a black rectangle corresponding to the visible portion of the physical LED monitor but at exactly the mirror position. The disc on the virtual monitor and the monitor surround itself (simply a dark grey boundary) are displayed on the HIVE screen with negative parallax, so the whole assembly appears to be in front of the screen, 1.22 m from the participant. Note, however, that the accommodation distance when observing this disc is on the HIVE screen, some 2.97 m away when the participant turns to face it. A physical tripod is placed below the virtual monitor.</p><p>All three discs are white with no texture, and all are observed through the shutter glasses. Efforts were made to ensure uniform brightness across the three discs, measured using a light meter at each display, through the shutter glasses. Brightness was kept within a range of <span class="mathjax-tex">\(\pm 1\)</span> lux.</p><p>To perform the task correctly, the participant must look at and focus on each disc with both eyes. Forming any kind of strategy such as placing both discs in the FOV but focusing on neither is undesirable because it eliminates the binocular cues of focus and vergence. The solution adopted was to render a disc only when the participant was facing it.</p><h3 class="c-article__sub-heading" id="Sec6">Procedure</h3><p>A potential weakness when measuring TR lies in knowing whether participants are actually attempting to match perspective or objective size. Thus, although the experimenter may have clear intentions, it requires the participant to have understood these in order to respond as required. The work of Gilinsky (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1955" title="Gilinsky AS (1955) The effect of attitude upon the perception of size. Am J Psychol 68:173–192" href="/article/10.1007/s10055-014-0257-x#ref-CR4" id="ref-link-section-d91675e1795">1955</a>) sheds light, by describing the effect of different participant instructions on the results of size-matching experiments in the physical environment. Explicit instruction is needed to ensure participants do actually match perspective or objective size, depending on the aims of the particular study.</p><p>In the present experiments, we need participants to attempt to match perspective size, that is, subjects are to report when they think the visual angles subtended by the discs are the same, not when the objective sizes of the discs match. This point is emphasised using two physical, white felt discs. The participant is invited to take a seat on the HIVE stage and swivel to face the left wall. A 27-cm disc is fixed to a black curtain and with the participant’s verbal assistance adjusted to their eye level. Holding a 19-cm disc at the participant’s eye level and next to the larger disc, the experimenter states ‘This is a large disc, this is a small disc, this one looks larger than the other, doesn’t it?’ The goal of this instruction is to make it clear that the smaller disc is really physically smaller than the large one and therefore when at the same distance it subtends a smaller angle. The participant acknowledges this and the smaller disc is then slowly carried by hand towards them at their eye level, being careful not to occlude the larger disc. As the experimenter does this, they state ‘As I bring this disc closer to your eyes it looks bigger in your field of view until finally it looks bigger than the other disc’.</p><p>The purpose of these instructions is to emphasise that the disc is not physically getting bigger, but it does fill a larger portion of the FOV as it moves closer. Once the participant acknowledges this, the experimenter states ‘Now if I slowly move the disc away from you is there a distance where both discs look the same size, i.e. where their perspective sizes match?’ Again, once the participant acknowledges a distance where this statement is true the experimenter states ‘During the experiment you will perform a similar task, but instead of moving a disc, both discs will be fixed at different distances. You will adjust the diameter of a far disc until its perspective size matches a near disc’.</p><p>Before the experiment, the participant’s interpupillary distance (IPD) is measured using software and a webcam, and the apparatus is calibrated to ensure the discs are at the correct distance, height and orientation. It is made clear to the participant that calibration is not part of the experiment. The distance between the participant’s chair and the HIVE screen is adjusted so that, when they turn to face each disc, the distance from their eyes to the near displays is 1.22 m and to the HIVE screen 2.72 m. The participant sits, turns to look at each disc and if adjustment is necessary stands up whilst the chair is moved forwards or back. This is repeated until the desired distances are achieved, and it takes 5–10 adjustments to get the desired position. The participant’s head is not held in position following this adjustment but is subsequently tracked continuously and their actual distances from the discs are used in the calculations of TR. We found that no participant performed the experiment at more (and most considerably less) than 7 cm from their initial calibration position and during measurements participants’ positions remained stable, with <span class="mathjax-tex">\(|l_{{\rm physical}}-l_{{\rm virtual}}|&lt;2\,{\hbox {cm}}\)</span>.</p><p>Next, the participant checks the height of their seat and if necessary adjusts it, so their feet are on the floor and they feel comfortably supported. At this point, the experimenter gives a command to the display software to match the height of the centre disc to the participant’s tracked eye level. The tripod holding the LED monitor is then raised or lowered manually until the participant reports that the disc height matches that of the centre disc. Because the LED monitor is tracked, the virtual monitor on the left also rises to the same position. The second tripod beneath the virtual disc is then adjusted to the same height as the first. As a final check, the participant is asked ‘If you glance across all three discs, they should all appear to be at the same height and level with your eyes?’ Finally, the angle of the LED monitor is adjusted until the participant states they are facing it directly and the disc therefore appears circular. The angle of the virtual monitor and disc automatically adjusts to mirror the LED monitor. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig7">7</a> illustrates the apparatus set-up following the calibration procedure.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Photograph of apparatus from the participant’s viewpoint following the calibration procedure. All three discs are at the same height, facing the viewer and level with the viewer’s eye. The black crosses drawn on the discs are to assist the calibration procedure. The stereo separation of the virtual disc and monitor (<i>left</i>) is not visible when the participant is wearing the LCD shutter glasses. This photograph illustrates how the virtual monitor appears to rest upon the physical tripod. The photograph has been manipulated for clarity of printing: the brightness and contrast do not match those of the experiment conditions; the image is desaturated; and the borders of the virtual monitor have been enhanced</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Once the apparatus has been calibrated, the participant is instructed on the experimental procedure. They are given the specific instruction ‘Upon each experimental trial you will be presented with two discs, one on the centre screen and one on either the left or right monitor’. No distinction is drawn between the virtual and physical monitors. It is explained that the discs are only visible when participants turn and face the display directly. They are handed a wireless mouse—‘With the mouse wheel you are to adjust the size of the centre disc until its perspective size matches the standard disc’. Participants are told that between each trial all the discs will disappear and a white box with a black cross will be displayed on the large screen in front for a period of three seconds, during which they are to look at the cross. It is explained that at the start of each trial the experimenter will say ‘left’ or ‘right’ to indicate which monitor the standard disc will appear on. They are reminded that for every trial the adjustable disc will be displayed on the centre screen.</p><p>Participants are told that they can turn back-and-forth between the two discs as many times as required and, to avoid any potential fatigue or strain from turning the head, are encouraged to do this by swivelling their chair. Once they are satisfied that both discs look the same, they are asked to acknowledge this verbally and the next trial commences. Finally, participants are told that they should not narrow or close their eyes and that they cannot use their fingers to aid them with the task. The participant is given two practice trials, one using the left monitor and the other using the right, and during these, they can ask any questions. After completing the two practice trials, the experiment begins.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Results</h2><div class="c-article-section__content" id="Sec7-content"><p>Sixteen participants were chosen at random from the postgraduate students and lecturers in the Computer Science Department at the University of Hull. All but three were male between the ages 23–58 with a mean age of 39. No participant was working within the field or knowledgeable of VEs or psychology, but all had had some exposure to head-tracked stereo displays at university open day events. Participants had normal or corrected-to-normal vision, but to maintain comparability with the Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e1909">1931a</a>) experiment we deliberately did not screen for stereo vision.</p><p>There are two experimental factors for the near disc—type and size. Disc type has two categorical levels—physical and virtual, whilst disc size has four interval levels—disc diameters were 15, 19, 23 and 27 cm. The chosen disc sizes are comparable with those of the Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e1915">1931a</a>) experiment, with 27 cm representing the maximum disc size that could be displayed on the available apparatus. Each participant performed five trials for each condition, making a total of 40 trials per experiment. During each trial, participants’ head positions were logged at each moment they faced a visible disc; these distances were used to calculate average values for <span class="mathjax-tex">\(l\)</span> and <span class="mathjax-tex">\(L\)</span>, in order to calculate the TR value for that trial. Participants were tested for all conditions within a single session with the order of conditions randomised.</p><p>Four datasets were eliminated prior to analysis: one participant ended the experiment prematurely due to eye strain caused by not wearing their corrective lenses; a second stated they had engaged in a strategy to use retinal after-images; a third participant said they had forgotten what they were supposed to do and might have performed objective instead of perspective matches; and a fourth was discovered to have performed the experiment at the wrong IPD due to an error in the procedure. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig8">8</a> shows the resulting means and standard deviations across five trials of physical and virtual TR values, per participant, per disc. Participants have been arranged in order of increasing overall mean physical TR value in order to demonstrate the range of TR values observed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Mean TR values per participant for each disc in the physical and virtual conditions, arranged in ascending order of overall mean value in the physical condition. <i>Error bars</i> denote <span class="mathjax-tex">\(\pm \sigma _{t}\)</span>, the trial standard deviation for each condition tested, showing that the regression effect exhibited by participants is considerably more variable in size than the variability in its measurement</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Mean TR values were subjected to a two-way repeated measures ANOVA. There were no outliers and the data were normally distributed for each group, as assessed by boxplot and Shapiro–Wilk test <span class="mathjax-tex">\((p &gt; .05)\)</span>, respectively. There was no significant interaction between disc type and disc size <span class="mathjax-tex">\(F(3, 33) = .281, p &gt; .05\)</span> and no significant main effect of disc type, <span class="mathjax-tex">\(F(1, 11) = 2.852, p &gt; .05\)</span>. However, for the disc size factor, Mauchly’s test indicated that the assumption of sphericity had been violated, <span class="mathjax-tex">\(\chi ^{2}(5) = 11.874, p = .037\)</span>, and therefore, the Greenhouse–Geisser correction was applied <span class="mathjax-tex">\((\varepsilon = .669)\)</span>, giving a significant main effect of disc size, <span class="mathjax-tex">\(F(2.008, 22.086) = 9.758, p &lt; .05\)</span>. For the disc size, post hoc pairwise analysis of TR with a Bonferroni adjustment revealed that TR increased statistically significantly from 15 to 19 cm (<i>M</i> = .026, 95 % CL [.009, .042], <i>p</i> = .002). TR decreased from size 19 to 27 cm (<i>M</i> = .048, 95 % CL[.013, .084], <i>p</i> = .007) and from size 23 to 27 cm (<i>M</i> = .046, 95 % CL [.085, .006], <i>p</i> = .021).</p><p>The degree of correlation of virtual and physical TR values was investigated by fitting a straight line, adopting the approach of Press et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Press WH, Teukolsky SA, Vetterling WT, Flannery BP, eds. (1992) Numerical Recipes in C: The Art of Scientific Computing, chap. Modelling of Data, 666–668. Cambridge University Press, 2nd edn" href="/article/10.1007/s10055-014-0257-x#ref-CR16" id="ref-link-section-d91675e2318">1992</a>, pp. 666–668) in order to take account of the presence of errors in both sets of measurements. Due to the significant main effect of disc size, this was done for each disc separately, with the results shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0257-x#Tab1">1</a> and the corresponding plots in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig9">9</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Slope, intercept, coefficient of determination <span class="mathjax-tex">\((R^2)\)</span> and <i>p</i>-value of virtual versus physical TR values, for each disc size</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0257-x/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0257-x/MediaObjects/10055_2014_257_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Correlation of physical and virtual TR values for disc sizes <b>a</b> 15 cm, <b>b</b> 19 cm, <b>c</b> 23 cm and <b>d</b> 27 cm. Model-fitting parameters, weighted <span class="mathjax-tex">\(R^2\)</span> and <span class="mathjax-tex">\(p\)</span> values can be found in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0257-x#Tab1">1</a>
                      </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0257-x/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Discussion</h2><div class="c-article-section__content" id="Sec8-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig8">8</a>a shows that participants matching perspective size in the physical world do indeed exhibit TR &gt; 0 as Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931a" title="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" href="/article/10.1007/s10055-014-0257-x#ref-CR21" id="ref-link-section-d91675e2659">1931a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e2662">b</a>) described and, furthermore, there is individual variation shown in this measure. In some individuals, the departure from TR = 0 is very marked (two with mean TR around or above 0.5), bearing in mind that TR = 1 equates to size constancy, or perfect objective matching. We are confident this is a genuine finding and not the result of inadvertent objective matching by participants, given the precautions we described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec6">4.2</a> and the disclosure of one participant who realised for themselves they had strayed from the required task. We also experienced one participant who asked to view their results after the experiment and could not believe they had not performed a perspective match. Thouless (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1931b" title="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" href="/article/10.1007/s10055-014-0257-x#ref-CR22" id="ref-link-section-d91675e2668">1931b</a>) mentioned that some of his participants made the same comment.</p><p>The error bars in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig8">8</a>a, b demonstrate that TR is generally reproducible for both the physical and virtual stimuli, at least for the duration of our experiment, except for two individuals A0 and A7 who exhibit rather large trial standard deviations. The reasons for this require more investigation, but indications from the other participants are encouraging, since searching for VE perceptual effects using TR will require it to be stable under experimental conditions. Also encouraging is the correlation demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig9">9</a> between physical and virtual TR values and the large coefficients of determination reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0257-x#Tab1">1</a>. Although the slopes and intercepts in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0257-x#Fig9">9</a> are not precisely unity and zero, these plots show sufficient relationship to warrant further investigation—no correlation would surely have ruled out TR as a potential quality metric.</p><p>The ANOVA results show no main effect on TR of disc type but do show an effect of disc size. The former was unexpected but on reflection perhaps not surprising in the AR set-up we describe. The virtual disc is augmented by a physical tripod as our aim was to make the appearance of the virtual and physical components as similar as possible. We surmise that participants converge to the correct distance of the physical tripod, outweighing the incorrect accommodation cue presented by the virtual monitor. This is consistent with the findings of Ellis and Menges (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431" href="/article/10.1007/s10055-014-0257-x#ref-CR3" id="ref-link-section-d91675e2689">1998</a>) and Singh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field augmented reality. In: Proceedings of the 7th symposium on applied perception in graphics and visualization, APGV ’10. ACM, New York, NY, pp 149–156" href="/article/10.1007/s10055-014-0257-x#ref-CR19" id="ref-link-section-d91675e2692">2010</a>), even though in our case the real element is only adjacent to the virtual object in the FOV, not actually occluding it. Whatever the mechanism, most participants remarked how the virtual monitor appeared uncannily real to them. Some had not even realised it was virtual and expressed their shock when, upon completion of the experiment, they removed the shutter glasses and saw that there was actually no monitor on the left of the stage. We consider the augmentation of the environment further in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec9">7</a> when we discuss our future experiments. The latter effect of disc size is surprising and requires more investigation. It might be due to weak control of the ambient light conditions, not from the room lights which were always set the same, but from variable light wash coming from the HIVE screen as participants adjusted the centre disc to phenomenal equality. Especially for participants with small TR, the far disc is physically large even though it subtends much the same angle at the eye as the small disc. We discuss how to mitigate this effect in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec9">7</a>.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Conclusions and future work</h2><div class="c-article-section__content" id="Sec9-content"><p>For TR to be a truly useful tool, we recognise that, as well as demonstrating it is measurable and comparable in the specific case described in this paper, we also have to establish whether it is sensitive to depth cue approximations in the VE. Thouless himself has proved this in the physical domain with his cue-reduction experiments, but it remains to be seen whether arguably more subtle effects, such as accommodation–vergence mismatch, have a measurable effect.</p><p>Our next experiment proposes to investigate precisely this. Using a variant of the apparatus described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0257-x#Sec5">4.1</a>, we will gradually mitigate the incorrect focus cue present in the virtual disc, by repeating the experiment at progressively shorter distances from the HIVE screen. We also intend to remove the tripod below the virtual disc, so that the only true cue to its distance is the vergence point as the participant fuses the stereo pairs.</p><p>We will also pay due regard to improving our overall experimental procedure. Variable ambient light level is one issue which we believe can be adequately controlled by the simple expedient of using grey rather than white discs. Additionally, we need to establish whether the current, mostly moderate variability in repeated trial values can be reduced still further. Several participants commented that perspective matching was a difficult task to perform and they started slowly; however, it became much easier as they relaxed. Therefore, it is feasible that more practice trials would prove beneficial.</p><p>In summary, the work of Thouless has often been inappropriately cited when investigating size constancy; it has been satisfying to reproduce his perspective-matching results many decades later with modern equipment and thereby re-read his accounts with fresh understanding. Our study is at a preliminary stage, but initial findings are encouraging that TR</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>can be determined independently of participants’ skill in spatial tasks, and</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>has potential as an intrinsic measure of VE spatial quality.</p>
                    
                  </li>
                </ol>
              </div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SH. Creem-Regehr, P. Willemsen, AA. Gooch, WB. Thompson, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Creem-Regehr SH, Willemsen P, Gooch AA, Thompson WB (2005) The influence of restricted viewing conditions on e" /><p class="c-article-references__text" id="ref-CR1">Creem-Regehr SH, Willemsen P, Gooch AA, Thompson WB (2005) The influence of restricted viewing conditions on egocentric distance perception: implications for real and virtual environments. Perception 34:191–204</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp5144" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20influence%20of%20restricted%20viewing%20conditions%20on%20egocentric%20distance%20perception%3A%20implications%20for%20real%20and%20virtual%20environments&amp;journal=Perception&amp;volume=34&amp;pages=191-204&amp;publication_year=2005&amp;author=Creem-Regehr%2CSH&amp;author=Willemsen%2CP&amp;author=Gooch%2CAA&amp;author=Thompson%2CWB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and " /><p class="c-article-references__text" id="ref-CR2">Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth. In: Handbook of perception and cognition; perception of space and motion, vol 5. Academic Press, San Diego, pp 69–117</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SR. Ellis, BM. Menges, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431" /><p class="c-article-references__text" id="ref-CR3">Ellis SR, Menges BM (1998) Localization of virtual objects in the near visual field. Hum Factors 40:415–431</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872098779591278" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Localization%20of%20virtual%20objects%20in%20the%20near%20visual%20field&amp;journal=Hum%20Factors&amp;volume=40&amp;pages=415-431&amp;publication_year=1998&amp;author=Ellis%2CSR&amp;author=Menges%2CBM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AS. Gilinsky, " /><meta itemprop="datePublished" content="1955" /><meta itemprop="headline" content="Gilinsky AS (1955) The effect of attitude upon the perception of size. Am J Psychol 68:173–192" /><p class="c-article-references__text" id="ref-CR4">Gilinsky AS (1955) The effect of attitude upon the perception of size. Am J Psychol 68:173–192</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F1418890" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20attitude%20upon%20the%20perception%20of%20size&amp;journal=Am%20J%20Psychol&amp;volume=68&amp;pages=173-192&amp;publication_year=1955&amp;author=Gilinsky%2CAS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Interrante V, Ries B, Anderson L (2006) Distance perception in immersive virtual environments, revisited. In: " /><p class="c-article-references__text" id="ref-CR5">Interrante V, Ries B, Anderson L (2006) Distance perception in immersive virtual environments, revisited. In: Proceedings of the IEEE conference on Virtual Reality, VR ’06. IEEE Computer Society, Washington, DC, pp 3–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Interrante, B. Ries, J. Lindquist, M. Kaeding, L. Anderson, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Interrante V, Ries B, Lindquist J, Kaeding M, Anderson L (2008) Elucidating factors that can facilitate veridi" /><p class="c-article-references__text" id="ref-CR6">Interrante V, Ries B, Lindquist J, Kaeding M, Anderson L (2008) Elucidating factors that can facilitate veridical spatial perception in immersive virtual environments. Presence Teleoper Virtual Environ 17:176–198</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.17.2.176" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Elucidating%20factors%20that%20can%20facilitate%20veridical%20spatial%20perception%20in%20immersive%20virtual%20environments&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=17&amp;pages=176-198&amp;publication_year=2008&amp;author=Interrante%2CV&amp;author=Ries%2CB&amp;author=Lindquist%2CJ&amp;author=Kaeding%2CM&amp;author=Anderson%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Knapp, JM. Loomis, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Knapp JM, Loomis JM (2004) Limited field of view of head-mounted displays is not the cause of distance underes" /><p class="c-article-references__text" id="ref-CR7">Knapp JM, Loomis JM (2004) Limited field of view of head-mounted displays is not the cause of distance underestimation in virtual environments. Presence Teleoper Virtual Environ 13:572–577</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F1054746042545238" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Limited%20field%20of%20view%20of%20head-mounted%20displays%20is%20not%20the%20cause%20of%20distance%20underestimation%20in%20virtual%20environments&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=13&amp;pages=572-577&amp;publication_year=2004&amp;author=Knapp%2CJM&amp;author=Loomis%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SA. Kuhl, WB. Thompson, SH. Creem-Regehr, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Kuhl SA, Thompson WB, Creem-Regehr SH (2009) HMD calibration and its effects on distance judgments. ACM Trans " /><p class="c-article-references__text" id="ref-CR8">Kuhl SA, Thompson WB, Creem-Regehr SH (2009) HMD calibration and its effects on distance judgments. ACM Trans Appl Percept 6:19:1–19:20</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1577755.1577762" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=HMD%20calibration%20and%20its%20effects%20on%20distance%20judgments&amp;journal=ACM%20Trans%20Appl%20Percept&amp;volume=6&amp;pages=19%3A1-19%3A20&amp;publication_year=2009&amp;author=Kuhl%2CSA&amp;author=Thompson%2CWB&amp;author=Creem-Regehr%2CSH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Loomis JM, Knapp JM (2003) Visual perception of egocentric distance in real and virtual environments. In: Virt" /><p class="c-article-references__text" id="ref-CR9">Loomis JM, Knapp JM (2003) Visual perception of egocentric distance in real and virtual environments. In: Virtual and adaptive environments applications, implications, and human performance issues. CRC Press, Boca Raton, pp 21–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mohler BJ, Bülthoff HH, Thompson WB, Creem-Regehr SH (2008) A full-body avatar improves egocentric distance ju" /><p class="c-article-references__text" id="ref-CR10">Mohler BJ, Bülthoff HH, Thompson WB, Creem-Regehr SH (2008) A full-body avatar improves egocentric distance judgments in an immersive virtual environment. In: Proceedings of the 5th symposium on applied perception in graphics and visualization, APGV ’08. ACM, New York, NY, p 194</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BJ. Mohler, SH. Creem-Regehr, WB. Thompson, HH. Bülthoff, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Mohler BJ, Creem-Regehr SH, Thompson WB, Bülthoff HH (2010) The effect of viewing a self-avatar on distance ju" /><p class="c-article-references__text" id="ref-CR11">Mohler BJ, Creem-Regehr SH, Thompson WB, Bülthoff HH (2010) The effect of viewing a self-avatar on distance judgments in an HMD-based virtual environment. Presence Teleoper Virtual Environ 19:230–242</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.19.3.230" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20viewing%20a%20self-avatar%20on%20distance%20judgments%20in%20an%20HMD-based%20virtual%20environment&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=19&amp;pages=230-242&amp;publication_year=2010&amp;author=Mohler%2CBJ&amp;author=Creem-Regehr%2CSH&amp;author=Thompson%2CWB&amp;author=B%C3%BClthoff%2CHH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Murgia, PM. Sharkey, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Murgia A, Sharkey PM (2009) Estimation of distances in virtual environments using size constancy. Int J Virtua" /><p class="c-article-references__text" id="ref-CR12">Murgia A, Sharkey PM (2009) Estimation of distances in virtual environments using size constancy. Int J Virtual Real 8:67–74</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20of%20distances%20in%20virtual%20environments%20using%20size%20constancy&amp;journal=Int%20J%20Virtual%20Real&amp;volume=8&amp;pages=67-74&amp;publication_year=2009&amp;author=Murgia%2CA&amp;author=Sharkey%2CPM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PE. Napieralski, BM. Altenhoff, JW. Bertrand, LO. Long, SV. Babu, CC. Pagano, J. Kern, TA. Davis, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Napieralski PE, Altenhoff BM, Bertrand JW, Long LO, Babu SV, Pagano CC, Kern J, Davis TA (2011) Near-field dis" /><p class="c-article-references__text" id="ref-CR13">Napieralski PE, Altenhoff BM, Bertrand JW, Long LO, Babu SV, Pagano CC, Kern J, Davis TA (2011) Near-field distance perception in real and virtual environments using both verbal and action responses. ACM Trans Appl Percept 8:18:1–18:19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2010325.2010328" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Near-field%20distance%20perception%20in%20real%20and%20virtual%20environments%20using%20both%20verbal%20and%20action%20responses&amp;journal=ACM%20Trans%20Appl%20Percept&amp;volume=8&amp;pages=18%3A1-18%3A19&amp;publication_year=2011&amp;author=Napieralski%2CPE&amp;author=Altenhoff%2CBM&amp;author=Bertrand%2CJW&amp;author=Long%2CLO&amp;author=Babu%2CSV&amp;author=Pagano%2CCC&amp;author=Kern%2CJ&amp;author=Davis%2CTA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IV. Piryankova, S. Rosa, U. Kloos, HH. Bulthoff, BJ. Mohler, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Piryankova IV, de la Rosa S, Kloos U, Bulthoff HH, Mohler BJ (2013) Egocentric distance perception in large sc" /><p class="c-article-references__text" id="ref-CR14">Piryankova IV, de la Rosa S, Kloos U, Bulthoff HH, Mohler BJ (2013) Egocentric distance perception in large screen immersive displays. Displays 34(2):153–164</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.displa.2013.01.001" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Egocentric%20distance%20perception%20in%20large%20screen%20immersive%20displays&amp;journal=Displays&amp;volume=34&amp;issue=2&amp;pages=153-164&amp;publication_year=2013&amp;author=Piryankova%2CIV&amp;author=Rosa%2CS&amp;author=Kloos%2CU&amp;author=Bulthoff%2CHH&amp;author=Mohler%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Ponto, M. Gleicher, RG. Radwin, HJ. Shin, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Ponto K, Gleicher M, Radwin RG, Shin HJ (2013) Perceptual calibration for immersive display environments. IEEE" /><p class="c-article-references__text" id="ref-CR15">Ponto K, Gleicher M, Radwin RG, Shin HJ (2013) Perceptual calibration for immersive display environments. IEEE Trans Visual Comput Graph 19:691–700</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2013.36" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20calibration%20for%20immersive%20display%20environments&amp;journal=IEEE%20Trans%20Visual%20Comput%20Graph&amp;volume=19&amp;pages=691-700&amp;publication_year=2013&amp;author=Ponto%2CK&amp;author=Gleicher%2CM&amp;author=Radwin%2CRG&amp;author=Shin%2CHJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Press WH, Teukolsky SA, Vetterling WT, Flannery BP, eds. (1992) Numerical Recipes in C: The Art of Scientific " /><p class="c-article-references__text" id="ref-CR16">Press WH, Teukolsky SA, Vetterling WT, Flannery BP, eds. (1992) Numerical Recipes in C: The Art of Scientific Computing, chap. Modelling of Data, 666–668. Cambridge University Press, 2nd edn</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ries B, Interrante V, Kaeding M, Anderson L (2008) The effect of self-embodiment on distance perception in imm" /><p class="c-article-references__text" id="ref-CR17">Ries B, Interrante V, Kaeding M, Anderson L (2008) The effect of self-embodiment on distance perception in immersive virtual environments. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST ’08. ACM, New York, NY, pp 167–170</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sedgwick HA (1986) Space perception. In: Handbook of perception and human performance, vol 1. Wiley, New York," /><p class="c-article-references__text" id="ref-CR18">Sedgwick HA (1986) Space perception. In: Handbook of perception and human performance, vol 1. Wiley, New York, pp 129–158</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field au" /><p class="c-article-references__text" id="ref-CR19">Singh G, Swan JE II, Jones JA, Ellis SR (2010) Depth judgment measures and occluding surfaces in near-field augmented reality. In: Proceedings of the 7th symposium on applied perception in graphics and visualization, APGV ’10. ACM, New York, NY, pp 149–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WB. Thompson, P. Willemsen, AA. Gooch, SH. Creem-Regehr, JM. Loomis, AC. Beall, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Thompson WB, Willemsen P, Gooch AA, Creem-Regehr SH, Loomis JM, Beall AC (2004) Does the quality of the comput" /><p class="c-article-references__text" id="ref-CR20">Thompson WB, Willemsen P, Gooch AA, Creem-Regehr SH, Loomis JM, Beall AC (2004) Does the quality of the computer graphics matter when judging distances in visually immersive environments. Presence Teleoper Virtual Environ 13:560–571</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F1054746042545292" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Does%20the%20quality%20of%20the%20computer%20graphics%20matter%20when%20judging%20distances%20in%20visually%20immersive%20environments&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=13&amp;pages=560-571&amp;publication_year=2004&amp;author=Thompson%2CWB&amp;author=Willemsen%2CP&amp;author=Gooch%2CAA&amp;author=Creem-Regehr%2CSH&amp;author=Loomis%2CJM&amp;author=Beall%2CAC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RH. Thouless, " /><meta itemprop="datePublished" content="1931" /><meta itemprop="headline" content="Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359" /><p class="c-article-references__text" id="ref-CR21">Thouless RH (1931a) Phenomenal regression to the real object. I. Br J Psychol Gen Sect 21:339–359</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.2044-8295.1931.tb00597.x" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Phenomenal%20regression%20to%20the%20real%20object.%20I&amp;journal=Br%20J%20Psychol%20Gen%20Sect&amp;volume=21&amp;pages=339-359&amp;publication_year=1931&amp;author=Thouless%2CRH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RH. Thouless, " /><meta itemprop="datePublished" content="1931" /><meta itemprop="headline" content="Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30" /><p class="c-article-references__text" id="ref-CR22">Thouless RH (1931b) Phenomenal regression to the real object. II. Br J Psychol Gen Sect 22:1–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.2044-8295.1931.tb00609.x" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Phenomenal%20regression%20to%20the%20real%20object.%20II&amp;journal=Br%20J%20Psychol%20Gen%20Sect&amp;volume=22&amp;pages=1-30&amp;publication_year=1931&amp;author=Thouless%2CRH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Willemsen P, Gooch AA (2002) An experimental comparison of perceived egocentric distance in real, image-based," /><p class="c-article-references__text" id="ref-CR23">Willemsen P, Gooch AA (2002) An experimental comparison of perceived egocentric distance in real, image-based, and traditional virtual environment using direct walking tasks. Tech. rep., University of Utah Computer Science</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Willemsen, AA. Gooch, WB. Thompson, SH. Creem-Regehr, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Willemsen P, Gooch AA, Thompson WB, Creem-Regehr SH (2008) Effects of stereo viewing conditions on distance pe" /><p class="c-article-references__text" id="ref-CR24">Willemsen P, Gooch AA, Thompson WB, Creem-Regehr SH (2008) Effects of stereo viewing conditions on distance perception in virtual environments. Presence Teleoper Virtual Environ 17:91–101</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.17.1.91" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20stereo%20viewing%20conditions%20on%20distance%20perception%20in%20virtual%20environments&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=17&amp;pages=91-101&amp;publication_year=2008&amp;author=Willemsen%2CP&amp;author=Gooch%2CAA&amp;author=Thompson%2CWB&amp;author=Creem-Regehr%2CSH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Willemsen, MB. Colton, SH. Creem-Regehr, WB. Thompson, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Willemsen P, Colton MB, Creem-Regehr SH, Thompson WB (2009) The effects of head-mounted display mechanical pro" /><p class="c-article-references__text" id="ref-CR25">Willemsen P, Colton MB, Creem-Regehr SH, Thompson WB (2009) The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments. ACM Trans Appl Percept 6:8:1–8:14</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1498700.1498702" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20head-mounted%20display%20mechanical%20properties%20and%20field%20of%20view%20on%20distance%20judgments%20in%20virtual%20environments&amp;journal=ACM%20Trans%20Appl%20Percept&amp;volume=6&amp;pages=8%3A1-8%3A14&amp;publication_year=2009&amp;author=Willemsen%2CP&amp;author=Colton%2CMB&amp;author=Creem-Regehr%2CSH&amp;author=Thompson%2CWB">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0257-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We acknowledge useful discussions with Derek Wills and James Ward of the Department of Computer Science at the University of Hull. KWE was supported by a doctoral student scholarship funded by the Department of Computer Science at the University of Hull.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, University of Hull, Hull, HU6 7RX, UK</p><p class="c-article-author-affiliation__authors-list">Kevin W. Elner &amp; Helen Wright</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Kevin_W_-Elner"><span class="c-article-authors-search__title u-h3 js-search-name">Kevin W. Elner</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kevin W.+Elner&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kevin W.+Elner" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kevin W.+Elner%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Helen-Wright"><span class="c-article-authors-search__title u-h3 js-search-name">Helen Wright</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Helen+Wright&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Helen+Wright" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Helen+Wright%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0257-x/email/correspondent/c1/new">Kevin W. Elner</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Phenomenal%20regression%20to%20the%20real%20object%20in%20physical%20and%20virtual%20worlds&amp;author=Kevin%20W.%20Elner%20et%20al&amp;contentID=10.1007%2Fs10055-014-0257-x&amp;publication=1359-4338&amp;publicationDate=2014-12-06&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0257-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0257-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Elner, K.W., Wright, H. Phenomenal regression to the real object in physical and virtual worlds.
                    <i>Virtual Reality</i> <b>19, </b>21–31 (2015). https://doi.org/10.1007/s10055-014-0257-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0257-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06-26">26 June 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11-24">24 November 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-12-06">06 December 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-03">March 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0257-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0257-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Depth and size perception</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Depth compression</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Index of phenomenal regression</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Thouless ratio</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Brunswik ratio</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0257-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=257;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

