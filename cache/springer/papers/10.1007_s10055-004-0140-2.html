<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Conceptualising mixed spaces of interaction for designing continuous i"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Recent progress in the overlay and registration of digital information on the user&#8217;s workspace in a spatially meaningful way has allowed mixed reality (MR) to become a more effective..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Conceptualising mixed spaces of interaction for designing continuous interaction"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-01-14"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Recent progress in the overlay and registration of digital information on the user&#8217;s workspace in a spatially meaningful way has allowed mixed reality (MR) to become a more effective operational medium. However, research in software structures, design methods and design support tools for MR systems is still in its infancy. In this paper, we propose a conceptual classification of the design space to support the development of MR systems. The proposed design space (DeSMiR) is an abstract tool for systematically exploring several design alternatives at an early stage of interaction design, without being biassed towards a particular modality or technology. Once the abstract design possibilities have been identified and a concrete design decision has been taken (i.e. a specific modality has been selected), a concrete MR application can be considered in order to analyse the interaction techniques in terms of continuous interaction properties. We suggest that our design space can be applied to the design of several kinds of MR applications, especially those in which very little user focus distraction can be tolerated, and where smooth connections and interactions between real and virtual worlds is critical for the system development. An image-guided surgery system (IGS) is used as a case study."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-01-14"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="83"/>

    <meta name="prism.endingPage" content="95"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0140-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0140-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0140-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0140-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Conceptualising mixed spaces of interaction for designing continuous interaction"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2005/01/14"/>

    <meta name="citation_firstpage" content="83"/>

    <meta name="citation_lastpage" content="95"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0140-2"/>

    <meta name="DOI" content="10.1007/s10055-004-0140-2"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0140-2"/>

    <meta name="description" content="Recent progress in the overlay and registration of digital information on the user&#8217;s workspace in a spatially meaningful way has allowed mixed realit"/>

    <meta name="dc.creator" content="Daniela Gorski Trevisan"/>

    <meta name="dc.creator" content="Jean Vanderdonckt"/>

    <meta name="dc.creator" content="Beno&#238;t Macq"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Akesson KP, Simsarian K (1999) Reality portals. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST&#8217;99), London, UK, December 1999. ACM Press, New York, ISBN 1-58113-141-0"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Maintaining knowledge about temporal intervals; citation_author=JF Allen; citation_volume=26; citation_issue=11; citation_publication_date=1993; citation_pages=832-843; citation_doi=10.1145/182.358434; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Focusing on the essential: considering attention in display design; citation_author=P Baudish, D DeCarlo, AT Duchowski, WS Geiser; citation_volume=46; citation_issue=3; citation_publication_date=2003; citation_pages=60-66; citation_doi=10.1145/636772.636799; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput&#8211;Human Interact; citation_title=Understanding and constructing shared spaces with mixed reality boundaries; citation_author=S Benford, C Greenhalt, G Reynard, C Brown, B Koleva; citation_volume=5; citation_issue=3; citation_publication_date=1998; citation_pages=185-223; citation_doi=10.1145/292834.292836; citation_id=CR4"/>

    <meta name="citation_reference" content="Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In: Proceedings of the IEEE international workshop on projector-camera systems (Procams 2003), Nice, France, October 2003"/>

    <meta name="citation_reference" content="Bier EA, Stone MC, Fishkin K, Buxton W, Baudel T (1994) A taxonomy of see-through tools. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI&#8217;94), Boston, Massachusetts, April 1994. ACM Press, New York, pp 358&#8211;364"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Shared space: an augmented reality approach for computer supported collaborative work; citation_author=M Billinghurst, S Weghorst, TA III Furness; citation_volume=3; citation_issue=1; citation_publication_date=1998; citation_pages=25-36; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGHCI Bull; citation_title=HCI and requirements engineering: exploring human&#8211;computer interaction and software engineering methodologies for the creation of interactive software; citation_author=J Brown; citation_volume=29; citation_issue=1; citation_publication_date=1997; citation_pages=32-35; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=J Univer Access Inf Soci; citation_title=Assessing continuity and compatibility in augmented reality systems; citation_author=E Dubois, L Nigay, J Troccaz; citation_volume=1; citation_issue=4; citation_publication_date=2002; citation_pages=263-273; citation_doi=10.1007/s10209-002-0024-8; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_author=null Dubois; citation_volume=15; citation_publication_date=2003; citation_pages=497; citation_doi=10.1016/S0953-5438(03)00037-7; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_title=Virtual reality: scientific and technological challenges; citation_publication_date=1995; citation_id=CR11; citation_author=NI Durlach; citation_author=AS Mavor; citation_publisher=National Academy Press"/>

    <meta name="citation_reference" content="Florins M, Trevisan D, Vanderdonckt J (2004) The continuity property in mixed reality and multiplatform systems: a comparative study. In: Proceedings of the 5th international conference on computer-aided design of user interfaces (CADUI 2004), Madeira Island, Portugal, January 2004, pp 328&#8211;339"/>

    <meta name="citation_reference" content="Graham TCN, Watts LA, Calvary G, Coutaz J, Dubois E, Nigay L (2000) A dimension space for the design of interactive systems within their physical environments. In: Proceedings of the conference on designing interactive systems (DIS 2000), New York City, August 2000. ACM Press, New York, pp 406&#8211;416"/>

    <meta name="citation_reference" content="Harrison BL, Ishii H, Vicente KJ, Buxton WAS (1995) Transparent layered user interfaces: an evaluation of a display design to enhance focused and divided attention. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI&#8217;95), Denver, Colorado, May 1995. ACM Press, New York, pp 317&#8211;324"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Iterative design of seamless collaboration media; citation_author=H Ishii, M Kobayashi, K Arita; citation_volume=37; citation_issue=8; citation_publication_date=1994; citation_pages=83-97; citation_id=CR15"/>

    <meta name="citation_reference" content="ISMAR (2003) Proceedings of the 2nd IEEE and ACM international symposium on mixed and augmented reality, Tokyo, Japan, October 2003. Accessible at 
                    http://www.ismar03.org/
                    
                  "/>

    <meta name="citation_reference" content="Klemmer SR (2003) Papier-m&#226;ch&#233;: toolkit support for tangible interaction. In: Proceedings of the 16th annual ACM symposium on user interface software and technology (UIST 2003), Vancouver, British Columbia, Canada, November 2003, doctoral consortium paper "/>

    <meta name="citation_reference" content="Marichal X, Macq B, Douxchamps D, Umeda T, art.live consortium (2003) Real-time segmentation of video objects for mixed-reality interactive applications. In: Proceedings of the SPIE conference on visual communication and image processing (VCIP 2003), Lugano, Switzerland, July 2003, vol 5150, pp 41&#8211;50"/>

    <meta name="citation_reference" content="citation_title=A taxonomy of real and virtual world display integration; citation_inbook_title=In: Mixed reality: merging real and virtual environments.; citation_publication_date=1999; citation_pages=1-16; citation_id=CR19; citation_author=P Milgran; citation_author=H Jr Colquhoun; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Dubois E, Gray P, Trevisan D, Vanderdonckt J (eds) (2004) MIXER&#8217;04. In: Proceedings of the IUI-CADUI 2004 international workshop on exploring the design and engineering of mixed reality systems, Island of Madeira, Portugal, January 2004. Also available at 
                    http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/
                    
                  , ISSN pp 1613&#8211;0073"/>

    <meta name="citation_reference" content="Nigay L, Dubois E, Renevier P, Pasqualetti L, Troccaz J (2003) Mixed systems: combining physical and digital worlds. In: Proceedings of the 10th international conference on human&#8211;computer interaction (HCI International 2003), Crete, Greece, June 2003, pp 1203&#8211;1207"/>

    <meta name="citation_reference" content="citation_title=User centered system design: new perspectives on human-computer interaction; citation_publication_date=1986; citation_id=CR22; citation_author=DA Norman; citation_author=SW Draper; citation_publisher=Lawrence Erlbaum"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Computer; citation_title=Developing a generic augmented reality interface; citation_author=I Poupyrev, DS Tan, M. Billinghurst, H Kato, H Regembrecht, N Tetsutani; citation_volume=35; citation_issue=3; citation_publication_date=2002; citation_pages=44-50; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_title=Software engineering: a practitioner&#8217;s approach; citation_publication_date=2001; citation_id=CR24; citation_author=RS Pressman; citation_publisher=McGraw-Hill"/>

    <meta name="citation_reference" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST&#8217;95), Pittsburgh, Pennsylvania, November 1995, pp 29&#8211;36"/>

    <meta name="citation_reference" content="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI&#8217;99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378&#8211;385"/>

    <meta name="citation_reference" content="Renevier P, Nigay L (2001) Mobile collaborative augmented reality: the augmented stroll. In: Proceedings of the 8th IFIP international conference on engineering for human&#8211;computer interaction (EHCI 2001), Toronto, Canada, May 2001. Lecture notes in computer science, vol 2254, Springer, Berlin Heidelberg New York, pp 315&#8211;334"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A conceptual framework for mixed reality environments: designing novel learning activities for young children; citation_author=Y Rogers, M Scaife, S Gabrielli, H Smith, E Harris; citation_volume=11; citation_issue=6; citation_publication_date=2002; citation_pages=677-686; citation_id=CR28"/>

    <meta name="citation_reference" content="STARS (2003) Proceedings of the international workshop on software technology for augmented reality systems, Tokyo, Japan, October 2003. Accessible ate 
                    http://stars2003.in.tum.de/
                    
                  "/>

    <meta name="citation_reference" content="citation_journal_title=Multimedia and virtual; citation_author=null Sutcliffe; citation_volume=reality; citation_publication_date=2003; citation_pages=designing; citation_id=CR30"/>

    <meta name="citation_reference" content="Tanriverdi V, Jacob RJK (2001) VRID: a design model and methodology for developing virtual reality interfaces. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST 2001), Banff, Alberta, Canada, November 2001. ACM Press, New York, pp 175&#8211;182"/>

    <meta name="citation_reference" content="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108&#8211;118"/>

    <meta name="citation_reference" content="Trevisan DG, Vanderdonckt J, Macq BM (2004) Designing interaction space for mixed reality systems. In: Proceedings of the international workshop on exploring the design and engineering of mixed reality systems (MIXER&#8217;04). Available at 
                    http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/paperD5.pdf
                    
                  , ISSN 1613&#8211;0073:27&#8211;34"/>

    <meta name="citation_reference" content="Vanderdonckt J, Bodart F (1993) Encapsulating knowledge for intelligent interaction objects selection. In: Adjunct proceedings of the joint conference of ACM SIGCHI and INTERACT (InterCHI&#8217;93), Amsterdam, The Netherlands, April 1993. ACM Press, New York, pp 424&#8211;429"/>

    <meta name="citation_reference" content="citation_journal_title=Multimedia Syst; citation_author=null Vazirgiannis; citation_volume=6; citation_publication_date=1998; citation_pages=284; citation_doi=10.1007/s005300050094; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_title=Attentive user interface Commun ACM vol 46(3); citation_publication_date=2003; citation_id=CR36; citation_author=R Vertegaal; citation_publisher=ACM Press"/>

    <meta name="citation_author" content="Daniela Gorski Trevisan"/>

    <meta name="citation_author_email" content="trevisan@tele.ucl.ac.be"/>

    <meta name="citation_author_institution" content="Communication and Remote Sensing Laboratory, Universit&#233; catholique de Louvain, Louvain-la-Neuve, Belgium"/>

    <meta name="citation_author_institution" content="Belgium Laboratory of Computer&#8211;Human Interaction, Universit&#233; catholique de Louvain, Louvain-la-Neuve, Belgium"/>

    <meta name="citation_author" content="Jean Vanderdonckt"/>

    <meta name="citation_author_institution" content="Belgium Laboratory of Computer&#8211;Human Interaction, Universit&#233; catholique de Louvain, Louvain-la-Neuve, Belgium"/>

    <meta name="citation_author" content="Beno&#238;t Macq"/>

    <meta name="citation_author_institution" content="Communication and Remote Sensing Laboratory, Universit&#233; catholique de Louvain, Louvain-la-Neuve, Belgium"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0140-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0140-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Conceptualising mixed spaces of interaction for designing continuous interaction"/>
        <meta property="og:description" content="Recent progress in the overlay and registration of digital information on the user’s workspace in a spatially meaningful way has allowed mixed reality (MR) to become a more effective operational medium. However, research in software structures, design methods and design support tools for MR systems is still in its infancy. In this paper, we propose a conceptual classification of the design space to support the development of MR systems. The proposed design space (DeSMiR) is an abstract tool for systematically exploring several design alternatives at an early stage of interaction design, without being biassed towards a particular modality or technology. Once the abstract design possibilities have been identified and a concrete design decision has been taken (i.e. a specific modality has been selected), a concrete MR application can be considered in order to analyse the interaction techniques in terms of continuous interaction properties. We suggest that our design space can be applied to the design of several kinds of MR applications, especially those in which very little user focus distraction can be tolerated, and where smooth connections and interactions between real and virtual worlds is critical for the system development. An image-guided surgery system (IGS) is used as a case study."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Conceptualising mixed spaces of interaction for designing continuous interaction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0140-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Design space, Mixed reality, Continuous interaction, Image-guided surgery","kwrd":["Design_space","Mixed_reality","Continuous_interaction","Image-guided_surgery"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0140-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0140-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=140;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0140-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Conceptualising mixed spaces of interaction for designing continuous interaction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0140-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0140-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-01-14" itemprop="datePublished">14 January 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Conceptualising mixed spaces of interaction for designing continuous interaction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniela_Gorski-Trevisan" data-author-popup="auth-Daniela_Gorski-Trevisan" data-corresp-id="c1">Daniela Gorski Trevisan<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université catholique de Louvain" /><meta itemprop="address" content="grid.7942.8, 000000012294713X, Communication and Remote Sensing Laboratory, Université catholique de Louvain, Place du Levant 2, 1348, Louvain-la-Neuve, Belgium" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université catholique de Louvain" /><meta itemprop="address" content="grid.7942.8, 000000012294713X, Belgium Laboratory of Computer–Human Interaction, Université catholique de Louvain, Place du Doyens 1, 1348, Louvain-la-Neuve, Belgium" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jean-Vanderdonckt" data-author-popup="auth-Jean-Vanderdonckt">Jean Vanderdonckt</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université catholique de Louvain" /><meta itemprop="address" content="grid.7942.8, 000000012294713X, Belgium Laboratory of Computer–Human Interaction, Université catholique de Louvain, Place du Doyens 1, 1348, Louvain-la-Neuve, Belgium" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Beno_t-Macq" data-author-popup="auth-Beno_t-Macq">Benoît Macq</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Université catholique de Louvain" /><meta itemprop="address" content="grid.7942.8, 000000012294713X, Communication and Remote Sensing Laboratory, Université catholique de Louvain, Place du Levant 2, 1348, Louvain-la-Neuve, Belgium" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">83</span>–<span itemprop="pageEnd">95</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">151 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0140-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Recent progress in the overlay and registration of digital information on the user’s workspace in a spatially meaningful way has allowed mixed reality (MR) to become a more effective operational medium. However, research in software structures, design methods and design support tools for MR systems is still in its infancy. In this paper, we propose a conceptual classification of the design space to support the development of MR systems. The proposed design space (DeSMiR) is an abstract tool for systematically exploring several design alternatives at an early stage of interaction design, without being biassed towards a particular modality or technology. Once the abstract design possibilities have been identified and a concrete design decision has been taken (i.e. a specific modality has been selected), a concrete MR application can be considered in order to analyse the interaction techniques in terms of continuous interaction properties. We suggest that our design space can be applied to the design of several kinds of MR applications, especially those in which very little user focus distraction can be tolerated, and where smooth connections and interactions between real and virtual worlds is critical for the system development. An image-guided surgery system (IGS) is used as a case study.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Emerging new interaction modalities, such as gesture, speech or other natural human–computer interactions (HCIs), allow applications to be created with a seamless integration of computer-based information into the real world. In such systems, the user interface can be moved from the classical computer screen to tangible interfaces<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> to create a smooth and continuous transition between the digital and the real world.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig1">1</a> shows how multimodal and augmented styles of interaction relate to those found in traditional graphical user interfaces (GUIs). In multimodal interfaces, menus and alerts are replaced, when necessary, by a delicate negotiation of turns with the user through peripheral channels (e.g. by using voice and gesture recognition). Foreground activities are distinguished from background activities by using seamless transitions in the density of the information supplied, rather than by a discrete windowing system. For instance, the display content can be adjusted by using different resolutions to direct the user’s attention to the desired place, or for gaze-contingent compression purposes (Baudish et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Baudish P, DeCarlo D, Duchowski AT, Geiser WS (2003) Focusing on the essential: considering attention in display design. Commun ACM 46(3):60–66" href="/article/10.1007/s10055-004-0140-2#ref-CR3" id="ref-link-section-d1802e337">2003</a>). By using multimodal and augmented reality (AR) systems, the user’s focus is shared between the real and the virtual world and the interaction becomes tangible (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan DS, Billinghurst M., Kato H, Regembrecht H, Tetsutani N (2002) Developing a generic augmented reality interface. IEEE Computer 35(3):44–50" href="/article/10.1007/s10055-004-0140-2#ref-CR23" id="ref-link-section-d1802e340">2002</a>; Rekimoto and Saitoh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378–385" href="/article/10.1007/s10055-004-0140-2#ref-CR26" id="ref-link-section-d1802e343">1999</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The way in which multimodal and augmented interaction styles relate to those found in traditional GUIs. Adapted from Vertegaal (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Vertegaal R (2003) Introduction to special issue on “Attentive user interfaces.” Commun ACM 46(3):pp 30–33" href="/article/10.1007/s10055-004-0140-2#ref-CR36" id="ref-link-section-d1802e356">2003</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This change of paradigm entails making a distinction between different kinds of systems with respect to their reality–virtuality poles (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb2.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb2.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Milgran’s reality–virtuality continuum (Milgran and Colquhoun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Milgran P, Colquhoun H Jr (1999) A taxonomy of real and virtual world display integration. In: Proceedings of the 1st international symposium on mixed reality (ISMR’99): merging real and virtual environments, Yokohama, Japan, March 1999. Springer, Berlin Heidelberg New York, pp 1–16" href="/article/10.1007/s10055-004-0140-2#ref-CR19" id="ref-link-section-d1802e383">1999</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Milgran and Colquhoun (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Milgran P, Colquhoun H Jr (1999) A taxonomy of real and virtual world display integration. In: Proceedings of the 1st international symposium on mixed reality (ISMR’99): merging real and virtual environments, Yokohama, Japan, March 1999. Springer, Berlin Heidelberg New York, pp 1–16" href="/article/10.1007/s10055-004-0140-2#ref-CR19" id="ref-link-section-d1802e397">1999</a>) argue that real environments (RE) and virtual environments (VE) are, in fact, two poles of a reality–virtuality continuum, RE being the left pole and VE the right pole. Mixed reality (MR) covers all the continuum between RE and VE, including augmented reality (AR), and augmented virtuality (AV), but excludes the end-points, perceived as limit conditions. Therefore, we can say that AR and AV are components of MR, and MR systems are any possible combination of real and virtual information<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig3">3</a> shows a microscope-image-guided surgery system (IGS). In such systems, complex surgical procedures can be navigated visually with great precision by overlaying a colour-coded plan extracted from pre-operative images (left image in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig3">3</a>) on a live video of the patient (right image in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig3">3</a>). The overlapped information can specify details such as the location of incisions, areas to be avoided and the diseased tissue. This is a typical application of an AR system, where the digital information corresponding to the pre-operative surgical planning is carefully aligned in real time with the real world, corresponding to the intra-operative information (i.e. the live video of the patient).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Image-guided surgery (IGS); an example of an AR system (Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108–118" href="/article/10.1007/s10055-004-0140-2#ref-CR32" id="ref-link-section-d1802e428">2003</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>An example of AV is the Transfiction system<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> (Marichal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Marichal X, Macq B, Douxchamps D, Umeda T, art.live consortium (2003) Real-time segmentation of video objects for mixed-reality interactive applications. In: Proceedings of the SPIE conference on visual communication and image processing (VCIP 2003), Lugano, Switzerland, July 2003, vol 5150, pp 41–50" href="/article/10.1007/s10055-004-0140-2#ref-CR18" id="ref-link-section-d1802e455">2003</a>) shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig4">4</a>: video images are analysed to capture the users’ movements and these are integrated into a virtual graphical scene, which reacts in an interactive manner to the behaviour of the filmed subject(s).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The Transfiction system; an example of an AV system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The development and implementation of such systems becomes very complex and the methodologies<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> developed for the design of conventional interfaces are no longer valid for modelling, analysing and designing virtual, augmented and mixed realities.</p><p>The ability to overlay and to register digital information on the user’s workspace in a spatially meaningful way allows MR to be an extremely effective operational medium. Research into software structures, design methods and design support tools for MR systems is still in its infancy. The first efforts in this direction can be found in the proceedings of the ISMAR (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="ISMAR (2003) Proceedings of the 2nd IEEE and ACM international symposium on mixed and augmented reality, Tokyo, Japan, October 2003. Accessible at &#xA;                    http://www.ismar03.org/&#xA;                    &#xA;                  " href="/article/10.1007/s10055-004-0140-2#ref-CR16" id="ref-link-section-d1802e496">2003</a>), MIXER (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Dubois E, Gray P, Trevisan D, Vanderdonckt J (eds) (2004) MIXER’04. In: Proceedings of the IUI-CADUI 2004 international workshop on exploring the design and engineering of mixed reality systems, Island of Madeira, Portugal, January 2004. Also available at &#xA;                    http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/&#xA;                    &#xA;                  , ISSN pp 1613–0073" href="/article/10.1007/s10055-004-0140-2#ref-CR20" id="ref-link-section-d1802e499">2004</a>) and STARS (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="STARS (2003) Proceedings of the international workshop on software technology for augmented reality systems, Tokyo, Japan, October 2003. Accessible ate &#xA;                    http://stars2003.in.tum.de/&#xA;                    &#xA;                  " href="/article/10.1007/s10055-004-0140-2#ref-CR29" id="ref-link-section-d1802e502">2003</a>) conferences and workshops, which have focussed mostly on AR systems. These early results show clearly that the lack of an efficient design methodology can lead to major drawbacks, such as discontinuous interactions, in MR systems. Indeed, most of the MR interfaces offer users two different interfaces—one to deal with the real world and another for the virtual one—and force them to switch between operation modes, so compromising the usability of the system (Ishii et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8)83–97" href="/article/10.1007/s10055-004-0140-2#ref-CR15" id="ref-link-section-d1802e505">15</a>).</p><p>In order to address these challenges, this research identifies the foundations of MR systems, as well as providing support for design and early evaluation of MR applications. For this, we propose a design space for MR systems (DeSMiR), which is based on the following key abstractions: “transform type,” “kind of augmentation,” “connection type between worlds,” “user’s interaction focus” and “insertion context of interaction spaces.” Once the abstract design possibilities have been identified and a concrete design decision has been taken (e.g. by electing a specific modality), a concrete MR application can be considered in order to analyse the interaction techniques in terms of continuous interaction properties.</p><p>We suggest that this methodology could be applied to the design of several kinds of MR applications, especially those for which smooth connections and interactions with different worlds are required. Examples might be in-vehicle information systems and image-guided surgery (IGS) systems, where the additional information should be provided without diverting attention from the task in hand (driving or surgery). An IGS system will be used to illustrate our approach.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related works</h2><div class="c-article-section__content" id="Sec2-content"><p>Several studies have tried to divide interactive applications into different classes in order to create a taxonomy of such systems. The interest of these approaches for the design step of an interactive system is primarily to help the designer to identify which existing applications have a similar behaviour and role to the one s/he is intending to develop. The designer may then explore the design solutions used in applications of the same class in the taxonomy, and may even try to reuse parts of the technical solution.</p><p>Klemmer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Klemmer SR (2003) Papier-mâché: toolkit support for tangible interaction. In: Proceedings of the 16th annual ACM symposium on user interface software and technology (UIST 2003), Vancouver, British Columbia, Canada, November 2003, doctoral consortium paper " href="/article/10.1007/s10055-004-0140-2#ref-CR17" id="ref-link-section-d1802e522">2003</a>) identifies four kinds of tangible user interfaces (TUI): spatial, topological, associative and forms. A spatial application augments, for example, walls, tables and whiteboards; a topological application is one in which acting on physical objects modifies the behaviour of the application; in systems of the associative class, physical artifacts serve as handles onto digital entities (URLs, files, etc.); finally, forms applications add digital services to natural physical interactions (e.g. scanning a paper).</p><p>The “dimension space” (Graham et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Graham TCN, Watts LA, Calvary G, Coutaz J, Dubois E, Nigay L (2000) A dimension space for the design of interactive systems within their physical environments. In: Proceedings of the conference on designing interactive systems (DIS 2000), New York City, August 2000. ACM Press, New York, pp 406–416" href="/article/10.1007/s10055-004-0140-2#ref-CR13" id="ref-link-section-d1802e528">2000</a>) classifies entities in an MR system according to the attention they receive, and their role, manifestation, I/O capacities and informational density. Using the axes of the dimension space, we can ask questions such as “what is the purpose of the entity (from the point of view of a user carrying out a task)?”, “how does the entity combine physical and virtual attributes?” and “how does the entity constrain the use of other entities?” The dimension space is not built to replace methods for describing the mechanisms by which people make sense of and organise their work. However, it includes, with the existing methods, a capability for generating a “design space analysis” tailored for hybrid physical–virtual collaborative systems. In terms of the development cycle, it fits into the system design activity, between system requirements and software requirements, as part of the process of refining the entities.</p><p>ASUR was initially designed for the description of the physical and digital entities that make up a mixed system, including user(s), physical and digital artifacts, and physical and informational relationships (Dubois <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Dubois E, Gray PD, Nigay L (2003) ASUR++: a design notation for mobile mixed systems. Interact Comput 15(4):497–520" href="/article/10.1007/s10055-004-0140-2#ref-CR10" id="ref-link-section-d1802e534">2003</a>). The purpose of ASUR is to help us understand how to combine the physical and digital worlds by identifying physical and digital objects involved in the system to be designed (the so-called ASUR components), and the boundaries between the two worlds (the ASUR relations). A list of characteristics refines the description of the ASUR components and relations. The use of this notation allows the designer to: (1) identify several design solutions; (2) describe them in a way that aides the comparison and (3) study ergonomic properties in a predictable way.</p><p>The virtual reality interface design (VRID) (Tanriverdi and Jacob <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Tanriverdi V, Jacob RJK (2001) VRID: a design model and methodology for developing virtual reality interfaces. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST 2001), Banff, Alberta, Canada, November 2001. ACM Press, New York, pp 175–182" href="/article/10.1007/s10055-004-0140-2#ref-CR31" id="ref-link-section-d1802e541">2001</a>) proposes a model and an associated methodology to design virtual reality interfaces. The model is based on components: graphics, behaviour, interaction and mediator. The methodology consists in applying the model at two design phases: the high and low levels. In the high-level design phase, the goal is to specify a design solution, at a high-level of abstraction, using the multi-component object architecture as a conceptual guide. In the low-level design phase, the goal is to provide fine-grained details of the high-level representations, and to provide procedural details as to how they will be formally represented. The outcome of the low-level design is a set of design specifications, which are represented in formal, implementation-oriented terminology, ready to be implemented by software developers.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Mixed interaction space</h2><div class="c-article-section__content" id="Sec3-content"><p>An interaction space (IS) is assumed to be the complete presentation environment required for carrying out a particular interactive task. The interaction space contains representations of the visual, haptic and auditory elements that a user interface offers to its users, as well as their relationships. It is very often required to deal with questions such as whether particular objects or scenes being displayed are real or virtual, whether images of scanned data should be considered real or virtual and whether a real object must look realistic, whereas a virtual one need not. For example, in some AR systems, it is difficult to label the remotely viewed video scene as “real” and the computer generated images as “virtual.” Compare this to an MR system in which the user reaches into a computer-generated scene with his or her own hand and “grabs” an object. There is no doubt, in this case, that the object being grabbed is “virtual” and the hand is “real.” In comparing these two examples, it is clear that the reality of the hand and the reality of a video image are quite different, which suggests that a decision must be made about whether it is appropriate to use the term “real” for both cases.</p><p>In this work, we adopt the distinction made by Milgran and Colquhoun (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Milgran P, Colquhoun H Jr (1999) A taxonomy of real and virtual world display integration. In: Proceedings of the 1st international symposium on mixed reality (ISMR’99): merging real and virtual environments, Yokohama, Japan, March 1999. Springer, Berlin Heidelberg New York, pp 1–16" href="/article/10.1007/s10055-004-0140-2#ref-CR19" id="ref-link-section-d1802e554">1999</a>) that real objects are those that have an actual objective existence and virtual objects are those that exist in essence or effect, but not formally or actually. In order for a real object to be viewed, it can either be observed directly or it can be sampled and then synthesized via some display device. In order for a virtual object to be viewed, it must be simulated, since, in essence, it does not exist. This entails the use of some sort of description or model of the object.</p><h3 class="c-article__sub-heading" id="Sec4">Concrete and abstract interaction objects</h3><p>The presentation of interaction spaces for MR systems is developed from the following concepts<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup>:</p><ul class="u-list-style-dash">
                    <li>
                      <p> <i>Concrete interaction object</i> (CIO): this is an object belonging to the interaction space that any user with the appropriate artifacts (e.g. a see-through head-mounted display (HMD)) can see. There are two types of CIO; real and virtual. A real CIO is part of the real interaction space (RIS) (e.g. live video or a physical object, such as a pen or a needle), which can have a representation in the virtual world, and so, can become a virtual concrete interaction object. The virtual CIO is part of the virtual interaction space (VIS) (e.g. text, image, animation, push button, a list box). The virtual CIO can also entail the virtual representation of the real CIO. A CIO is said to be “simple” if it cannot be decomposed into smaller CIOs, or “composite” if it can be further decomposed. Two categories are distinguished: “presentation CIO,” which is any static CIO allowing no user interaction and “control CIO,” which supports some interaction or user interface control by the user. Both presentation and control CIOs can be part of the RIS and/or the VIS.</p>
                    </li>
                    <li>
                      <p><i>Abstract interaction object</i> (AIO): this is an abstraction of all the CIOs, which is independent of any given computing platform from both presentation and behavioural viewpoints. By definition, an AIO does not have any graphical appearance, but each AIO is connected to one or more CIOs with different names and presentations on various computing platforms.</p>
                    </li>
                  </ul><h3 class="c-article__sub-heading" id="Sec5">Composing mixed reality space</h3><p>Taking into account Milgram’s continuum (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig2">2</a>), the interaction space is classified as:</p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Real interaction space</i> (RIS) if it consists only of real components, e.g. real concrete interaction objects, such as physical objects</p>
                    </li>
                    <li>
                      <p><i>Virtual interaction space</i> (VIS) if it consists only of virtual concrete interaction objects</p>
                    </li>
                    <li>
                      <p><i>Mixed interaction space</i> (MIS) if it consists of virtual concrete interaction objects added to the real environment, i.e. combined with real concrete interaction objects</p>
                    </li>
                  </ul><p>Each mixed interaction space (MIS) is composed of a VIS and an RIS, which are physically constrained by the user’s workspace and are displayed on the workspace simultaneously. Each workspace is composed of at least one IS, called the basic IS, from which it is possible to derive the other ISs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig5">5</a>). This configuration is necessary for the user to manipulate objects in the virtual world through the VIS or objects in the real world through the RIS. Basically, the RIS is composed of information from the real world (e.g. a live video source), and when the user interacts with the real world, it has some effect on the virtual world representation (i.e. on the VIS).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb5.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb5.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Representation of interaction spaces in an MR workspace</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6">Spatial–temporal relationships</h3><p>The IS may involve a large number of media objects that should be integrated into the mixed interaction unit (MIU). This integration covers the spatial ordering and topological features of the abstract interaction objects. At the abstract level, the designer is interested in expressing only the high-level relationships between AIOs, if any, and not the low-level details of the relationships, such as specific distance or time. Spatial–temporal relationships characterise the physical links between AIOs as they are presented in time and space. Since an abstract user interface (AUI) does not preclude the use of any particular modality, we do not know whether a particular AUI will be further reified into a concrete user interface (CUI) that is graphical, vocal, multimodal or virtual. Therefore, spatial–temporal relationships should be expressed in a way that is independent of any modality.</p><p>Allen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Allen JF (1993) Maintaining knowledge about temporal intervals. Commun ACM 26(11):832–843" href="/article/10.1007/s10055-004-0140-2#ref-CR2" id="ref-link-section-d1802e663">1993</a>) identified 13 possible temporal relationships and these are listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0140-2#Tab1">1</a>. Basically, there are two types of temporal relationships: sequential (<i>before</i> the relationship) and simultaneous (which can be <i>equal</i>, <i>meets</i>, <i>overlaps</i>, <i>during</i>, <i>starts</i> or <i>finishes</i>). Each basic relationship has an inverse, except for the <i>equal</i> relationship, which is symmetric. Although Allen’s relationships have been introduced to characterise temporal intervals, they are suitable for expressing constraints in both space and time, thanks to a space–time value. All simultaneous relationships (such as <i>overlaps</i>, <i>during</i>, <i>starts</i> and <i>finishes</i>) can be generalised as an <i>equal</i> relationship by inserting some delay time when needed. For example, in the <i>x before y</i> relationship, the space–time value between <i>x</i> and <i>y</i> is greater than zero, while in the <i>x meets y</i> relationship the space–time value between <i>x</i> and <i>y</i> is equal to zero. As relationships are abstract at the AUI level, the space–time value is left unspecified until needed at the CUI level. For instance, the <i>meets</i> relationship can be used in a vocal user interface to specify that a question is first presented to the user, and then immediately after a prompt to recognise the vocal answer of the user. In this case, the relationship represents a time interval.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Temporal Allen relationships (Allen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Allen JF (1993) Maintaining knowledge about temporal intervals. Commun ACM 26(11):832–843" href="/article/10.1007/s10055-004-0140-2#ref-CR2" id="ref-link-section-d1802e742">1993</a>)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0140-2/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>One-dimensional Allen relationships can be used to specify the temporal composition of <i>n</i> objects. Let <i>A</i> and <i>B</i> be two AIOs. The temporal relationship between <i>A</i> and <i>B</i> is defined as:</p><ul class="u-list-style-none">
                    <li>
                      <p><i>Temporal_Composition</i> (<i>A</i>, <i>B</i>)=(<i>R</i><sub>
                          <i>i</i>
                        </sub>), where <i>i</i>∈{1,..., 13}, as defined in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0140-2#Tab1">1</a>.</p>
                    </li>
                  </ul><p>Allen relationships can be generalised to <i>n</i> dimensions to express similar constraints in an <i>n</i>D space (Vazirgiannis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Vazirgiannis M, Theodoridis Y, Sellis TK (1998) Spatio-temporal composition and indexing for large multimedia applications. Multimedia Syst 6(4):284–298" href="/article/10.1007/s10055-004-0140-2#ref-CR35" id="ref-link-section-d1802e812">1998</a>). Here, the 2D generalisation is used to express space relationships more precisely in any type of UI involving spatial expressions. To exemplify this, let us assume two AIOs <i>A</i> and <i>B</i>. The spatial relationship between <i>A</i> and <i>B</i> is defined as:</p><ul class="u-list-style-none">
                    <li>
                      <p><i>Spatial_Composition</i> (<i>A</i>, <i>B</i>)=(<i>R</i><sub>
                          <i>i</i>
                        </sub>, <i>R</i><sub>
                          <i>j</i>
                        </sub>), where <i>i</i>, <i>j</i>∈{1,..., 13}, as defined in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0140-2#Tab1">1</a>, <i>R</i><sub>
                          <i>i</i>
                        </sub> is the identifier of the spatial relationship between <i>A</i> and <i>B</i> according to the <i>X</i> axis and <i>R</i><sub>
                          <i>j</i>
                        </sub> is the identifier of the spatial relationship between <i>A</i> and <i>B</i> according to the <i>Y</i> axis in the matrix reproduced in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>. When a spatial arrangement is expressed only according to one dimension, <i>R</i><sub>
                          <i>i</i>
                        </sub>=∅ or <i>R</i><sub>
                          <i>j</i>
                        </sub>=∅.</p>
                    </li>
                  </ul><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Areas of the Allen 2D matrix suitable for GUIs (grey sections near outer edge of grid) and multimodal and mixed UIs (white section in the centre)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a> depicts graphically areas of the Allen matrix that are appropriate for representing the spatial composition of certain types of UIs. The two first lines (i.e. <i>R</i><sub>1, 1</sub>–<i>R</i><sub>13, 1</sub> and <i>R</i><sub>1, 2</sub>–<i>R</i><sub>13, 2</sub>) and columns (i.e. <i>R</i><sub>1, 1</sub>–<i>R</i><sub>1, 13</sub> and <i>R</i><sub>2, 1</sub>–<i>R</i><sub>2, 13</sub>), and the two last lines (i.e. <i>R</i><sub>1, 12</sub>–<i>R</i><sub>13, 12</sub> and <i>R</i><sub>1, 13</sub>–<i>R</i><sub>13, 13</sub>) and columns (i.e. <i>R</i><sub>12, 1</sub>–<i>R</i><sub>12, 13</sub> and <i>R</i><sub>13, 1</sub>–<i>R</i><sub>13, 13</sub>) of the matrix allow the spatial composition of GUIs to be expressed, since widgets in GUIs do not overlap. They can be completely separated from each other (first and last lines and columns, represented in light grey in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>). In certain specific cases, they touch each other or share one part of the convex envelope (area represented in dark grey in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>). The remaining area, represented in white in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>, allows spatial arrangement for semi-transparent and see-through GUIs to be expressed (Bier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Bier EA, Stone MC, Fishkin K, Buxton W, Baudel T (1994) A taxonomy of see-through tools. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’94), Boston, Massachusetts, April 1994. ACM Press, New York, pp 358–364" href="/article/10.1007/s10055-004-0140-2#ref-CR6" id="ref-link-section-d1802e1026">1994</a>), where widgets can overlap, thanks to transparency layers (Harrison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Harrison BL, Ishii H, Vicente KJ, Buxton WAS (1995) Transparent layered user interfaces: an evaluation of a display design to enhance focused and divided attention. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’95), Denver, Colorado, May 1995. ACM Press, New York, pp 317–324" href="/article/10.1007/s10055-004-0140-2#ref-CR14" id="ref-link-section-d1802e1030">1995</a>), for multimedia UIs or for MR UIs (Poupyrev <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan DS, Billinghurst M., Kato H, Regembrecht H, Tetsutani N (2002) Developing a generic augmented reality interface. IEEE Computer 35(3):44–50" href="/article/10.1007/s10055-004-0140-2#ref-CR23" id="ref-link-section-d1802e1033">2002</a>).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Design space for mixed reality systems (DeSMiR)</h2><div class="c-article-section__content" id="Sec7-content"><p>DeSMiR is our proposed design space. It considers six aspects of the conception of MR systems while a specific task is being carried out. These are the “transform type,” “connection type,” “insertion context,” “media,” “interaction focus” and “kind of augmentation.” We use the Kiviat diagram to display several data variations on several axes for a given item (the task item).</p><p>DeSMiR is an abstract tool which can be used to systematically explore several design alternatives in the early phases of interaction design, without being biassed towards a particular modality (device plus media) or technology. Taking into account this design space during the conception phase of MR systems, it can help designers manage the large number of options that a mixed interaction space comprises, as well as think about the continuous interaction aspects. We will show that, on the basis of the system requirements, several abstractions using the design space can be explored and then the most suitable modality can be chosen. Once the modality has been selected, the concrete design can be analysed in terms of its continuous interaction properties.</p><p>The <i>transform type</i> axis is arranged according to users’ level of familiarity with the types. Thus, real action with real effect is highly familiar, while virtual action with virtual effect is highly unfamiliar. The <i>kind of augmentation</i> axis explores the possible types of augmentation that the system can provide. Augmenting perception (e.g. by adding, removing, etc. virtual objects to/from the real world) is the most common event, while augmenting action is uncommon since it requires more sophisticated equipment. The <i>connection type</i> axis is sorted by level of complexity in registering information. Environments with static links (i.e. where links between the real and virtual world are established during design time) are considerably less complex than environments in which all links are established during execution time. The <i>interaction focus</i> axis is sorted by degree of reality–virtuality. The <i>insertion context</i> axis is sorted according to the distance at which each device displaying the interaction space is inserted in the environment relative to the user’s position and the user’s task focus. A device is a piece of hardware used to access and interact with an application. It is capable of providing input to the computer, receiving output or both. The device class consists of two categories: input and output devices. Input devices (such as a keyboard, mouse, touch screen, data glove etc.) are used to enter commands or information into a computer. For example, a sensor is a kind of input device that can track users and objects and it is essential in AR environments. Output devices are used to give acoustic, haptic or visual feedback, in a form intelligible to the user, on the results of processing carried out by a computer. For the central zone, we have devices inserted from 0 cm to 45 from the user’s position and this provides a potential induced continuity. For the public zone, we have devices at distances greater than 3.6 m from the user’s position and the user’s task focus, and this provides a potential induced discontinuity. The <i>media axis</i> is sorted by level of complexity and dimensionality, starting with basic media such as text (1D) and image (2D), and finishing with more complex and structured media type, such as those found in 3D animation and immersive environments.</p><p>The items situated at the extreme of the design space axes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig7">7</a>) have a greater probability of being near the VE pole of Milgran’s diagram (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig2">2</a>). On the other hand, the items situated at the centre of the design space are more likely to be near the RE pole of Milgran’s diagram.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb7.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2flb7.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>DeSMiR: design space for mixed reality systems</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">Kind of augmentation</h3><p>Current work has been focussed on adding virtual objects to a real environment. However, graphic overlays might also be used to remove or hide parts of the real environment from a user. Another way to augment systems is by including sound in such systems. The user could wear headphones equipped with microphones on the outside. The headphones would add synthetic, directional 3D sound, while the external microphones would detect incoming sounds from the environment. This would give the system a chance to mask or cover up selected real sounds from the environment by generating a masking signal that exactly cancels out the incoming real sound (Durlach and Mavor <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Durlach NI, Mavor AS (1995) Virtual reality: scientific and technological challenges. National Academy Press, Washington, DC" href="/article/10.1007/s10055-004-0140-2#ref-CR11" id="ref-link-section-d1802e1102">1995</a>).</p><p>Many applications use the paradigm of interaction based on mixed reality (Berard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In: Proceedings of the IEEE international workshop on projector-camera systems (Procams 2003), Nice, France, October 2003" href="/article/10.1007/s10055-004-0140-2#ref-CR5" id="ref-link-section-d1802e1108">2003</a>; Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M, Weghorst S, Furness TA III (1998) Shared space: an augmented reality approach for computer supported collaborative work. Virtual Reality 3(1):25–36" href="/article/10.1007/s10055-004-0140-2#ref-CR7" id="ref-link-section-d1802e1111">1998</a>; Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36" href="/article/10.1007/s10055-004-0140-2#ref-CR25" id="ref-link-section-d1802e1114">1995</a>; Rekimoto and Saitoh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378–385" href="/article/10.1007/s10055-004-0140-2#ref-CR26" id="ref-link-section-d1802e1117">1999</a>). The augmentation type provided by these applications can be of three types: interaction, user’s action and user’s perception (Dubois et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Univer Access Inf Soci 1(4):263–273" href="/article/10.1007/s10055-004-0140-2#ref-CR9" id="ref-link-section-d1802e1120">2002</a>).</p><p>Interaction augmentation is a style of human–computer interface that tries to make computers as transparent as possible. For instance, by using the concept of tangible interfaces, in which the interaction is based on physical objects. For example, Rekimoto and Saitoh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378–385" href="/article/10.1007/s10055-004-0140-2#ref-CR26" id="ref-link-section-d1802e1126">1999</a>) show how users can smoothly exchange digital information between their portable computers and a computerised table and wall using a technique called hyper-dragging.</p><p>User’s actions augmentation increases the number and/or quality of tasks that the user can perform. For example, in the MagicBoard application (Berard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In: Proceedings of the IEEE international workshop on projector-camera systems (Procams 2003), Nice, France, October 2003" href="/article/10.1007/s10055-004-0140-2#ref-CR5" id="ref-link-section-d1802e1132">2003</a>), the user can perform cut-and-paste operations on real drawings—an action which is not possible in the real world.</p><p>User’s perception is augmented by new or more realistic information that is provided to the user. For example, in the Museum project (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36" href="/article/10.1007/s10055-004-0140-2#ref-CR25" id="ref-link-section-d1802e1139">1995</a>), the user can perceive additional information (such as historical data on the paint used for a picture), which is unavailable in the real world.</p><h3 class="c-article__sub-heading" id="Sec9">Transform type</h3><p>The term transform type is used to identify the different types of transformation occurring within the real, virtual and virtuality-enhanced trio identified in the definition of MR environments. According to Rogers et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Rogers Y, Scaife M, Gabrielli S, Smith H, Harris E (2002) A conceptual framework for mixed reality environments: designing novel learning activities for young children. Presence 11(6):677–686" href="/article/10.1007/s10055-004-0140-2#ref-CR28" id="ref-link-section-d1802e1150">2002</a>), the conceptual space is divided into four transform types, linking different combinations of action and effect, along real and virtual dimensions:</p><ul class="u-list-style-dash">
                    <li>
                      <p>Real action with a real effect (RARE)</p>
                    </li>
                    <li>
                      <p>Real action with a virtual effect (RAVE)</p>
                    </li>
                    <li>
                      <p>Virtual action with a virtual effect (VAVE)</p>
                    </li>
                    <li>
                      <p>Virtual action with a real effect (VARE)</p>
                    </li>
                  </ul><p>For example, if we take the MagicBoard application (Berard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In: Proceedings of the IEEE international workshop on projector-camera systems (Procams 2003), Nice, France, October 2003" href="/article/10.1007/s10055-004-0140-2#ref-CR5" id="ref-link-section-d1802e1178">2003</a>), a RARE transform type might be making a draw on the board with a pencil. Alternatively, using fingers (real action) to produce a digital copy of a drawing (virtual effect) is a RAVE transformation. Another example of RAVE is the Transfiction system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig3">3</a>), where the user performs real actions, which have a virtual effect (e.g. the user’s movements in the real world change the virtual world). A VAVE transformation occurs, for instance, when we select a region of interest in an image (e.g. a tumour) using a mouse, and the selected region changes its colour. VARE transformations can be found in the experiments explored by Rogers et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Rogers Y, Scaife M, Gabrielli S, Smith H, Harris E (2002) A conceptual framework for mixed reality environments: designing novel learning activities for young children. Presence 11(6):677–686" href="/article/10.1007/s10055-004-0140-2#ref-CR28" id="ref-link-section-d1802e1184">2002</a>). They used the Mimio<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> setup to enable a digitally potentiated action to trigger a physical effect. A bi-coloured (e.g. blue and white) virtual windmill was displayed on the desk surface. Moving the arms of the virtual windmill triggered the spinning of a physical windmill placed nearby, which had corresponding sets of coloured arms.</p><p>This classification can be complete by including two other possible kinds of transformations:</p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Real action with a shared effect</i> (RASE): this kind of transformation can be found in guided surgical systems. By tracking the surgical instruments in the frame of reference of the medical imagery, the surgeon acts in real world, but has feedback in both worlds (e.g. changes in the actual patient and in the virtual representation).</p>
                    </li>
                    <li>
                      <p><i>Virtual action with a shared effect</i> (VASE): this can be found in systems such as remote environment visualisation and manipulation for monitoring and exploration in distant or hazardous locations. For example, Akesson and Simsarian (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Akesson KP, Simsarian K (1999) Reality portals. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST’99), London, UK, December 1999. ACM Press, New York, ISBN 1-58113-141-0" href="/article/10.1007/s10055-004-0140-2#ref-CR1" id="ref-link-section-d1802e1219">1999</a>) developed an augmented virtual world that contains real world images as object textures that are created in an automatic way. They call these features reality portals. Using these reality portals with the robotic system, a human supervisor can control a remote robot assistant by issuing commands using the virtual environment as a medium.</p>
                    </li>
                  </ul><h3 class="c-article__sub-heading" id="Sec10">Connection types between worlds</h3><p>According to the taxonomy proposed by Nigay et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nigay L, Dubois E, Renevier P, Pasqualetti L, Troccaz J (2003) Mixed systems: combining physical and digital worlds. In: Proceedings of the 10th international conference on human–computer interaction (HCI International 2003), Crete, Greece, June 2003, pp 1203–1207" href="/article/10.1007/s10055-004-0140-2#ref-CR21" id="ref-link-section-d1802e1232">2003</a>), the links between the real and digital worlds can be characterised by two axes: the owner of the link (e.g. the person defining the link) and its nature (e.g. static, dynamic). For example, the designer of the microscope-image-guided surgery system (Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108–118" href="/article/10.1007/s10055-004-0140-2#ref-CR32" id="ref-link-section-d1802e1235">2003</a>) decided to combine the digital representation of a tumour with the representation of the current position of the tumour using optical trackers and registration procedures. Such a link is static and it was defined by its designer (during design time). On the other hand, using MAGIC (mobile augmented reality, group interactive, in context), the users dynamically define new digital objects that are combined with physical objects. Examples of applications using this technology can be found in Renevier and Nigay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Renevier P, Nigay L (2001) Mobile collaborative augmented reality: the augmented stroll. In: Proceedings of the 8th IFIP international conference on engineering for human–computer interaction (EHCI 2001), Toronto, Canada, May 2001. Lecture notes in computer science, vol 2254, Springer, Berlin Heidelberg New York, pp 315–334" href="/article/10.1007/s10055-004-0140-2#ref-CR27" id="ref-link-section-d1802e1238">2001</a>).</p><p>We propose an extension of this taxonomy to take account of the fact that the link between real and digital worlds can be defined by:</p><ul class="u-list-style-dash">
                    <li>
                      <p>The designer (e.g. a static link created in the design process)</p>
                    </li>
                    <li>
                      <p>The user (e.g. a dynamic link defined during execution time)</p>
                    </li>
                    <li>
                      <p>A third part (e.g. an agent system which is capable of making decisions and initiating actions independently, during execution time)</p>
                    </li>
                    <li>
                      <p>Mixed initiative (e.g. a combination of the above)</p>
                    </li>
                  </ul><h3 class="c-article__sub-heading" id="Sec11">Interaction focus</h3><p>When there are multiple sources of information and two worlds of interaction (real and virtual), we must make choices about what to attend to and when. At times, we need to focus our attention exclusively on a single item without interference from other items. At other times, we may need to time-share or divide our attention between two (or more) items of interest, which can be part of the same or different worlds.</p><p>For example, in the Museum project (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36" href="/article/10.1007/s10055-004-0140-2#ref-CR25" id="ref-link-section-d1802e1276">1995</a>), the user wears a see-through HMD in which information about an exhibit is displayed. The user is, thus, able to perceive real objects (the exhibit) as well as added synthetic information. The object of the task here is the painting in the exhibit. Therefore, the task focus belongs either to the virtual world or to the real world.</p><p>The user could be either performing a task in order to manipulate or modify an object in the real world (when the task focus is on the real world), or an object in the virtual world (when the task focus is on the virtual world). By considering all the possibilities of interaction focus while the user is performing a specific task, we have identified five possible combinations:</p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Interaction focus in real world without shared attention</i>: in this type of interaction, attention is focussed on only one object in the real world. There is no other object competing for the user’s attention.</p>
                    </li>
                    <li>
                      <p><i>Interaction focus in virtual world without shared attention</i>: in this type of interaction, attention is focussed on only one item in the virtual world. There is no other object competing for the user’s attention.</p>
                    </li>
                    <li>
                      <p><i>Interaction focus shared in real world</i> (<i>intra</i>-<i>world interaction focus</i>): in this case, the interaction focus is shared between real objects in the real world.</p>
                    </li>
                    <li>
                      <p><i>Interaction focus shared in virtual world</i> (<i>intra</i>-<i>world interaction focus</i>): in this case, the interaction focus is shared between virtual objects in the virtual world.</p>
                    </li>
                    <li>
                      <p><i>Interaction focus shared between worlds</i> (<i>inter</i>-<i>world interaction focus</i>): here, the interaction focus is shared between objects belonging to different worlds (real and virtual).</p>
                    </li>
                  </ul><h3 class="c-article__sub-heading" id="Sec12">Insertion context</h3><p>Sutcliffe (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sutcliffe A (2003) Multimedia and virtual reality: designing multisensory user interfaces. Lawrence Erlbaum, Mahwah, New Jersey" href="/article/10.1007/s10055-004-0140-2#ref-CR30" id="ref-link-section-d1802e1350">2003</a>) states, “We all like to have personal space surrounding us, generally about 0.5 m, although this is culturally dependent.” Here, we want to define the space around us according to the user’s focus while performing a task. An interaction space can be concretised using any device (screen, HMD etc.) or any physical object (projected at a table, wall etc.). We have identified four spatial zones (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig8">8</a>) for describing the IS, depending on the level of periphery:</p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Central zone</i>: corresponds to an insertion distance of 0 to 45 cm from the user</p>
                    </li>
                    <li>
                      <p><i>Personal zone</i>: corresponds to an insertion distance of 46 cm to 1.2 m from the user</p>
                    </li>
                    <li>
                      <p><i>Social zone</i>: corresponds to an insertion distance of 1.3 m to 3.6 m from the user</p>
                    </li>
                    <li>
                      <p><i>Public zone</i>: corresponds to an insertion distance greater than 3.6 m from the user</p>
                    </li>
                  </ul> <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Insertion zones of interaction spaces regarding the user’s task focus. 1=central zone, 2=personal zone, 3=social zone, 4=public zone</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>If the IS is inserted in the central zone of the user’s task, she/he does not need to change her/his attention focus to perform the task. If the user’s attention focus is changing all the time, then it is probable that the IS has been inserted outside the central zone, in a peripheral context of use. For instance, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig9">9</a> shows a potential source of discontinuity in IGS<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> systems: while a surgeon is operating on a patient lying on a table (the primary task and, thus, the main focus of attention), additional information is displayed on TV screens and monitors. Those devices are not necessarily located close to the patient’s location, thus, forcing the surgeon to switch attention from the patient to the various devices and back again. The farther the devices are from the main focus of attention, the more discontinuity will be induced.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Example of potential discontinuity</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In the Museum project, which is one application of the NaviCam system (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36" href="/article/10.1007/s10055-004-0140-2#ref-CR25" id="ref-link-section-d1802e1443">1995</a>), the IS is inserted in the central context of the user’s tasks; she doesn’t need to change her attention focus to perform the task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Example of potential continuity during user’s interaction with NaviCam system (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36" href="/article/10.1007/s10055-004-0140-2#ref-CR25" id="ref-link-section-d1802e1459">1995</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>A similar approach based on the spatial model has been used by Benford et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Benford S, Greenhalt C, Reynard G, Brown C, Koleva B (1998) Understanding and constructing shared spaces with mixed reality boundaries. ACM Trans Comput–Human Interact 5(3):185–223" href="/article/10.1007/s10055-004-0140-2#ref-CR4" id="ref-link-section-d1802e1473">1998</a>) to create highly interactive environments where objects dynamically react to the presence of other objects (e.g. a tool can be activated simply by approaching it).</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Design aspects and continuity</h2><div class="c-article-section__content" id="Sec13-content"><p>In modern interaction techniques such as gesture recognition, speech recognition, animation and haptic feedback, the user is in constant and tightly coupled interaction with the computing system over a period of time. The interaction is no longer based only on the exchange of discrete messages that could be considered as atomic actions. Instead, the input provided by the user and/or the outputs provided by the computing system are in a continuous process of exchanging information.</p><h3 class="c-article__sub-heading" id="Sec14">Defining continuity</h3><p>Ishii et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8)83–97" href="/article/10.1007/s10055-004-0140-2#ref-CR15" id="ref-link-section-d1802e1491">1994</a>) define a seam as a spatial, temporal or functional constraint that forces the user to shift among a variety of spaces or modes of operation. For example, the seam between word processing using a computer and traditional pen and paper makes it difficult to produce digital copies of handwritten documents without a translation step. All authors have agreed that systems asking users to abandon their acquired skills and to learn a new protocol are likely to encounter strong resistance.</p><p>Dubois et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Univer Access Inf Soci 1(4):263–273" href="/article/10.1007/s10055-004-0140-2#ref-CR9" id="ref-link-section-d1802e1497">2002</a>) consider continuity at the perceptual and cognitive levels. Perceptual continuity is present if the user perceives the different representations of a given entity directly and smoothly. Cognitive continuity is present if the cognitive processes that are involved in the interpretation of the different perceived representations are similar.</p><p>Here, we follow Florins et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Florins M, Trevisan D, Vanderdonckt J (2004) The continuity property in mixed reality and multiplatform systems: a comparative study. In: Proceedings of the 5th international conference on computer-aided design of user interfaces (CADUI 2004), Madeira Island, Portugal, January 2004, pp 328–339" href="/article/10.1007/s10055-004-0140-2#ref-CR12" id="ref-link-section-d1802e1503">2004</a>) in defining continuity as the capability of the system to promote a smooth interaction with the user during task accomplishment considering perceptual, cognitive and functional aspects. <i>Perceptual continuity</i> is defined as the ability of the system to provide information to the user in one perceptual environment (e.g. when the user is wearing a see-through HMD). <i>Cognitive continuity</i> is defined as the ability of the system to ensure that the user will interpret the perceived information correctly and that the perceived information is correct with regards to the internal state of the system (e.g. by using similar representations of the real and virtual objects). <i>Functional continuity</i> is defined as the adaptability of the user to change or learn new modes of interaction. Consequently, the functional property is related to the modality used (e.g. language and device).</p><h3 class="c-article__sub-heading" id="Sec15">Norman’s model and continuous interaction</h3><p>The theory of action proposed by Norman and Draper (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Norman DA, Draper SW (1986) User centered system design: new perspectives on human–computer interaction. Lawrence Erlbaum, Hillsdale, New Jersey" href="/article/10.1007/s10055-004-0140-2#ref-CR22" id="ref-link-section-d1802e1523">1986</a>) assumes that, for people to reach the goal of their tasks, they have to perform actions. Actions have two distinct aspects: they have to be executed and their results have to be evaluated. The stages of execution (intention, action specification and execution) are coupled to the stages of evaluation (perception, interpretation and evaluation). With this model, Norman shows that problems in the use of objects and interfaces can be explained as the discrepancy between the intention of the user and the actions made available by the system (execution flow), and the discrepancy between the physical representation of the system and the expectation and intentions of the user (evaluation flow). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0140-2#Tab2">2</a> shows how the phases of Norman’s model are related to the design aspects of mixed systems.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Characteristics and their associated continuous interaction properties according to the Norman’s theory</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0140-2/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Implications and applications</h2><div class="c-article-section__content" id="Sec16-content"><p>In this section, we present a practical study to show how the design space can be used to help designers to think about how to combine real and virtual objects, and to consider how decisions in this area are directly linked to evaluate interaction continuity.</p><h3 class="c-article__sub-heading" id="Sec17">Case study</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig11">11</a> shows the general scenario involved in image-guided surgery (IGS). This work is being conducted in collaboration with the MRI intra-operative group of the Neurology department at Hospital Saint Luc, Brussels (Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108–118" href="/article/10.1007/s10055-004-0140-2#ref-CR32" id="ref-link-section-d1802e1642">2003</a>). In such systems, complex surgical procedures can be navigated visually with great precision by overlaying on an image of the patient a colour-coded pre-operative plan specifying details such as the location of incisions, areas to be avoided and the diseased tissue. Augmented reality techniques applied to IGS systems allow users to retain their environmental perception while having intuitive access to more contextual information. In such systems, the focussed and shared attention should be addressed as well as the kind of interaction that the system will provide: continuous or discrete. A multimodal interface to couple the boundaries between different worlds should be considered (Nigay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nigay L, Dubois E, Renevier P, Pasqualetti L, Troccaz J (2003) Mixed systems: combining physical and digital worlds. In: Proceedings of the 10th international conference on human–computer interaction (HCI International 2003), Crete, Greece, June 2003, pp 1203–1207" href="/article/10.1007/s10055-004-0140-2#ref-CR21" id="ref-link-section-d1802e1645">2003</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Image-guided surgery (IGS) scenario where images acquired pre-operatively are registered to the patient images intra-operatively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Let us assume the following scenario in IGS (Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Trevisan DG, Vanderdonckt J, Macq BM (2004) Designing interaction space for mixed reality systems. In: Proceedings of the international workshop on exploring the design and engineering of mixed reality systems (MIXER’04). Available at &#xA;                    http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/paperD5.pdf&#xA;                    &#xA;                  , ISSN 1613–0073:27–34" href="/article/10.1007/s10055-004-0140-2#ref-CR33" id="ref-link-section-d1802e1668">2004</a>). The display starts with the background presentation of a live video image A (located at point 0 relative to the application origin Φ, as represented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig12">12</a>). At the same time, a path line graphic B overlaps image A according to the registration procedures. At time <i>t</i>1, determined by tracking system procedures, the Menu_options containing the texts C, D and E are displayed. Object C appears partially overlapping the right side of Object B, 10 cm lower than the top of B and less than 8 cm from the right of B. Object D appears 25 cm below and 6 cm to the right of Object C. Object E appears 30 cm below the bottom of Object D and less than 7 cm to the left of D. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig12">12</a> graphically depicts the initial UI in MR (left), plus its corresponding spatial (centre) and temporal (right) compositions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Composition of objects in IGS: initial interface (<i>left</i>), spatial composition (<i>centre</i>) and temporal composition (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The spatial composition of this example, following the guidelines given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>, are described as follows:</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; {\text{Spatial\_Composition}}\;{\left( {\Phi ,\;A} \right)} = {\left( {R_{6} ,\;R_{6} } \right)} \\ &amp; {\text{Spatial\_Composition}}\;{\left( {A,\;B} \right)} = {\left( {R_{3} ,\;R_{3} } \right)} \\ &amp; {\text{Spatial\_Composition}}\;{\left( {B,\;{\text{Menu\_options}}} \right)} = {\left( {R_{{11}} ,\;R_{{11}} } \right)} \\ \end{aligned} $$</span></div></div><p> These spatial compositions are extracted from the inter-relationships existing between the CIOs (i.e. <i>A</i>, <i>B</i> and Menu_options). Note that these relations pertain to the multimodal (light coloured central) area shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>.</p><div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; {\text{Menu\_options}}: \\ &amp; {\text{Spatial\_Composition}}\;{\left( {C,\;D} \right)} = {\left( {R_{{13}} ,\;R_{{13}} } \right)} \\ &amp; {\text{Spatial\_Composition}}\;{\left( {D,\;E} \right)} = {\left( {R_{1} ,\;R_{{13}} } \right)} \\ \end{aligned} $$</span></div></div><p> These spatial compositions are extracted from the intra-relationships existing between the CIOs in the Menu_options (i.e. <i>C</i>, <i>D</i> and <i>E</i>). Note that these relations pertain to the GUI (darker) area shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig6">6</a>.</p><p>The temporal compositions for the example, taking into account the relationships outlined in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0140-2#Tab1">1</a>, are described as follow:</p><div id="Eque" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; {\text{Temporal\_Composition}}\;{\left( {\Phi ,\;A} \right)} = {\left( {R_{7} } \right)} \\ &amp; {\text{Temporal\_Composition}}\;{\left( {A,\;B} \right)} = {\left( {R_{{12}} } \right)} \\ &amp; {\text{Temporal\_Composition}}\;{\left( {B,\;{\text{Menu\_options}}} \right)} = {\left( {R_{{12}} } \right)} \\ \end{aligned} $$</span></div></div><p> These temporal compositions are extracted from the inter-relationships existing between the CIOs (i.e. <i>A</i>, <i>B</i> and Menu_options).</p><div id="Equf" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; {\text{Menu\_options}}: \\ &amp; {\text{Temporal\_Composition}}\;{\left( {C,\;D} \right)} = {\left( {R_{7} } \right)} \\ &amp; {\text{Temporal\_Composition}}\;{\left( {D,\;E} \right)} = {\left( {R_{7} } \right)} \\ \end{aligned} $$</span></div></div><p> These temporal compositions are extracted from the intra-relationships existing between the CIOs in the Menu_options (i.e. <i>C</i>, <i>D</i> and <i>E</i>).</p><p>It is important to stress that, when each host composition (for instance, Φ, <i>A</i>)<i>ψ</i> ends, all spatial–temporal relationships related to or started by it are also terminated (i.e. <i>B</i> and Menu_options).</p><h3 class="c-article__sub-heading" id="Sec18">Exploring the design space</h3><p>By considering its spatial–temporal composition, the design space (DeSMiR) can be explored to extract some potential setups. Two possibilities are illustrated in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig13">13</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig14">14</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb13.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb13.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>DeSMiR applied to the IGS case study where the media are inserted in the central zone of the user’s task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb14.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0140-2/MediaObjects/s10055-004-0140-2fhb14.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>DeSMiR applied to the IGS case study where the media are inserted in the personal zone of the user’s task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0140-2/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig13">13</a> shows a potential design space where a graphic path line indicating the tumour region and some text information are overlapped on the live video image. In this case, since all this information is inserted in the central zone of the user’s task, the most suitable display devices would be either a microscope or an HMD. With this kind of display device, the user’s interaction focus is shared between the real and virtual worlds by using only one visible environment.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig14">14</a> describes another design space using the same information (i.e. path line and text overlapped on the live video). In this case, the information is inserted in the personal zone of the user’s task and a multiple screen display should be considered. Consequently, the user’s interaction focus is shared in the virtual world while accessing the additional information and focussed on the real world while focussing on the patient or on the live image video. In this situation, the user has to deal with two visible environments.</p><p>In both cases, the system is augmenting the user’s perception by providing additional information, the user is performing real actions with shared effect and the connection type between the worlds is static (i.e. the registration procedure and tracking system were established previously).</p><p>The interaction can be analysed in terms of continuity properties: perceptive, cognitive and functional. Regarding the cognitive property, we can say that the media language (video, graphic and text) is used to transmit information to the surgeon to allow easy interpretation, providing a cognitive continuity (by using similar representations for real and virtual objects). In the case described in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig14">14</a>, the perceptive property forces the user to change his/her attention focus to access all the available information, and so, causing a perceptive rupture during the interaction. This discontinuity is eliminated in the setup described in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig13">13</a>, where all the information is perceived in the central zone of the user’s task (i.e. in the same perceptive environment). As regards to functionality, the interaction device chosen for the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig13">13</a> case (for instance, an HMD) allows the user to perform real actions with shared perceived effects without changing his/her interaction focus. This does not happen with the device chosen for the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0140-2#Fig14">14</a> case (where information is distributed through multiple screens) and, for that reason, the user needs to stop one task (the surgical task) to start another (the information access).</p><p>Many other configurations using the design space could be envisaged and analyses made of their interactions. In addition, as the next step, practical experiments exploring the possibilities indicated by the design space should be undertaken. These will contribute to our ability to assess and evaluate the interaction technique in terms of continuous interaction, as well as providing some useful guidelines for designing MR systems.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Final considerations</h2><div class="c-article-section__content" id="Sec19-content"><p>A main objective of this theoretical study is to provide a conceptualisation of mixed realities that could give a means to systematically investigate and inform the design of mixed user interactions. As preliminary results of this study, we have proposed a design space (DeSMiR) for modelling mixed reality (MR) applications. The design space is an abstract tool to explore several design alternatives at early phases of the interaction design, without being biassed towards a particular modality or technology. Once a particular modality has been chosen, designers can also measure the interaction technique according to the continuous interaction properties mentioned. This approach provides a better understanding of how to design MR systems in a manner that is intuitive and effective. We suggest that this methodology can be applied for the design of several kinds of MR applications. However, we mainly address those applications in which smooth connections and interactions with worlds is critical for the system; i.e. image-guide surgery, driver applications or pilot simulations.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>According to Poupyrev et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan DS, Billinghurst M., Kato H, Regembrecht H, Tetsutani N (2002) Developing a generic augmented reality interface. IEEE Computer 35(3):44–50" href="/article/10.1007/s10055-004-0140-2#ref-CR23" id="ref-link-section-d1802e323">2002</a>) and Rekimoto and Saitoh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378–385" href="/article/10.1007/s10055-004-0140-2#ref-CR26" id="ref-link-section-d1802e326">1999</a>), tangible interfaces are those in which each virtual object is linked to a (tangible) physical object and the user interacts with the virtual object by manipulating the corresponding physical object.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>We use the terms “digital” and “virtual” indiscriminately to refer to a world that is not physical or real. We also consider that “real” and “physical” share the same meaning of “not digital or virtual.”</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>More details of this technology can be found at <a href="http://www.alterface.com">http://www.alterface.com</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>Following Brown (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Brown J (1997) HCI and requirements engineering: exploring human–computer interaction and software engineering methodologies for the creation of interactive software. ACM SIGHCI Bull 29(1):32–35" href="/article/10.1007/s10055-004-0140-2#ref-CR8" id="ref-link-section-d1802e485">1997</a>) and Pressman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Pressman RS (2001) Software engineering: a practitioner’s approach, 5th edn. McGraw-Hill, New York" href="/article/10.1007/s10055-004-0140-2#ref-CR24" id="ref-link-section-d1802e488">2001</a>), we are assuming here that the methodology corresponds to the process description necessary for the system development based on human–computer interaction approaches or based on paradigms of the software engineer.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>This approach was developed from the definitions given by Vanderdonckt and Bodart (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Vanderdonckt J, Bodart F (1993) Encapsulating knowledge for intelligent interaction objects selection. In: Adjunct proceedings of the joint conference of ACM SIGCHI and INTERACT (InterCHI’93), Amsterdam, The Netherlands, April 1993. ACM Press, New York, pp 424–429" href="/article/10.1007/s10055-004-0140-2#ref-CR34" id="ref-link-section-d1802e568">1993</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>More details about this technology can be found at <a href="http://www.mimio.com">http://www.mimio.com</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>The complete analysis of this system can be found in Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108–118" href="/article/10.1007/s10055-004-0140-2#ref-CR32" id="ref-link-section-d1802e1418">2003</a>).</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Akesson KP, Simsarian K (1999) Reality portals. In: Proceedings of the ACM symposium on virtual reality softwa" /><p class="c-article-references__text" id="ref-CR1">Akesson KP, Simsarian K (1999) Reality portals. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST’99), London, UK, December 1999. ACM Press, New York, ISBN 1-58113-141-0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JF. Allen, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Allen JF (1993) Maintaining knowledge about temporal intervals. Commun ACM 26(11):832–843" /><p class="c-article-references__text" id="ref-CR2">Allen JF (1993) Maintaining knowledge about temporal intervals. Commun ACM 26(11):832–843</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F182.358434" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Maintaining%20knowledge%20about%20temporal%20intervals&amp;journal=Commun%20ACM&amp;volume=26&amp;issue=11&amp;pages=832-843&amp;publication_year=1993&amp;author=Allen%2CJF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Baudish, D. DeCarlo, AT. Duchowski, WS. Geiser, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Baudish P, DeCarlo D, Duchowski AT, Geiser WS (2003) Focusing on the essential: considering attention in displ" /><p class="c-article-references__text" id="ref-CR3">Baudish P, DeCarlo D, Duchowski AT, Geiser WS (2003) Focusing on the essential: considering attention in display design. Commun ACM 46(3):60–66</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F636772.636799" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Focusing%20on%20the%20essential%3A%20considering%20attention%20in%20display%20design&amp;journal=Commun%20ACM&amp;volume=46&amp;issue=3&amp;pages=60-66&amp;publication_year=2003&amp;author=Baudish%2CP&amp;author=DeCarlo%2CD&amp;author=Duchowski%2CAT&amp;author=Geiser%2CWS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Benford, C. Greenhalt, G. Reynard, C. Brown, B. Koleva, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Benford S, Greenhalt C, Reynard G, Brown C, Koleva B (1998) Understanding and constructing shared spaces with " /><p class="c-article-references__text" id="ref-CR4">Benford S, Greenhalt C, Reynard G, Brown C, Koleva B (1998) Understanding and constructing shared spaces with mixed reality boundaries. ACM Trans Comput–Human Interact 5(3):185–223</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F292834.292836" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20and%20constructing%20shared%20spaces%20with%20mixed%20reality%20boundaries&amp;journal=ACM%20Trans%20Comput%E2%80%93Human%20Interact&amp;volume=5&amp;issue=3&amp;pages=185-223&amp;publication_year=1998&amp;author=Benford%2CS&amp;author=Greenhalt%2CC&amp;author=Reynard%2CG&amp;author=Brown%2CC&amp;author=Koleva%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In:" /><p class="c-article-references__text" id="ref-CR5">Berard F (2003) The Magic Table: computer-vision based augmentation of a whiteboard for creative meetings. In: Proceedings of the IEEE international workshop on projector-camera systems (Procams 2003), Nice, France, October 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bier EA, Stone MC, Fishkin K, Buxton W, Baudel T (1994) A taxonomy of see-through tools. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR6">Bier EA, Stone MC, Fishkin K, Buxton W, Baudel T (1994) A taxonomy of see-through tools. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’94), Boston, Massachusetts, April 1994. ACM Press, New York, pp 358–364</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, S. Weghorst, TA III. Furness, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Billinghurst M, Weghorst S, Furness TA III (1998) Shared space: an augmented reality approach for computer sup" /><p class="c-article-references__text" id="ref-CR7">Billinghurst M, Weghorst S, Furness TA III (1998) Shared space: an augmented reality approach for computer supported collaborative work. Virtual Reality 3(1):25–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Shared%20space%3A%20an%20augmented%20reality%20approach%20for%20computer%20supported%20collaborative%20work&amp;journal=Virtual%20Reality&amp;volume=3&amp;issue=1&amp;pages=25-36&amp;publication_year=1998&amp;author=Billinghurst%2CM&amp;author=Weghorst%2CS&amp;author=Furness%2CTA%20III">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Brown, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Brown J (1997) HCI and requirements engineering: exploring human–computer interaction and software engineering" /><p class="c-article-references__text" id="ref-CR8">Brown J (1997) HCI and requirements engineering: exploring human–computer interaction and software engineering methodologies for the creation of interactive software. ACM SIGHCI Bull 29(1):32–35</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=HCI%20and%20requirements%20engineering%3A%20exploring%20human%E2%80%93computer%20interaction%20and%20software%20engineering%20methodologies%20for%20the%20creation%20of%20interactive%20software&amp;journal=ACM%20SIGHCI%20Bull&amp;volume=29&amp;issue=1&amp;pages=32-35&amp;publication_year=1997&amp;author=Brown%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Dubois, L. Nigay, J. Troccaz, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Uni" /><p class="c-article-references__text" id="ref-CR9">Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Univer Access Inf Soci 1(4):263–273</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10209-002-0024-8" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessing%20continuity%20and%20compatibility%20in%20augmented%20reality%20systems&amp;journal=J%20Univer%20Access%20Inf%20Soci&amp;volume=1&amp;issue=4&amp;pages=263-273&amp;publication_year=2002&amp;author=Dubois%2CE&amp;author=Nigay%2CL&amp;author=Troccaz%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Dubois, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Dubois E, Gray PD, Nigay L (2003) ASUR++: a design notation for mobile mixed systems. Interact Comput 15(4):49" /><p class="c-article-references__text" id="ref-CR10">Dubois E, Gray PD, Nigay L (2003) ASUR++: a design notation for mobile mixed systems. Interact Comput 15(4):497–520</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0953-5438%2803%2900037-7" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Interact%20Comput&amp;volume=15&amp;publication_year=2003&amp;author=Dubois%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="NI. Durlach, AS. Mavor, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Durlach NI, Mavor AS (1995) Virtual reality: scientific and technological challenges. National Academy Press, " /><p class="c-article-references__text" id="ref-CR11">Durlach NI, Mavor AS (1995) Virtual reality: scientific and technological challenges. National Academy Press, Washington, DC</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20scientific%20and%20technological%20challenges&amp;publication_year=1995&amp;author=Durlach%2CNI&amp;author=Mavor%2CAS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Florins M, Trevisan D, Vanderdonckt J (2004) The continuity property in mixed reality and multiplatform system" /><p class="c-article-references__text" id="ref-CR12">Florins M, Trevisan D, Vanderdonckt J (2004) The continuity property in mixed reality and multiplatform systems: a comparative study. In: Proceedings of the 5th international conference on computer-aided design of user interfaces (CADUI 2004), Madeira Island, Portugal, January 2004, pp 328–339</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Graham TCN, Watts LA, Calvary G, Coutaz J, Dubois E, Nigay L (2000) A dimension space for the design of intera" /><p class="c-article-references__text" id="ref-CR13">Graham TCN, Watts LA, Calvary G, Coutaz J, Dubois E, Nigay L (2000) A dimension space for the design of interactive systems within their physical environments. In: Proceedings of the conference on designing interactive systems (DIS 2000), New York City, August 2000. ACM Press, New York, pp 406–416</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harrison BL, Ishii H, Vicente KJ, Buxton WAS (1995) Transparent layered user interfaces: an evaluation of a di" /><p class="c-article-references__text" id="ref-CR14">Harrison BL, Ishii H, Vicente KJ, Buxton WAS (1995) Transparent layered user interfaces: an evaluation of a display design to enhance focused and divided attention. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’95), Denver, Colorado, May 1995. ACM Press, New York, pp 317–324</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Ishii, M. Kobayashi, K. Arita, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8)83–97" /><p class="c-article-references__text" id="ref-CR15">Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8)83–97</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Iterative%20design%20of%20seamless%20collaboration%20media&amp;journal=Commun%20ACM&amp;volume=37&amp;issue=8&amp;pages=83-97&amp;publication_year=1994&amp;author=Ishii%2CH&amp;author=Kobayashi%2CM&amp;author=Arita%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ISMAR (2003) Proceedings of the 2nd IEEE and ACM international symposium on mixed and augmented reality, Tokyo" /><p class="c-article-references__text" id="ref-CR16">ISMAR (2003) Proceedings of the 2nd IEEE and ACM international symposium on mixed and augmented reality, Tokyo, Japan, October 2003. Accessible at <a href="http://www.ismar03.org/">http://www.ismar03.org/</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klemmer SR (2003) Papier-mâché: toolkit support for tangible interaction. In: Proceedings of the 16th annual A" /><p class="c-article-references__text" id="ref-CR17">Klemmer SR (2003) Papier-mâché: toolkit support for tangible interaction. In: Proceedings of the 16th annual ACM symposium on user interface software and technology (UIST 2003), Vancouver, British Columbia, Canada, November 2003, doctoral consortium paper </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marichal X, Macq B, Douxchamps D, Umeda T, art.live consortium (2003) Real-time segmentation of video objects " /><p class="c-article-references__text" id="ref-CR18">Marichal X, Macq B, Douxchamps D, Umeda T, art.live consortium (2003) Real-time segmentation of video objects for mixed-reality interactive applications. In: Proceedings of the SPIE conference on visual communication and image processing (VCIP 2003), Lugano, Switzerland, July 2003, vol 5150, pp 41–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Milgran, H Jr. Colquhoun, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Milgran P, Colquhoun H Jr (1999) A taxonomy of real and virtual world display integration. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR19">Milgran P, Colquhoun H Jr (1999) A taxonomy of real and virtual world display integration. In: Proceedings of the 1st international symposium on mixed reality (ISMR’99): merging real and virtual environments, Yokohama, Japan, March 1999. Springer, Berlin Heidelberg New York, pp 1–16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=In%3A%20Mixed%20reality%3A%20merging%20real%20and%20virtual%20environments.&amp;pages=1-16&amp;publication_year=1999&amp;author=Milgran%2CP&amp;author=Colquhoun%2CH%20Jr">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dubois E, Gray P, Trevisan D, Vanderdonckt J (eds) (2004) MIXER’04. In: Proceedings of the IUI-CADUI 2004 inte" /><p class="c-article-references__text" id="ref-CR20">Dubois E, Gray P, Trevisan D, Vanderdonckt J (eds) (2004) MIXER’04. In: Proceedings of the IUI-CADUI 2004 international workshop on exploring the design and engineering of mixed reality systems, Island of Madeira, Portugal, January 2004. Also available at <a href="http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/">http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/</a>, ISSN pp 1613–0073</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nigay L, Dubois E, Renevier P, Pasqualetti L, Troccaz J (2003) Mixed systems: combining physical and digital w" /><p class="c-article-references__text" id="ref-CR21">Nigay L, Dubois E, Renevier P, Pasqualetti L, Troccaz J (2003) Mixed systems: combining physical and digital worlds. In: Proceedings of the 10th international conference on human–computer interaction (HCI International 2003), Crete, Greece, June 2003, pp 1203–1207</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Norman, SW. Draper, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Norman DA, Draper SW (1986) User centered system design: new perspectives on human–computer interaction. Lawre" /><p class="c-article-references__text" id="ref-CR22">Norman DA, Draper SW (1986) User centered system design: new perspectives on human–computer interaction. Lawrence Erlbaum, Hillsdale, New Jersey</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=User%20centered%20system%20design%3A%20new%20perspectives%20on%20human-computer%20interaction&amp;publication_year=1986&amp;author=Norman%2CDA&amp;author=Draper%2CSW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, DS. Tan, M.. Billinghurst, H. Kato, H. Regembrecht, N. Tetsutani, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Poupyrev I, Tan DS, Billinghurst M., Kato H, Regembrecht H, Tetsutani N (2002) Developing a generic augmented " /><p class="c-article-references__text" id="ref-CR23">Poupyrev I, Tan DS, Billinghurst M., Kato H, Regembrecht H, Tetsutani N (2002) Developing a generic augmented reality interface. IEEE Computer 35(3):44–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20a%20generic%20augmented%20reality%20interface&amp;journal=IEEE%20Computer&amp;volume=35&amp;issue=3&amp;pages=44-50&amp;publication_year=2002&amp;author=Poupyrev%2CI&amp;author=Tan%2CDS&amp;author=Billinghurst%2CM.&amp;author=Kato%2CH&amp;author=Regembrecht%2CH&amp;author=Tetsutani%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="RS. Pressman, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Pressman RS (2001) Software engineering: a practitioner’s approach, 5th edn. McGraw-Hill, New York" /><p class="c-article-references__text" id="ref-CR24">Pressman RS (2001) Software engineering: a practitioner’s approach, 5th edn. McGraw-Hill, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Software%20engineering%3A%20a%20practitioner%E2%80%99s%20approach&amp;publication_year=2001&amp;author=Pressman%2CRS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world envi" /><p class="c-article-references__text" id="ref-CR25">Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Proceedings of the 8th ACM symposium on user interface software and technology (UIST’95), Pittsburgh, Pennsylvania, November 1995, pp 29–36</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environ" /><p class="c-article-references__text" id="ref-CR26">Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: Proceedings of the ACM SIGCHI conference on human factors in computing systems (CHI’99), Pittsburgh, Pennsylvania, May 1999.ACM Press, New York, pp 378–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Renevier P, Nigay L (2001) Mobile collaborative augmented reality: the augmented stroll. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR27">Renevier P, Nigay L (2001) Mobile collaborative augmented reality: the augmented stroll. In: Proceedings of the 8th IFIP international conference on engineering for human–computer interaction (EHCI 2001), Toronto, Canada, May 2001. Lecture notes in computer science, vol 2254, Springer, Berlin Heidelberg New York, pp 315–334</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Rogers, M. Scaife, S. Gabrielli, H. Smith, E. Harris, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Rogers Y, Scaife M, Gabrielli S, Smith H, Harris E (2002) A conceptual framework for mixed reality environment" /><p class="c-article-references__text" id="ref-CR28">Rogers Y, Scaife M, Gabrielli S, Smith H, Harris E (2002) A conceptual framework for mixed reality environments: designing novel learning activities for young children. Presence 11(6):677–686</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20conceptual%20framework%20for%20mixed%20reality%20environments%3A%20designing%20novel%20learning%20activities%20for%20young%20children&amp;journal=Presence&amp;volume=11&amp;issue=6&amp;pages=677-686&amp;publication_year=2002&amp;author=Rogers%2CY&amp;author=Scaife%2CM&amp;author=Gabrielli%2CS&amp;author=Smith%2CH&amp;author=Harris%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="STARS (2003) Proceedings of the international workshop on software technology for augmented reality systems, T" /><p class="c-article-references__text" id="ref-CR29">STARS (2003) Proceedings of the international workshop on software technology for augmented reality systems, Tokyo, Japan, October 2003. Accessible ate <a href="http://stars2003.in.tum.de/">http://stars2003.in.tum.de/</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Sutcliffe, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Sutcliffe A (2003) Multimedia and virtual reality: designing multisensory user interfaces. Lawrence Erlbaum, M" /><p class="c-article-references__text" id="ref-CR30">Sutcliffe A (2003) Multimedia and virtual reality: designing multisensory user interfaces. Lawrence Erlbaum, Mahwah, New Jersey</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Multimedia%20and%20virtual&amp;volume=reality&amp;publication_year=2003&amp;author=Sutcliffe%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tanriverdi V, Jacob RJK (2001) VRID: a design model and methodology for developing virtual reality interfaces." /><p class="c-article-references__text" id="ref-CR31">Tanriverdi V, Jacob RJK (2001) VRID: a design model and methodology for developing virtual reality interfaces. In: Proceedings of the ACM symposium on virtual reality software and technology (VRST 2001), Banff, Alberta, Canada, November 2001. ACM Press, New York, pp 175–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. I" /><p class="c-article-references__text" id="ref-CR32">Trevisan DG, Vanderdonckt J, Macq BM, Raftopoulos C (2003) Modeling interaction for image-guided procedures. In: Proceedings of the SPIE conference on medical imaging, San Diego, California, February 2003, vol 5029, pp 108–118</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trevisan DG, Vanderdonckt J, Macq BM (2004) Designing interaction space for mixed reality systems. In: Proceed" /><p class="c-article-references__text" id="ref-CR33">Trevisan DG, Vanderdonckt J, Macq BM (2004) Designing interaction space for mixed reality systems. In: Proceedings of the international workshop on exploring the design and engineering of mixed reality systems (MIXER’04). Available at <a href="http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/paperD5.pdf">http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-91/paperD5.pdf</a>, ISSN 1613–0073:27–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vanderdonckt J, Bodart F (1993) Encapsulating knowledge for intelligent interaction objects selection. In: Adj" /><p class="c-article-references__text" id="ref-CR34">Vanderdonckt J, Bodart F (1993) Encapsulating knowledge for intelligent interaction objects selection. In: Adjunct proceedings of the joint conference of ACM SIGCHI and INTERACT (InterCHI’93), Amsterdam, The Netherlands, April 1993. ACM Press, New York, pp 424–429</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Vazirgiannis, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Vazirgiannis M, Theodoridis Y, Sellis TK (1998) Spatio-temporal composition and indexing for large multimedia " /><p class="c-article-references__text" id="ref-CR35">Vazirgiannis M, Theodoridis Y, Sellis TK (1998) Spatio-temporal composition and indexing for large multimedia applications. Multimedia Syst 6(4):284–298</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs005300050094" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Multimedia%20Syst&amp;volume=6&amp;publication_year=1998&amp;author=Vazirgiannis%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Vertegaal, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Vertegaal R (2003) Introduction to special issue on “Attentive user interfaces.” Commun ACM 46(3):pp 30–33" /><p class="c-article-references__text" id="ref-CR36">Vertegaal R (2003) Introduction to special issue on “Attentive user interfaces.” Commun ACM 46(3):pp 30–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Attentive%20user%20interface%20Commun%20ACM%20vol%2046%283%29&amp;pages=30-33&amp;publication_year=2003&amp;author=Vertegaal%2CR">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0140-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We gratefully acknowledge the support of the Belgian Région Wallonne under contract WALEO 21/5129. The work described here is a part of the VISME/MERCATOR projects, available at <a href="http://www.isys.ucl.ac.be/bchi/research/visme.htm">http://www.isys.ucl.ac.be/bchi/research/visme.htm</a>.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Communication and Remote Sensing Laboratory, Université catholique de Louvain, Place du Levant 2, 1348, Louvain-la-Neuve, Belgium</p><p class="c-article-author-affiliation__authors-list">Daniela Gorski Trevisan &amp; Benoît Macq</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Belgium Laboratory of Computer–Human Interaction, Université catholique de Louvain, Place du Doyens 1, 1348, Louvain-la-Neuve, Belgium</p><p class="c-article-author-affiliation__authors-list">Daniela Gorski Trevisan &amp; Jean Vanderdonckt</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Daniela_Gorski-Trevisan"><span class="c-article-authors-search__title u-h3 js-search-name">Daniela Gorski Trevisan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daniela Gorski+Trevisan&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniela Gorski+Trevisan" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniela Gorski+Trevisan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jean-Vanderdonckt"><span class="c-article-authors-search__title u-h3 js-search-name">Jean Vanderdonckt</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jean+Vanderdonckt&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jean+Vanderdonckt" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jean+Vanderdonckt%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Beno_t-Macq"><span class="c-article-authors-search__title u-h3 js-search-name">Benoît Macq</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Beno%C3%AEt+Macq&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Beno%C3%AEt+Macq" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Beno%C3%AEt+Macq%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0140-2/email/correspondent/c1/new">Daniela Gorski Trevisan</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Conceptualising%20mixed%20spaces%20of%20interaction%20for%20designing%20continuous%20interaction&amp;author=Daniela%20Gorski%20Trevisan%20et%20al&amp;contentID=10.1007%2Fs10055-004-0140-2&amp;publication=1359-4338&amp;publicationDate=2005-01-14&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Trevisan, D.G., Vanderdonckt, J. &amp; Macq, B. Conceptualising mixed spaces of interaction for designing continuous interaction.
                    <i>Virtual Reality</i> <b>8, </b>83–95 (2004). https://doi.org/10.1007/s10055-004-0140-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0140-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-12">12 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-10-21">21 October 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-01-14">14 January 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0140-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0140-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Design space</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Continuous interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Image-guided surgery</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0140-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=140;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

