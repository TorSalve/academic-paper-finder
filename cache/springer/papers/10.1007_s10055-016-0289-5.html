<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Computerized spatial language generation for object location"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, which in turn may be aided by a reference..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/20/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Computerized spatial language generation for object location"/>

    <meta name="dc.source" content="Virtual Reality 2016 20:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-06-22"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, which in turn may be aided by a reference object, other than the one to be located. This reference object is commonly selected based on its perceptual salience, that is, its more prominent features. Computer systems linked to various research areas have been developed to facilitate the communication and/or interpretation of spatial language for localization tasks. In this paper is presented a literature review of computer systems that adopt spatial language and perceptual salience for object location."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2016-06-22"/>

    <meta name="prism.volume" content="20"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="183"/>

    <meta name="prism.endingPage" content="192"/>

    <meta name="prism.copyright" content="2016 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-016-0289-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-016-0289-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-016-0289-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-016-0289-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Computerized spatial language generation for object location"/>

    <meta name="citation_volume" content="20"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2016/09"/>

    <meta name="citation_online_date" content="2016/06/22"/>

    <meta name="citation_firstpage" content="183"/>

    <meta name="citation_lastpage" content="192"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-016-0289-5"/>

    <meta name="DOI" content="10.1007/s10055-016-0289-5"/>

    <meta name="citation_doi" content="10.1007/s10055-016-0289-5"/>

    <meta name="description" content="Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, whic"/>

    <meta name="dc.creator" content="Graciela Lara"/>

    <meta name="dc.creator" content="Ang&#233;lica De Antonio"/>

    <meta name="dc.creator" content="Adriana Pe&#241;a"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Abella A, Kender JR (1999) From Images to Sentences via Spatial Relations. In: Proceedings of the W on Integration of Speech and Image Understanding, pp 117&#8211;146"/>

    <meta name="citation_reference" content="citation_title=Characterizing trajectories of moving objects using natural language path descriptions; citation_publication_date=1986; citation_id=CR2; citation_author=E Andr&#233;; citation_author=G Bosch; citation_author=G Herzog; citation_author=T Rist; citation_publisher=Project VITRA, Universit&#228;t des Saarlandes"/>

    <meta name="citation_reference" content="citation_title=Coping with the intrinsic and deictic uses of spatial prepositions. Artificial intelligence II: methodology, systems, applications (AIMSA); citation_publication_date=1987; citation_id=CR3; citation_author=E Andr&#233;; citation_author=G Bosch; citation_author=G Herzog; citation_author=T Rist; citation_publisher=North-Holland"/>

    <meta name="citation_reference" content="Andr&#233; E, Herzog G, Rist T (1988) On the simultaneous interpretation of real world image sequences and their natural language description: the system SOCCER. In: Proceeding of 8th European Conference on Artificial Intelligence (ECAI-88). Pitmann Publishing, London, Munich, pp 449&#8211;454"/>

    <meta name="citation_reference" content="Andr&#233; E, Herzong G, Rist T (1989) Natural language access to visual data: dealing with space and movement. In: Proceedings of the 1st Workshop on Logical Semantics of Time, Space and Movement in Natural Language. Universit&#228;t des Saarlandes, Toulouse, pp 1&#8211;21"/>

    <meta name="citation_reference" content="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis"/>

    <meta name="citation_reference" content="citation_journal_title=ACM J Name; citation_title=Computational visual attention systems and their cognitive foundations: a survey; citation_author=S Frintrop, E Rome; citation_volume=7; citation_publication_date=2010; citation_pages=1-46; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_title=Object localization: selection of optimal reference objects; citation_publication_date=1995; citation_id=CR8; citation_author=K-P Gapp; citation_publisher=Universit&#228;t des Saarlandes"/>

    <meta name="citation_reference" content="citation_title=Selection of best reference objects in object localizations; citation_publication_date=1996; citation_id=CR9; citation_author=K-P Gapp; citation_publisher=University of Saarbr&#252;cken"/>

    <meta name="citation_reference" content="citation_journal_title=J Artif Intell Res; citation_title=Grounded semantic composition for visual scenes; citation_author=P Gorniak, D Roy; citation_volume=21; citation_publication_date=2004; citation_pages=429-470; citation_id=CR10"/>

    <meta name="citation_reference" content="Hall D, Leibe B, Schile B (2002) Saliency of interest points under scale changes. British Machine Vision Conference (BMVC&#8217;02), Cardiff, pp 646&#8211;655"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Cognit; citation_title=Spatial language and spatial representation; citation_author=WG Hayward, MJ Tarr; citation_volume=55; citation_publication_date=1995; citation_pages=39-84; citation_doi=10.1016/0010-0277(94)00643-Y; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_title=Visualization methods for the VITRA workbench; citation_publication_date=1992; citation_id=CR13; citation_author=G Herzog; citation_publisher=Universit&#228;t des Saarlandes"/>

    <meta name="citation_reference" content="Herzog G (1995) From visual input to verbal output in the visual translator. In: Proceedings of the AAAI Fall Symposium on Computational Models for Integrating Language and Vision. Universit&#228;t des Saarlandes, Cambridge, pp 1&#8211;15"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Vis Res; citation_title=Quantifying object salience by equating distractor effects; citation_author=L Huang, H Pashler; citation_volume=45; citation_publication_date=2005; citation_pages=1909-1920; citation_doi=10.1016/j.visres.2005.01.013; citation_id=CR15"/>

    <meta name="citation_reference" content="Kelleher JD (2003) A Perceptually Based Computational Framework&#160;for the Interpretation of Spatial Language Dublin City University. PhD. Thesis"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Linguist Assoc Comput Linguist; citation_title=Applying computational models of spatial prepositions to visually situated dialog; citation_author=JD Kelleher, FJ Costello; citation_volume=35; citation_issue=2; citation_publication_date=2009; citation_pages=271-306; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Doyma Revista de Psiquiatr&#237;a y Salud Mental; citation_title=Asignaci&#243;n de relevancia (salience) y desregulaci&#243;n del sistema dopamin&#233;rgico; citation_author=G Lahera, N Freund, J S&#225;iz-Ruiz; citation_volume=6; citation_publication_date=2013; citation_pages=45-51; citation_doi=10.1016/j.rpsm.2012.05.003; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Brain Sci; citation_title=&#8220;What&#8221; and &#8220;where&#8221; in spatial language and spatial cognition; citation_author=B Landau, R Jackendoff; citation_volume=16; citation_publication_date=1993; citation_pages=255-265; citation_doi=10.1017/S0140525X00029927; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_title=Space in language and cognition: explorations in cognitive diversity, LCC5. Language, culture and cognition; citation_publication_date=2003; citation_id=CR20; citation_author=SC Levinson; citation_publisher=University Press"/>

    <meta name="citation_reference" content="Lockwood K, Forbus K, Usher J (2005) SpaceCase: A Model of Spatial Preposition Use. In: Proceedings of the 27th Annual Conference of the Cognitive Science Society, Stressa, Italy"/>

    <meta name="citation_reference" content="Lockwood K, Forbus K, Halstead DT, Usher J (2006) Automatic categorization of spatial prepositions. In: Proceedings of the 28th Annual Conference of the Cognitive Science Society, Vancouver, Canada"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Neurosci Biobehav Rev; citation_title=The egocentric spatial reference frame used in dorsal&#8211;lateral prefrontal workingmemory in primates; citation_author=Y Ma, X Hu, FA Wilson; citation_volume=36; citation_publication_date=2012; citation_pages=26-33; citation_doi=10.1016/j.neubiorev.2011.03.011; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Psychon Soc Inc Mem Cognit; citation_title=The spatial frame of reference in object naming and discrimination of left-right reflections; citation_author=PA McMullen, P Jolicoeur; citation_volume=18; citation_publication_date=1990; citation_pages=99-115; citation_doi=10.3758/BF03202650; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Spat Cogn Comput; citation_title=Spatial reference in linguistic human-robot interaction: iterative, empirically supported development of a model of projective relations; citation_author=R Moratz, T Tenbrik; citation_volume=6; citation_issue=1; citation_publication_date=2006; citation_pages=63-106; citation_id=CR25"/>

    <meta name="citation_reference" content="Moratz R, Tenbrink T, Bateman J, Fischer K (2003) Spatial knowledge representation for human&#8211;robot interaction Springer-Verlag Berlin Heidelberg Spatial Cognition III Lecture Notes in Computer Science 2685:263&#8211;283"/>

    <meta name="citation_reference" content="citation_journal_title=Am Psychol Assoc J Exp Psychol: Learn Mem Cognit; citation_title=Intrinsic frames of reference in spatial memory; citation_author=W Mou, TP McNamara; citation_volume=28; citation_publication_date=2002; citation_pages=162-170; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Image Vis Comput; citation_title=Conceptual description of visual scenes from linguistic models; citation_author=A Mukerjee, K Gupta, S Nautiyal, MP Singh, N Mishra; citation_volume=18; citation_publication_date=2000; citation_pages=173-187; citation_doi=10.1016/S0262-8856(99)00022-0; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Lang Sci; citation_title=Spatial frames of reference in Mesoamerican languages; citation_author=C O&#8217;Meara, BG P&#233;rez; citation_volume=33; citation_publication_date=2011; citation_pages=837-852; citation_doi=10.1016/j.langsci.2011.06.013; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Language; citation_title=Semantic typology and spatial conceptualization linguistic society of America JSTOR; citation_author=E Pederson, E Danziger, D Wilkins, S Levinson, S Kita, G Senft; citation_volume=74; citation_publication_date=1998; citation_pages=557-589; citation_doi=10.1353/lan.1998.0074; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Brain Sci; citation_title=Natural language and natural selection; citation_author=S Pinker, P Bloom; citation_volume=13; citation_publication_date=1990; citation_pages=707-727; citation_doi=10.1017/S0140525X00081061; citation_id=CR31"/>

    <meta name="citation_reference" content="Raubal M, Winter S (2002) Enriching Wayfinding Instructions with Local Landmarks. In: Egenhofer MJ, Mark DM (eds) Second International Conference, GIScience Proceedings. Springer Berlin Heidelberg, Boulder, CO, USA, pp 243&#8211;259"/>

    <meta name="citation_reference" content="citation_journal_title=Computat Linguist; citation_title=The human semantic potential: spatial language and constrained connectionism; citation_author=T Regier; citation_volume=23; citation_issue=3; citation_publication_date=1996; citation_pages=483-486; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Gen; citation_title=Grounding spatial language in perception: an empirical and computational investigation; citation_author=T Regier, LA Carlson; citation_volume=130; citation_publication_date=2001; citation_pages=273-298; citation_doi=10.1037/0096-3445.130.2.273; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_title=Situated communication; citation_publication_date=2006; citation_id=CR35; citation_author=G Rickheit; citation_author=I Wachsmuth; citation_publisher=Gruyter"/>

    <meta name="citation_reference" content="R&#246;ser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: Markus K, Natalie S, Michael P, Ipke W (eds) Cooperative Minds: Social Interaction and Group Dynamics Proceedings of the 35th Annual Meeting of the Cognitive Science Society, Austin, TX. USA, pp 3315&#8211;3320"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Speech Lang; citation_title=Learning visually-grounded words and syntax for a scene description task; citation_author=DK Roy; citation_volume=16; citation_publication_date=2002; citation_pages=1-39; citation_doi=10.1016/S0885-2308(02)00024-4; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Sci Cognit Psychol; citation_title=Systems of spatial reference in human memory; citation_author=AL Shelton, TP McNamara; citation_volume=43; citation_publication_date=2001; citation_pages=274-310; citation_doi=10.1006/cogp.2001.0758; citation_id=CR38"/>

    <meta name="citation_reference" content="Skubic M, Perzanowski D, Blisard S, Schultz A, Adams W (2002) Spatial Language for Human-Robot Dialogs. IEEE Transactions on SMC, Part C, Special Issue on Human-Robot Interaction: 1&#8211;39"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Acta Psycol; citation_title=Semantic consistency versus perceptual salience in visual scenes: findings from change detection; citation_author=S Spotorno, BW Tatler, S Faure; citation_volume=142; citation_publication_date=2013; citation_pages=168-176; citation_doi=10.1016/j.actpsy.2012.12.009; citation_id=CR40"/>

    <meta name="citation_reference" content="Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis"/>

    <meta name="citation_reference" content="Tenbrink T, Ragni M (2012) Relevance in Spatial Navigation and Communication. Springer Spatial Cognition VIII Lecture Notes in Computer Science 7463:279&#8211;298"/>

    <meta name="citation_reference" content="citation_title=Lectures on the elementary psychology of feeling and attention; citation_publication_date=1908; citation_id=CR43; citation_author=EB Titchener; citation_publisher=The MacMillan Company"/>

    <meta name="citation_reference" content="Trinh T-H (2013) A Constraint-based Approach to Modelling Spatial Semantics of Virtual Environments, Universit&#233; de Bretagne Occidentale. PhD. Thesis"/>

    <meta name="citation_reference" content="citation_title=Asignaci&#243;n de relavancia: Una propuesta para el t&#233;rmino ingl&#233;s &#8220;salience&#8221;; citation_publication_date=2011; citation_id=CR45; citation_author=ML Vargas; citation_author=G Lahera; citation_publisher=Actas Esp Psiquiatr&#237;a"/>

    <meta name="citation_reference" content="Williams P, Miikkulainen R (2006) Grounding Language in Descriptions of Scenes. In: Proceedings of the 28th Annual Conference of the Cognitive Science Society"/>

    <meta name="citation_reference" content="Winograd T (1971) Procedures as a representation for data in a computer program for understanding natural language, Massachusetts Institute of Technology. PhD. Thesis"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier Acta Psychol; citation_title=The influence of spatial reference frames on imagined object and viewer rotations; citation_author=M Wraga, SH Creem, DR Proffitt; citation_volume=102; citation_publication_date=1998; citation_pages=247-264; citation_doi=10.1016/S0001-6918(98)00057-2; citation_id=CR48"/>

    <meta name="citation_author" content="Graciela Lara"/>

    <meta name="citation_author_email" content="graciela.lara@red.cucei.udg.mx"/>

    <meta name="citation_author_institution" content="CUCEI of the Universidad de Guadalajara, Guadalajara, Mexico"/>

    <meta name="citation_author" content="Ang&#233;lica De Antonio"/>

    <meta name="citation_author_institution" content="Escuela T&#233;cnica Superior de Ingenieros Inform&#225;tico of the Universidad Polit&#233;cnica de Madrid, Boadilla Del Monte, Spain"/>

    <meta name="citation_author" content="Adriana Pe&#241;a"/>

    <meta name="citation_author_institution" content="CUCEI of the Universidad de Guadalajara, Guadalajara, Mexico"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-016-0289-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2016/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-016-0289-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Computerized spatial language generation for object location"/>
        <meta property="og:description" content="Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, which in turn may be aided by a reference object, other than the one to be located. This reference object is commonly selected based on its perceptual salience, that is, its more prominent features. Computer systems linked to various research areas have been developed to facilitate the communication and/or interpretation of spatial language for localization tasks. In this paper is presented a literature review of computer systems that adopt spatial language and perceptual salience for object location."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Computerized spatial language generation for object location | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-016-0289-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Spatial language, Spatial reference frames, Computer systems, Virtual environment, Objects location and perceptual salience","kwrd":["Spatial_language","Spatial_reference_frames","Computer_systems","Virtual_environment","Objects_location_and_perceptual_salience"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-016-0289-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-016-0289-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=289;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-016-0289-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Computerized spatial language generation for object location
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0289-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0289-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-06-22" itemprop="datePublished">22 June 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Computerized spatial language generation for object location</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Graciela-Lara" data-author-popup="auth-Graciela-Lara" data-corresp-id="c1">Graciela Lara<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CUCEI of the Universidad de Guadalajara" /><meta itemprop="address" content="grid.412890.6, 0000000121580196, CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara, Jalisco, Mexico" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ang_lica-Antonio" data-author-popup="auth-Ang_lica-Antonio">Angélica De Antonio</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Escuela Técnica Superior de Ingenieros Informático of the Universidad Politécnica de Madrid" /><meta itemprop="address" content="Escuela Técnica Superior de Ingenieros Informático of the Universidad Politécnica de Madrid, Campus de Montegancedo, 28660, Boadilla Del Monte, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adriana-Pe_a" data-author-popup="auth-Adriana-Pe_a">Adriana Peña</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CUCEI of the Universidad de Guadalajara" /><meta itemprop="address" content="grid.412890.6, 0000000121580196, CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara, Jalisco, Mexico" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">183</span>–<span itemprop="pageEnd">192</span>(<span data-test="article-publication-year">2016</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">316 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-016-0289-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, which in turn may be aided by a reference object, other than the one to be located. This reference object is commonly selected based on its perceptual salience, that is, its more prominent features. Computer systems linked to various research areas have been developed to facilitate the communication and/or interpretation of spatial language for localization tasks. In this paper is presented a literature review of computer systems that adopt spatial language and perceptual salience for object location.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>An important feature of living beings and their surrounding objects, all of which have a place in space, is undoubtedly their spatial position (Moratz and Tenbrik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Moratz R, Tenbrik T (2006) Spatial reference in linguistic human-robot interaction: iterative, empirically supported development of a model of projective relations. Spat Cogn Comput 6(1):63–106" href="/article/10.1007/s10055-016-0289-5#ref-CR25" id="ref-link-section-d980e330">2006</a>). Accordingly, people develop spatial knowledge, a basic skill helpful for the localization process. The representations underlying object recognition, object search and navigation through space are fundamental components of this spatial knowledge. Albeit a seemingly simple task, spatial language calls for a mix of human knowledge theories and an accessible visual representation for the linguistic system. Through our ability to use these representations of our spatial experience, we formulate a spatial language to express where objects are located (Landau and Jackendoff <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Landau B, Jackendoff R (1993) “What” and “where” in spatial language and spatial cognition. Behav Brain Sci 16:255–265" href="/article/10.1007/s10055-016-0289-5#ref-CR19" id="ref-link-section-d980e333">1993</a>).</p><p>Spatial language is then, natural language with spatial references, useful for giving directions about an object location. In the formulation of spatial language, the spatial reasoning task is based mainly on a small subset of relational terms, with a large collection of linguistic expressions offered by each language. Spatial linguistic expressions vary also on the basis of the nature of the environment, the goal of the communication, the basic features of the object, the number of objects, the location of the objects and the interaction among objects (Skubic et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Skubic M, Perzanowski D, Blisard S, Schultz A, Adams W (2002) Spatial Language for Human-Robot Dialogs. IEEE Transactions on SMC, Part C, Special Issue on Human-Robot Interaction: 1–39" href="/article/10.1007/s10055-016-0289-5#ref-CR39" id="ref-link-section-d980e339">2002</a>).</p><p>A common practice in spatial language is the use of reference objects. In fact, it can be very difficult to state the position of an object without referring to another (Mou and McNamara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Mou W, McNamara TP (2002) Intrinsic frames of reference in spatial memory. Am Psychol Assoc J Exp Psychol: Learn Mem Cognit 28:162–170" href="/article/10.1007/s10055-016-0289-5#ref-CR27" id="ref-link-section-d980e345">2002</a>). This situation implies the representation of the relation between two objects: the <i>locatum</i> or located object (LO<i>)</i>, the <i>relatum</i> or reference object (RO<i>)</i> and their spatial relation (Tenbrink and Ragni <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Tenbrink T, Ragni M (2012) Relevance in Spatial Navigation and Communication. Springer Spatial Cognition VIII Lecture Notes in Computer Science 7463:279–298" href="/article/10.1007/s10055-016-0289-5#ref-CR42" id="ref-link-section-d980e361">2012</a>). An object localization is then divided in three steps: (1) identify the object to be located (LO); (2) select a useful object to serve as a reference (RO); and (3) develop a linguistic expression in relation to both objects (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. Universität des Saarlandes, Saarbrücken, pp 1–18" href="/article/10.1007/s10055-016-0289-5#ref-CR8" id="ref-link-section-d980e364">1995</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gapp K-P (1996) Selection of best reference objects in object localizations. University of Saarbrücken, Saarbrücken, pp 1–6" href="/article/10.1007/s10055-016-0289-5#ref-CR9" id="ref-link-section-d980e367">1996</a>).</p><p>Likewise, because the location of an object is inherently relative, it cannot be referenced without establishing a frame of reference (Shelton and McNamara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Shelton AL, McNamara TP (2001) Systems of spatial reference in human memory. Elsevier Sci Cognit Psychol 43:274–310" href="/article/10.1007/s10055-016-0289-5#ref-CR38" id="ref-link-section-d980e373">2001</a>; Mou and McNamara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Mou W, McNamara TP (2002) Intrinsic frames of reference in spatial memory. Am Psychol Assoc J Exp Psychol: Learn Mem Cognit 28:162–170" href="/article/10.1007/s10055-016-0289-5#ref-CR27" id="ref-link-section-d980e376">2002</a>). An object can be specified relative to the observer, to the environment, to its own intrinsic structure or to other objects in the environment, which requires the adoption of a specific spatial frame of reference (Wraga et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Wraga M, Creem SH, Proffitt DR (1998) The influence of spatial reference frames on imagined object and viewer rotations. Elsevier Acta Psychol 102:247–264" href="/article/10.1007/s10055-016-0289-5#ref-CR48" id="ref-link-section-d980e379">1998</a>). In such a way that the linguistic relation between the LO and the RO is constructed upon a spatial reference frame, also known as spatial reference system, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0289-5#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0289-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0289-5/MediaObjects/10055_2016_289_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0289-5/MediaObjects/10055_2016_289_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Spatial language elements with reference object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0289-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The spatial reference frames provide a structure to specify the object’s spatial composition and position; a coordinate system to give directions from different points in space or a mental representation of positions, such as up, down or side (McMullen and Jolicoeur <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="McMullen PA, Jolicoeur P (1990) The spatial frame of reference in object naming and discrimination of left-right reflections. Psychon Soc Inc Mem Cognit 18:99–115" href="/article/10.1007/s10055-016-0289-5#ref-CR24" id="ref-link-section-d980e407">1990</a>; Levinson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Levinson SC (2003) Space in language and cognition: explorations in cognitive diversity, LCC5. Language, culture and cognition. University Press, Cambridge" href="/article/10.1007/s10055-016-0289-5#ref-CR20" id="ref-link-section-d980e410">2003</a>; O’Meara and Pérez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="O’Meara C, Pérez BG (2011) Spatial frames of reference in Mesoamerican languages. Elsevier Lang Sci 33:837–852" href="/article/10.1007/s10055-016-0289-5#ref-CR29" id="ref-link-section-d980e413">2011</a>; Tenbrink and Ragni <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Tenbrink T, Ragni M (2012) Relevance in Spatial Navigation and Communication. Springer Spatial Cognition VIII Lecture Notes in Computer Science 7463:279–298" href="/article/10.1007/s10055-016-0289-5#ref-CR42" id="ref-link-section-d980e416">2012</a>).</p><p>A common categorization of reference frames based on Levinson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Levinson SC (2003) Space in language and cognition: explorations in cognitive diversity, LCC5. Language, culture and cognition. University Press, Cambridge" href="/article/10.1007/s10055-016-0289-5#ref-CR20" id="ref-link-section-d980e422">2003</a>) contemplates three types: intrinsic, relative and absolute. The intrinsic frame of reference occurs when the spatial reference is specified with respect to the reference object, for example: “the pencil is in <i>front of the box.</i>” It is relative, when it focuses on the viewpoint of the listener or the speaker, for example, “the car is on <i>your left.</i>” And absolute, when it describes a fixed bearing provided by cardinal points or the visual horizon, that is, focused on the environment and its salient feature, for example, “the house is <i>north of the city.</i>” Another common classification distinguishes whether the frame of reference is egocentric, represented focused on the peripheral aspects in reference to the observer’s body, or allocentric, when the object location is focused with respect to the environment (Shelton and McNamara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Shelton AL, McNamara TP (2001) Systems of spatial reference in human memory. Elsevier Sci Cognit Psychol 43:274–310" href="/article/10.1007/s10055-016-0289-5#ref-CR38" id="ref-link-section-d980e434">2001</a>; Mou and McNamara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Mou W, McNamara TP (2002) Intrinsic frames of reference in spatial memory. Am Psychol Assoc J Exp Psychol: Learn Mem Cognit 28:162–170" href="/article/10.1007/s10055-016-0289-5#ref-CR27" id="ref-link-section-d980e438">2002</a>; Ma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ma Y, Hu X, Wilson FA (2012) The egocentric spatial reference frame used in dorsal–lateral prefrontal workingmemory in primates. Elsevier Neurosci Biobehav Rev 36:26–33" href="/article/10.1007/s10055-016-0289-5#ref-CR23" id="ref-link-section-d980e441">2012</a>).</p><p>Regarding the selection of the reference object, it involves the recognition of its prominent characteristic or characteristics. Several cognitive criteria are used to select the reference objects, where perceptual saliency is a key concept of psychology early mentioned by Titchener (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1908" title="Titchener EB (1908) Lectures on the elementary psychology of feeling and attention. The MacMillan Company, New York" href="/article/10.1007/s10055-016-0289-5#ref-CR43" id="ref-link-section-d980e447">1908</a>), one of the first writers in this field. In recent years, this concept has been applied in information technology for object analysis and vision (Huang and Pashler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Huang L, Pashler H (2005) Quantifying object salience by equating distractor effects. Elsevier Vis Res 45:1909–1920" href="/article/10.1007/s10055-016-0289-5#ref-CR15" id="ref-link-section-d980e450">2005</a>).</p><p>The characteristic or characteristics that provoke perceptual saliency of an object have been described as those that somehow draw our attention (Frintrop and Rome <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Frintrop S, Rome E (2010) Computational visual attention systems and their cognitive foundations: a survey. ACM J Name 7:1–46" href="/article/10.1007/s10055-016-0289-5#ref-CR7" id="ref-link-section-d980e456">2010</a>; Vargas and Lahera <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vargas ML, Lahera G (2011) Asignación de relavancia: Una propuesta para el término inglés “salience”. Actas Esp Psiquiatría, España, pp 271–272" href="/article/10.1007/s10055-016-0289-5#ref-CR45" id="ref-link-section-d980e459">2011</a>; Lahera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Lahera G, Freund N, Sáiz-Ruiz J (2013) Asignación de relevancia (salience) y desregulación del sistema dopaminérgico. Elsevier Doyma Revista de Psiquiatría y Salud Mental 6:45–51" href="/article/10.1007/s10055-016-0289-5#ref-CR18" id="ref-link-section-d980e462">2013</a>). Some authors refer to them as those characteristics that are rare or just different in the scenario (Hall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Hall D, Leibe B, Schile B (2002) Saliency of interest points under scale changes. British Machine Vision Conference (BMVC’02), Cardiff, pp 646–655" href="/article/10.1007/s10055-016-0289-5#ref-CR11" id="ref-link-section-d980e465">2002</a>; Röser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: Markus K, Natalie S, Michael P, Ipke W (eds) Cooperative Minds: Social Interaction and Group Dynamics Proceedings of the 35th Annual Meeting of the Cognitive Science Society, Austin, TX. USA, pp 3315–3320" href="/article/10.1007/s10055-016-0289-5#ref-CR36" id="ref-link-section-d980e468">2013</a>), although, as Gapp (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. Universität des Saarlandes, Saarbrücken, pp 1–18" href="/article/10.1007/s10055-016-0289-5#ref-CR8" id="ref-link-section-d980e472">1995</a>) mentioned, in some cases, the selection might obey only to the distance between the target and the reference object.</p><p>Specifically about the object features, research on visual representation has addressed the implicit functionality of objects and their familiarity to the user (Hayward <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Hayward WG, Tarr MJ (1995) Spatial language and spatial representation. Elsevier Cognit 55:39–84" href="/article/10.1007/s10055-016-0289-5#ref-CR12" id="ref-link-section-d980e478">1995</a>; Pinker and Bloom <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Pinker S, Bloom P (1990) Natural language and natural selection. Behav Brain Sci 13:707–727" href="/article/10.1007/s10055-016-0289-5#ref-CR31" id="ref-link-section-d980e481">1990</a>). However, the most common prominent characteristics of an object closely related to our visual perception seem to be its color, size and shape (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. Universität des Saarlandes, Saarbrücken, pp 1–18" href="/article/10.1007/s10055-016-0289-5#ref-CR8" id="ref-link-section-d980e484">1995</a>; Stoia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR41" id="ref-link-section-d980e487">2007</a>; Spotorno et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Spotorno S, Tatler BW, Faure S (2013) Semantic consistency versus perceptual salience in visual scenes: findings from change detection. Elsevier Acta Psycol 142:168–176" href="/article/10.1007/s10055-016-0289-5#ref-CR40" id="ref-link-section-d980e490">2013</a>; Raubal and Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Raubal M, Winter S (2002) Enriching Wayfinding Instructions with Local Landmarks. In: Egenhofer MJ, Mark DM (eds) Second International Conference, GIScience Proceedings. Springer Berlin Heidelberg, Boulder, CO, USA, pp 243–259" href="/article/10.1007/s10055-016-0289-5#ref-CR32" id="ref-link-section-d980e494">2002</a>). An object can stand out from its surroundings on color alone; take, for example, a white cube in the midst of a group of red cubes. Color is a difficult property to measure, identify and compare, although some studies use perceived light as a complex function of illumination, reflectance/absorption on surfaces of objects, and receptive abilities of the visual sense. An object can also be striking because of its size, which usually comprises the length, width and height of the object. For some objects, however, size refers only to a specific dimension, for example, width for a road or height for a building or a person (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gapp K-P (1996) Selection of best reference objects in object localizations. University of Saarbrücken, Saarbrücken, pp 1–6" href="/article/10.1007/s10055-016-0289-5#ref-CR9" id="ref-link-section-d980e497">1996</a>). If the visual interest of an object is its shape, the measure can be specified by the deviation of its shape from a rectangle or by its shape factor, which represents the height-to-width ratio. For example, skyscrapers have a high shape factor, whereas long and low buildings have a low shape factor (Raubal and Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Raubal M, Winter S (2002) Enriching Wayfinding Instructions with Local Landmarks. In: Egenhofer MJ, Mark DM (eds) Second International Conference, GIScience Proceedings. Springer Berlin Heidelberg, Boulder, CO, USA, pp 243–259" href="/article/10.1007/s10055-016-0289-5#ref-CR32" id="ref-link-section-d980e500">2002</a>).</p><p>Worthy of note is that a good perception of an object depends on factors such as visual acuity, clarity of vision and the viewpoint of the observer, as well as on the social and psychological impact that these features might have on the viewer.</p><p>The interest in directing people to an object in a physical or virtual space has led to the development of a number of studies and experiments in this research area. With the support of computer systems and the development of virtual environments, it is possible to simulate real spaces in order to represent this process not only for the purpose of study, but also for the generation of artificial spatial language.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Spatial language systems</h2><div class="c-article-section__content" id="Sec2-content"><p>Spatial language systems are mainly intended for the artificial generation of language and not for its interpretation. However, many of the algorithms developed for computer systems can be used for both purposes. Same as spatial language, the automatic generation of spatial language requires the combination of a spatial reference frame and the linguistic expression, for which might be required the selection of objects according to their perceptual salience. This is a far from straightforward task that represents a number of limitations, such as:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>
                                    <i>Absolute references may be ambiguous with respect to relative references</i>. In some cases, it is difficult to distinguish between the behavior of the absolute reference frame and its behavior in terms of a relative frame (Pederson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Pederson E, Danziger E, Wilkins D, Levinson S, Kita S, Senft G (1998) Semantic typology and spatial conceptualization linguistic society of America JSTOR. Language 74:557–589" href="/article/10.1007/s10055-016-0289-5#ref-CR30" id="ref-link-section-d980e531">1998</a>). Therefore, relative references can be misinterpreted; to solve this problem, priority schemes have been designed with alternative references and their own linguistic expressions (Moratz and Tenbrik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Moratz R, Tenbrik T (2006) Spatial reference in linguistic human-robot interaction: iterative, empirically supported development of a model of projective relations. Spat Cogn Comput 6(1):63–106" href="/article/10.1007/s10055-016-0289-5#ref-CR25" id="ref-link-section-d980e534">2006</a>).</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>A similar problem is <i>the ambiguity of the linguistic expressions used to understand the context and generate directions</i>. Ambiguity can emerge when describing one of the possible target objects available to use as a reference. This situation can render the linguistic expressions redundant or imprecise, generating a high computational cost in objects’ description, search and location processes (Stoia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR41" id="ref-link-section-d980e551">2007</a>).</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Another potential problem can arise <i>when the use of reference objects is not taken into account</i> or when there are several candidates because there is no way of knowing why one object is preferred to others in the described scenes (Barclay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR6" id="ref-link-section-d980e568">2010</a>).</p>
                    
                  </li>
                </ol>
                     <p>In order to deal with some of these limitations, conceptual schemes have been introduced with linguistic expressions as a key element for their implementation, such as the design of spatial language as close as possible to natural language and with a planned syntax, using common semantics with a logical formalism, a likely solution to be applied in virtual environments (Trinh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Trinh T-H (2013) A Constraint-based Approach to Modelling Spatial Semantics of Virtual Environments, Université de Bretagne Occidentale. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR44" id="ref-link-section-d980e578">2013</a>). For the generation of linguistic expressions with an efficient description, it is important to take advantage of the semantic rules of the used language; the multiple meanings of words and sentences allow generating multiple interpretations from which the listener can pick up the best, according to the surrounding environment (Winograd <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Winograd T (1971) Procedures as a representation for data in a computer program for understanding natural language, Massachusetts Institute of Technology. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR47" id="ref-link-section-d980e581">1971</a>; Stoia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR41" id="ref-link-section-d980e584">2007</a>).</p><p>Despite limitations, a number of applications have been developed, including, for example, descriptions of scenes from video input for blind people, the generation of automatic comments and the search for images or objects (Kelleher and Costello <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kelleher JD, Costello FJ (2009) Applying computational models of spatial prepositions to visually situated dialog. Comput Linguist Assoc Comput Linguist 35(2):271–306" href="/article/10.1007/s10055-016-0289-5#ref-CR17" id="ref-link-section-d980e590">2009</a>; Barclay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR6" id="ref-link-section-d980e593">2010</a>), where virtual environments play an important role. Based on Barclay (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR6" id="ref-link-section-d980e596">2010</a>), we now describe several applications with spatial language systems in different research areas:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>
                                    <i>Graphic design and drawing programs</i>. It might be complex to visually interpret the design of complex 3D graphics with a particular sequence development, but a language system could help by providing advice and an incentive for the creation of this type of representations.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>
                                    <i>Computer games</i>. Video game systems were one of the first areas to generate spatial languages because of the need for realistic software agents and their interaction with humans.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>
                                    <i>Navigation aids</i>. The use of wayfinding instructions within these software solutions help users to navigate better, possibly by providing auditory cues, such as “turn left and walk 100 meters” or even more specific expressions like “to your right, walk 5 meters, and turn left at an angle of 45°.”</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>
                                    <i>Robot systems</i>. This is certainly the easiest type of application to perceive. Like human beings, a robot can be guided by audio messages or wayfinding instructions. Work developed in the robotics field has relied heavily on spatial language systems. Also, natural language is a key resource for interaction between robots and humans, that is, for instructing and programming robots in localization tasks. An important aspect in this scenario is to establish the spatial references to the target, recognizing scene descriptions, so that the robot can identify objects or sites. To do this, however, it is necessary to solve two problems: robot perception and the ambiguity of human language (Moratz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Moratz R, Tenbrink T, Bateman J, Fischer K (2003) Spatial knowledge representation for human–robot interaction Springer-Verlag Berlin Heidelberg Spatial Cognition III Lecture Notes in Computer Science 2685:263–283" href="/article/10.1007/s10055-016-0289-5#ref-CR26" id="ref-link-section-d980e654">2003</a>).</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">5.</span>
                    
                      <p>
                                    <i>Training simulators</i>. The key to the information required by these applications is the type of support they provide for the users. Imagine, for example, a virtual training exercise, with an online instructor providing a soldier with the support required for locating a wounded comrade in a danger zone.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">6.</span>
                    
                      <p>
                                    <i>Geographic information system interfaces</i>. This type of tools is used to map and analyze events that occur in geographic areas, and they give local map references in response to specific queries.</p>
                    
                  </li>
                </ol>
                     <p>In the next section, we analyze a set of computational systems that integrate spatial language and discuss the progress made in recent years in this research line.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Computer systems with spatial language</h2><div class="c-article-section__content" id="Sec3-content"><p>The criteria for the selection of computer systems were: the use of real or virtual (2D or 3D) environments, object location, the use of spatial reference frames and reference objects, the implementation of a perceptual saliency model and the use of spatial natural language.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0289-5#Tab1">1</a> singles out three important aspects of the selected documents. Column 1 lists the name and year of publication. Column 2 gives the authors and the paper reference, as well as some authors that have cited the project; this is interesting because it provides insight into the maturity and the impact of the project on the research community. Column 3 gives a general description of the key aspects of the project, like its organizational structure, the programming language for development, advantages and disadvantages. Note that projects like Barclay’s Reference Object Choice in Spatial Language: Machine and Human Models (Barclay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR6" id="ref-link-section-d980e707">2010</a>) and Thanh-Hai Trinh’s Moscaret (Trinh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Trinh T-H (2013) A Constraint-based Approach to Modelling Spatial Semantics of Virtual Environments, Université de Bretagne Occidentale. PhD. Thesis" href="/article/10.1007/s10055-016-0289-5#ref-CR44" id="ref-link-section-d980e710">2013</a>) were not cited by other projects, but were included because they cover several of the topics of interest in this review.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Computational systems applying spatial language</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0289-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0289-5#Tab2">2</a> are identified the specific areas addressed by each computer system for these same projects. Column 1 lists the name of the system; column 2 specifies whether syntax and semantics were used to create the spatial language; column 3 indicates whether the system includes object localization; column 4 is concerned with whether the concept of object salience is used; column 5 refers to whether VEs were built for the system; column 6 describes whether the system interacts with humans or robots; and, finally, column 7 shows whether artificial intelligence was applied for the algorithm.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Other areas addressed by computational systems</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0289-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec4">Discussion</h3><p>In most of the analyzed systems, the use of semantics stands out as a key aspect. Fifteen of the 18 systems developed a semantic model for the correct interpretation of symbols and words. Syntax is another important aspect underlying interest in the selection and combination of the right words.</p><p>The main goal of 11 of the systems is object location. However, all systems were analyzed in search of key aspects for describing objects and scenes, such as a semantic model for studying linguistic behavior.</p><p>The reviewed papers describe spatial relations based on visual perception and natural language, thereby generating linguistic expressions.</p><p>As Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0289-5#Tab2">2</a> shows, only nine systems consider a model of perceptual salience based on the basic characteristics of objects.</p><p>All the systems used a virtual environment, albeit with different dimensionality (i.e., textual, 2D or 3D).</p><p>Human–computer interaction is the most common interaction mode (16 systems), and only a couple of the projects used interaction with robots.</p><p>Projects like 2D Images, Abella and Kender’s Scene Describer, the Virtual Director System, System for Spatial Knowledge Representation for Human–Robot Interaction, the GLIDES System and the Bishop System explain the design of their architecture and their components, giving a clear description of their responsibilities. However, other systems, like Situated Artificial Communicators, provide very limited information so that it is impossible either to visualize technical details and development tools or to gather evidence from the results of the respective experiments.</p><p>Several of these systems consider the use of reference objects, linguistic expressions and/or spatial reference frames. However, only 11 applications, namely the CITYTOUR System, SOCCER System, Abella and Kender’s Scene Describer, the Virtual Director System, the Describer System, Kelleher’s Situated Language Interpreter System, the System for Spatial Knowledge Representation for Human–Robot Interaction, the Bishop System, Space Case, Reference Object Choice in Spatial Language: Machine and Human Models, and Moscaret, precisely specify how they are used. Of these applications, seven explicitly specify the implementation process. The other applications, including SHRDLU, 2D Images, 3D Images Workbench, Regier’s Constrained Connectionist System, the Attentional Vector Sum Model, the GLIDES System, and Situated Artificial Communicators, describe the use and implementation of these aspects implicitly.</p><p>Artificial intelligence is a multidisciplinary area applied in the development of six of these projects: SHRDLU, 2D Images, Regier’s Constrained Connectionist System, the GLIDES System, Situated Artificial Communicators and Moscaret. Through computing and logic, artificial intelligence has helped to implement real dialog between the user and the computer. Furthermore, through the support of neural networks, knowledge bases and Bayesian networks, it has been possible to develop models that simulate human intelligence linked to the object location process.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Conclusions and future work</h2><div class="c-article-section__content" id="Sec5-content"><p>In this paper, we presented a review of computer systems that use spatial language for object location. The process of locating objects has been conducted in both real and virtual, and 2D and 3D environments. From linguistics and psychology, semantics and syntax are two of the fields involved in the study of frames of reference, perceptual salience and cognitive maps that have supported the comprehension and generation of artificial spatial languages, which, as mentioned, is a seemingly simple but actually a highly complex process.</p><p>The analyzed systems present some limitations, barriers to a complete and efficient interaction with humans in the localization process. However, important advances have been made in this ongoing research area.</p><p>This review presents a framework for addressing the listed aspects and limitations with the intention of developing a computational application to more efficiently enact the object location process in 3D virtual environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Abella A, Kender JR (1999) From Images to Sentences via Spatial Relations. In: Proceedings of the W on Integra" /><p class="c-article-references__text" id="ref-CR1">Abella A, Kender JR (1999) From Images to Sentences via Spatial Relations. In: Proceedings of the W on Integration of Speech and Image Understanding, pp 117–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. André, G. Bosch, G. Herzog, T. Rist, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="André E, Bosch G, Herzog G, Rist T (1986) Characterizing trajectories of moving objects using natural language" /><p class="c-article-references__text" id="ref-CR2">André E, Bosch G, Herzog G, Rist T (1986) Characterizing trajectories of moving objects using natural language path descriptions. Project VITRA, Universität des Saarlandes, Brighton, pp 1–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterizing%20trajectories%20of%20moving%20objects%20using%20natural%20language%20path%20descriptions&amp;pages=1-8&amp;publication_year=1986&amp;author=Andr%C3%A9%2CE&amp;author=Bosch%2CG&amp;author=Herzog%2CG&amp;author=Rist%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. André, G. Bosch, G. Herzog, T. Rist, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="André E, Bosch G, Herzog G, Rist T (1987) Coping with the intrinsic and deictic uses of spatial prepositions. " /><p class="c-article-references__text" id="ref-CR3">André E, Bosch G, Herzog G, Rist T (1987) Coping with the intrinsic and deictic uses of spatial prepositions. Artificial intelligence II: methodology, systems, applications (AIMSA). North-Holland, Amsterdam, pp 375–382</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Coping%20with%20the%20intrinsic%20and%20deictic%20uses%20of%20spatial%20prepositions.%20Artificial%20intelligence%20II%3A%20methodology%2C%20systems%2C%20applications%20%28AIMSA%29&amp;pages=375-382&amp;publication_year=1987&amp;author=Andr%C3%A9%2CE&amp;author=Bosch%2CG&amp;author=Herzog%2CG&amp;author=Rist%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="André E, Herzog G, Rist T (1988) On the simultaneous interpretation of real world image sequences and their na" /><p class="c-article-references__text" id="ref-CR4">André E, Herzog G, Rist T (1988) On the simultaneous interpretation of real world image sequences and their natural language description: the system SOCCER. In: Proceeding of 8th European Conference on Artificial Intelligence (ECAI-88). Pitmann Publishing, London, Munich, pp 449–454</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="André E, Herzong G, Rist T (1989) Natural language access to visual data: dealing with space and movement. In:" /><p class="c-article-references__text" id="ref-CR5">André E, Herzong G, Rist T (1989) Natural language access to visual data: dealing with space and movement. In: Proceedings of the 1st Workshop on Logical Semantics of Time, Space and Movement in Natural Language. Universität des Saarlandes, Toulouse, pp 1–21</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. " /><p class="c-article-references__text" id="ref-CR6">Barclay M (2010) reference object choice in spatial language: machine and human models, University of Exeter. PhD. Thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Frintrop, E. Rome, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Frintrop S, Rome E (2010) Computational visual attention systems and their cognitive foundations: a survey. AC" /><p class="c-article-references__text" id="ref-CR7">Frintrop S, Rome E (2010) Computational visual attention systems and their cognitive foundations: a survey. ACM J Name 7:1–46</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20visual%20attention%20systems%20and%20their%20cognitive%20foundations%3A%20a%20survey&amp;journal=ACM%20J%20Name&amp;volume=7&amp;pages=1-46&amp;publication_year=2010&amp;author=Frintrop%2CS&amp;author=Rome%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="K-P. Gapp, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Gapp K-P (1995) Object localization: selection of optimal reference objects. Universität des Saarlandes, Saarb" /><p class="c-article-references__text" id="ref-CR8">Gapp K-P (1995) Object localization: selection of optimal reference objects. Universität des Saarlandes, Saarbrücken, pp 1–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Object%20localization%3A%20selection%20of%20optimal%20reference%20objects&amp;pages=1-18&amp;publication_year=1995&amp;author=Gapp%2CK-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="K-P. Gapp, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Gapp K-P (1996) Selection of best reference objects in object localizations. University of Saarbrücken, Saarbr" /><p class="c-article-references__text" id="ref-CR9">Gapp K-P (1996) Selection of best reference objects in object localizations. University of Saarbrücken, Saarbrücken, pp 1–6</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Selection%20of%20best%20reference%20objects%20in%20object%20localizations&amp;pages=1-6&amp;publication_year=1996&amp;author=Gapp%2CK-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Gorniak, D. Roy, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Gorniak P, Roy D (2004) Grounded semantic composition for visual scenes. J Artif Intell Res 21:429–470" /><p class="c-article-references__text" id="ref-CR10">Gorniak P, Roy D (2004) Grounded semantic composition for visual scenes. J Artif Intell Res 21:429–470</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grounded%20semantic%20composition%20for%20visual%20scenes&amp;journal=J%20Artif%20Intell%20Res&amp;volume=21&amp;pages=429-470&amp;publication_year=2004&amp;author=Gorniak%2CP&amp;author=Roy%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hall D, Leibe B, Schile B (2002) Saliency of interest points under scale changes. British Machine Vision Confe" /><p class="c-article-references__text" id="ref-CR11">Hall D, Leibe B, Schile B (2002) Saliency of interest points under scale changes. British Machine Vision Conference (BMVC’02), Cardiff, pp 646–655</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WG. Hayward, MJ. Tarr, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Hayward WG, Tarr MJ (1995) Spatial language and spatial representation. Elsevier Cognit 55:39–84" /><p class="c-article-references__text" id="ref-CR12">Hayward WG, Tarr MJ (1995) Spatial language and spatial representation. Elsevier Cognit 55:39–84</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0010-0277%2894%2900643-Y" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20language%20and%20spatial%20representation&amp;journal=Elsevier%20Cognit&amp;volume=55&amp;pages=39-84&amp;publication_year=1995&amp;author=Hayward%2CWG&amp;author=Tarr%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Herzog, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Herzog G (1992) Visualization methods for the VITRA workbench. Universität des Saarlandes, Saarbrücken, pp 1–1" /><p class="c-article-references__text" id="ref-CR13">Herzog G (1992) Visualization methods for the VITRA workbench. Universität des Saarlandes, Saarbrücken, pp 1–16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visualization%20methods%20for%20the%20VITRA%20workbench&amp;pages=1-16&amp;publication_year=1992&amp;author=Herzog%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Herzog G (1995) From visual input to verbal output in the visual translator. In: Proceedings of the AAAI Fall " /><p class="c-article-references__text" id="ref-CR14">Herzog G (1995) From visual input to verbal output in the visual translator. In: Proceedings of the AAAI Fall Symposium on Computational Models for Integrating Language and Vision. Universität des Saarlandes, Cambridge, pp 1–15</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Huang, H. Pashler, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Huang L, Pashler H (2005) Quantifying object salience by equating distractor effects. Elsevier Vis Res 45:1909" /><p class="c-article-references__text" id="ref-CR15">Huang L, Pashler H (2005) Quantifying object salience by equating distractor effects. Elsevier Vis Res 45:1909–1920</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.visres.2005.01.013" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantifying%20object%20salience%20by%20equating%20distractor%20effects&amp;journal=Elsevier%20Vis%20Res&amp;volume=45&amp;pages=1909-1920&amp;publication_year=2005&amp;author=Huang%2CL&amp;author=Pashler%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kelleher JD (2003) A Perceptually Based Computational Framework for the Interpretation of Spatial Language Dub" /><p class="c-article-references__text" id="ref-CR16">Kelleher JD (2003) A Perceptually Based Computational Framework for the Interpretation of Spatial Language Dublin City University. PhD. Thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JD. Kelleher, FJ. Costello, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Kelleher JD, Costello FJ (2009) Applying computational models of spatial prepositions to visually situated dia" /><p class="c-article-references__text" id="ref-CR17">Kelleher JD, Costello FJ (2009) Applying computational models of spatial prepositions to visually situated dialog. Comput Linguist Assoc Comput Linguist 35(2):271–306</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Applying%20computational%20models%20of%20spatial%20prepositions%20to%20visually%20situated%20dialog&amp;journal=Comput%20Linguist%20Assoc%20Comput%20Linguist&amp;volume=35&amp;issue=2&amp;pages=271-306&amp;publication_year=2009&amp;author=Kelleher%2CJD&amp;author=Costello%2CFJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Lahera, N. Freund, J. Sáiz-Ruiz, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Lahera G, Freund N, Sáiz-Ruiz J (2013) Asignación de relevancia (salience) y desregulación del sistema dopamin" /><p class="c-article-references__text" id="ref-CR18">Lahera G, Freund N, Sáiz-Ruiz J (2013) Asignación de relevancia (salience) y desregulación del sistema dopaminérgico. Elsevier Doyma Revista de Psiquiatría y Salud Mental 6:45–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.rpsm.2012.05.003" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asignaci%C3%B3n%20de%20relevancia%20%28salience%29%20y%20desregulaci%C3%B3n%20del%20sistema%20dopamin%C3%A9rgico&amp;journal=Elsevier%20Doyma%20Revista%20de%20Psiquiatr%C3%ADa%20y%20Salud%20Mental&amp;volume=6&amp;pages=45-51&amp;publication_year=2013&amp;author=Lahera%2CG&amp;author=Freund%2CN&amp;author=S%C3%A1iz-Ruiz%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Landau, R. Jackendoff, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Landau B, Jackendoff R (1993) “What” and “where” in spatial language and spatial cognition. Behav Brain Sci 16" /><p class="c-article-references__text" id="ref-CR19">Landau B, Jackendoff R (1993) “What” and “where” in spatial language and spatial cognition. Behav Brain Sci 16:255–265</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1017%2FS0140525X00029927" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CWhat%E2%80%9D%20and%20%E2%80%9Cwhere%E2%80%9D%20in%20spatial%20language%20and%20spatial%20cognition&amp;journal=Behav%20Brain%20Sci&amp;volume=16&amp;pages=255-265&amp;publication_year=1993&amp;author=Landau%2CB&amp;author=Jackendoff%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SC. Levinson, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Levinson SC (2003) Space in language and cognition: explorations in cognitive diversity, LCC5. Language, cultu" /><p class="c-article-references__text" id="ref-CR20">Levinson SC (2003) Space in language and cognition: explorations in cognitive diversity, LCC5. Language, culture and cognition. University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Space%20in%20language%20and%20cognition%3A%20explorations%20in%20cognitive%20diversity%2C%20LCC5.%20Language%2C%20culture%20and%20cognition&amp;publication_year=2003&amp;author=Levinson%2CSC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lockwood K, Forbus K, Usher J (2005) SpaceCase: A Model of Spatial Preposition Use. In: Proceedings of the 27t" /><p class="c-article-references__text" id="ref-CR21">Lockwood K, Forbus K, Usher J (2005) SpaceCase: A Model of Spatial Preposition Use. In: Proceedings of the 27th Annual Conference of the Cognitive Science Society, Stressa, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lockwood K, Forbus K, Halstead DT, Usher J (2006) Automatic categorization of spatial prepositions. In: Procee" /><p class="c-article-references__text" id="ref-CR22">Lockwood K, Forbus K, Halstead DT, Usher J (2006) Automatic categorization of spatial prepositions. In: Proceedings of the 28th Annual Conference of the Cognitive Science Society, Vancouver, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Ma, X. Hu, FA. Wilson, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Ma Y, Hu X, Wilson FA (2012) The egocentric spatial reference frame used in dorsal–lateral prefrontal workingm" /><p class="c-article-references__text" id="ref-CR23">Ma Y, Hu X, Wilson FA (2012) The egocentric spatial reference frame used in dorsal–lateral prefrontal workingmemory in primates. Elsevier Neurosci Biobehav Rev 36:26–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neubiorev.2011.03.011" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20egocentric%20spatial%20reference%20frame%20used%20in%20dorsal%E2%80%93lateral%20prefrontal%20workingmemory%20in%20primates&amp;journal=Elsevier%20Neurosci%20Biobehav%20Rev&amp;volume=36&amp;pages=26-33&amp;publication_year=2012&amp;author=Ma%2CY&amp;author=Hu%2CX&amp;author=Wilson%2CFA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PA. McMullen, P. Jolicoeur, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="McMullen PA, Jolicoeur P (1990) The spatial frame of reference in object naming and discrimination of left-rig" /><p class="c-article-references__text" id="ref-CR24">McMullen PA, Jolicoeur P (1990) The spatial frame of reference in object naming and discrimination of left-right reflections. Psychon Soc Inc Mem Cognit 18:99–115</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03202650" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20spatial%20frame%20of%20reference%20in%20object%20naming%20and%20discrimination%20of%20left-right%20reflections&amp;journal=Psychon%20Soc%20Inc%20Mem%20Cognit&amp;volume=18&amp;pages=99-115&amp;publication_year=1990&amp;author=McMullen%2CPA&amp;author=Jolicoeur%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Moratz, T. Tenbrik, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Moratz R, Tenbrik T (2006) Spatial reference in linguistic human-robot interaction: iterative, empirically sup" /><p class="c-article-references__text" id="ref-CR25">Moratz R, Tenbrik T (2006) Spatial reference in linguistic human-robot interaction: iterative, empirically supported development of a model of projective relations. Spat Cogn Comput 6(1):63–106</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20reference%20in%20linguistic%20human-robot%20interaction%3A%20iterative%2C%20empirically%20supported%20development%20of%20a%20model%20of%20projective%20relations&amp;journal=Spat%20Cogn%20Comput&amp;volume=6&amp;issue=1&amp;pages=63-106&amp;publication_year=2006&amp;author=Moratz%2CR&amp;author=Tenbrik%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moratz R, Tenbrink T, Bateman J, Fischer K (2003) Spatial knowledge representation for human–robot interaction" /><p class="c-article-references__text" id="ref-CR26">Moratz R, Tenbrink T, Bateman J, Fischer K (2003) Spatial knowledge representation for human–robot interaction Springer-Verlag Berlin Heidelberg Spatial Cognition III Lecture Notes in Computer Science 2685:263–283</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Mou, TP. McNamara, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Mou W, McNamara TP (2002) Intrinsic frames of reference in spatial memory. Am Psychol Assoc J Exp Psychol: Lea" /><p class="c-article-references__text" id="ref-CR27">Mou W, McNamara TP (2002) Intrinsic frames of reference in spatial memory. Am Psychol Assoc J Exp Psychol: Learn Mem Cognit 28:162–170</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intrinsic%20frames%20of%20reference%20in%20spatial%20memory&amp;journal=Am%20Psychol%20Assoc%20J%20Exp%20Psychol%3A%20Learn%20Mem%20Cognit&amp;volume=28&amp;pages=162-170&amp;publication_year=2002&amp;author=Mou%2CW&amp;author=McNamara%2CTP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Mukerjee, K. Gupta, S. Nautiyal, MP. Singh, N. Mishra, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Mukerjee A, Gupta K, Nautiyal S, Singh MP, Mishra N (2000) Conceptual description of visual scenes from lingui" /><p class="c-article-references__text" id="ref-CR28">Mukerjee A, Gupta K, Nautiyal S, Singh MP, Mishra N (2000) Conceptual description of visual scenes from linguistic models. Elsevier Image Vis Comput 18:173–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0262-8856%2899%2900022-0" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Conceptual%20description%20of%20visual%20scenes%20from%20linguistic%20models&amp;journal=Elsevier%20Image%20Vis%20Comput&amp;volume=18&amp;pages=173-187&amp;publication_year=2000&amp;author=Mukerjee%2CA&amp;author=Gupta%2CK&amp;author=Nautiyal%2CS&amp;author=Singh%2CMP&amp;author=Mishra%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. O’Meara, BG. Pérez, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="O’Meara C, Pérez BG (2011) Spatial frames of reference in Mesoamerican languages. Elsevier Lang Sci 33:837–852" /><p class="c-article-references__text" id="ref-CR29">O’Meara C, Pérez BG (2011) Spatial frames of reference in Mesoamerican languages. Elsevier Lang Sci 33:837–852</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.langsci.2011.06.013" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20frames%20of%20reference%20in%20Mesoamerican%20languages&amp;journal=Elsevier%20Lang%20Sci&amp;volume=33&amp;pages=837-852&amp;publication_year=2011&amp;author=O%E2%80%99Meara%2CC&amp;author=P%C3%A9rez%2CBG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Pederson, E. Danziger, D. Wilkins, S. Levinson, S. Kita, G. Senft, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Pederson E, Danziger E, Wilkins D, Levinson S, Kita S, Senft G (1998) Semantic typology and spatial conceptual" /><p class="c-article-references__text" id="ref-CR30">Pederson E, Danziger E, Wilkins D, Levinson S, Kita S, Senft G (1998) Semantic typology and spatial conceptualization linguistic society of America JSTOR. Language 74:557–589</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1353%2Flan.1998.0074" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Semantic%20typology%20and%20spatial%20conceptualization%20linguistic%20society%20of%20America%20JSTOR&amp;journal=Language&amp;volume=74&amp;pages=557-589&amp;publication_year=1998&amp;author=Pederson%2CE&amp;author=Danziger%2CE&amp;author=Wilkins%2CD&amp;author=Levinson%2CS&amp;author=Kita%2CS&amp;author=Senft%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Pinker, P. Bloom, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Pinker S, Bloom P (1990) Natural language and natural selection. Behav Brain Sci 13:707–727" /><p class="c-article-references__text" id="ref-CR31">Pinker S, Bloom P (1990) Natural language and natural selection. Behav Brain Sci 13:707–727</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1017%2FS0140525X00081061" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20language%20and%20natural%20selection&amp;journal=Behav%20Brain%20Sci&amp;volume=13&amp;pages=707-727&amp;publication_year=1990&amp;author=Pinker%2CS&amp;author=Bloom%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raubal M, Winter S (2002) Enriching Wayfinding Instructions with Local Landmarks. In: Egenhofer MJ, Mark DM (e" /><p class="c-article-references__text" id="ref-CR32">Raubal M, Winter S (2002) Enriching Wayfinding Instructions with Local Landmarks. In: Egenhofer MJ, Mark DM (eds) Second International Conference, GIScience Proceedings. Springer Berlin Heidelberg, Boulder, CO, USA, pp 243–259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Regier, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Regier T (1996) The human semantic potential: spatial language and constrained connectionism. Computat Linguis" /><p class="c-article-references__text" id="ref-CR33">Regier T (1996) The human semantic potential: spatial language and constrained connectionism. Computat Linguist 23(3):483–486</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20human%20semantic%20potential%3A%20spatial%20language%20and%20constrained%20connectionism&amp;journal=Computat%20Linguist&amp;volume=23&amp;issue=3&amp;pages=483-486&amp;publication_year=1996&amp;author=Regier%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Regier, LA. Carlson, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Regier T, Carlson LA (2001) Grounding spatial language in perception: an empirical and computational investiga" /><p class="c-article-references__text" id="ref-CR34">Regier T, Carlson LA (2001) Grounding spatial language in perception: an empirical and computational investigation. J Exp Psychol Gen 130:273–298</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-3445.130.2.273" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grounding%20spatial%20language%20in%20perception%3A%20an%20empirical%20and%20computational%20investigation&amp;journal=J%20Exp%20Psychol%20Gen&amp;volume=130&amp;pages=273-298&amp;publication_year=2001&amp;author=Regier%2CT&amp;author=Carlson%2CLA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Rickheit, I. Wachsmuth, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Rickheit G, Wachsmuth I (2006) Situated communication. Gruyter, Berlin, pp 7–30" /><p class="c-article-references__text" id="ref-CR35">Rickheit G, Wachsmuth I (2006) Situated communication. Gruyter, Berlin, pp 7–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Situated%20communication&amp;pages=7-30&amp;publication_year=2006&amp;author=Rickheit%2CG&amp;author=Wachsmuth%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: Markus K, Nat" /><p class="c-article-references__text" id="ref-CR36">Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: Markus K, Natalie S, Michael P, Ipke W (eds) Cooperative Minds: Social Interaction and Group Dynamics Proceedings of the 35th Annual Meeting of the Cognitive Science Society, Austin, TX. USA, pp 3315–3320</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DK. Roy, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Roy DK (2002) Learning visually-grounded words and syntax for a scene description task. Comput Speech Lang 16:" /><p class="c-article-references__text" id="ref-CR37">Roy DK (2002) Learning visually-grounded words and syntax for a scene description task. Comput Speech Lang 16:1–39</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0885-2308%2802%2900024-4" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20visually-grounded%20words%20and%20syntax%20for%20a%20scene%20description%20task&amp;journal=Comput%20Speech%20Lang&amp;volume=16&amp;pages=1-39&amp;publication_year=2002&amp;author=Roy%2CDK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AL. Shelton, TP. McNamara, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Shelton AL, McNamara TP (2001) Systems of spatial reference in human memory. Elsevier Sci Cognit Psychol 43:27" /><p class="c-article-references__text" id="ref-CR38">Shelton AL, McNamara TP (2001) Systems of spatial reference in human memory. Elsevier Sci Cognit Psychol 43:274–310</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcogp.2001.0758" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Systems%20of%20spatial%20reference%20in%20human%20memory&amp;journal=Elsevier%20Sci%20Cognit%20Psychol&amp;volume=43&amp;pages=274-310&amp;publication_year=2001&amp;author=Shelton%2CAL&amp;author=McNamara%2CTP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Skubic M, Perzanowski D, Blisard S, Schultz A, Adams W (2002) Spatial Language for Human-Robot Dialogs. IEEE T" /><p class="c-article-references__text" id="ref-CR39">Skubic M, Perzanowski D, Blisard S, Schultz A, Adams W (2002) Spatial Language for Human-Robot Dialogs. IEEE Transactions on SMC, Part C, Special Issue on Human-Robot Interaction: 1–39</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Spotorno, BW. Tatler, S. Faure, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Spotorno S, Tatler BW, Faure S (2013) Semantic consistency versus perceptual salience in visual scenes: findin" /><p class="c-article-references__text" id="ref-CR40">Spotorno S, Tatler BW, Faure S (2013) Semantic consistency versus perceptual salience in visual scenes: findings from change detection. Elsevier Acta Psycol 142:168–176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.actpsy.2012.12.009" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Semantic%20consistency%20versus%20perceptual%20salience%20in%20visual%20scenes%3A%20findings%20from%20change%20detection&amp;journal=Elsevier%20Acta%20Psycol&amp;volume=142&amp;pages=168-176&amp;publication_year=2013&amp;author=Spotorno%2CS&amp;author=Tatler%2CBW&amp;author=Faure%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis" /><p class="c-article-references__text" id="ref-CR41">Stoia L (2007) Noun phrase generation for situated dialogs, Ohio State University. PhD. Thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tenbrink T, Ragni M (2012) Relevance in Spatial Navigation and Communication. Springer Spatial Cognition VIII " /><p class="c-article-references__text" id="ref-CR42">Tenbrink T, Ragni M (2012) Relevance in Spatial Navigation and Communication. Springer Spatial Cognition VIII Lecture Notes in Computer Science 7463:279–298</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="EB. Titchener, " /><meta itemprop="datePublished" content="1908" /><meta itemprop="headline" content="Titchener EB (1908) Lectures on the elementary psychology of feeling and attention. The MacMillan Company, New" /><p class="c-article-references__text" id="ref-CR43">Titchener EB (1908) Lectures on the elementary psychology of feeling and attention. The MacMillan Company, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lectures%20on%20the%20elementary%20psychology%20of%20feeling%20and%20attention&amp;publication_year=1908&amp;author=Titchener%2CEB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trinh T-H (2013) A Constraint-based Approach to Modelling Spatial Semantics of Virtual Environments, Universit" /><p class="c-article-references__text" id="ref-CR44">Trinh T-H (2013) A Constraint-based Approach to Modelling Spatial Semantics of Virtual Environments, Université de Bretagne Occidentale. PhD. Thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="ML. Vargas, G. Lahera, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Vargas ML, Lahera G (2011) Asignación de relavancia: Una propuesta para el término inglés “salience”. Actas Es" /><p class="c-article-references__text" id="ref-CR45">Vargas ML, Lahera G (2011) Asignación de relavancia: Una propuesta para el término inglés “salience”. Actas Esp Psiquiatría, España, pp 271–272</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asignaci%C3%B3n%20de%20relavancia%3A%20Una%20propuesta%20para%20el%20t%C3%A9rmino%20ingl%C3%A9s%20%E2%80%9Csalience%E2%80%9D&amp;pages=271-272&amp;publication_year=2011&amp;author=Vargas%2CML&amp;author=Lahera%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Williams P, Miikkulainen R (2006) Grounding Language in Descriptions of Scenes. In: Proceedings of the 28th An" /><p class="c-article-references__text" id="ref-CR46">Williams P, Miikkulainen R (2006) Grounding Language in Descriptions of Scenes. In: Proceedings of the 28th Annual Conference of the Cognitive Science Society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Winograd T (1971) Procedures as a representation for data in a computer program for understanding natural lang" /><p class="c-article-references__text" id="ref-CR47">Winograd T (1971) Procedures as a representation for data in a computer program for understanding natural language, Massachusetts Institute of Technology. PhD. Thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Wraga, SH. Creem, DR. Proffitt, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Wraga M, Creem SH, Proffitt DR (1998) The influence of spatial reference frames on imagined object and viewer " /><p class="c-article-references__text" id="ref-CR48">Wraga M, Creem SH, Proffitt DR (1998) The influence of spatial reference frames on imagined object and viewer rotations. Elsevier Acta Psychol 102:247–264</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0001-6918%2898%2900057-2" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20influence%20of%20spatial%20reference%20frames%20on%20imagined%20object%20and%20viewer%20rotations&amp;journal=Elsevier%20Acta%20Psychol&amp;volume=102&amp;pages=247-264&amp;publication_year=1998&amp;author=Wraga%2CM&amp;author=Creem%2CSH&amp;author=Proffitt%2CDR">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-016-0289-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>Graciela Lara holds a PROMEP scholarship in partnership with the Universidad de Guadalajara (UDG-685), Mexico.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara, Jalisco, Mexico</p><p class="c-article-author-affiliation__authors-list">Graciela Lara &amp; Adriana Peña</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Escuela Técnica Superior de Ingenieros Informático of the Universidad Politécnica de Madrid, Campus de Montegancedo, 28660, Boadilla Del Monte, Spain</p><p class="c-article-author-affiliation__authors-list">Angélica De Antonio</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Graciela-Lara"><span class="c-article-authors-search__title u-h3 js-search-name">Graciela Lara</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Graciela+Lara&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Graciela+Lara" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Graciela+Lara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ang_lica-Antonio"><span class="c-article-authors-search__title u-h3 js-search-name">Angélica De Antonio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ang%C3%A9lica+De+Antonio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ang%C3%A9lica+De+Antonio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ang%C3%A9lica+De+Antonio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Adriana-Pe_a"><span class="c-article-authors-search__title u-h3 js-search-name">Adriana Peña</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Adriana+Pe%C3%B1a&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Adriana+Pe%C3%B1a" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adriana+Pe%C3%B1a%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-016-0289-5/email/correspondent/c1/new">Graciela Lara</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Computerized%20spatial%20language%20generation%20for%20object%20location&amp;author=Graciela%20Lara%20et%20al&amp;contentID=10.1007%2Fs10055-016-0289-5&amp;publication=1359-4338&amp;publicationDate=2016-06-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-016-0289-5" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-016-0289-5" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lara, G., De Antonio, A. &amp; Peña, A. Computerized spatial language generation for object location.
                    <i>Virtual Reality</i> <b>20, </b>183–192 (2016). https://doi.org/10.1007/s10055-016-0289-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-016-0289-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-12-03">03 December 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-06-10">10 June 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-06-22">22 June 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-09">September 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-016-0289-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-016-0289-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Spatial language</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Spatial reference frames</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Computer systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Objects location and perceptual salience</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0289-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=289;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

