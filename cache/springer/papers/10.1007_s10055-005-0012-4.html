<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Stroke-based modeling and haptic skill display for Chinese calligraphy"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The goal of this paper is to study haptic skill representation and display in a Chinese calligraphy training system. The challenge is to model haptic skill during the writing of different strokes..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system"/>

    <meta name="dc.source" content="Virtual Reality 2005 9:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-01-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The goal of this paper is to study haptic skill representation and display in a Chinese calligraphy training system. The challenge is to model haptic skill during the writing of different strokes in Chinese characters and to achieve haptic rendering with high fidelity and stability. The planning of the writing process is organized at three levels: task, representation and device level to describe the haptic handwriting skill. State transition graph (STG) is proposed to describe switches between tasks during the handwriting. Chinese characters are modeled using 39 typical strokes, which are further grouped into basic and compound strokes. The compound stroke is considered to be sequential combination of the basic strokes. Straight and curve strokes are modeled using line segment and the Bezier curve, respectively. Information from STG is used for real-time collision detection and haptic rendering. Ambiguity of the collision detection at stroke-corner points is prevented using active stroke combined with local nearest point computation. A modified virtual fixture method is developed for haptic rendering. The approach is tested on a prototype training system using Phantom desktop. Initial experiments suggest that the proposed modeling and rendering method is effective."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-01-11"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="118"/>

    <meta name="prism.endingPage" content="132"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0012-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0012-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0012-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0012-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2006/03"/>

    <meta name="citation_online_date" content="2006/01/11"/>

    <meta name="citation_firstpage" content="118"/>

    <meta name="citation_lastpage" content="132"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0012-4"/>

    <meta name="DOI" content="10.1007/s10055-005-0012-4"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0012-4"/>

    <meta name="description" content="The goal of this paper is to study haptic skill representation and display in a Chinese calligraphy training system. The challenge is to model haptic skill"/>

    <meta name="dc.creator" content="Daniel Wang"/>

    <meta name="dc.creator" content="Yuru Zhang"/>

    <meta name="dc.creator" content="Chong Yao"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Solis J, Avizzano CA, Bergamasco M (2002) Teaching to write Japanese characters using a haptic interface. In: Proceedings of the 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 255&#8211;262"/>

    <meta name="citation_reference" content="Henmi K, Yoshikawa T (1998) Virtual lesson and its application to virtual calligraphy system. In: Proceedings of the 1998 IEEE international conference on robotics and automation. 2:1275&#8211;1280"/>

    <meta name="citation_reference" content="Teo CL, Burdet E, Lim HP (2002) A robotic teacher of chinese handwriting. In: Proceedings of 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 335&#8211;341"/>

    <meta name="citation_reference" content="Yang H-M, Lu J-J, Lee H-J (2001) A bezier curve-based approach to shape description for chinese calligraphy characters. In: Proceedings of the 6th international conference on document analysis and recognition, pp 276&#8211;280"/>

    <meta name="citation_reference" content="Chu NS-H, Tai C-L (2002) An efficient brush model for physically-based 3D painting. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 413&#8211;421"/>

    <meta name="citation_reference" content="Yeh J-S, Lien T-Y, Ouhyoung M (2002) On the effects of haptic display in brush and ink simulation for Chinese painting and calligraphy. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 439&#8211;441"/>

    <meta name="citation_reference" content="SagaS, Kawakami N, Tachi S (2005) Haptic teaching using opposite force presentation. In: Proceeding of the 1st World Haptics conference, Pisa, Italy, pp 18&#8211;20"/>

    <meta name="citation_reference" content="Hennion B, Gentaz E, Gouagout P, Bara F (2005) Telemaque, a new visuo-haptic interface for remediation of dysgraphic children. In: Proceedings of the 1st World Haptics conference, Pisa, Italy, pp 18&#8211;20"/>

    <meta name="citation_reference" content="Sakuma M et al (1999) System for Japanese calligraphy lesson with force feedback. In: Proceedings of the 8th annual symposium on Haptic interfaces for virtual environment and teleoperator systems"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Haptics in virtual environments: taxonomy, research status, and challenges; citation_author=MA Srinivasan, C Basdogan; citation_volume=21; citation_issue=4; citation_publication_date=1997; citation_pages=393-404; citation_doi=10.1016/S0097-8493(97)00030-7; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Rob Autom; citation_title=&#8220;Stable Haptic interaction with virtual environments&#8221;; citation_author=RJ Adams, B Hannaford; citation_volume=15; citation_publication_date=1999; citation_pages=3; citation_doi=10.1109/70.744598; citation_id=CR11"/>

    <meta name="citation_author" content="Daniel Wang"/>

    <meta name="citation_author_email" content="hapticwang@buaa.edu.cn"/>

    <meta name="citation_author_institution" content="Robotics Institute, Beihang University, Beijing, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Yuru Zhang"/>

    <meta name="citation_author_institution" content="Robotics Institute, Beihang University, Beijing, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Chong Yao"/>

    <meta name="citation_author_institution" content="Robotics Institute, Beihang University, Beijing, People&#8217;s Republic of China"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0012-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0012-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system"/>
        <meta property="og:description" content="The goal of this paper is to study haptic skill representation and display in a Chinese calligraphy training system. The challenge is to model haptic skill during the writing of different strokes in Chinese characters and to achieve haptic rendering with high fidelity and stability. The planning of the writing process is organized at three levels: task, representation and device level to describe the haptic handwriting skill. State transition graph (STG) is proposed to describe switches between tasks during the handwriting. Chinese characters are modeled using 39 typical strokes, which are further grouped into basic and compound strokes. The compound stroke is considered to be sequential combination of the basic strokes. Straight and curve strokes are modeled using line segment and the Bezier curve, respectively. Information from STG is used for real-time collision detection and haptic rendering. Ambiguity of the collision detection at stroke-corner points is prevented using active stroke combined with local nearest point computation. A modified virtual fixture method is developed for haptic rendering. The approach is tested on a prototype training system using Phantom desktop. Initial experiments suggest that the proposed modeling and rendering method is effective."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0012-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Feature Point, Chinese Character, Collision Detection, Haptic Feedback, Haptic Device","kwrd":["Feature_Point","Chinese_Character","Collision_Detection","Haptic_Feedback","Haptic_Device"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0012-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0012-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=12;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0012-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0012-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0012-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-01-11" itemprop="datePublished">11 January 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniel-Wang" data-author-popup="auth-Daniel-Wang" data-corresp-id="c1">Daniel Wang<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Beihang University" /><meta itemprop="address" content="grid.64939.31, 0000000099991211, Robotics Institute, Beihang University, 100083, Beijing, People’s Republic of China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yuru-Zhang" data-author-popup="auth-Yuru-Zhang">Yuru Zhang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Beihang University" /><meta itemprop="address" content="grid.64939.31, 0000000099991211, Robotics Institute, Beihang University, 100083, Beijing, People’s Republic of China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chong-Yao" data-author-popup="auth-Chong-Yao">Chong Yao</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Beihang University" /><meta itemprop="address" content="grid.64939.31, 0000000099991211, Robotics Institute, Beihang University, 100083, Beijing, People’s Republic of China" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">118</span>–<span itemprop="pageEnd">132</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">308 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">17 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0012-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The goal of this paper is to study haptic skill representation and display in a Chinese calligraphy training system. The challenge is to model haptic skill during the writing of different strokes in Chinese characters and to achieve haptic rendering with high fidelity and stability. The planning of the writing process is organized at three levels: task, representation and device level to describe the haptic handwriting skill. State transition graph (STG) is proposed to describe switches between tasks during the handwriting. Chinese characters are modeled using 39 typical strokes, which are further grouped into basic and compound strokes. The compound stroke is considered to be sequential combination of the basic strokes. Straight and curve strokes are modeled using line segment and the Bezier curve, respectively. Information from STG is used for real-time collision detection and haptic rendering. Ambiguity of the collision detection at stroke-corner points is prevented using active stroke combined with local nearest point computation. A modified virtual fixture method is developed for haptic rendering. The approach is tested on a prototype training system using Phantom desktop. Initial experiments suggest that the proposed modeling and rendering method is effective.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Learning to write Chinese characters by hand is difficult for beginners to learn because various skills are involved in order to co-ordinate motor output with sensory (visual and haptic) feedback for producing perfect strokes. The sensory-motor skills of wrist and fingers need to be trained over a long period before the structure and sequence of strokes become familiar. It is valuable to explore ways to accelerate the learning process.</p><p>Traditionally, there are mainly two training methods. The first is hands-on training by a teacher. The second is to practice on character templates sculpted on wood or printed on paper. Each method has its disadvantages. The first method is highly dependent on the skill of the teacher and is very time-consuming. The second method tends to be boring to the student who has to spend a long time repeating the same word over and over. Therefore, a haptically enhanced virtual reality system is proposed as an alternative for handwriting skill training.</p><p>Solis et al. developed a Japanese character training system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Solis J, Avizzano CA, Bergamasco M (2002) Teaching to write Japanese characters using a haptic interface. In: Proceedings of the 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 255–262" href="/article/10.1007/s10055-005-0012-4#ref-CR1" id="ref-link-section-d20069e307">1</a>]. The emphasis of their system was to identify the intent of a user in real-time and to compare the user’s trajectory with those in a pre-defined character library to find the best match. Henmi and Yoshikawa recorded the motion and force information of a teacher during the writing of Japanese characters and used the recorded data to train students [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Henmi K, Yoshikawa T (1998) Virtual lesson and its application to virtual calligraphy system. In: Proceedings of the 1998 IEEE international conference on robotics and automation. 2:1275–1280" href="/article/10.1007/s10055-005-0012-4#ref-CR2" id="ref-link-section-d20069e310">2</a>]. Teo et al. established a Chinese calligraphy training system using a six degree of freedom Delta haptic device [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Teo CL, Burdet E, Lim HP (2002) A robotic teacher of chinese handwriting. In: Proceedings of 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 335–341" href="/article/10.1007/s10055-005-0012-4#ref-CR3" id="ref-link-section-d20069e313">3</a>] and a mirror that collocated visual and force information. The learning process was decomposed into motion guidance and path constraining.</p><p>To develop a stroke-trajectory based training system of Chinese characters, it is necessary to study the kinesthetic cues that can lead to realistic force sensations. The kinesthetic cues should be quantified and be incorporated into an impedance controller of a haptic device. The modeling of haptic handwriting skills is still an open research problem.</p><p>There currently exists no model of Chinese characters for collision detection between a virtual pen and virtual characters. Hsi-Ming Yang et al. adopted piecewise Bezier curves to model the contour of Chinese brush pen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Yang H-M, Lu J-J, Lee H-J (2001) A bezier curve-based approach to shape description for chinese calligraphy characters. In: Proceedings of the 6th international conference on document analysis and recognition, pp 276–280" href="/article/10.1007/s10055-005-0012-4#ref-CR4" id="ref-link-section-d20069e322">4</a>]. Chu and Chiew-Lan Tai utilized a skeleton model to simulate handwriting effects [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Chu NS-H, Tai C-L (2002) An efficient brush model for physically-based 3D painting. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 413–421" href="/article/10.1007/s10055-005-0012-4#ref-CR5" id="ref-link-section-d20069e325">5</a>]. Information in these models are not suitable for computing virtual forces during handwriting [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Yeh J-S, Lien T-Y, Ouhyoung M (2002) On the effects of haptic display in brush and ink simulation for Chinese painting and calligraphy. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 439–441" href="/article/10.1007/s10055-005-0012-4#ref-CR6" id="ref-link-section-d20069e328">6</a>]. Henmi and Yoshikawa focused on skill transfer of hands-on teaching from teacher to students [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Henmi K, Yoshikawa T (1998) Virtual lesson and its application to virtual calligraphy system. In: Proceedings of the 1998 IEEE international conference on robotics and automation. 2:1275–1280" href="/article/10.1007/s10055-005-0012-4#ref-CR2" id="ref-link-section-d20069e331">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="SagaS, Kawakami N, Tachi S (2005) Haptic teaching using opposite force presentation. In: Proceeding of the 1st World Haptics conference, Pisa, Italy, pp 18–20" href="/article/10.1007/s10055-005-0012-4#ref-CR7" id="ref-link-section-d20069e334">7</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Sakuma M et al (1999) System for Japanese calligraphy lesson with force feedback. In: Proceedings of the 8th annual symposium on Haptic interfaces for virtual environment and teleoperator systems" href="/article/10.1007/s10055-005-0012-4#ref-CR9" id="ref-link-section-d20069e338">9</a>]. Because modeling issue in hands-on teaching is different from standard character based training, therefore, modeling and training of writing skill based on standard characters is an unsolved problem.</p><p>High efficient memory cost data structure is expected to store large number of Chinese characters and access speed for characters model information should meet 1 KHz update rate for stable haptic rendering [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Yeh J-S, Lien T-Y, Ouhyoung M (2002) On the effects of haptic display in brush and ink simulation for Chinese painting and calligraphy. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 439–441" href="/article/10.1007/s10055-005-0012-4#ref-CR6" id="ref-link-section-d20069e344">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Srinivasan MA, Basdogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. Comput Graph 21(4):393–404" href="/article/10.1007/s10055-005-0012-4#ref-CR10" id="ref-link-section-d20069e347">10</a>]. How to prevent ambiguity of collision detection at corners of complex strokes and maintain stability between interaction switches among different strokes, are still open problems.</p><p>The goal of this paper is to model a user’s haptic skills during the Chinese handwriting process and to develop a training system for studying motor skill learning. It is expected that the system can be used to train young Chinese pupils and non-Chinese students to learn handwriting. Such a system can also be used as a platform for studying human haptic skills.</p><p>The remainder of the paper is organized as follows. Section 2 describes a model of haptic skills and a three-level task planner. Section 3 presents the modeling of Chinese strokes and characters. Section 4 proposes a haptic rendering architecture and algorithm for collision detection and force modeling. Section 5 discusses an experiment using a force-feedback haptic device. Section 6 presents conclusions and future work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Task planning</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Architecture</h3><p>In a real-time force-feedback Chinese calligraphy simulation system, a task-planning model is the fundamental component to ensure realistic force interaction. The goal of task planning is to establish a representation model of human haptic skills and fidelity evaluation criteria through an analysis of human actions during a Chinese hand writing process, and furthermore to decompose the skills into meaningful control commands for a haptic device. As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig1">1</a> shows, a hierarchical task planning method is proposed to describe the hand writing process.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A hierarchical task plan method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig1">1</a>, the term task space refers to an abstract space for describing the handwriting process, and the term device space refers to the physical workspace of a haptic device.</p><p>At the task level, the haptic skills involved in handwriting are decomposed into perceptible elements, that is, independent element perceptible by human operator. The inputs to the task level are Chinese characters and the outputs are perceptible elements of the interaction process.</p><p>At the representation level, the perceptible elements are divided into interaction status to realize dynamic simulation and status switching control during the interaction process. The inputs are task language units and the outputs are interaction status series between a virtual pen’s tip and a virtual paper.</p><p>At the device level, the interaction status series are mapped into control signals in the device space that includes motion commands and control goals for the haptic device. Its inputs are interaction status series between a virtual pen’s tip and a virtual paper. The output is the control goals of the haptic device. The result supplies the planning goal for force rendering and device control.</p><p>The task and representation levels are independent of the haptic device. Haptic skills and the interaction status, are respectively represented by these two levels. The device level is dependent on the specific device to enable stable simulation on the device.</p><p>There are two advantages of using the three-level task planner. The first is that task planning is independent of the haptic device and extensible to different devices. The second is ease of debugging and control of the entire simulation, because the effect of each component on system performance can be tested independently and therefore, the overall system performance can be optimized.</p><h3 class="c-article__sub-heading" id="Sec4">Task level</h3><p>The task level focuses on the factors related to the fidelity of haptic simulation. Based on the characteristics of human haptic skills, a factor combination and status switching model is proposed to represent haptic skills of the handwriting process.</p><p>Standard Chinese character templates are predefined as the goal and the reference of haptic simulation. Force display is divided into the display of structural information and the display of sequences among strokes. Task planning and status series during handwriting are established based on the status switching method. Evaluation criteria are proposed for each perceptible element. The combined precision of all perceptible elements displayed on the haptic device is defined as the fidelity of the entire simulation system.</p><p>As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig2">2</a> shows, the perceptible elements during haptic simulation of Chinese calligraphy using a pen include a character contour element and a stroke sequence element.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Perceptible elements of haptic skill</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The character contour element refers to the force instructions along each stroke to reflect the trajectory of the stroke. As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig3">3</a> shows, a user can feel the shape of each stroke and the writing direction within each stroke; therefore, haptic memory of the character’s shape can be enhanced.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Force feedback of shape of a curve stroke</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The stroke sequence refers to the movement patterns made by the user while writing a character according to the sequence of different strokes for the character. Following the standard order of strokes for each Chinese character is an important feature of writing skill.</p><p>Two kinds of sequence should be classified. One belongs to the external sequence; i.e., the sequence ordering among different strokes. The other belongs to the internal sequence; i.e., the movement direction within a single stroke. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig4">4</a> shows the internal sequence by a horizontal stroke (the gray arrow). When the virtual pen is moving along the stroke from left to right, it is regarded as in the right direction. Otherwise, resistance force (the blue arrow) will be imposed on the user’s hand if the pen is moving from right to left (the red arrow). The internal sequence is realized by comparing the moving direction of the virtual pen with the predefined internal sequence stored in the model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Force feedback of internal sequence</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The simulation procedure of the perceptible elements simulation process is as follows:</p><ul class="u-list-style-none">
                    <li>
                      <p>Step 1: Input the Chinese character;</p>
                    </li>
                    <li>
                      <p>Step 2: Load the required character data, including contour and sequence data. Parametric points describing the shape of the character is stored in the contour data. Identity (ID) of each stroke and index of each feature points are stored in the sequence data;</p>
                    </li>
                    <li>
                      <p>Step 3: The standard character is displayed graphically and the haptic loop is started;</p>
                    </li>
                    <li>
                      <p>Step 4: The currently active stroke is decided according to collision detection between virtual pen’s tip and the virtual character. Feature point array of active stroke is loaded and the user is guided by computed force of this stroke. The next stroke is set to be active when the current stroke is finished and all other strokes are disabled. The aforementioned computation is repeated until all strokes in the character have been written.</p>
                    </li>
                  </ul>
                <h3 class="c-article__sub-heading" id="Sec5">Representation level</h3><p>The representation level discusses the interaction types between virtual pen’s tip point (VTP) and virtual paper’s surface, or between VTP and each stroke. The switching condition that leads to a change of the status will also be described at this level.</p><p>Action of virtual pen’s tip during writing of a single character includes moving the pen to contact the paper, moving the pen to retreat from the paper, writing on the paper, moving the pen’s tip with the help of visual feedback, moving the pen’s tip with the help of haptic feedback. Possible interaction status resulting from the actions includes virtual pen’s tip in contact or not in contact with the paper.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig5">5</a> shows the bounding box of a reference character and the neighboring area of the start point of the first stroke of the reference standard character. Because the goal of haptic simulation is to guide the user along the strokes of a standard Chinese character, only movements within the bounding box of a standard Chinese character are regarded to be valid writing attempts. Interaction status during the contact phase can be further classified according to the distance from the VTP to start point of the first stroke of the reference standard character.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Bounding box and neighbor area hint</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0012-4#Tab1">1</a> gives definition of all interaction states, where “system behavior” refers to expected result of haptic device and graphical device under each interaction status. State transition graph (STG) in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig6">6</a> is proposed to describe the real-time interaction status between virtual pen and the virtual paper. The STG can be used to describe information flow among different status and switch condition between these states.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Definition of all interaction state</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>State transfer graph of interaction state</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec6">Device level</h3><p>Device level discuss mapping of motion and force signal from task space to device space and focus of this level is to realize representation of interaction status and real-time haptic simulation. Two aspects are included in this level: (1) internal description of each status about motion and force series; (2) description of switch performance between every two status about stability and bandwidth of such switch.</p><p>Considering the impedance display characteristic of haptic device, virtual pen’s tip will deviate from haptic interface point when the haptic probe enters constraint space [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Adams RJ, Hannaford B (1999) “Stable Haptic interaction with virtual environments”. IEEE Trans Rob Autom 15:3" href="/article/10.1007/s10055-005-0012-4#ref-CR11" id="ref-link-section-d20069e702">11</a>]. In force rendering algorithm, mapping from haptic interface point to virtual pen’s tip should be established to transfer interaction status from device co-ordination system to virtual co-ordination system. Device’s behavior under each interaction status can be derived based on status of representation level.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0012-4#Tab2">2</a> gives characteristic of device’s output force and mapping of position signal. In order to describe concisely, HIP is used to represent haptic interface point and VTP is used to represent virtual pen’s tip.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Behavior of haptic device under each status</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Stability problem will occur on the switch point of two states such as contact or non-contact of pen’s tip with the virtual paper, writing from current stroke to next stroke, or writing from current character to next character, and writing on stroke with large curvature or at corner point of strokes, etc. Output force of haptic device will increase or decrease violently and will mismatch with human’s active force. Therefore, the device will manifest vibration. Special force rendering algorithm will be discussed in Sect. 4 to ensure stability and continuity during transform between strokes.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Stroke-based character modeling</h2><div class="c-article-section__content" id="Sec7-content"><p>Digital model of Chinese character is the fundamental component to simulate realistic and real-time handwriting process. Challenge of modeling is to meet requirement of fidelity and high force computation rate, and to meet adaptability for writing various characters.</p><p>It is well known that Chinese character is composed of strokes; naturally, modeling of character is transferred into modeling of strokes. Modeling method of Chinese character based on combination of sequential strokes and additional physical property is proposed in this paper. Within geometrical property subgroup, Chinese character is classified into combination of basic and complex strokes, while basic strokes is described by control points and complex strokes are modeled using sequential series of feature points. Within physical property subgroup, attraction factor is defined to reflect force magnitude difference on different control point.</p><h3 class="c-article__sub-heading" id="Sec8">Typical strokes library</h3><p>Chinese character is combined from sequence of strokes. In order to improve storage efficiency and maintain arbitrary character modeling requirement, 39 typical strokes are selected to form arbitrary Chinese characters and stroke structure is modeled using stroke ID and stack sequence. The typical strokes library is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig7">7</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>List of typical strokes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Each typical stroke is assigned a unique ID. For a specified typical stroke, the ID is constant no matter the variation of the size and no matter the location of the stroke within the character. The ID is corresponding to inherent structural characteristic of the stroke instead of its location. The ID will be used to quickly check out each stroke in proper sequence for collision detection and force rendering.</p><h3 class="c-article__sub-heading" id="Sec9">Stroke combination method</h3><p>In this system, graphical rendering program is developed based on OpenGL, and force rendering is based on GHOST SDK and Phantom desktop device. Memory cost will be a large burden if different models are used for graphical and force rendering respectively. Therefore, it is needed to propose a unified model for both graphical and force rendering.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig8">8</a> gives modeling hierarchical logic of character. Chinese character is modeled as sequential combination of typical strokes and each typical stroke can be modeled using combination of stroke elements. There are two kinds of stroke element: line segment stroke element (LSE) and curve segment stroke element (CSE). According to complexity level, each typical stroke can be classified into two groups, basic stroke or compound stroke. Each one is defined as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">(a)</span>
                      
                        <p>Basic stroke means LSE and CSE. The model of the former is a line segment, and the model of the latter is third order Bezier curve.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">(b)</span>
                      
                        <p>Complex stroke is described by combination of several basic strokes using geometric transformation.</p>
                      
                    </li>
                  </ol><p>Within geometrical property subgroup, modeling data of a character can be divided into two sets: structural data and sequence data in order to reflect the different property of the character. Structural data means relative co-ordinate of each control point within its parent character, and sequence data means sequence of control points within each character, which include index of each feature point and ID of each child stroke.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Modeling hierarchical logic of character</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Group of control points are used for model the structural data of each stroke, which also decide position, size, and orientation of each stroke within its parent character. All the strokes are defined to be constant width.</p><p>Feature points are defined to describe sequence among the control points within their parent stroke. Feature point is special control points, which connect basic strokes into a compound stroke. Therefore, all typical strokes can be modeled to form arbitrary Chinese character while maintaining compact data structure.</p><h3 class="c-article__sub-heading" id="Sec10">Modeling of basic stroke</h3><p>Basic strokes include points, horizontal stroke, vertical stroke, etc. According to shape difference between strokes, basic strokes can be classified into line segment and curve segment strokes.</p><p>Line segment stroke include points, horizontal stroke, vertical stroke, etc. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig9">9</a>a–c give model of these strokes. Control points are defined to describe geometric property of the strokes and force attraction factor is defined to describe physical property of the strokes. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig9">9</a>, control points are show using circle dot and red dot means that the control point is also a feature point.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Model of basic strokes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Curve segment stroke include left-hand and right-hand arc, etc. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig10">10</a>a–b give the model of these strokes. The third order Bezier curve is used to describe curve segment strokes in order to enable arbitrary curve strokes modeling requirement. Mathematical model is </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {P} {\left( u \right)} = {\sum\limits_{i = 0}^4 {\frac{{4!}} {{{\left( {4 - i} \right)}!i!}}} }{\left( {1 - u} \right)}^{{4 - i}} u^{i} \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{i} \quad u \in [0,1], $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {P} {\left( u \right)} \)</span> is co-ordinate of each point on the curve, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{i} \)</span> is control vertex of Bezier curve, and dotted line means the convex bounding polygon of the Bezier curve.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Model of basic strokes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">Modeling of compound stroke</h3><p>Compound strokes are modeled using combination of several stroke elements and feature points are defined to connect all the stroke elements. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig11">11</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig12">12</a> gives example of two compound strokes, respectively. Each one consist several basic strokes and is defined by sequence of feature points and control points. The first stroke is combined through two stroke elements and the second stroke is combined through five stroke elements.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Model of compound stroke (ID=16)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Model of compound stroke (ID=20)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>It can be found that feature points are some special points, which are usually locates at special places, such as the start point, the end point of a stroke and connection point between two basic strokes within a typical stroke. Because the location of feature point in its parent stroke is constant, feature points can be stored using its sequence index in the control points instead of their co-ordinate. The co-ordinate can be loaded through search in the control points co-ordinate information.</p><p>Feature point locates at the connection points between basic strokes, and the goal is to achieve two-order continuity between two strokes. Not only position of connection point but also derivative of two adjacent strokes is same. Therefore, the change of curvature and change of gradient do not have negative influence on system performance.</p><h3 class="c-article__sub-heading" id="Sec12">Sample of character</h3><p>Hierarchical stroke combination method is used to describe Chinese character. Model of character “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figa_HTML.gif" alt="" />
” is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig13">13</a>. Each feature point is marked by combination of ID of typical stroke with index of the feature point. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0012-4#Tab3">3</a> gives detailed data structure of this character.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Digital model of character “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figj_HTML.gif" alt="" />
”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Data structure of character “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figi_HTML.gif" alt="" />
”</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Haptic rendering</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Architecture</h3><p>Haptic rendering is a key element in haptic display systems, which directly decides fidelity and stability of the simulation. Strokes-sequence based rendering architecture shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig14">14</a> is proposed to enable haptic Chinese calligraphy training.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Architecture of calligraphy simulation system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Position of the virtual pen’s tip is computed according to mapping from position of haptic device. Collision detection is carried out according to relative position between virtual pen’s tip and virtual paper, which is used to find the currently activated stroke in current character. Line segment model and Bezier curve model is adopted to ensure rapid collide detection for straight and curve stroke, respectively. Virtual force can be computed using modified virtual fixture method and pre-defined force attraction factor at feature points along each stroke.</p><p>Challenge of Chinese calligraphy is detection of various interaction status and corresponding system control under different interaction between virtual pen’s tip and virtual paper and under switch point between these statuses. Furthermore, compound strokes exist in character and virtual pen will move in different area of the character and will be near to several strokes at same time at some stroke-clustered areas. Therefore, it is the responsibility of haptic rendering algorithm to maintain consistent result of collision detection within stroke clustering areas and at some corner points. All the above requirements propose challenge for system’s stability and fidelity.</p><p>Based on above requirement, simulation procedure for haptic rendering is proposed as Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig15">15</a> shows. In this architecture, there are several main components: motion mapping from device’s position to virtual pen’s tip, collision detection, force computation model and force rendering algorithm for ensuring stability.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Procedure for real-time simulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Stroke stack based force rendering architecture is proposed to ensure consistent result of collision detection. When the haptic device is moved by human operator, position of the virtual pen’s tip is computed by motion mapping algorithm. Collision detection is carried out according to position of virtual pen’s tip and decide the active stroke of active character, thus to ensure uniqueness of collision detection result within neighboring area of several clustered strokes or at corner point of compound strokes.</p><h3 class="c-article__sub-heading" id="Sec15">Motion mapping</h3><p>Two spaces are defined to describe motion mapping from human’s hand to virtual pen, which are device space and virtual space. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig16">16</a> gives signal mapping flow from human operator to virtual pen.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Signal mapping flow from human to virtual pen</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Motion scale <i>K</i>
                    <sub>S</sub> is defined to describe velocity transformation between two spaces. Velocity of human’s hand in device space is <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{h}}} , \)</span> and velocity of haptic interface point in virtual space is <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{{\text{HIP}}}} . \)</span> Transformation between these two velocities is defined: </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{{\text{HIP}}}} = K_{{\text{S}}} \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{h}}} . $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                <p>In this system, constant motion scale is adopted and the scale is set to be 1. Furthermore, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} \)</span> is position of haptic interface point (Avatar) and it can be computed by numerical integration: </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} = {\int {\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{{\text{HIP}}}} dt} } $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \)</span> is the position of virtual pen’s tip in virtual space and it is used to be graphically displayed for writing trajectory. Based on collision response algorithm, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \)</span> can be decided according to dynamic response computation in free space and constraint space: </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} = \left\{ \begin{aligned}{} &amp; \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} \notin R_{{{\text{res}}}} \\ &amp; g_{{{\text{CR}}}} {\left( {\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} } \right)}\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} \in R_{{{\text{res}}}} \\ \end{aligned} \right., $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p> where 
<span class="mathjax-tex">\( g_{{{\text{CR}}}} {\left( \bullet \right)} \)</span> is collision response function that transform from position of HIP to position of VTP. <i>R</i>
                    <sub>res</sub> means constraint space, which will be defined in Sect. 4.3 according to difference strokes. In this system, following simplification is adopted: </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \equiv \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{{\text{HIP}}}} . $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                <h3 class="c-article__sub-heading" id="Sec16">Collision detection</h3><p>Feature points are used to maintain smooth transition between different basic strokes during handwriting of a character. The index of the feature points in current active stroke is stored in an array before simulation. The co-ordinate of each feature point is loaded through its index during the writing process. In each stroke, first feature point is always the start point of this stroke and its child basic stroke.</p><p>As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig17">17</a> shows, feature plane is defined at each stroke’s ending feature point, which is always perpendicular to current basic stroke. Real-time collision detection is computed according to relative position between the virtual pen’s tip and each feature plane. When a feature plane is popped through, it means that the writing on first basic stroke finished and writing on second stroke begins. As a result, the control points of the second basic stroke will be loaded into the memory and collision detection will be carried out between the pen and the second basic stroke. The above process will be repeated until the virtual pen’s tip stepped over the last feature plane and reached the last feature point, and the stack for feature point’s array will be cleared. Handwriting on one compound stroke can be simulated using above process.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Location of feature plane</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>If virtual pen’s tip retreats from virtual paper’s surface during writing state, it means operator attempts to give up writing of current stroke. Therefore, all feature points of this stroke will be loaded into the stack again. The graphical trajectory of the unfinished stroke will be cleared and the operator can re-write the stroke.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Collision detection against LSE</h4><p>When the virtual pen’s tip contact with line segment stroke, difference between operator’s real-time trajectory and referenced standard character’s trajectory is computed based on collision detection algorithm.</p><p>The first step is to compute the perpendicular foot of virtual pen’s tip projected on the linear segment. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig18">18</a> shows the relative position of HIP and linear segment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Relative positions of HIP and linear segment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Because each line segment stroke is limited length, valid collision detection area should be defined to avoid force feedback when writing on adjoining strokes. For every line segment stroke, constraint space is defined in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig19">19</a>. where <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {P} _{{\text{h}}} \)</span> is possible position of virtual pen’s tip, <span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> is vector from start point of the stroke to the position of virtual pen’s tip, 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{2} \)</span> is vector from end point of the stroke to the position of virtual pen’s tip, 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} \)</span> is vector from start point of the stroke to the end point of the stroke.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig19_HTML.gif?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig19_HTML.gif" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Constraint space definition</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Constraint space is computed according to following procedures:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(a)</span>
                        
                          <p>If dot product of <span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> and <span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} \)</span> is less than zero, it means HIP is not within constraint space;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(b)</span>
                        
                          <p>If dot product of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> and 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} \)</span> is bigger than 0, but norm of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> is bigger than norm of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} , \)</span> it means HIP is not within constraint space;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(c)</span>
                        
                          <p>If dot product of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> and 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} \)</span> is bigger than 0, and norm of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{1} \)</span> is less than norm of 
<span class="mathjax-tex">\( \ifmmode\expandafter\vec\else\expandafter\vec\fi{P}_{{{\text{Edge}}}} , \)</span> it means HIP is within constraint space;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(d)</span>
                        
                          <p>If start point of the stroke is the same with the end point of the stroke, it means HIP is within constraint space.</p>
                        
                      </li>
                    </ol><p>Only when the virtual pen’s tip is within the constraint space, force feedback for the stroke will be enabled. Otherwise, it is regarded as the virtual pen’s tip is within the free space.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Collision detection against CSE</h4><p>Spherical neighboring area around the start point of each stroke is set as valid area of collision detection. Nearest point on the Bezier curve from the virtual pen’s tip is searched by the collision detection. Force feedback is computed based on distance between the solution point and the virtual pen’s tip.</p><p>Based on subdivision algorithm of Bezier curve shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig20">20</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig21">21</a>, middle point of Bezier curve is computed by first division and two child segments can be produced. Middle point of each child segment can be got and distances from HIP to the three points on each child segment are compared. The child segment that owns smallest distance will be selected as new parent segment. Above subdivision is repeated until the subtraction of the distances from HIP to middle point of each child segment is less than <i>ε</i>. Collision detection is finished and middle point of the segment that connects final two middle points is set as the solution point.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig20_HTML.gif?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig20_HTML.gif" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>Definition of control points</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig21_HTML.gif?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig21_HTML.gif" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Subdivision algorithm of Bezier curve</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Collision detection against compound stroke</h4><p>For compound strokes, collision detection needs to consider relative position between virtual pen’s tip and strokes with various shapes. Furthermore, collision detection ambiguity needs to be avoided within stroke-clustered area and at switch point between two adjoining strokes while maintaining high update rate requirement.</p><p>Compound stroke is described as combination of line and curve segment stroke and feature point is used to connect basic strokes. During real-time writing, relative position between virtual pen’s tip and feature point need to be computed. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig22">22</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig23">23</a> gives collision detection against feature plane on LSE and CSE, respectively. Feature plane is defined to decide the interaction result and mathematical model is: </p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \left\{ \begin{aligned}{} &amp; \phi _{i} &gt; 0\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{t}}} \bullet \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{f}}} &gt; 0 \\ &amp; \phi _{i} &lt; 0\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{t}}} \bullet \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{f}}} \le 0 \\ \end{aligned} \right., $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p> where 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{t}}} \)</span> is tangential vector of the stroke at current feature point, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {V} _{{\text{f}}} \)</span> is vector from current feature point to virtual pen’s tip. <i>φ</i>
                        <sub>
                      <i>i</i>
                    </sub> means whether current feature point has been written or not and <i>φ</i>
                        <sub>
                      <i>i</i>
                    </sub>&gt;0 means current feature point has been written already.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-22"><figure><figcaption><b id="Fig22" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 22</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/22" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig22_HTML.gif?as=webp"></source><img aria-describedby="figure-22-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig22_HTML.gif" alt="figure22" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-22-desc"><p>Feature plane detection against line segment stroke element (<i>LSE</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/22" data-track-dest="link:Figure22 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-23"><figure><figcaption><b id="Fig23" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 23</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/23" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig23_HTML.gif?as=webp"></source><img aria-describedby="figure-23-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig23_HTML.gif" alt="figure23" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-23-desc"><p>Feature plane detection against curve segment stroke element (<i>CSE</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/23" data-track-dest="link:Figure23 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    </div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Force model</h2><div class="c-article-section__content" id="Sec20-content"><p>In order to achieve realistic force simulation, force model during handwriting process need to be analyzed. All the active and resistance forces on a pen are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig24">24</a>, where human’s force are divided into tangential and normal components, i. e. <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{\text{t}}} \)</span> and 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{\text{n}}} . \)</span> Normal resistance force 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{nr}}}} \)</span> and frictional forces 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {f} \)</span> between pen and paper is passive force. In our system, new virtual force within the paper’s surface is added to constraint operator from deviation from the reference character template. Furthermore, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {f} \)</span> is ignored to simplify force-rendering algorithm. Therefore, feedback force in simulation system can be defined: </p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{\text{e}}} = \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{en}}}} + \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{et}}}} , $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p> where 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{\text{e}}} \)</span> is total feedback force, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{et}}}} \)</span> is force within the paper’s surface, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{en}}}} \)</span> is force perpendicular to the paper’s surface and 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{en}}}} \)</span> equals to 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{nr}}}} . \)</span>
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-24"><figure><figcaption><b id="Fig24" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 24</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/24" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig24_HTML.gif?as=webp"></source><img aria-describedby="figure-24-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig24_HTML.gif" alt="figure24" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-24-desc"><p>Active and resistance forces on a pen</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/24" data-track-dest="link:Figure24 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>Feedback forces are computed based on result of real-time collision detection and typical virtual fixture force model is adopted [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="SagaS, Kawakami N, Tachi S (2005) Haptic teaching using opposite force presentation. In: Proceeding of the 1st World Haptics conference, Pisa, Italy, pp 18–20" href="/article/10.1007/s10055-005-0012-4#ref-CR7" id="ref-link-section-d20069e2079">7</a>]. However, modification has been made to improve the linear spring force model. For the constraint force within paper’s surface, different force attraction factors can be set at different feature point to reflect characteristic of each stroke as follows: </p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{et}}}} = \left\{ {\begin{array}{*{20}c} {{K_{{{\text{et}}}} (\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} - \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{S}}} )}} &amp; {{\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \in R_{{{\text{res}}}} {\text{ }}}} \\ {0} &amp; {{\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \notin R_{{{\text{res}}}} }} \\ \end{array} } \right., $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p> where <i>K</i>
                <sub>et</sub> is force attraction factor at current feature point, <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{p}}} \)</span> is position of virtual pen’s tip, 
<span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {X} _{{\text{S}}} \)</span> is perpendicular projection of virtual pen’s tip on current active stroke, <i>R</i>
                <sub>res</sub> means constraint space, which is defined in Sect. 4.3 according to difference strokes.</p><p>Force perpendicular to the paper’s surface can be computed: </p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} _{{{\text{en}}}} = \left\{ {\begin{array}{*{20}c} {{K_{{{\text{en}}}} (z_{{\text{P}}} - z_{0} )}} &amp; {{z_{{\text{P}}} \ge z_{0} {\text{ }}}} \\ {0} &amp; {{z_{{\text{P}}} &lt; z_{0} }} \\ \end{array} } \right., $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p> where <i>K</i>
                <sub>en</sub> is stiffness of the virtual paper’s plane, <i>z</i>
                <sub>p</sub> is position of device’s avatar along normal of the paper’s plane, <i>z</i>
                <sub>0</sub> is position of virtual paper along Z-axis.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Stability issues</h2><div class="c-article-section__content" id="Sec21-content"><p>According to spring based force model, instability will arise because of two possible cases, initial distance from pen’s tip penetrating neighboring circle of stroke is too big, and big stiffness in the force model.</p><p>Furthermore, unsuitable character model will lead to ambiguity of virtual force during collision detection process. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig25">25</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig26">26</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig27">27</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig28">28</a> gives several possible cases of force ambiguity. In Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig25">25</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig26">26</a>, multiple minimum distances will arise when virtual pen’s tip moves to neighboring area of big curvature strokes or stroke’s corner. Furthermore, in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig27">27</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig28">28</a>, multiple minimum distances will also arise when virtual pen’s tip moves to neighboring area of two closely located strokes. Multiple minimum distances will lead to multiple force result, therefore lead to instability.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-25"><figure><figcaption><b id="Fig25" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 25</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/25" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig25_HTML.gif?as=webp"></source><img aria-describedby="figure-25-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig25_HTML.gif" alt="figure25" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-25-desc"><p>Minimum distances on big curvature strokes or stroke’s corner</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/25" data-track-dest="link:Figure25 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-26"><figure><figcaption><b id="Fig26" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 26</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/26" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig26_HTML.gif?as=webp"></source><img aria-describedby="figure-26-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig26_HTML.gif" alt="figure26" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-26-desc"><p>Minimum distances at stroke’s corner</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/26" data-track-dest="link:Figure26 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-27"><figure><figcaption><b id="Fig27" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 27</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/27" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig27_HTML.gif?as=webp"></source><img aria-describedby="figure-27-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig27_HTML.gif" alt="figure27" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-27-desc"><p>Minimum distances at parallel strokes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/27" data-track-dest="link:Figure27 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-28"><figure><figcaption><b id="Fig28" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 28</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/28" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig28_HTML.gif?as=webp"></source><img aria-describedby="figure-28-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig28_HTML.gif" alt="figure28" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-28-desc"><p>Minimum distances at crossed strokes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/28" data-track-dest="link:Figure28 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>Finally, transition among different interaction status will also lead to instability of haptic device. Such transition includes: pen’s tip contact or separate from the paper’s plane, and transition from writing along one stroke to next stroke, transition from writing along one character to next character, etc.</p><p>For the above three kinds of instability, control methods are proposed, respectively.</p><p>For the first kind of force magnitude step change, constraint space of each stroke is proposed to maintain the step change of force magnitude within allowed limit.</p><p>For the second kind of instability, i.e., the force ambiguity problem, strokes element and feature point are proposed to determine active stroke during collision detection between pen’s tip and current character. Active stroke element is determined through collision detection to avoid force ambiguity.</p><p>For the third kind of instability, i.e., status transition problem, active character and active stroke are proposed to ensure smooth force signal between different statuses. All other characters will be masked when active character is been written with. Furthermore, the written strokes cannot be written twice to ensure force result consistency.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Experiment</h2><div class="c-article-section__content" id="Sec22-content"><h3 class="c-article__sub-heading" id="Sec23">Configuration</h3><p>There are two goals for the experiment. The first goal is to validate stability and fidelity of simulation architecture, task planning method, collision detection and haptic rendering algorithm under various strokes. The second goal is to compare the handwriting effect with or without haptic feedback. Effect on human’s motor skill will not be studied in this paper, which will be carried out on large number of participants in our future work.</p><p>Experiment platform is constructed using Phantom desktop<sup>®</sup>, and Chinese calligraphy simulation software is developed using the configuration in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0012-4#Tab4">4</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig29">29</a> gives the scenario of handwriting operation. Wrist of the operator is required to be fixed on the desk and pen-grasp is required during writing process.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Hardware and software configuration</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-29"><figure><figcaption><b id="Fig29" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 29</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/29" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig29_HTML.jpg?as=webp"></source><img aria-describedby="figure-29-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig29_HTML.jpg" alt="figure29" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-29-desc"><p>Scenario of handwriting operation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/29" data-track-dest="link:Figure29 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Experiments are designed based on two modes: vision feedback only, vision feedback with haptic feedback. Five native (Skilled and unskilled) students and one foreign student are invited to write using the system.</p><p>Before formal trial and evaluation, participants are allowed to write a specified character for a few times to get familiar with the system. When formal trials begin, new characters different from the learning character are loaded into the system. Participants are required to write each character for several times and average score will be computed.</p><h3 class="c-article__sub-heading" id="Sec24">Result</h3><p>As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig30">30</a> shows, specified Chinese character group—“
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figb_HTML.gif" alt="" />
” is selected to validate system performance. Trajectory of reference template is displayed in blue color. Current active character is displayed in yellow color and actual trajectory result of the virtual pen is displayed in light green color. Graphical display of the writing result is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig31">31</a>, where error of actual trajectory and referenced trajectory can be seen clearly. In addition, a red ellipse is drawn at beginning control point of current active stroke to give a hint for the operator.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-30"><figure><figcaption><b id="Fig30" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 30</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/30" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig30_HTML.gif?as=webp"></source><img aria-describedby="figure-30-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig30_HTML.gif" alt="figure30" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-30-desc"><p>Specified Chinese character group</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/30" data-track-dest="link:Figure30 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-31"><figure><figcaption><b id="Fig31" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 31</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/31" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig31_HTML.gif?as=webp"></source><img aria-describedby="figure-31-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig31_HTML.gif" alt="figure31" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-31-desc"><p>Error of actual trajectory and referenced trajectory</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/31" data-track-dest="link:Figure31 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Force signals within the virtual paper plane during writing on basic stroke element are recorded to analyze stability of the system. The plane of virtual paper is defined to be XOY plane. As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig31">31</a> shows, there are line and curve shaped stroke element within character “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figc_HTML.gif" alt="" />
”. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig32">32</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig33">33</a> gives force signal of stroke “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figd_HTML.gif" alt="" />
” and “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fige_HTML.gif" alt="" />
”, respectively. Collision detection is proved to be rapid enough to deal with interaction status computation within 1 ms. Effectiveness of modified virtual fixture force model to train handwriting is also validated.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-32"><figure><figcaption><b id="Fig32" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 32</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/32" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig32_HTML.gif?as=webp"></source><img aria-describedby="figure-32-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig32_HTML.gif" alt="figure32" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-32-desc"><p>Force signal of stroke “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figk_HTML.gif" alt="" />
”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/32" data-track-dest="link:Figure32 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-33"><figure><figcaption><b id="Fig33" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 33</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/33" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig33_HTML.gif?as=webp"></source><img aria-describedby="figure-33-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig33_HTML.gif" alt="figure33" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-33-desc"><p>Force signal of stroke “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figl_HTML.gif" alt="" />
<b>”</b>
                                </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/33" data-track-dest="link:Figure33 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Force signals within the virtual paper plane during writing on compound stroke are recorded to test the function of feature point in haptic rendering algorithm. As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig31">31</a> shows, there is one compound stroke “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figf_HTML.gif" alt="" />
” within character “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figg_HTML.gif" alt="" />
”, where four feature points exist in this stroke. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig34">34</a> gives force signal of writing process on the compound stroke. Ambiguity of collision detection could be avoided by using active stroke method. Stability can be ensured when feature point is passed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-34"><figure><figcaption><b id="Fig34" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 34</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/34" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig34_HTML.gif?as=webp"></source><img aria-describedby="figure-34-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig34_HTML.gif" alt="figure34" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-34-desc"><p>Force signal of compound stroke</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/34" data-track-dest="link:Figure34 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig35">35</a> gives writing result on a single character “
<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figh_HTML.gif" alt="" />
”. There are five strokes in the character, which will lead to five times status transition between pen and the virtual paper. No matter the pen contact or separate from the virtual paper, or the pen transit from a stroke to next stroke, the haptic device always works smoothly. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig36">36</a> is detailed view of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig35">35</a> to display force within paper’s plane clearly. Stability is ensured at interaction status switch time during writing process. Writing experiments on multiple characters are also been carried out and stability can be maintained when avatar of haptic device change among characters.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-35"><figure><figcaption><b id="Fig35" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 35</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/35" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig35_HTML.gif?as=webp"></source><img aria-describedby="figure-35-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig35_HTML.gif" alt="figure35" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-35-desc"><p>Writing result on a single characters “<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Figm_HTML.gif" alt="" />
”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/35" data-track-dest="link:Figure35 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-36"><figure><figcaption><b id="Fig36" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 36</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/36" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig36_HTML.gif?as=webp"></source><img aria-describedby="figure-36-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig36_HTML.gif" alt="figure36" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-36-desc"><p>Detailed view of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig35">35</a> </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/36" data-track-dest="link:Figure36 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig37">37</a> give results of writing without haptic feedback and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0012-4#Fig38">38</a> give result of writing with haptic feedback. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0012-4#Tab5">5</a> gives the time difference of four characters between two modes. The unit of the time is second. It can be seen clearly that the trajectory error is reduced with help of force feedback. The same results can be observed on other Chinese characters.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-37"><figure><figcaption><b id="Fig37" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 37</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/37" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig37_HTML.gif?as=webp"></source><img aria-describedby="figure-37-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig37_HTML.gif" alt="figure37" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-37-desc"><p>Result of writing without haptic feedback</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/37" data-track-dest="link:Figure37 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-38"><figure><figcaption><b id="Fig38" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 38</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/38" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig38_HTML.gif?as=webp"></source><img aria-describedby="figure-38-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0012-4/MediaObjects/10055_2005_12_Fig38_HTML.gif" alt="figure38" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-38-desc"><p>Result of writing with haptic feedback</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0012-4/figures/38" data-track-dest="link:Figure38 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Time difference between two modes</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec25">Discussion</h3><p>Experiment results preliminarily indicate validity of the proposed task planning, collision detection and haptic rendering method. Writing on typical character illustrates validity of the model to reflect force display of calligraphy of character shape and strokes sequence while maintaining small storage memory and high computation rate.</p><p>Experiment on typical character illustrates validity of the rendering algorithm to achieve expected collision detection result within neighboring domain of strokes clusters and corner points of complex strokes, and stability can be maintained even when frequent switch occur between the virtual pen’s tip and the virtual paper, or between the virtual pen’s tip and the strokes.</p><p>Advantage of haptic feedback is apparent according to experiment results. Written time can be reduced for each character with help of haptic feedback. The time difference is especially apparent for foreign student.</p><p>From the experiment, some limitations are also found. Friction force between virtual pen and virtual paper should be added to prevent slippery sensation. Furthermore, haptic rendering algorithm cannot cope with larger deviations, which might be generated by a less skilled novice. Under this case, magnitude of guided force within the virtual paper’s plane need be enlarged, which is now limited by stiffness threshold of Phantom desktop. It is necessary to discuss new algorithm to enlarge force magnitude while maintaining stability.</p></div></div></section><section aria-labelledby="Sec26"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Conclusion</h2><div class="c-article-section__content" id="Sec26-content"><p>A prototype of haptic training system for Chinese calligraphy is developed, which can be used as a platform for studying human haptic skill representation and skill transfer.</p><p>Chinese characters are modeled as a sequential array of typical strokes consisting of compound and basic strokes elements. In the process of task planning, the haptic skill is decomposed as perceptible elements at the first level and further transformed into control signals of the haptic device in the following two levels.</p><p>In the haptic rendering, collision detection algorithm based on nearest point is implemented successfully for various strokes and the modified virtual fixture method is introduced to obtain high stability. This combines the strategy of activating the stroke under writing makes a stable haptic simulation.</p><p>Stability of the rendering algorithm operating on different characters and strokes is validated by experiment. Writing simulation with/without force feedback has been compared and the results indicate using haptic feedback in Chinese calligraphy training system is helpful to reduce writing error and to improve writing speed.</p><p>Our future work will investigate how to evaluate training effect of the system quantitatively. Large number of human subjects will be invited to use the system and their writing information will be recorded and analyzed using statistical methods.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Solis J, Avizzano CA, Bergamasco M (2002) Teaching to write Japanese characters using a haptic interface. In: " /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Solis J, Avizzano CA, Bergamasco M (2002) Teaching to write Japanese characters using a haptic interface. In: Proceedings of the 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 255–262</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Henmi K, Yoshikawa T (1998) Virtual lesson and its application to virtual calligraphy system. In: Proceedings " /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Henmi K, Yoshikawa T (1998) Virtual lesson and its application to virtual calligraphy system. In: Proceedings of the 1998 IEEE international conference on robotics and automation. 2:1275–1280</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Teo CL, Burdet E, Lim HP (2002) A robotic teacher of chinese handwriting. In: Proceedings of 10th internationa" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Teo CL, Burdet E, Lim HP (2002) A robotic teacher of chinese handwriting. In: Proceedings of 10th international symposium on Haptic interfaces for virtual environment and teleoperator systems, HAPTICS, pp 335–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang H-M, Lu J-J, Lee H-J (2001) A bezier curve-based approach to shape description for chinese calligraphy ch" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Yang H-M, Lu J-J, Lee H-J (2001) A bezier curve-based approach to shape description for chinese calligraphy characters. In: Proceedings of the 6th international conference on document analysis and recognition, pp 276–280</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chu NS-H, Tai C-L (2002) An efficient brush model for physically-based 3D painting. In: Proceedings of the 10t" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Chu NS-H, Tai C-L (2002) An efficient brush model for physically-based 3D painting. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 413–421</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yeh J-S, Lien T-Y, Ouhyoung M (2002) On the effects of haptic display in brush and ink simulation for Chinese " /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Yeh J-S, Lien T-Y, Ouhyoung M (2002) On the effects of haptic display in brush and ink simulation for Chinese painting and calligraphy. In: Proceedings of the 10th Pacific conference on computer graphics and applications, pp 439–441</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SagaS, Kawakami N, Tachi S (2005) Haptic teaching using opposite force presentation. In: Proceeding of the 1st" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">SagaS, Kawakami N, Tachi S (2005) Haptic teaching using opposite force presentation. In: Proceeding of the 1st World Haptics conference, Pisa, Italy, pp 18–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hennion B, Gentaz E, Gouagout P, Bara F (2005) Telemaque, a new visuo-haptic interface for remediation of dysg" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Hennion B, Gentaz E, Gouagout P, Bara F (2005) Telemaque, a new visuo-haptic interface for remediation of dysgraphic children. In: Proceedings of the 1st World Haptics conference, Pisa, Italy, pp 18–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sakuma M et al (1999) System for Japanese calligraphy lesson with force feedback. In: Proceedings of the 8th a" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Sakuma M et al (1999) System for Japanese calligraphy lesson with force feedback. In: Proceedings of the 8th annual symposium on Haptic interfaces for virtual environment and teleoperator systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Srinivasan, C. Basdogan, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Srinivasan MA, Basdogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. C" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Srinivasan MA, Basdogan C (1997) Haptics in virtual environments: taxonomy, research status, and challenges. Comput Graph 21(4):393–404</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2897%2900030-7" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptics%20in%20virtual%20environments%3A%20taxonomy%2C%20research%20status%2C%20and%20challenges&amp;journal=Comput%20Graph&amp;volume=21&amp;issue=4&amp;pages=393-404&amp;publication_year=1997&amp;author=Srinivasan%2CMA&amp;author=Basdogan%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RJ. Adams, B. Hannaford, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Adams RJ, Hannaford B (1999) “Stable Haptic interaction with virtual environments”. IEEE Trans Rob Autom 15:3" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Adams RJ, Hannaford B (1999) “Stable Haptic interaction with virtual environments”. IEEE Trans Rob Autom 15:3</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.744598" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CStable%20Haptic%20interaction%20with%20virtual%20environments%E2%80%9D&amp;journal=IEEE%20Trans%20Rob%20Autom&amp;volume=15&amp;publication_year=1999&amp;author=Adams%2CRJ&amp;author=Hannaford%2CB">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0012-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work is supported by the Science Foundation of China Post-doctoral Research and the National Science Foundation of China under the grant No. 50275003. Their support is greatly appreciated. Authors would also like to thank Professor Hong Z. Tan from Purdue Univ. for her proof reading and suggestions during revision phase of the paper.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Robotics Institute, Beihang University, 100083, Beijing, People’s Republic of China</p><p class="c-article-author-affiliation__authors-list">Daniel Wang, Yuru Zhang &amp; Chong Yao</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Daniel-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">Daniel Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daniel+Wang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniel+Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniel+Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Yuru-Zhang"><span class="c-article-authors-search__title u-h3 js-search-name">Yuru Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yuru+Zhang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yuru+Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yuru+Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Chong-Yao"><span class="c-article-authors-search__title u-h3 js-search-name">Chong Yao</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Chong+Yao&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chong+Yao" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chong+Yao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0012-4/email/correspondent/c1/new">Daniel Wang</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Stroke-based%20modeling%20and%20haptic%20skill%20display%20for%20Chinese%20calligraphy%20simulation%20system&amp;author=Daniel%20Wang%20et%20al&amp;contentID=10.1007%2Fs10055-005-0012-4&amp;publication=1359-4338&amp;publicationDate=2006-01-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Wang, D., Zhang, Y. &amp; Yao, C. Stroke-based modeling and haptic skill display for Chinese calligraphy simulation system.
                    <i>Virtual Reality</i> <b>9, </b>118–132 (2006). https://doi.org/10.1007/s10055-005-0012-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0012-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-29">29 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-10-07">07 October 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-01-11">11 January 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03">March 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0012-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0012-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Feature Point</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Chinese Character</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collision Detection</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic Feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic Device</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0012-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=12;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

