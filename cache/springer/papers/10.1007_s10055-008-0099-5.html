<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Learning medicine through collaboration and action: collaborative, exp"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The SUMMIT Lab and William LeRoy Heinrichs, at Stanford University, were honored to be the 2002 awardees of the Satava Award for Virtual Reality in Medicine. Since the award, the group has followed..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/12/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Learning medicine through collaboration and action: collaborative, experiential, networked learning environments"/>

    <meta name="dc.source" content="Virtual Reality 2008 12:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2008-10-28"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The SUMMIT Lab and William LeRoy Heinrichs, at Stanford University, were honored to be the 2002 awardees of the Satava Award for Virtual Reality in Medicine. Since the award, the group has followed two main threads of research, which we describe below. The first, &#8220;building a high-performance, network-aware, collaborative learning environment&#8221; has investigated the framework and components needed when students in multiple locations collaborate using computation-intensive simulations and large image datasets. The second thread, &#8220;online, interactive human physiology for medical education and training&#8221;, has focused on the application of interactive physiology models embedded in 3D visualizations of virtual patients in naturalistic medical environments. These environments support immersive, experiential learning where students act as medical providers and manage authentic medical events and crises. These research efforts, and our conclusions, are presented in the chapter below."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2008-10-28"/>

    <meta name="prism.volume" content="12"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="215"/>

    <meta name="prism.endingPage" content="234"/>

    <meta name="prism.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-008-0099-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-008-0099-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-008-0099-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-008-0099-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Learning medicine through collaboration and action: collaborative, experiential, networked learning environments"/>

    <meta name="citation_volume" content="12"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2008/12"/>

    <meta name="citation_online_date" content="2008/10/28"/>

    <meta name="citation_firstpage" content="215"/>

    <meta name="citation_lastpage" content="234"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-008-0099-5"/>

    <meta name="DOI" content="10.1007/s10055-008-0099-5"/>

    <meta name="citation_doi" content="10.1007/s10055-008-0099-5"/>

    <meta name="description" content="The SUMMIT Lab and William LeRoy Heinrichs, at Stanford University, were honored to be the 2002 awardees of the Satava Award for Virtual Reality in Medicin"/>

    <meta name="dc.creator" content="Parvati Dev"/>

    <meta name="dc.creator" content="Wm. LeRoy Heinrichs"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Teach Learn Med; citation_title=Growing use of standardized patients in teaching and evaluation in medical education; citation_author=MB Anderson, PL Stillman, Y Wang; citation_volume=6; citation_issue=1; citation_publication_date=1994; citation_pages=15-22; citation_doi=10.1080/10401339409539637; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Educ; citation_title=Interactive simulated patient&#8212;an advanced tool for student-activated learning in medicine and healthcare; citation_author=RA Bergin, UGH Fors; citation_volume=40; citation_publication_date=2003; citation_pages=361-76; citation_doi=10.1016/S0360-1315(02)00167-7; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Clin Anat; citation_title=Collaborative learning using Internet2 and remote collections of stereo dissection images; citation_author=P Dev, S Srivastava, S Senger; citation_volume=19; citation_issue=3; citation_publication_date=2006; citation_pages=275-283; citation_doi=10.1002/ca.20313; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_title=Virtual patient model for multi-person virtual medical environments; citation_publication_date=2007; citation_id=CR4; citation_author=P Dev; citation_author=WL Heinrichs; citation_author=P Youngblood; citation_author=S Kung; citation_author=R Cheng; citation_author=L Kusumoto; citation_author=A Hendrick; citation_publisher=AMIA"/>

    <meta name="citation_reference" content="citation_journal_title=Anesthesiol Clin; citation_title=Virtual worlds &amp; team training; citation_author=P Dev, P Youngblood, WL Heinrichs, L Kusumoto; citation_volume=25; citation_publication_date=2007; citation_pages=321-336; citation_doi=10.1016/j.anclin.2007.03.001; citation_id=CR5"/>

    <meta name="citation_reference" content="Ellaway R (2005) Modeling virtual patients and virtual cases. 
                    http://meld.medbiq.org/primers/virtual_patients_cases_ellaway.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Med Educ; citation_title=An evaluation of the effectiveness of a computer simulation of anaesthetic uptake and distribution as a teaching tool; citation_author=JM Garfield, S Paskin, JH Philip; citation_volume=23; citation_publication_date=1989; citation_pages=457-462; citation_doi=10.1111/j.1365-2923.1989.tb00902.x; citation_id=CR7"/>

    <meta name="citation_reference" content="Heinrichs WL, Kung S-Y, Dev P (2008a) Design and implementation of rule-based medical models: an in silico patho-physiological trauma model for hypovolemic shock. In: Proceedings, MMVR 2008, January 2008, IOS Press, Long Beach, pp 159&#8211;164"/>

    <meta name="citation_reference" content="Heinrichs WL, Youngblood P, Harter PM, Dev P (2008b) Simulation for team training and assessment: case studies of online training with virtual worlds. World J Surg Spec Issue 32:161&#8211;170. 
                    http://www.springerlink.com/content/v86175717562/?p=d92acbf4aabf4fea9187acdb5cc053b3&amp;pi
                    
                  = 4"/>

    <meta name="citation_reference" content="
                  
                    http://labyrinth.mvm.ed.ac.uk/
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://research.bidmc.harvard.edu/VPTutorials/
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://tusk.tufts.edu/view/url/H1185C/471802/490012/
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://websp.lime.ki.se/about
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.aamc.org/mededportal
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.acssurgery.com/abstracts/acs/acs0104.htm
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.entelos.com/virtualPatients.php
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;hl=en&amp;q=human+physiology&amp;btnG=Google+Search
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.netmedicine.com/cyberpt/cyberptframe.htm
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.tinkering.net/vp/
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.virtualpatients.net/
                    
                  
                "/>

    <meta name="citation_reference" content="
                  
                    http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Anesth Analg; citation_title=The effect of graded hemorrhage and intravascular volume replacement on systolic pressure variation in humans during mechanical and spontaneous ventilation; citation_author=A Rooke, H Schwid, Y Shapira; citation_volume=80; citation_publication_date=1995; citation_pages=925-932; citation_doi=10.1097/00000539-199505000-00012; citation_id=CR22"/>

    <meta name="citation_reference" content="SBIR FY04.3 A04-182 Phases II &amp; III Contract W81XWH-05-C-0040"/>

    <meta name="citation_reference" content="citation_journal_title=Am J Pharm Educ; citation_title=Learning motivational interviewing: scripting a virtual patient; citation_author=WA Villaume, BA Berger, BN Barker; citation_volume=70; citation_issue=2; citation_publication_date=2006; citation_pages=33; citation_id=CR24"/>

    <meta name="citation_reference" content="Youngblood P, Harter P, Srivastava S, Wallin C-J, Fellander-Tsai L, Moffet S, Heinrichs WL (2008) Design, development and evaluation of an online virtual emergency department for training trauma teams. J Sim Med (in press)"/>

    <meta name="citation_author" content="Parvati Dev"/>

    <meta name="citation_author_email" content="parvati@parvatidev.org"/>

    <meta name="citation_author_institution" content="Innovation in Learning, Inc, Los Altos Hills, USA"/>

    <meta name="citation_author" content="Wm. LeRoy Heinrichs"/>

    <meta name="citation_author_institution" content="Innovation in Learning, Inc, Los Altos Hills, USA"/>

    <meta name="citation_author_institution" content="SUMMIT and Department of Obstetrics/Gynaecology, Stanford University School of Medicine, Stanford, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-008-0099-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2008/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-008-0099-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Learning medicine through collaboration and action: collaborative, experiential, networked learning environments"/>
        <meta property="og:description" content="The SUMMIT Lab and William LeRoy Heinrichs, at Stanford University, were honored to be the 2002 awardees of the Satava Award for Virtual Reality in Medicine. Since the award, the group has followed two main threads of research, which we describe below. The first, “building a high-performance, network-aware, collaborative learning environment” has investigated the framework and components needed when students in multiple locations collaborate using computation-intensive simulations and large image datasets. The second thread, “online, interactive human physiology for medical education and training”, has focused on the application of interactive physiology models embedded in 3D visualizations of virtual patients in naturalistic medical environments. These environments support immersive, experiential learning where students act as medical providers and manage authentic medical events and crises. These research efforts, and our conclusions, are presented in the chapter below."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Learning medicine through collaboration and action: collaborative, experiential, networked learning environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-008-0099-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Collaborative learning, Human anatomy, Human physiology, Online, Distance learning, Virtual patients, Virtual physiology models, Virtual worlds, Stereo anatomy","kwrd":["Collaborative_learning","Human_anatomy","Human_physiology","Online","Distance_learning","Virtual_patients","Virtual_physiology_models","Virtual_worlds","Stereo_anatomy"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-008-0099-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-008-0099-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=99;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-008-0099-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Learning medicine through collaboration and action: collaborative, experiential, networked learning environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0099-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0099-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2008-10-28" itemprop="datePublished">28 October 2008</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Learning medicine through collaboration and action: collaborative, experiential, networked learning environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Parvati-Dev" data-author-popup="auth-Parvati-Dev" data-corresp-id="c1">Parvati Dev<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Innovation in Learning, Inc" /><meta itemprop="address" content="Innovation in Learning, Inc, Los Altos Hills, CA, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Wm__LeRoy-Heinrichs" data-author-popup="auth-Wm__LeRoy-Heinrichs">Wm. LeRoy Heinrichs</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Innovation in Learning, Inc" /><meta itemprop="address" content="Innovation in Learning, Inc, Los Altos Hills, CA, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Stanford University School of Medicine" /><meta itemprop="address" content="grid.168010.e, 0000000419368956, SUMMIT and Department of Obstetrics/Gynaecology, Stanford University School of Medicine, Stanford, CA, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 12</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">215</span>–<span itemprop="pageEnd">234</span>(<span data-test="article-publication-year">2008</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">348 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">7 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-008-0099-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The SUMMIT Lab and William LeRoy Heinrichs, at Stanford University, were honored to be the 2002 awardees of the Satava Award for Virtual Reality in Medicine. Since the award, the group has followed two main threads of research, which we describe below. The first, “building a high-performance, network-aware, collaborative learning environment” has investigated the framework and components needed when students in multiple locations collaborate using computation-intensive simulations and large image datasets. The second thread, “online, interactive human physiology for medical education and training”, has focused on the application of interactive physiology models embedded in 3D visualizations of virtual patients in naturalistic medical environments. These environments support immersive, experiential learning where students act as medical providers and manage authentic medical events and crises. These research efforts, and our conclusions, are presented in the chapter below.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Building a high-performance, network aware, collaborative learning environment</h2><div class="c-article-section__content" id="Sec1-content"><h3 class="c-article__sub-heading" id="Sec2">Introduction</h3><p>The HAVnet project (Haptic, Audio, Visual Network for Education), funded by the National Library of Medicine, supported our research and development of a framework for an advanced network infrastructure for health education and medical research. We have shown (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig1">1</a>) that such a network infrastructure requires a <i>Middleware</i> system that monitors and reports network conditions to network-aware <i>Applications</i> that can self-scale and self-optimize based on network “weather reports.” The core system and applications have been developed within the context of two medical <i>testbeds</i>, a clinical anatomy testbed and a clinical skills testbed. Each testbed focuses on applications that challenge networks in unique ways. Beginning with local testbeds, we have extended our testbeds to national and international scope, and have <i>evaluated</i> them for educational, technical and enterprise impact. In this section of the chapter, we provide insight into the new directions that will be required for networked access, through powerful interfaces, to complex anatomy and clinical data supporting medical research and education. A complete report is available at <a href="http://havnet.stanford.edu/resources/report.html">http://havnet.stanford.edu/resources/report.html</a> with a summary report at <a href="http://havnet.stanford.edu/">http://havnet.stanford.edu/</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Framework for a networked, collaborative learning environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec3">Infrastructure</h3><p>Development of a networked, collaborative environment depends on the availability of a robust, high performance computing and communication infrastructure, such as the following:</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec4">Remote computation and storage</h4><p>The applications and infrastructure developed in this project are based on an architecture in which large databases like the visible human data set, and the core applications that work with them, reside on remote servers. Learners, on client workstations, interact with the server applications to provide a collaborative working environment that can encompass multiple sites.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Multicast</h4><p>The client applications permit multiple views of the same data set, and collaborative interactions led from any workstation. The leader, a role that is easily passed between students, can control the views displayed at all workstations and can position a pointer visible to all participants. The ability to do this depends on the Multicast protocol. This is a key infrastructure component that is not uniformly implemented across the global network. Experimental alternatives to multicast, such as tunneling, exist and are in use. A robust multicast capability, or a viable option, will be essential in developing large-scale collaborative learning environments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Internet2</h4><p>In all of our work we have assumed the availability of Internet2 level service. Shortly after commencing the project we upgraded our connectivity from 100 Mb/s to 1 Gb/s. This involved a solution to the “last mile problem,” upgrading the connectivity from the SUMMIT laboratory to the university’s gateway, and thence to the CENIC backbone.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Network traffic data</h4><p>A networked, collaborative environment that adapts to the available connectivity, requires frequent monitoring of network traffic. Network traffic conditions change by time of day and day of week. They also change as the network architecture evolves within the institution, regionally, and on the national and international backbone. In Fall 2004, we monitored network traffic over many weeks with 100 MB/s connectivity, and again in 2006 with 1 GB/s connectivity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig2">2</a>). Detailed traffic pattern graphs are available in the HAVnet Final Report. (See Appendix 1: Experiments to characterize networks based on RSV requirements <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Monitoring network traffic parameters in local, national and international testbeds. The figure shows the network layout, the average network traffic parameters of throughput, delay, packet loss and jitter, and the variation of one of these parameters, delay, over the course of a single day</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec8">Middleware</h3><p>A middleware environment is needed for collaborative learning environments. We describe the suite of middleware we developed to support self-scaling and self-optimizing applications for the end-to-end performance experience of the learner. (See relevant reports in Appendix 2: Middleware. <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">InformationChannels and the registration server</h4><p>The InformationChannels framework supports the formation of complex networks of communicating application components (clients and servers) with minimal direct learner intervention. The framework accomplishes this by focusing on the channels of information exchanged between application components.</p><p>Applications announce their ability to provide or consume channels of information by periodically sending channel announcements on a multicast address. A channel is identified by the provider’s name, the channel’s name, and the channel’s application type. A channel announcement specifies the time interval used for resending the announcement. The announcement contains the network parameters (IP address, port number etc.) required to establish the connection.</p><p>A registration server (possibly with multiple instances for redundancy) listens to its multicast address and maintains a list of all active channels. The registration server responds to queries about currently active channels. Application components can use this capability to discover the availability of remote services. An automatically-generated web based query page also allows learners to discover the existence of channels.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">A collaboration framework</h4><p>A collaborative application, such as the Remote Stereo Viewer (RSV), supports the formation of a group of learners through the following process (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig3">3</a>) (Dev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Dev P, Srivastava S, Senger S (2006) Collaborative learning using Internet2 and remote collections of stereo dissection images. Clin Anat 19(3):275–283" href="/article/10.1007/s10055-008-0099-5#ref-CR3" id="ref-link-section-d84052e470">2006</a>). The <i>RSV server</i>, with the RSV server application and the RSV-formatted image sets, announces its ability to provide access to an anatomy image set. The <i>Registration server</i> receives this announcement. A learner, through the <i>web interface</i>, queries for active RSV channels (Anderson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Anderson MB, Stillman PL, Wang Y (1994) Growing use of standardized patients in teaching and evaluation in medical education. Teach Learn Med 6(1):15–22" href="/article/10.1007/s10055-008-0099-5#ref-CR1" id="ref-link-section-d84052e483">1994</a>). The learner selects the link for one of the advertised image sets. The <i>RSV client</i> on the learner’s computer (<i>Host A</i>) is automatically launched and connected to the RSV server for the image set. The learner can now interact with the anatomy image set (Ellaway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ellaway R (2005) Modeling virtual patients and virtual cases. &#xA;                    http://meld.medbiq.org/primers/virtual_patients_cases_ellaway.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-008-0099-5#ref-CR6" id="ref-link-section-d84052e492">2005</a>). The learner indicates a desire to collaborate with other learners and the <i>RSV client on Host A</i> begins announcing an <i>active collaboration channel</i> (William et al. 2006). A second learner (on <i>Host B</i>) finds the collaboration channel through the web interface and selects the link. This automatically launches the RSV client on the second learner’s computer, which now connects to the image set server as well as to the multicast address used for communication with the first client. The actions of the first learner are now seen by the second learner. Continuing this one step further, either client can query the registration server and discover the existence of other services, such as, a <i>label set server</i> for the image set being viewed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Illustration of the process of discovering available applications and setting up collaborations. For details, see the section “A collaboration framework”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">WeatherStations</h4><p>The ability of our applications to adapt to network traffic conditions depends, in part, on the availability of data on current network conditions. Network performance is monitored by WeatherStations, developed as part of the HAVnet project, that make use of the InformationChannels collaboration framework. To monitor end-to-end performance across the national testbeds used in the HAVnet project we have operated weather stations at Stanford, CENIC—Sunnyvale, CA, WISCNet–Milwaukee, UW—La Crosse, the Michigan Center for Biological Information, and the University of Michigan Medical School.</p><p>Each weather station announces a channel indicating its ability to conduct various network measurements. By querying the registration server each weather station discovers the existence of peer stations and schedules tests. The tests consist of</p><ul class="u-list-style-dash">
                      <li>
                        <p>Ping loss and roundtrip time</p>
                      </li>
                      <li>
                        <p>TCP throughput</p>
                      </li>
                      <li>
                        <p>UDP maximum send rate with less than 1% loss</p>
                      </li>
                      <li>
                        <p>Multicast group setup time and loss rate</p>
                      </li>
                    </ul>
                           <p>Weather stations are periodically polled for the results, which are placed into the performance database as well as being used to update performance graphs.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">MultiServ: multicast lease server</h4><p>This server provides a mechanism for “leasing” multicast addresses from an available pool. This allows RSV to obtain a multicast address to use with collaborating peers and allows WeatherStations to obtain multicast addresses for use in conducting network tests. The server utilizes the InformationChannels facility to create the lease mechanism. When an application is given an address to use the application begins to announce an “MLease” channel where the name of the channel is the multicast address. As long as the channel continues to be reasserted by the application the multicast lease server marks the address as being unavailable.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">LogServ: logging performance data</h4><p>This server advertises a channel that applications can use to relay performance data, which this server then logs into the performance database. Applications can log their performance by communicating with the LogServ through a client side API. The client queues log messages, sends them as single UDP packets to the LogServ and continues to resend them until acknowledged by the LogServ. If the application quits before all log messages are acknowledged the remaining queue of messages is written to a local file. The next time the application is run it will attempt to resend these messages.</p><p>Both RSV and the WeatherStations query for the presence of a log server and, if found, use it to log all performance data.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">COR and CORQ: correlating performance data</h4><p>This server correlates the weather station measured metrics and application performance data. Since specific performance data are unique to each application the COR server creates correlation tables unique to each application. The general principle is to aggregate application performance information based upon the prevailing weather conditions at the time the application was running. As a secondary correlation, application data is also aggregated by the time of day and day of week. The correlation mechanism allows individual computers to be treated separately or for the performance data from a group of machines (e.g., a teaching lab) to be combined.</p><p>The correlation query server, CORQ, allows applications to ask for their aggregate performance experience for the weather conditions currently prevailing between a specific client and server locations. Clients can ask for the performance experience from a specific computer or for the combined experience of all machines within their local zone.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Scalable transport protocols</h4><p>Each application will have unique ways in which it can adapt its behavior to changing network performance conditions as a result of design choices. RSV is designed to support unpredictable navigation through large image sets by transporting images on demand. This implies a transport protocol that attempts to send image packets at as high of a rate as possible while balancing this against the goal of zero-packet loss since lost packets incur a round-trip time penalty to recover.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Integrating middleware into a network-aware scalable application</h4><p>The middleware components described are integrated into an adaptive system where the application’s features are selected based on network weather conditions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig4">4</a>). For example, a learner selects an anatomy image resource, such as “Lung and Pleura” on the course’s iAnatomy web page. The click on the data link automatically launches the RSV client program on the learner’s computer, and links to the corresponding program on the server. The lung images begin displaying on the learner’s screen.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Illustration of the process underlying network-aware scalability. <b>a</b> The user selects an image resource on the iAnatomy web page. <b>b</b> The Remote Stereo Viewer client is automatically launched on the user’s computer. <b>c</b> In the background, the RSV client queries the Log Server and the Performance Database to determine the viewing and transmission parameters for optimal viewing by the user</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In the background, the application sends its network performance data to the LogServ program, and queries (CorQ) the performance database about any changes it needs to make to its transmission settings. The current network weather (bandwidth, delay, packet loss, jitter, multicast setup) is checked against the information stored in the LogServ database.</p><p>If CorQ suggests that current network weather requires changes to the application settings, the scalable application selects some of the many possibilities in a predetermined priority order. The possible changes we have tested include a change in image resolution, frame transmission rate, and in the rate of interleaving of prefetch data according to the application’s speed in decoding compressed image data.</p><p>Experiments conducted by us suggest that responsiveness of the application is one of the most important characteristics sought by the user. Other scalable features, such as image resolution, can be modified as long as they remain adequate for the learning task.</p><h3 class="c-article__sub-heading" id="Sec17">Applications</h3><p>Applications were selected and customized for the HAVnet project to support collaborative and distance learning. Selected applications were enhanced significantly so as to support our research in creating network-aware, scalable applications. They are described below.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Remote stereo viewer</h4>
                    <h3 class="c-article__sub-heading">Description:</h3>
                    <p>The RSV is a client/server application that provides immediate access to large sets of high-resolution stereo images. The image sets are organized as multi-dimensional grids. Each image set has at least one dimension, consisting of a series of images forming a 360° revolution of viewpoint around an anatomical subject. Consecutive images around this rotation axis are used to construct a stereo view of the object. Horizontal mouse motion moves the image in this dimension through the image set. Additional dimensions, mapped to vertical mouse motion, may show the structure at various levels of dissection or magnification. The application assumes that the learner will view only a “small” selection of the images and consequently does not warrant downloading the entire image set. RSV 2.0 uses the InformationChannels framework and includes the ability to form collaborative groups of learners. The content includes both hand and knee image sets, labeled for use in anatomical study (Dev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Dev P, Srivastava S, Senger S (2006) Collaborative learning using Internet2 and remote collections of stereo dissection images. Clin Anat 19(3):275–283" href="/article/10.1007/s10055-008-0099-5#ref-CR3" id="ref-link-section-d84052e658">2006</a>). (See also Appendix 3: Remote Stereo Viewer at <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p>
                  
                    <h3 class="c-article__sub-heading">Network characteristics:</h3>
                    <p>Image sets are stored on the server as JPEG images and transported to the client on demand using a UDP layer protocol. The transport rate is configured to be equal to the JPEG decompression rate of the client, allowing the client to interleave receipt and decompression of data. Between Stanford and University of Wisconsin La Crosse, transport produces bursts of between 30 and 40 MB/s.</p>
                  
                    <h3 class="c-article__sub-heading">End-to-end performance support:</h3>
                    <p>We further developed this application for use with our testbed networks. RSV 3.0 incorporates the collaboration and end-to-end performance support of the core infrastructure. It reports actual image transport performance information to the WeatherHistory mechanism. The client also queries current weather conditions and uses this information to optimize its behavior accordingly.</p>
                  <p>All clients in a collaboration group can have independent connections to the image server and submit separate image requests. RSV also incorporates the ability to multicast an image to all collaborating clients, significantly improving the application’s scalability.</p>
                    <h3 class="c-article__sub-heading">Scalable transport in RSV:</h3>
                    <p>Remote Stereo Viewer is designed to support unpredictable navigation through large image sets. Typically, images are sent in high bandwidth bursts where the burst duration is a fraction of the total round-trip time. Since this means that the transport link would be idle for significant periods of time, the client pre-fetches images in the neighborhood of the last request to utilize the available capacity of the transport link as long as this does not negatively impact transport efficiency. The client can also adjust the image resolution requested to better balance use of the transport link. The performance data logged by RSV consists of its transport success experience (packet loss percent, percent of images transported with zero loss) at different send rates.</p>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Exploratory haptics trainer: SPRING</h4><p>SPRING is a software platform for developing surgical simulation applications, initially developed by Dr Kevin Montgomery of the National Biocomputation Center in 1999. The set of networked, cooperating applications in SPRING offers an interactive 3D world of tissue and tool models. The models use standard data formats and support both rigid and deformable models with customized physical tissue parameters. We have enhanced the software of SPRING while making it more accessible to potential learners and developers. Our major activities have included updating the code base and development environments for use with modern software engineering tools, improved algorithms, and resolution of many software problems. The software is networked, cross-platform, and open source, with a web site, availability on SourceForge, and extensive documentation. SPRING has been presented at workshops and conferences as a development platform on Open Source Surgical Simulation. Partly as a result of these efforts, SPRING is now in use as a research tool and simulator development platform at multiple university sites around the world. We intend that improved availability and support will allow computer science groups and medical schools to design and develop simulation applications for medical education and training (see Appendix 3: Exploratory Haptics Trainer, at <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><p>A series of haptic experiments have been hosted on SPRING, including comparison between actual and perceived stiffness of virtual membranes, and perceptual experiments on the range of haptic feedback that people can detect. We have also studied the role of network parameters on haptically-enabled task-oriented activities performed by subjects with a variety of medical experience and training.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">The Collab Room</h4><p>The <i>Collaboration Room</i> opened in 2005 has provided the HAVnet team with a greatly improved ability to participate in and host video collaborative networked events and research (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig5">5</a>). It is a 400-sq.ft. experimental space to study the impact of new collaboration technologies and new teaching methods on medical education of the future. This space is used for planning and staging interactive high bandwidth educational events, virtual reality team training sessions, and pilot studies for simulator validation. The room is equipped with Access Grid and other high-resolution videoconferencing system, capable of connecting teams simultaneously from multiple locations worldwide. Rich media data streams, such as large projected stereo images from remote simulators are part of the learning experience. The infrastructure supports easy switching to other collaboration tools, such as DVTS, standard H.323 videoconferencing, desktop-sharing and videoconferencing from many vendors (Microsoft, Marratech, Skype, and others) or other web-based communication tools such as Adobe Connect. (See Appendix 3: Collab Room at <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The Collab Room is outfitted with simulation equipment, ceiling mounted computer projectors, and multiple video cameras, with the ability for flexible re-organization of the space for a variety of learning activities</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Around the room are mobile carts with state-of-the-art surgical simulators where residents and fellows evaluate modules on clipping, dissecting, grasping and other basic surgical skills. Key to the room’s design, therefore, is flexibility so that the room can be set up and torn down for the variety of activities required for their educational research.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Remote tactile sensor</h4><p>The dermatologist, conducting an examination of a remote patient, needs both high quality video and high quality haptics for diagnostic purposes. We have designed and constructed a system for remote diagnosis of dermatological disorders employing both visual examination and palpation. It employs a haptic master–slave robot system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig6">6</a>). This sub-project turned out to be more challenging than was originally envisaged, and the full system is still being tested. However, we did make significant progress in many different, and relevant, areas (see Appendix 3: Remote Tactile Sensor, <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Connectivity diagram for the Remote Tactile Sensor</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>We developed and tested a tactile probe suitable for the detection of skin texture, and the evaluation of skin profile. The unit is compact and light in weight suitable for mounting on a slave robotic arm. The sensing is done by an array of piezoelectric sensors formed on a piece of PZT film. As part of the design of this sensor we did extensive work on mathematical modeling of tactile sensors interacting with compliant substrates, such as skin for the purpose of evaluating texture. The tactile probe has been tested on human subjects and has been successful at discriminating between the textures of skin on the back of the arm and on the front of the arm and hand. We developed separate means of sensing of skin profile: that is the palpation of lumps in order to evaluate their geometry, location and mechanical properties. This requires a combination of the tactile probe and the native haptics of the slave manipulator. We have successfully operated the tactile probe mounted on a Sensable Technologies Omni slave robot and programmed to maintain constant pressure on the skin, in order to sense the skin profile.</p><p>The network parameter of particular relevance to the Remote Tactile Sensor is latency. Closing a control loop around a delay of the order of that generated by speed of light and switching delays (10’s or 100’s of milliseconds) results in dynamic instability. Control algorithms, such as the wave variable algorithm, cause a softening of the haptic sensation. We are investigating calibration methods based on the average latency for each specific session. We are also investigating a model-driven approach to collecting haptic data where the learner then interacts with that model, rather than the actual patient.</p><h3 class="c-article__sub-heading" id="Sec22">Clinical anatomy testbed</h3><p>We designed and implemented local, national, and global testbeds to link learners at distributed sites through Internet2, and provided access to remote media resources through richly interactive interfaces. We used these testbeds to demonstrate, investigate and evaluate technical and pedagogic requirements for collaborative learning among students at distributed sites. Lessons learnt in each use of a testbed were used to improve the configuration and process for subsequent testbed experiments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Local testbed</h4><p>The <i>local testbed</i> was set up at Stanford University, between the Anatomy division’s classrooms and our laboratory. Faculty in the laboratory guided students in the classrooms through a series of stereo views of anatomy. The actual teaching sessions were preceded by numerous test sessions that were needed to set up the technology.</p><p>The primary application used in the testbed was the RSV. Using RSV, students accessed stereo images of the dissected hand and the knee, to get a three-dimensional view of dissected anatomy. The viewpoint could be moved interactively, allowing rotation of the anatomy and dissection through multiple layers. RSV supports collaboration by allowing students to form groups to observe others’ interactions with the images.</p><p>The local testbed was critical in identifying the technical infrastructure essential for remote collaboration in learning. Setting up the collaborative group, simultaneous viewing of an image with its cursor, and transfer of leadership, all required use of a key feature of the network—Multicast. Our local testbed showed that, even within Stanford, the multicast protocol was not implemented uniformly over the network, leading to failure of multicast between some pairs of learners. A second requirement for collaborative learning was the use of video collaboration. The teacher found it essential to see the tacit body language cues that were indicative of failure to understand or follow the lesson. A third requirement was good audio, with no echo. This was achieved either through the use of headsets or through echo-canceling microphones. A fourth requirement was the configuration of identical equipment and versions of software at each site, in order to reduce the chances of system incompatibility for complex applications. A major outcome of the experiments with the local testbed was the specification of a new experimental learning space, the Collab Room, as a flexibly configurable space, suitable for testing many types of distributed learning (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0099-5#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Comparisons of technical results of two networked teaching events</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-008-0099-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">National testbed</h4><p>For our <i>national testbed</i>, we linked faculty at Stanford University with students at University of Michigan, Ann Arbor, using Internet2 at 1 GB/s. Both accessed image resources located at Wisconsin and at Stanford. The goal of this testbed was to determine the issues that would arise as we scaled up the testbed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>In the national testbed, students at the University of Michigan, Ann Arbor, and faculty at Stanford University, accessed anatomy images from University of Wisconsin, La Crosse</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Creating a stable network infrastructure that would support our application, RSV, proved even more difficult in the national testbed than in the local testbed. Each site worked with its institutional network provider to obtain adequate bandwidth. However, obtaining multicast functionality required testing of every point-to-point connection, with the involvement of the regional networks, CENIC, WiscNet and MERIT, as well as the network engineers at each institution. Intervention of Internet2 network engineering leadership was required for multicast problems to be addressed. This heroic effort made it clear that the network infrastructure for collaboration is inadequate, and that, until these services are available at the network level, collaboration may have to be a function of each application.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">International testbed</h4><p>For our <i>international testbed</i>, we partnered with ORION, Ontario’s high performance network group, and the Northern Ontario School of Medicine (NOSM), a new medical school, with an inherently distributed student body. Classes are held simultaneously at their two campuses, Sudbury and Thunder Bay, 500 miles apart.</p><p>Faculty at NOSM accessed the Bassett collection of stereo images of dissection remotely, to prepare image sets that were uploaded to the RSV server at Stanford. The images were labeled in RSV and then used in interactive small group learning by both faculty and students. Since NOSM provides only prosected dissection specimens, and no actual dissection experience, the three-dimensional stereo images proved attractive to the students. NOSM continues to use this collection for teaching even though the testbed experiment is complete.</p><p>In the evaluation, all students agreed or strongly agreed that the use of the images in RSV was worthwhile. Eighty-eight percent agreed or strongly agreed that 3D virtual reality and the Bassett collection should be incorporated into their curriculum resources. The students had some excellent practical suggestions on how the technology and the material could be incorporated into their curriculum.</p><p>As an extension to our testbed teaching experiments, we undertook numerous <i>demonstrations</i> to institutions around the world. Demonstrations to most parts of the world were over Internet connections of 2 MB/s or less, and with significant delay. In these situations, slide presentations using videoconferencing applications were the most effective. In some demonstrations, Internet2 quality bandwidth was available. The remote site could access our resources, such as RSV images, interactively, though only in mono-viewing, not stereo. However, true multi-site collaboration was not available. Screen-sharing applications, such as vnc, supported collaboration but interaction was slow. Some sites had stereo-viewing capability, enhancing the viewing experience. Others, such as the KISTI and MEDRIC sites in Korea, had high-definition videoconferencing capability, thus providing an extremely strong sense of presence between our laboratory and theirs. However, only those sites, such as NOSM and a few others, who had installed the RSV client and had multicast capability, experienced the collaborative interaction that our application and content was able to provide.</p><h3 class="c-article__sub-heading" id="Sec26">Clinical skills testbed</h3><p>The clinical skills testbed encompassed many areas, most including a component of haptics perception. (See Appendix 5: Clinical Skills Testbed, <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Skills training with simulators</h4><p>Procedure simulators support practice of surgical tasks, from camera control, to grasping and clipping, to conducting a procedure such as excision of an ectopic pregnancy. To understand the technical and pedagogic issues of training with simulators, we conducted training sessions with ob-gyn and other residents. Most simulators are still stand-alone, with no collaboration, say, between the learner and a remote teacher. We have experimented with networked surgical simulators, based on Australia’s CSIRO-developed software and on Stanford’s open source SPRING software. The potential for networked training with surgical simulators is significant but still unrealized.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Video-supported learning events</h4><p>Video telecast of real-time surgery is a common distance training method. To create a greater sense of presence and immediacy, we experimented with high-resolution two-way video (DVTS) and stereo video from the laparoscope camera. A laparoscopic appendectomy was conducted in an operating room at Stanford University, and was observed by residents in Sydney Australia. The project was a technical success. Since then, we have implemented and participated in numerous DVTS-supported distributed learning events, as part of conference sessions (multiple sessions for the Asia-Pacific Advanced Network and for Internet2) and as infrastructure for courses that demand high quality video and images. The DVTS infrastructure is free, reliable, and has high quality transmission of video. It does need knowledgeable technical support, both for network issues and for audio-visual quality, as well as high bandwidth, 30 MB/s per one-way video stream.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec29">Performance score in simulation tasks</h4><p>In collaboration with the Society for Laparoendoscopic Surgeons, we developed a collaborative, formal study for acquiring benchmark data for criterion-based training of basic technical surgical skills with five widely-used training simulators. Seventeen laparoscopic surgeons spent three half-days learning to use the simulators. We collected data from which Performance Scores (on 4th-attempt data) could be rigorously calculated. Scores were a linear weighted sum of the results from metrics embedded in the simulator, and were of the form <span class="mathjax-tex">\( b0 + b1 \cdot X1 + b2 \cdot X2 + \cdots + bk \cdot Xk. \)</span> The coefficients, <i>b</i>0, <i>b</i>1, … <i>bk</i>, were adjusted for each task, using a linear regression process, to generate a performance score for each task that improved from a value of 50 on the first attempt and to an asymptote of 100. Fourth attempt data, using this score, was determined to be an accurate predictor of a subject’s performance. This statistical method will be a valuable tool for evaluating performance scores for subsequent studies of networked simulators (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig8">8</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Graphs of practice attempts by individual subjects (surgeons). Analysis indicates that, for most subjects, the performance in the fourth attempt is an accurate predictor of later performance on the simulator</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec30">Experiments in haptic perception</h4><p>We performed a series of experiments to measure haptic perception using both real and simulated forces. We first created a durometer containing latex sheets of various thicknesses. Learners probed these using laparoscopic tools, but without visual feedback. Subjects were able to feel differences in elasticity and could correctly rank the sheets in order of stiffness.</p><p>Virtual membranes with haptic output were modeled using the SPRING platform, comparing objects with four different constant stiffnesses to an unknown that matched one of the other four. The task was to pick the closest match to the center object by probing and sensing force differences. Network latency was also introduced, adding a time delay from 0 to 100 ms between probing and the force feedback. This apparently simple task proved to be difficult for most subjects.</p><p>We tested several profiles of force feedback as a function of penetration depth. Although learners judged the profile that increases force as the square of the depth (quadratic) to be most realistic, subjects performed at the same level with both linear and quadratic force profiles.</p><p>In an apparently simpler task, comparing two virtual surfaces using a low-friction, finger held haptic interface, subjects usually picked the difference in the force levels reliably when one force was twice the magnitude of the other. A significant number of errors were made with smaller force differences.</p><p>When both membranes resisted with similar force, but a network delay greater than 50 ms was introduced in sensing force on one membrane, subjects usually interpreted the two forces as being different. This is compatible with our earlier studies that indicated 100 ms as the maximum loop delay before a position-following task became unstable. (See Appendix 5: Experiments in Haptic Perception, <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><h3 class="c-article__sub-heading" id="Sec31">Evaluation</h3><p>(See Appendix 6: Evaluation, <a href="http://havnet.stanford.edu/resources/report_appendices.html">http://havnet.stanford.edu/resources/report_appendices.html</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec32">Impact on technology</h4>
                    <h3 class="c-article__sub-heading">Infrastructure and middleware:</h3>
                    <p>The work performed under this project in transport protocols, performance monitoring and end-to-end performance has had several impacts on the research community.</p>
                  <p>Within the Internet2 community, the network requirements of our applications influenced the development of an API design for a new transport protocol in the Bulk Transport working group of Internet2. This transport protocol will have better performance characteristics than TCP by attempting to differentiate congestive loss from non-congestive loss by monitoring changes in connection latency.</p><p>Our extensive use of multicast has demonstrated that monitoring multicast performance should become a standard part of network monitoring. The project’s use of multicast also resulted in multicast being made an operational priority within the Wiscnet network. The unique performance needs of the applications developed by this project have been used in planning and soliciting for a new state network backbone.</p>
                    <h3 class="c-article__sub-heading">User response to network performance degradation:</h3>
                    <p>We conducted a study of student perception of RSV images under degraded network conditions and under degraded image quality. The degradations included reduction of the available bandwidth for the transport of the images through the network links and two different latencies between server and client.</p>
                  <p>The analysis of the data provides some important insight into the impact of network performance measures on learner perception. For example, medical students rated with scores around 4 (considered very good) the scenario in which the RSV application worked with a network connection set at 5 MB/s and with an image set at 50% resolution. This is a striking finding considering that in the usual non-degraded scenario this application works with a bandwidth above 70 MB/s and 100% image resolution. This opens the opportunity to extend the use of RSV on IEEE 802.11 Wireless LAN environments, where bandwidth is scarce. We also investigated the impact of responsiveness versus image quality on the overall learner perception as we degraded network bandwidth and latency. We found that learners could tolerate and use images of slightly degraded quality (50% resolution) as long as the system was perceived as highly responsive, with little or no perceived latency as new images were selected.</p>
                    <h3 class="c-article__sub-heading">Open source surgical simulation software:</h3>
                    <p>SPRING: In December 2005, we released the SPRING package as a robust, publicly usable suite of software applications, with freely available development code, on SourceForge.net. We have developed and maintained a website for SPRING learners that describes surgical simulation, applications, and the use of SPRING for developing practical simulation. Documentation on the architecture and details of SPRING and its deployment are available, as well as links to the SourceForge project. We have hosted a 3-day workshop, and presented at conferences, to build awareness of this powerful software.</p>
                  <p>SPRING as an open source software has opened a technology platform to students, research groups, and developers around the world. In addition to raising awareness of surgical simulation, SPRING’s availability has lowered the barrier to incorporating 3D modeling, deformable surface physics, and haptic interaction into projects around the world. We have initiated and facilitated growth of a community of learners, developers, and researchers in open source surgical simulation through this project.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec33">Impact on learning</h4><p>The use of advanced technologies in medical and surgical education has a significant impact on learning. The impact is most easily recognized in terms of access. For example, in many schools and many parts of the world, dissection is no longer a part of the medical school curriculum, so the opportunity to do a “virtual” dissection becomes essential for these students. Richly interactive, distributed learning technologies, such as the RSV application, give learners access to complex datasets that use powerful learning resources, 7 days a week, from any location—on campus, across town, or from another city!</p><p>Surgical education, at the residency level, is being transformed by the introduction of simulators—both “stand alone” part-task trainers and networked simulations which give trainees the opportunity to work “one on one” with a master surgeon—manipulating virtual tissue and organs of the same simulated patient—even when trainee and surgeon are in different geographic locations. These new learning technologies also make it possible for surgeons to watch a live surgical demonstration in 3D stereo, as if they had a front row seat in the operating theatre, regardless of their actual location.</p><p>The addition of force feedback, or haptics, to surgical simulators makes it possible for trainees to “feel” what the instructor feels, receive “hand on hand” guidance and respond to individualized instruction from a master surgeon in a distributed learning environment.</p><p>Evaluation of these technologies shows considerable acceptance in spite of technical difficulties. After a surgical learning event (SimTech, 2004) between Stanford University and surgical residents at Canberra, Australia, 100% of the residents rated the learning value of the stereo images as acceptable or better, and 97% felt that the performance of the networked application, over Internet2 and its global equivalent, was acceptable or better. Medical students at Northern Ontario School of Medicine assessed the RSV application, the images and the labeling capability as useful or better. Many suggested that their use in small group learning and teaching between students would be very useful.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec34">Impact on enterprise</h4><p>Through the HAVnet project, SUMMIT has impacted the local enterprise, at Stanford University, as well as institutions and organizations globally.</p>
                    <h3 class="c-article__sub-heading">Learning and Knowledge Center:</h3>
                    <p>A newly funded building program, scheduled for opening in early 2010, will incorporate state-of-the-art learning technologies in classroom configurations that take advantage of networked facilities. SUMMIT’s leadership, through HAVnet and other projects, has contributed the vision for learning spaces that enable networked visualization of media-rich anatomy, of simulations with standardized and virtual patients, surgical simulation for individual skills development, and virtual worlds for team training.</p>
                  
                    <h3 class="c-article__sub-heading">Center for Immersive and Simulation-based Learning:</h3>
                    <p>The strong focus on simulation at SUMMIT, the VA, Childrens Hospital, and in the surgery department, has led to the formation of the new Center for Immersive and Simulation-based Learning. The center’s mission is to improve patient safety, patient care, education, and research through innovations in immersive and simulation-based learning techniques and tools.</p>
                  
                    <h3 class="c-article__sub-heading">Institutions outside Stanford:</h3>
                    <p>As part of the clinical anatomy testbed activities, The University of Michigan’s medical school has developed a new learning space for collaborative and visualization-based learning. Network performance has been a key requirement for this space, including the requirement for multicast. This room is now in routine use at Michigan.</p>
                  <p>Similarly, the Northern Ontario School of Medicine has pressed for 10 GB/s connectivity to both their campuses, with a requirement that this support simultaneous teaching at both facilities. The stereo images of anatomy will be the backbone of the visualization-based teaching component at NOSM.</p>
                    <h3 class="c-article__sub-heading">Collaborative learning spaces:</h3>
                    <p>The Collab Room at SUMMIT was based on an innovative collaborative learning facility at Stanford in Wallenberg Hall. We customized the concept to create a room suitable for medical teaching and learning. Since the space opened in 2005, SUMMIT has hosted about 100 groups each year, interested in the space and the applications it houses. Sites at Stanford and elsewhere have modified the design and implemented their own collaborative learning spaces.</p>
                  <h3 class="c-article__sub-heading" id="Sec35">Future directions</h3><p>We see several directions for our future research. The first is the technical infrastructure and the user interfaces to support the increasingly complex data and algorithms that will be accessed and used by future healthcare workers. Collaborative learning, access to remote experts, and transparent access to computing, storage and collaboration resources will be essential to future research. Our past and current work provides an excellent foundation for future research in this direction.</p><p>New technologies that promise to be relevant to medical education continue to be developed. The consequence is a continually increasing demand on Internet performance. When we started the current project we had 100 MB/s internet connectivity. That was expanded to 1 GB/s for the purposes of this project. Several universities now have 10 GB/s connectivity, and for purposes of planning only a few years ahead we need to think in terms of 100 GB/s. The implications in terms of the ways in which we will run interactive applications, and the ways in which software and hardware infrastructure will be affected remain to be explored. There will certainly be issues like the multicast protocol of the second generation internet that has not been consistently implemented in the way that was originally envisaged, and consequently remains a major limitation on interactive networked applications.</p><p>New visualization systems, high-resolution display walls, and high-definition video will become essential. Wave-front displays are becoming commercially available and offer a very viable alternative to stereo video. There are issues to be explored as to how one interacts with a wave-front model, and what are the implications for display of the very complex data sets characteristic of anatomical and physiological medical models.</p></div></div></section><section aria-labelledby="Sec36"><div class="c-article-section" id="Sec36-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec36">Online, interactive human physiology for medical education and training</h2><div class="c-article-section__content" id="Sec36-content"><h3 class="c-article__sub-heading" id="Sec37">Introduction</h3><p>Anatomy and physiology are fundamental building blocks in the study of biology and the health sciences of nursing, medicine, and dentistry. Yesteryear’s educational methods of cadaveric dissections, and animal experiments in physiology laboratories, have, or are being replaced in many training institutions. These changes are prompted by factors of cost and maintenance, animal rights, and time allocation in the curriculum, in competition with expanding knowledge. One of many consequences of the changed emphasis in these basic and pre-clinical disciplines is the paucity of graduate students and teachers to provide instruction in these “gross” topics—structural biology and genomics are the current focus. Another is the loss of opportunities for clinical skills development of dissection, a defining experience in the professional development of medical professionals, and of monitoring pharmaco-therapeutic interventions in anesthetized animals. These trends, and how SUMMIT’s work during the past 6 years is contributing new options, learning technologies and online educational tools are the topics that follow.</p><h3 class="c-article__sub-heading" id="Sec38">Online models of human physiology</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec39">The online information smorgasbord</h4><p>A “Google search” for <i>human physiology</i> (<a href="http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;hl=en&amp;q=human+physiology&amp;btnG=Google+Search">http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;hl=en&amp;q=human+physiology&amp;btnG=Google+Search</a>) yields over 7 million websites where one can find books, animations, videos, secondary websites, etc. Some of these are offered by colleges and universities around the world, others by companies, and still others by individuals. Multiple levels of complexity are offered for different learners; biology for high schoolers’ and pre-professional study, nursing, and medical students. Some information is free, and other arrangements require registration, subscription, or online purchase. Advertisements for information available on CD/DVD formats abound.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec40">Standardized patients</h4><p>Simulation-based learning environments in medicine usually require some graphical personalization of one or more patients. The classic example of the simulated or <i>Virtual Patient</i> is the “Standardized Patient,” represented physically by a trained human actor who feigns a clinical condition that is probed and resolved by a trainee in an OSCE (Anderson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Anderson MB, Stillman PL, Wang Y (1994) Growing use of standardized patients in teaching and evaluation in medical education. Teach Learn Med 6(1):15–22" href="/article/10.1007/s10055-008-0099-5#ref-CR1" id="ref-link-section-d84052e1345">1994</a>; Ellaway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ellaway R (2005) Modeling virtual patients and virtual cases. &#xA;                    http://meld.medbiq.org/primers/virtual_patients_cases_ellaway.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-008-0099-5#ref-CR6" id="ref-link-section-d84052e1348">2005</a>; <a href="http://www.virtualpatients.net/">http://www.virtualpatients.net/</a>). The simulations of physiology in such simulations are “by-report” in the clinical scenario. This decade’s long practice in medical education is the model for the computer-based technologies that follow.</p><p>Virtual representations of patients are familiar in healthcare education, in medical records, and in pharmaceutical development domains. These representations range from anatomical structures to scripted verbal communication, static descriptive data, or real-time processes of biomedical reactions (William et al. 2006). An example of an interactive Cyberpatient (<a href="http://www.netmedicine.com/cyberpt/cyberptframe.htm">http://www.netmedicine.com/cyberpt/cyberptframe.htm</a>) presents a “patient report” webpage prompting the user to select an appropriate action by clicking on one of several graphical button’s that execute the decision selected; a new screen advises if the action was either incorrect or correct; if correct, the user is informed of the correct result, reported on a new screen, perhaps with an ECG tracing for indicating the physiological consequence, or presenting a new query. Students interact with descriptions, video images, videos, and displays of radiological films that they select.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec41">Virtual patients</h4><p>In another type of virtual patient, functional models are continuously interactive. Users are able at any time to manipulate data to demonstrate structural features, cognitive or social attributes, or surgical manipulations (<a href="http://www.acssurgery.com/abstracts/acs/acs0104.htm">http://www.acssurgery.com/abstracts/acs/acs0104.htm</a>), however few of those available have physiological systems.</p><p>Another learning technology is the computer-based or online “virtual patient” that provides static vignettes of selected clinical conditions represented by graphical interfaces, textual, and auditory feedback (Garfield et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Garfield JM, Paskin S, Philip JH (1989) An evaluation of the effectiveness of a computer simulation of anaesthetic uptake and distribution as a teaching tool. Med Educ 23:457–462" href="/article/10.1007/s10055-008-0099-5#ref-CR7" id="ref-link-section-d84052e1386">1989</a>). An early entry into advanced learning systems for interactive simulation of patient (ISP) cases was introduced to meet the key pedagogical goals wherein students activate and construct the cases. The innovative ISP-VR system features a video-based illness history-taking function using keyboard input, highly interactive physical examination procedures, extensive laboratory tests and detailed user feedback (Bergin and Fors <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bergin RA, Fors UGH (2003) Interactive simulated patient—an advanced tool for student-activated learning in medicine and healthcare. Comput Educ 40:361–76" href="/article/10.1007/s10055-008-0099-5#ref-CR2" id="ref-link-section-d84052e1389">2003</a>).</p><p>This type of online physiology application involves clinical scenario simulations in which the virtual patient is represented as a video clip. Users are able to query the virtual patients who respond audibly with pre-recorded answers based upon recognition of keywords. The user may be provided graphical evidence of a condition, or physiological sounds, e.g., breathe sounds. This style of virtual patient has an embedded database of laboratory tests that can be selected; the results retrieved are consistent with the diagnosis in individual clinical scenarios. These laboratory values are pre-selected and static, and not responsive to therapeutic interventions. Nevertheless, development of therapy plans is required for completion of the lesson. Such virtual patients are well suited for the learning of diagnostic reasoning and medical management (Bergin and Fors <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bergin RA, Fors UGH (2003) Interactive simulated patient—an advanced tool for student-activated learning in medicine and healthcare. Comput Educ 40:361–76" href="/article/10.1007/s10055-008-0099-5#ref-CR2" id="ref-link-section-d84052e1395">2003</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec42">Libraries of virtual patients</h4><p>However two recently aggregated, large libraries of virtual patients are recently available, one (<a href="http://www.aamc.org/mededportal">http://www.aamc.org/mededportal</a>) from the MedEdPORTAL of the AAMC (American Association of Medical Colleges), and the other from the European-based Web-SP project (<a href="http://websp.lime.ki.se/about">http://websp.lime.ki.se/about</a>). For the former peer-reviewed virtual patient cases, of which over one-hundred have been submitted,</p><blockquote class="c-blockquote"><div class="c-blockquote__body">
                      <p>“… learners can virtually experience a patient interview, perform many aspects of the physical exam, and even make diagnostic and therapeutic decisions.”</p>
                    </div></blockquote>
                           <p>In the latter, case simulations features include</p><blockquote class="c-blockquote"><div class="c-blockquote__body">
                      <p>“… interactive history taking, complete physical examination (inspection, auscultation, palpation, percussion, vitals, neurological exams etc.), complete lab section including chemical labs, X-ray, MRI, ultrasound, CT, physiology lab, pharmacology lab, pathology lab etc.), diagnosis and differentials, therapy, interactive session feedback, integrated references and online database sources. All functions are freely chosen by the user and are available at any time, except the detailed feedback, which only is displayed when the user has submitted a diagnosis and therapy proposal”.</p>
                    </div></blockquote>
                           <p>Other medical institutions have also embarked on the path of developing virtual patients, including Harvard Medical School (<a href="http://research.bidmc.harvard.edu/VPTutorials/">http://research.bidmc.harvard.edu/VPTutorials/</a>), New York University (<a href="http://www.tinkering.net/vp/">http://www.tinkering.net/vp/</a>), the University of Edinburgh (<a href="http://labyrinth.mvm.ed.ac.uk/">http://labyrinth.mvm.ed.ac.uk/</a>), and Tufts University School of Medicine (<a href="http://tusk.tufts.edu/view/url/H1185C/471802/490012/">http://tusk.tufts.edu/view/url/H1185C/471802/490012/</a>).</p><p>Such efforts and collections indicate the need and acceptance of using virtual patients in early clinical education. However most groups indicate the labor and resource-intensive effort needed to develop media-rich programs. Several offer authoring systems to facilitate the creation of virtual patients.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec43">Dynamic virtual patients</h4><p>In contrast to the above type of cases, Entelos, Inc., a company with expertise in bio-physiology has modeled virtual patients with mathematical systems labeled <i>PhysioLab disease systems</i> operating intrinsically (<a href="http://www.entelos.com/virtualPatients.php">http://www.entelos.com/virtualPatients.php</a>). After modeling human health,</p><blockquote class="c-blockquote"><div class="c-blockquote__body">
                      <p>“… these researchers draw from a large volume of published data about normal physiology and disease data, ensuring that <i>PhysioLab disease systems</i> are of a high enough quality to maintain the stability and dynamics of a homeostatic system. The model has been validated by running simulations and comparing virtual experimental results to known experimental results on the molecular, cellular, tissue, and whole patient levels.” “In simple terms, a representative healthy human is “given” a disease, creating a virtual patient. Entelos can create many virtual patients that represent known and hypothesized causes for the disease and, using bio-simulation experiments, test therapies to understand a patient’s likely response to treatment. Virtual patients respond differently to treatment depending on their lifestyle and why they have the disease just as subpopulations of actual patients would in the clinic” (<a href="http://www.entelos.com/virtualPatients.php">http://www.entelos.com/virtualPatients.php</a>). These virtual patients have no graphical representations.</p>
                    </div></blockquote>
                           <p>We have amplified another category of virtual patients that affords a dynamic patho-physiology <i>in silico</i> model operating intrinsically, simulating appropriate representations of a victim/avatar’s gross physiology reflected in the vital signs (Heinrichs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Heinrichs WL, Kung S-Y, Dev P (2008a) Design and implementation of rule-based medical models: an in silico patho-physiological trauma model for hypovolemic shock. In: Proceedings, MMVR 2008, January 2008, IOS Press, Long Beach, pp 159–164" href="/article/10.1007/s10055-008-0099-5#ref-CR8" id="ref-link-section-d84052e1507">2008a</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Heinrichs WL, Youngblood P, Harter PM, Dev P (2008b) Simulation for team training and assessment: case studies of online training with virtual worlds. World J Surg Spec Issue 32:161–170. &#xA;                    http://www.springerlink.com/content/v86175717562/?p=d92acbf4aabf4fea9187acdb5cc053b3&amp;pi&#xA;                    &#xA;                  = 4" href="/article/10.1007/s10055-008-0099-5#ref-CR9" id="ref-link-section-d84052e1510">2008b</a>). These patients could be considered the “gross physiological” counterpart to the “metabolic physiological” virtual patients described by Entelos, Inc.</p><p>These virtual patients include responses to therapeutic interventions for virtual pathological conditions. Thus, a dynamic virtual patient’s system can be initiated by displaying the initial vital signs, which upon initiation in a clinical scenario at time zero, the models begin operating continuously, anticipating that a user will accurately diagnose and manage a patient’s clinical condition, preventing patient demise within time-constraints. If accurate assessment and timely interventions are introduced, the deteriorating patho-physiological condition is reversed, and the patient’s vital signs return to normal. In other words, the clinical condition deteriorates during the interval of diagnosing the problem or if appropriate interventions are not applied in a timely manner, and conversely, if appropriate and timely remedial measures are applied, the impending demise is averted. Such simulations provide immediate feedback for the application of knowledge of clinical management. As such, they extend the cognitive domain into the practice arena where immediate feedback is obtained, options are available, and performance can be measured. Such simulations are similar to those provided by physical patient simulators, serving for learning crisis management on a case-by-case basis. A few online simulations with different degrees of complexity focus on trauma patients in virtual Emergency Departments, as well as patients in other “critical care” areas of a virtual hospital (Heinrichs et al. 2008).</p><h3 class="c-article__sub-heading" id="Sec44">SUMMIT’s first virtual emergency department</h3><p>Our initial Virtual ED (VED) developed with <i>Atmosphere</i>, an Adobe Systems, Inc (San Jose, CA, USA) <i>beta</i> software product, offered six virtual patients whose physiological model were written in <i>Java script</i>. This simulation platform was very simple in concept, intended primarily for demonstrating computer modeling, but we extended that purpose by introducing dynamic virtual patients for learning (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig9">9</a>). Our online virtual cases were modeled after clinical trauma scenarios developed for the patient simulator in the Clinical Simulation Center at the Karolinska Institutet by collaborators in Stockholm. Four physiological parameters, including heart rate, blood pressure, respiratory rate, and oxygen saturation were programmed into the <i>Atmosphere</i> system using <i>Java script</i>. Teams of students/interns selected avatars that represented their decisions and actions in the VED. Virtual patients responded immediately to team members’ clinical interventions; if they were timely and appropriate, virtual patients survived. Changes in patients’ vital signs were displayed on a monitor in the VED (Youngblood et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Youngblood P, Harter P, Srivastava S, Wallin C-J, Fellander-Tsai L, Moffet S, Heinrichs WL (2008) Design, development and evaluation of an online virtual emergency department for training trauma teams. J Sim Med (in press)" href="/article/10.1007/s10055-008-0099-5#ref-CR25" id="ref-link-section-d84052e1543">2008</a>). Users interact with the VED I by selecting diagnostic and therapeutic action from a menu by clicking on their choice. (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A virtual ED scene with a patient on a gurney surrounded by six avatars hearing the ‘patient report’. The vital signs are reported in the monitor screen</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>The menu bar by which users can select diagnostic and therapeutic actions that control the vital signs displayed. The choices are purposefully disordered to avoid guiding users</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec45">Impact on learning</h4><p>Formative research among 31 senior medical students and interns recruited from Stanford and the University of California San Francisco, demonstrated in a pre-test and post-test design with four training cases, that cognitive learning was similar in online virtual patients as with those same cases programmed into a patient simulator. A significant difference between these two types of simulations is the lack of psychomotor interactions of the online users; using the mouse and keyboard for executing one’s actions produces slightly less immersion during practice sessions, compared with the physical patient simulators.</p><h3 class="c-article__sub-heading" id="Sec46">Beginning Anew with OLIVE—VED II</h3><p>The very favorable results of this 2003–2004 project utilizing “bare-bones” physiology provided the academic impetus for SUMMIT’s continuing studies of online virtual cases. The practical impetus came after Forterra Systems, Inc., a video game company in San Mateo, near by Stanford University, identified SUMMIT’s work with virtual medical worlds, and connected with us, providing opportunity to extend our physiology models via a successful, TATRC-sponsored SBIR application (SBIR FY04.3 A04-182 Phases II &amp; III Contract W81XWH-05-C-0040). The title of the SBIR was Medical Simulation Training for First Response to Chemical, Biological, Radiological, and Nuclear Events. The Forterra game platform, OnLine, Interactive, Virtual Environment (OLIVE), is a sophisticated system that provides the needed security needed now for addressing medical issues without interruption of onlookers, sometimes with disruptive behaviors. This type of anonymous individual is called a “griefer”. As simulations become more robust and applicable to deeper levels of medical simulation. OLIVE has been the platform preferred by the military, and an increasing number of corporations such as IBM, Cisco, and BP that are developing virtual worlds for communication and training.</p><p>In the VED II system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0099-5#Fig11">11</a>), each virtual victim has unique gender, age, severity of injury, and rate of hemorrhage modifiers that produce unique models that operate simultaneously. For example, the model specifies that all victims will begin having seizures if and when the systolic blood pressure reaches 200 mmHg. The role player activates an animation of an avatar convulsing, and continues that action until hearing a caregiver’s report to the team that the drug <i>diazepam</i> has been administered; the animation is then stopped by the role player, allowing the victim avatar to lie <i>still</i>, or to respond to other scripted actions (<a href="http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1">http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0099-5/MediaObjects/10055_2008_99_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>A virtual Emergency Department scene constructed by artists at Forterra Systems, Inc., based upon photographs of the Stanford University Medical Center’s Emergency Department. A virtual patient lies in the bed while a virtual caregiver reads the status report, views the patient’s appearance, and observes the vital signs from the monitor. The controls in the lower left allow the user to make queries about the patient’s clinical condition, introduce remedial actions, administer fluid resuscitation measures, and select appropriate medications</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0099-5/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec47">A new challenge: “Families” of virtual patients for MOS</h3><p>From the earlier genre of MMORPG’s (Massively, Multiple, Online, Role-playing Games) originally designed as social environments with the capacity for hundreds or even thousands of players simultaneously, has emerged the generic acronym MOS, acronym for Multi-player Online Simulation. Among the many video games under this banner, a sub-set is labeled Serious Games as non-entertainment video game venues, and a smaller segment is dubbed Serious Virtual Worlds. In all of these virtual worlds, each avatar is the in-world fictional representative of a real person who plays an assigned team role, or in personal “play,” whatever role they choose. Play-acting is akin to dramatic expression, and each virtual victim requires a role-player to provide the accompanying emotive behaviors, audible sounds, and some actions. In this latest group of emerging learning environments, requiring the representations of many patient victims simultaneously, we offer a new example, a “family” of virtual patients that has shared a common experience, but with unique injuries for each. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0099-5#Tab2">2</a> presents examples for five such victims.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Five patient profiles for in-hospital response (Dirty Bomb Scenario II)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-008-0099-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>For simulations of mass casualty events, as may occur after exposure either to bomb explosions, to nerve toxins or other catastrophes such as earthquakes, airplane or train crashes, etc., multiple virtual patients are required to provide authentic practice. In these emergency situations, clinical conditions deteriorate rapidly, necessitating efficient and accurate “life-saving” diagnosis and treatment in a virtual emergency department. To meet the need for training healthcare providers for management of such events, unthinkable before 9/11, we devised an effective method of developing patho-physiological models. We offer the method of organization that leads to programming in an expert-friendly, rule-based approach for creating virtual patient cases with temporal evolution of selected clinical conditions. We are now able to report on our first two models, asphyxial shock and traumatic, hemorrhagic, hypovolemic shock. These models have in formative research, provided realistic, clinically relevant responses to user interventions that are unanimously satisfactory to experienced clinicians.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec48">Models for CBRNE training</h4><p>The Forterra/SUMMIT collaboration developed two potential training approaches; a pre-hospital, and an in-hospital simulation. The former focused on training first responders to a mass casualty. The trainees would be firemen, police, and emergency medical technicians. We decided to select for further development the in-hospital simulation that would address the training needs of physicians, nurses, para-professionals, and hospital administrators. And we chose the chemical, sarin, a nerve toxin agent as the causal agent in one of the CBRNE mass disaster incidents. The other simulation was for a radioactive bomb blast.</p><p>The patho-physiological model for sarin toxicity was driven by the blood oxygenation deficit described in an “asphyxia” model cascade; decreased SaO<sub>2</sub> results in increased respiratory rates, falling blood pressure from hypoxia, with tachycardia as the consequence. This sarin model accommodated one null and six exposure level doses of a nerve toxin by two routes, inhalation and/or dermal. It incorporated response data of SaO<sub>2</sub>, respiratory rate, blood pressure, and pulse rate. These clinical indicators (vital signs) responded appropriately to the different doses in a time-limited manner if treated with O<sub>2</sub>, atropine, 2-PAM, and/or benzodiazepine. For realism and creating interesting scenarios, the “asphyxia” model also afforded modifications for co-morbidities such as asthma, obesity, heart disease, diabetes, and heavy cigarette smoking, as well as for age (adults) and gender (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0099-5#Tab3">3</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Rules that comprise the asphyxial model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-008-0099-5/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The SUMMIT trauma model was more complicated. It incorporated four summary scales used routinely by clinicians during trauma assessments; a severity scale, a score for different organs, a scale that includes the standard Glasgow Coma Score for head injuries, and the standard ACS hypovolemia stages due to hemorrhage. The physiological cascade is hypovolemia from hemorrhage, prompting hypotension, tachycardia, hypoxia, and tachypnea (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0099-5#Tab4">4</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 An example of the rule for the relationship between hemorrhage and vital signs states</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-008-0099-5/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec49">HFSM-based modeling</h4><p>Numerous methods are available to implement dynamic physiology models. Models have been developed based on detailed mathematical equations linking vital signs to detailed internal physiologic models. The metabolic models by Entelos (<a href="http://www.entelos.com/virtualPatients.php">http://www.entelos.com/virtualPatients.php</a>) are an example of the richness and complexity of such models. However, these complex models cannot be run in real-time for user interaction in a virtual world. We have chosen to implement fairly simple dynamic models, as described above (Sects. B.5, B.5.1). We use a computational method known as Hierarchical Finite State Machines where we represent a series of physiological states, and allow the patient to transition between these states based on the severity of their injuries and on the interventions taken by the caregivers.</p><p>One of the challenges of enabling multiple instances of virtual patients to operate simultaneously and in real-time in the simulation, is reducing the complexity of the simulations. We have successfully used the HFSM method to operate the patho-physiology models of two types, one for asphyxial shock, and the other, more complex, of hemorrhagic hypovolemic shock. For each type of shock, ten virtual patient victims are delivered into the VED, where they receive resuscitative care by three or four physicians and six or seven experienced RN’s. Unlike real-world Incident Command practice sessions in which patho-physiology cannot for ethical reasons be implemented in the role-players, the vital signs of injured victims in virtual worlds deteriorate continuously and at selected rates unless timely and appropriate remediation is introduced. Although virtual victims can be studied individually for practicing pre-surgical management of trauma victims, the affordance of multiple unique victims functioning simultaneously allows training in patient triage, critical care management, team communication, resource management, etc.</p><h3 class="c-article__sub-heading" id="Sec50">Results of formative research</h3><p>Ten physicians with an average of 4 years of post-training experience, and 12 nurses with an average of 9.5 years of ED experience at Stanford University Medical Center, and San Mateo County Hospital provided the data that follows. An exit questionnaire constructed as a Likert Scale was the test instrument. The study had IRB approval, and the participants were compensated with stipends. The majority never plays videogames, and a few individuals play only occasionally. All individuals were given an orientation to the game interface, and opportunity to practice using it. Two-thirds had prior <i>Code Triage</i> training, and one-half had prior CBRNE training. One group practiced with both trauma and sarin scenarios, and the other group played the sarin scenarios twice.</p><p>Among these trainees, 68 percent felt that they were immersed in the virtual world much or all of the time. Only three individuals found it difficult to communicate with colleagues in-world. Prior to the training, only four trainees were confident about managing mass casualty incidents of these types, but after training, nineteen felt either “confident” or “very confident”: thirteen attributed this change to practicing in the VED. Not surprisingly, 21 of the trainees reported that the scenarios were useful for improving healthcare team skills training, the primary objective for creating them. But a surprising response was that eighteen trainees believed that the cases were also instructive in learning about clinical skills management of such incidents.</p><p>We interpret that training healthcare teams in virtual worlds with dynamic virtual patients is an effective method of training, at least for uncommonly occurring incidents. Comments of trainees indicated their expectation that persons with videogames experience would be even more likely to find this method of training to be attractive.</p><h3 class="c-article__sub-heading" id="Sec51">Developing virtual patients for various acute clinical scenarios</h3><p>Virtual patients are designed empirically with unique histories (or back-stories, in gaming vocabulary), each with gender and age, different BMI’s, disease profiles, vital signs, and drug therapy lists (Dev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007a" title="Dev P, Heinrichs WL, Youngblood P, Kung S, Cheng R, Kusumoto L, Hendrick A (2007a) Virtual patient model for multi-person virtual medical environments. AMIA, Bethesda" href="/article/10.1007/s10055-008-0099-5#ref-CR4" id="ref-link-section-d84052e2650">2007a</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007b" title="Dev P, Youngblood P, Heinrichs WL, Kusumoto L (2007b) Virtual worlds &amp; team training. Anesthesiol Clin 25:321–336" href="/article/10.1007/s10055-008-0099-5#ref-CR5" id="ref-link-section-d84052e2653">2007b</a>). Selecting among these patient “features” from a database of real-patients allows an instructor to design hundreds of virtual individuals by choosing among four to six variables, based upon learning goals. For example, virtual obese patients express population-based risk factors for diabetes mellitus and hypertension; add a genetic risk factor, and the probability increases significantly that the physiology model will randomly exhibit those complications. For external actions such as exposure to an environmental chemical, the immediate clinical condition reflects a “dose” and route of administration of a toxic agent, or proximity to a blast, all in relative, but arbitrary terms because of the absence of experimental data from humans. Changes in vital signs are approximations based upon clinical experience and “expert knowledge” except when experimental data are available to guide development.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec52">Developing patho-physiological models for sub-acute clinical conditions</h4><p>In work-in-progress, patho-physiological models are being developed for sub-acute conditions, such as sepsis, that evolve over days to weeks, for diseases such as SARS, pandemic flu, cholera and plague. For human pregnancy, that evolves over months, avatars can be assigned the physiological attributes of the different weeks of gestation. Adding the “weeks of gestation” variable to the physiology model allows for the creation of dozens of unique virtual pregnancy avatars with different physiques, basal metabolic indicators, and hematocrits. These attributes are associated with different plasma volumes, red-cell mass, blood volumes, and cardiac stroke volume and output values, all calculated from published databases. Pregnancies that terminate abnormally in the first trimester are subject to additional maternal complications such as hemorrhage that can be chronicled with the physiologic changes appropriate for early pregnancy. Those that proceed into the second and third trimesters require both maternal and fetal physiological systems. These physiologic models are in development.</p></div></div></section><section aria-labelledby="Sec53"><div class="c-article-section" id="Sec53-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec53">Conclusion</h2><div class="c-article-section__content" id="Sec53-content"><p>Since the Satava award in 2002, our research group has developed two threads of research, the software framework and components needed when students in multiple locations collaborate using computation-intensive simulations and large image datasets, and online learning environments that support immersive experiential learning in multi-person online virtual worlds. These collaborative, immersive learning environments are able to support the learners of the future such as medical students or community workers coming together online from scattered rural locations, student groups learning medical protocols in authentic immersive settings, professionals practicing new procedures without leaving their site of work, and teams learning to rehearse and work together before disasters or catastrophes strike. Our experiments and evaluations show that many of these environments have significant technical and logistic improvements needed, and yet, even in this nascent state, they have been proven to be useful and effective learning environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MB. Anderson, PL. Stillman, Y. Wang, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Anderson MB, Stillman PL, Wang Y (1994) Growing use of standardized patients in teaching and evaluation in med" /><p class="c-article-references__text" id="ref-CR1">Anderson MB, Stillman PL, Wang Y (1994) Growing use of standardized patients in teaching and evaluation in medical education. Teach Learn Med 6(1):15–22</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10401339409539637" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Growing%20use%20of%20standardized%20patients%20in%20teaching%20and%20evaluation%20in%20medical%20education&amp;journal=Teach%20Learn%20Med&amp;volume=6&amp;issue=1&amp;pages=15-22&amp;publication_year=1994&amp;author=Anderson%2CMB&amp;author=Stillman%2CPL&amp;author=Wang%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Bergin, UGH. Fors, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Bergin RA, Fors UGH (2003) Interactive simulated patient—an advanced tool for student-activated learning in me" /><p class="c-article-references__text" id="ref-CR2">Bergin RA, Fors UGH (2003) Interactive simulated patient—an advanced tool for student-activated learning in medicine and healthcare. Comput Educ 40:361–76</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0360-1315%2802%2900167-7" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20simulated%20patient%E2%80%94an%20advanced%20tool%20for%20student-activated%20learning%20in%20medicine%20and%20healthcare&amp;journal=Comput%20Educ&amp;volume=40&amp;pages=361-76&amp;publication_year=2003&amp;author=Bergin%2CRA&amp;author=Fors%2CUGH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Dev, S. Srivastava, S. Senger, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Dev P, Srivastava S, Senger S (2006) Collaborative learning using Internet2 and remote collections of stereo d" /><p class="c-article-references__text" id="ref-CR3">Dev P, Srivastava S, Senger S (2006) Collaborative learning using Internet2 and remote collections of stereo dissection images. Clin Anat 19(3):275–283</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fca.20313" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20learning%20using%20Internet2%20and%20remote%20collections%20of%20stereo%20dissection%20images&amp;journal=Clin%20Anat&amp;volume=19&amp;issue=3&amp;pages=275-283&amp;publication_year=2006&amp;author=Dev%2CP&amp;author=Srivastava%2CS&amp;author=Senger%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Dev, WL. Heinrichs, P. Youngblood, S. Kung, R. Cheng, L. Kusumoto, A. Hendrick, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Dev P, Heinrichs WL, Youngblood P, Kung S, Cheng R, Kusumoto L, Hendrick A (2007a) Virtual patient model for m" /><p class="c-article-references__text" id="ref-CR4">Dev P, Heinrichs WL, Youngblood P, Kung S, Cheng R, Kusumoto L, Hendrick A (2007a) Virtual patient model for multi-person virtual medical environments. AMIA, Bethesda</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20patient%20model%20for%20multi-person%20virtual%20medical%20environments&amp;publication_year=2007&amp;author=Dev%2CP&amp;author=Heinrichs%2CWL&amp;author=Youngblood%2CP&amp;author=Kung%2CS&amp;author=Cheng%2CR&amp;author=Kusumoto%2CL&amp;author=Hendrick%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Dev, P. Youngblood, WL. Heinrichs, L. Kusumoto, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Dev P, Youngblood P, Heinrichs WL, Kusumoto L (2007b) Virtual worlds &amp; team training. Anesthesiol Clin 25:321–" /><p class="c-article-references__text" id="ref-CR5">Dev P, Youngblood P, Heinrichs WL, Kusumoto L (2007b) Virtual worlds &amp; team training. Anesthesiol Clin 25:321–336</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.anclin.2007.03.001" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20worlds%20%26%20team%20training&amp;journal=Anesthesiol%20Clin&amp;volume=25&amp;pages=321-336&amp;publication_year=2007&amp;author=Dev%2CP&amp;author=Youngblood%2CP&amp;author=Heinrichs%2CWL&amp;author=Kusumoto%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ellaway R (2005) Modeling virtual patients and virtual cases. http://meld.medbiq.org/primers/virtual_patients_" /><p class="c-article-references__text" id="ref-CR6">Ellaway R (2005) Modeling virtual patients and virtual cases. <a href="http://meld.medbiq.org/primers/virtual_patients_cases_ellaway.htm">http://meld.medbiq.org/primers/virtual_patients_cases_ellaway.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Garfield, S. Paskin, JH. Philip, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Garfield JM, Paskin S, Philip JH (1989) An evaluation of the effectiveness of a computer simulation of anaesth" /><p class="c-article-references__text" id="ref-CR7">Garfield JM, Paskin S, Philip JH (1989) An evaluation of the effectiveness of a computer simulation of anaesthetic uptake and distribution as a teaching tool. Med Educ 23:457–462</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1365-2923.1989.tb00902.x" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20evaluation%20of%20the%20effectiveness%20of%20a%20computer%20simulation%20of%20anaesthetic%20uptake%20and%20distribution%20as%20a%20teaching%20tool&amp;journal=Med%20Educ&amp;volume=23&amp;pages=457-462&amp;publication_year=1989&amp;author=Garfield%2CJM&amp;author=Paskin%2CS&amp;author=Philip%2CJH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heinrichs WL, Kung S-Y, Dev P (2008a) Design and implementation of rule-based medical models: an in silico pat" /><p class="c-article-references__text" id="ref-CR8">Heinrichs WL, Kung S-Y, Dev P (2008a) Design and implementation of rule-based medical models: an in silico patho-physiological trauma model for hypovolemic shock. In: Proceedings, MMVR 2008, January 2008, IOS Press, Long Beach, pp 159–164</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heinrichs WL, Youngblood P, Harter PM, Dev P (2008b) Simulation for team training and assessment: case studies" /><p class="c-article-references__text" id="ref-CR9">Heinrichs WL, Youngblood P, Harter PM, Dev P (2008b) Simulation for team training and assessment: case studies of online training with virtual worlds. World J Surg Spec Issue 32:161–170. <a href="http://www.springerlink.com/content/v86175717562/?p=d92acbf4aabf4fea9187acdb5cc053b3&amp;pi">http://www.springerlink.com/content/v86175717562/?p=d92acbf4aabf4fea9187acdb5cc053b3&amp;pi</a>= 4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://labyrinth.mvm.ed.ac.uk/&#xA;                " /><p class="c-article-references__text" id="ref-CR10">
                  <a href="http://labyrinth.mvm.ed.ac.uk/">http://labyrinth.mvm.ed.ac.uk/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://research.bidmc.harvard.edu/VPTutorials/&#xA;                " /><p class="c-article-references__text" id="ref-CR11">
                  <a href="http://research.bidmc.harvard.edu/VPTutorials/">http://research.bidmc.harvard.edu/VPTutorials/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://tusk.tufts.edu/view/url/H1185C/471802/490012/&#xA;                " /><p class="c-article-references__text" id="ref-CR12">
                  <a href="http://tusk.tufts.edu/view/url/H1185C/471802/490012/">http://tusk.tufts.edu/view/url/H1185C/471802/490012/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://websp.lime.ki.se/about&#xA;                " /><p class="c-article-references__text" id="ref-CR13">
                  <a href="http://websp.lime.ki.se/about">http://websp.lime.ki.se/about</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.aamc.org/mededportal&#xA;                " /><p class="c-article-references__text" id="ref-CR14">
                  <a href="http://www.aamc.org/mededportal">http://www.aamc.org/mededportal</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.acssurgery.com/abstracts/acs/acs0104.htm&#xA;                " /><p class="c-article-references__text" id="ref-CR15">
                  <a href="http://www.acssurgery.com/abstracts/acs/acs0104.htm">http://www.acssurgery.com/abstracts/acs/acs0104.htm</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.entelos.com/virtualPatients.php&#xA;                " /><p class="c-article-references__text" id="ref-CR16">
                  <a href="http://www.entelos.com/virtualPatients.php">http://www.entelos.com/virtualPatients.php</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;" /><p class="c-article-references__text" id="ref-CR17">
                  <a href="http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;hl=en&amp;q=human+physiology&amp;btnG=Google+Search">http://www.google.com/search?client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;channel=s&amp;hl=en&amp;q=human+physiology&amp;btnG=Google+Search</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.netmedicine.com/cyberpt/cyberptframe.htm&#xA;                " /><p class="c-article-references__text" id="ref-CR18">
                  <a href="http://www.netmedicine.com/cyberpt/cyberptframe.htm">http://www.netmedicine.com/cyberpt/cyberptframe.htm</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.tinkering.net/vp/&#xA;                " /><p class="c-article-references__text" id="ref-CR19">
                  <a href="http://www.tinkering.net/vp/">http://www.tinkering.net/vp/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.virtualpatients.net/&#xA;                " /><p class="c-article-references__text" id="ref-CR20">
                  <a href="http://www.virtualpatients.net/">http://www.virtualpatients.net/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;                  http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1&#xA;                " /><p class="c-article-references__text" id="ref-CR21">
                  <a href="http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1">http://www.youtube.com/watch?v=RwQlHNlpVcE&amp;NR=1</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rooke, H. Schwid, Y. Shapira, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Rooke A, Schwid H, Shapira Y (1995) The effect of graded hemorrhage and intravascular volume replacement on sy" /><p class="c-article-references__text" id="ref-CR22">Rooke A, Schwid H, Shapira Y (1995) The effect of graded hemorrhage and intravascular volume replacement on systolic pressure variation in humans during mechanical and spontaneous ventilation. Anesth Analg 80:925–932</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F00000539-199505000-00012" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20graded%20hemorrhage%20and%20intravascular%20volume%20replacement%20on%20systolic%20pressure%20variation%20in%20humans%20during%20mechanical%20and%20spontaneous%20ventilation&amp;journal=Anesth%20Analg&amp;volume=80&amp;pages=925-932&amp;publication_year=1995&amp;author=Rooke%2CA&amp;author=Schwid%2CH&amp;author=Shapira%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SBIR FY04.3 A04-182 Phases II &amp; III Contract W81XWH-05-C-0040" /><p class="c-article-references__text" id="ref-CR23">SBIR FY04.3 A04-182 Phases II &amp; III Contract W81XWH-05-C-0040</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WA. Villaume, BA. Berger, BN. Barker, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Villaume WA, Berger BA, Barker BN (2006) Learning motivational interviewing: scripting a virtual patient. Am J" /><p class="c-article-references__text" id="ref-CR24">Villaume WA, Berger BA, Barker BN (2006) Learning motivational interviewing: scripting a virtual patient. Am J Pharm Educ 70(2):33. <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1636931">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1636931</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20motivational%20interviewing%3A%20scripting%20a%20virtual%20patient&amp;journal=Am%20J%20Pharm%20Educ&amp;volume=70&amp;issue=2&amp;publication_year=2006&amp;author=Villaume%2CWA&amp;author=Berger%2CBA&amp;author=Barker%2CBN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Youngblood P, Harter P, Srivastava S, Wallin C-J, Fellander-Tsai L, Moffet S, Heinrichs WL (2008) Design, deve" /><p class="c-article-references__text" id="ref-CR25">Youngblood P, Harter P, Srivastava S, Wallin C-J, Fellander-Tsai L, Moffet S, Heinrichs WL (2008) Design, development and evaluation of an online virtual emergency department for training trauma teams. J Sim Med (in press)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-008-0099-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The SUMMIT team that developed the information in the first thread of this report was comprised of the authors, and numerous collaborators: They were Ken Waldron, Patricia Youngblood, Sakti Srivastava, Margaret Krebs, Craig Cornelius, Sean Kung, and Robert Cheng at Stanford University; Steven Senger at University of Wisconsin-La Crosse; Brian Athey, Ameed Raoof, Alex Terzian and Ted Hanss at University of Michigan; Donna Newhouse, David Topps, Mike Korolenko and Kevin Smith at Northern Ontario School of Medicine; Duncan Stevenson and Chris Gunn at CSIRO, Canberra, Australia; Min Suk Chung at Ajou University, Suwon, Korea, and Young Sung Lee at Chungbuk National University, Chungbuk, Korea. The program was partially funded by the National Library of Medicine, contract number N01-LM-3-3512, with Michael Ackerman, Director of the High Performance Computing and Communication, as the NLM program manager of the Scalable Information Infrastructure and the Next Generation Internet programs. The second thread of the report was the work of the authors as well as Patricia Youngblood, Sakti Srivastava, Sean Kung, Robert Cheng, and Kingsley Willis at Stanford University, and Li Fellander-Tsai and Carl-Johan Wallin at Karolinska Institute in Stockholm. The Emergency Medicine consultants were Phillip Harter and Eric A. Weiss at Stanford University, amd Michael Aratow from San Mateo Medical Center. The Forterra Systems, Inc. team was comprised of Robert Gehorsam, President; Laura Kusumoto, VP Studios; Arnold Hendrick, Project Director; Karen Laur, Art Design; Evan Driscoll, VP Engineering; Steve Hansted, Producer. This research was funded through multiple programs from USAMRMC’s TATRC organization (Harvey Magee, program manager), the Wallenberg Foundation, and Adobe Systems Inc. These individuals and groups provided extraordinary leadership and effort that made these projects successful. To them, we are eternally grateful. The subjects in the various studies were medical students from Stanford University, University of California San Francisco, University of Michigan and Karolinska Institute in Stockholm, as well as residents and nurses from Stanford University and San Mateo Medical Center. Participants in the videoconference-mediated learning sessions were physicians and residents from many of the sites listed above.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Innovation in Learning, Inc, Los Altos Hills, CA, USA</p><p class="c-article-author-affiliation__authors-list">Parvati Dev &amp; Wm. LeRoy Heinrichs</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">SUMMIT and Department of Obstetrics/Gynaecology, Stanford University School of Medicine, Stanford, CA, USA</p><p class="c-article-author-affiliation__authors-list">Wm. LeRoy Heinrichs</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Parvati-Dev"><span class="c-article-authors-search__title u-h3 js-search-name">Parvati Dev</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Parvati+Dev&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Parvati+Dev" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Parvati+Dev%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Wm__LeRoy-Heinrichs"><span class="c-article-authors-search__title u-h3 js-search-name">Wm. LeRoy Heinrichs</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Wm. LeRoy+Heinrichs&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wm. LeRoy+Heinrichs" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wm. LeRoy+Heinrichs%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-008-0099-5/email/correspondent/c1/new">Parvati Dev</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Learning%20medicine%20through%20collaboration%20and%20action%3A%20collaborative%2C%20experiential%2C%20networked%20learning%20environments&amp;author=Parvati%20Dev%20et%20al&amp;contentID=10.1007%2Fs10055-008-0099-5&amp;publication=1359-4338&amp;publicationDate=2008-10-28&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Dev, P., Heinrichs, W.L. Learning medicine through collaboration and action: collaborative, experiential, networked learning environments.
                    <i>Virtual Reality</i> <b>12, </b>215–234 (2008). https://doi.org/10.1007/s10055-008-0099-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-008-0099-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-09-08">08 September 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-09-09">09 September 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-10-28">28 October 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-12">December 2008</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-008-0099-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-008-0099-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human anatomy</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human physiology</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Online</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Distance learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual patients</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual physiology models</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual worlds</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Stereo anatomy</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0099-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=99;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

