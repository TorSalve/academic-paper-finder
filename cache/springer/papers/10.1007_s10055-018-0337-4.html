<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An evaluation of multimodal interaction techniques for 3D layout const"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our method uses an open-source constraint solver..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment"/>

    <meta name="dc.source" content="Virtual Reality 2018 22:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-02-24"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our method uses an open-source constraint solver integrated in a popular 3D game engine. We designed multimodal interaction techniques for the system, based on gesture and voice input. We conducted a user study with an interactive task of laying out room furniture to compare and evaluate the mono- and multimodal interaction techniques. Results showed that voice command provided the best performance and was most preferred by participants, based on the analysis of both objective and subjective data. Results also revealed that there was no significant difference between the voice and multimodal input (voice and gesture). Our original approach opens the way to multidisciplinary theoretical work and promotes the development of high-level applications for the VR applications."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-02-24"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="339"/>

    <meta name="prism.endingPage" content="351"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0337-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0337-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0337-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0337-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2018/11"/>

    <meta name="citation_online_date" content="2018/02/24"/>

    <meta name="citation_firstpage" content="339"/>

    <meta name="citation_lastpage" content="351"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0337-4"/>

    <meta name="DOI" content="10.1007/s10055-018-0337-4"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0337-4"/>

    <meta name="description" content="We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our meth"/>

    <meta name="dc.creator" content="Marounene Kefi"/>

    <meta name="dc.creator" content="Thuong N. Hoang"/>

    <meta name="dc.creator" content="Paul Richard"/>

    <meta name="dc.creator" content="Eulalie Verhulst"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Azenkot S, Lee NB (2013) Exploring the use of speech input by blind people on mobile devices. In: Proceedings of the 15th international ACM SIGACCESS conference on computers and accessibility. ACM, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=SIGGRAPH Comput. Graph.; citation_title=Put that where? Voice and gesture at the graphics interface; citation_author=M Billinghurst; citation_volume=32; citation_issue=4; citation_publication_date=1998; citation_pages=60-63; citation_doi=10.1145/307710.307730; citation_id=CR2"/>

    <meta name="citation_reference" content="Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on user interface software and technology, pp 7&#8211;14"/>

    <meta name="citation_reference" content="citation_journal_title=J Comp Neurol; citation_title=Threedimensional immersive virtual reality for studying cellular compartments in 3D models from EM preparations of neural tissues; citation_author=C Cal, J Baghabra, DJ Boges, GR Holst, A Kreshuk, FA Hamprecht, PJ Magistretti; citation_volume=524; citation_issue=1; citation_publication_date=2016; citation_pages=23-38; citation_doi=10.1002/cne.23852; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Lect Notes Comput Sci; citation_title=A new approach to the interactive resolution of configuration problems in virtual environments; citation_author=C Calderon, M Cavazza, D Diaz; citation_volume=2733; citation_publication_date=2003; citation_pages=112-122; citation_doi=10.1007/3-540-37620-8_11; citation_id=CR6"/>

    <meta name="citation_reference" content="Chun LM, Arshad H, Piumsomboon T, Billinghurst M (2015) A combination of static and stroke gesture with speech for multimodal interaction in a virtual environment. In: 2015 International conference on electrical engineering and informatics (ICEEI). IEEE, pp 59&#8211;64"/>

    <meta name="citation_reference" content="citation_journal_title=Constraints; citation_title=CLPGUI: a generic graphical user interface for constraint logic programming; citation_author=F Fages, S Soliman, R Coolenand; citation_volume=9; citation_publication_date=2004; citation_pages=241-262; citation_doi=10.1023/B:CONS.0000049203.53383.c1; citation_id=CR9"/>

    <meta name="citation_reference" content="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 147&#8211;154"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=An approach to solid modeling in a semi-immersive virtual environment; citation_author=S Gao, H Wan, Q Peng; citation_volume=24; citation_publication_date=2000; citation_pages=191-202; citation_doi=10.1016/S0097-8493(99)00154-5; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=J Netw Comput Appl; citation_title=Voice interactive classroom, a service-oriented software architecture for speech-enabled learning; citation_author=VM Garcia, MP Ru&#237;z, JR P&#233;rez; citation_volume=33; citation_publication_date=2010; citation_pages=603-610; citation_doi=10.1016/j.jnca.2010.03.005; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Eur J Oper Res; citation_title=Constraint programming for LNG ship scheduling and inventory management; citation_author=V Goel, M Slusky, WJ Hoeve, KC Furman, Y Shao; citation_volume=241; citation_issue=3; citation_publication_date=2015; citation_pages=662-673; citation_doi=10.1016/j.ejor.2014.09.048; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_title=Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research; citation_inbook_title=Advances in Psychology; citation_publication_date=1988; citation_pages=139-183; citation_id=CR14; citation_author=Sandra G. Hart; citation_author=Lowell E. Staveland"/>

    <meta name="citation_reference" content="Honda K, Mizoguchi F (1995) Constraint-based approach for automatic spatial layout planning. Conference on artificial intelligence for applications. IEEE Press, Ne York"/>

    <meta name="citation_reference" content="ISO/IEC (2011) 25010:2011 Systems and software engineering&#8212;systems and software quality requirements and evaluation (SQuaRE)&#8212;system and software quality models, ISO"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Hum Interact; citation_title=Integrality and separability of input devices; citation_author=R Jacob, L Sibert, D MacFarlane, P Mullen; citation_volume=1; citation_publication_date=1994; citation_pages=3-26; citation_doi=10.1145/174630.174631; citation_id=CR19"/>

    <meta name="citation_reference" content="Jacquenot G (2009) M&#233;thode g&#233;n&#233;rique pour l&#8217;optimisation d&#8217;agencement g&#233;ometrique et fonctionnel. Th&#233;se de Doctorat, Ecole Centrale de Nantes"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Multimodal humancomputer interaction: a survey; citation_author=A Jaimes, N Sebe; citation_volume=108; citation_issue=1; citation_publication_date=2007; citation_pages=116-134; citation_doi=10.1016/j.cviu.2006.10.019; citation_id=CR21"/>

    <meta name="citation_reference" content="Kefi M, Barichard V, Richard P (2012) A constraint-solver based tool for user-assisted interactive 3D layout. In: ICTAI, pp 199&#8211;206"/>

    <meta name="citation_reference" content="citation_title=Ergonomics Design on Expert Convenience of Voice-Based Interface for Vehicle&#8217;s AV Systems; citation_inbook_title=Human-Computer Interaction. Applications and Services; citation_publication_date=2013; citation_pages=606-611; citation_id=CR25; citation_author=Pei-Ying Ku; citation_author=Sheue-Ling Hwang; citation_author=Hsin-Chang Chang; citation_author=Jian-Yung Hung; citation_author=Chih-Chung Kuo; citation_publisher=Springer Berlin Heidelberg"/>

    <meta name="citation_reference" content="citation_journal_title=Autonomous Robots; citation_title=Human-Robot Interaction Through Gesture-Free Spoken Dialogue; citation_author=Vladimir Kulyukin; citation_volume=16; citation_issue=3; citation_publication_date=2004; citation_pages=239-257; citation_doi=10.1023/B:AURO.0000025789.33843.6d; citation_id=CR26"/>

    <meta name="citation_reference" content="Laviola JJ Jr (1999) Whole-hand and speech input in virtual environments (Doctoral dissertation, Brown University)"/>

    <meta name="citation_reference" content="citation_journal_title=J Inf Technol Constr; citation_title=Constraint-based adaptation for complew space configuration in building services; citation_author=B Medjdoub; citation_volume=243; citation_publication_date=2004; citation_pages=627-636; citation_id=CR30"/>

    <meta name="citation_reference" content="Patrick E et al (2000) Using a large projection screen as an alternative to head-mounted displays for virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=A heuristic problem solving design system for equipment or furniture layouts; citation_author=C Pfefferkorn; citation_volume=18; citation_issue=5; citation_publication_date=1975; citation_pages=286-297; citation_doi=10.1145/360762.360817; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_title=Industrial robots programming: building applications for the factories of the future; citation_publication_date=2006; citation_id=CR34; citation_author=JN Pires; citation_publisher=Springer"/>

    <meta name="citation_reference" content="R&#233;gin JC (2004) Modlisation et Contraintes Globales en Programmation par Contraintes. Habilitation diriger des recherches, Universit de Nice"/>

    <meta name="citation_reference" content="citation_journal_title=Aerosp Sci Technol; citation_title=A concept of voice guided general aviation aircraft; citation_author=T Rogalski, R Wielgat; citation_volume=14; citation_publication_date=2010; citation_pages=321-328; citation_doi=10.1016/j.ast.2010.02.006; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Comput Integr Manuf; citation_title=A industrially oriented voice control system; citation_author=A Rogowski; citation_volume=28; citation_publication_date=2012; citation_pages=303-315; citation_doi=10.1016/j.rcim.2011.09.010; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Comput Integr Manuf; citation_title=Web-based remote voice control of robotized cells; citation_author=A Rogowski; citation_volume=4; citation_publication_date=2012; citation_pages=77-89; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_title=Handbook of constraint programming; citation_publication_date=2006; citation_id=CR39; citation_publisher=Elsevier"/>

    <meta name="citation_reference" content="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA"/>

    <meta name="citation_reference" content="Schulte C, Tack G, Lagerkvist M (2013) Modeling with Gecode"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Virtual reality for assembly methods prototyping: a review; citation_author=A Seth, JM Vance, JH Oliver; citation_volume=15; citation_issue=1; citation_publication_date=2011; citation_pages=5-20; citation_doi=10.1007/s10055-009-0153-y; citation_id=CR42"/>

    <meta name="citation_reference" content="Tim T, Rafael B, Ruben MS, Klaas JK (2009) Rule-based layout solving and its application to procedural interior generation. In: Proceedings of the CASA workshop on 3D advanced media in gaming and simulation (3AMIGAS), pp 212&#8211;227"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn Lett; citation_title=Multimodal interaction: a review; citation_author=M Turk; citation_volume=36; citation_publication_date=2014; citation_pages=189-195; citation_doi=10.1016/j.patrec.2013.07.003; citation_id=CR44"/>

    <meta name="citation_reference" content="Unity 3D Game Engine (2013) 
                    http://Unity3d.com
                    
                  
                        "/>

    <meta name="citation_reference" content="Vacher M, Lecouteux B, Istrate D, Joubert T, Portet F et al (2013) Experimental evaluation of speech recognition technologies for voice-based home automation control in a smart home. In: 4th Workshop on speech and language processing for assistive technologies, Grenoble, France, pp 99&#8211;105"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Ind; citation_title=An interactive environment for virtual manufacturing: the virtual workbench; citation_author=M Weyrich, P Drews; citation_volume=38; citation_publication_date=1999; citation_pages=5-15; citation_doi=10.1016/S0166-3615(98)00104-3; citation_id=CR47"/>

    <meta name="citation_reference" content="Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics interface proceedings, University of Calgary, pp 25&#8211;34"/>

    <meta name="citation_reference" content="Zhao W, Madhavan V (2005) Integration of voice commands into a virtual reality environment for assembly design. In: Proceedings of the 10th annual international conference on industrial engineering theory, applications &amp; practice, Clearwater Beach, FL, USA"/>

    <meta name="citation_author" content="Marounene Kefi"/>

    <meta name="citation_author_institution" content="LARIS, University of Angers, Angers, France"/>

    <meta name="citation_author" content="Thuong N. Hoang"/>

    <meta name="citation_author_email" content="thuong.hoang@deakin.edu.au"/>

    <meta name="citation_author_institution" content="School of Information Technology, Deakin University, Melbourne, Australia"/>

    <meta name="citation_author" content="Paul Richard"/>

    <meta name="citation_author_institution" content="LARIS, University of Angers, Angers, France"/>

    <meta name="citation_author" content="Eulalie Verhulst"/>

    <meta name="citation_author_institution" content="LARIS, University of Angers, Angers, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0337-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0337-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment"/>
        <meta property="og:description" content="We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our method uses an open-source constraint solver integrated in a popular 3D game engine. We designed multimodal interaction techniques for the system, based on gesture and voice input. We conducted a user study with an interactive task of laying out room furniture to compare and evaluate the mono- and multimodal interaction techniques. Results showed that voice command provided the best performance and was most preferred by participants, based on the analysis of both objective and subjective data. Results also revealed that there was no significant difference between the voice and multimodal input (voice and gesture). Our original approach opens the way to multidisciplinary theoretical work and promotes the development of high-level applications for the VR applications."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0337-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Interaction techniques, Constraint solver, 3D layout, Virtual environments, Multimodality","kwrd":["Interaction_techniques","Constraint_solver","3D_layout","Virtual_environments","Multimodality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0337-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0337-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=337;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0337-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0337-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0337-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-02-24" itemprop="datePublished">24 February 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marounene-Kefi" data-author-popup="auth-Marounene-Kefi">Marounene Kefi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Angers" /><meta itemprop="address" content="0000 0001 2248 3363, grid.7252.2, LARIS, University of Angers, Angers, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thuong_N_-Hoang" data-author-popup="auth-Thuong_N_-Hoang" data-corresp-id="c1">Thuong N. Hoang<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Deakin University" /><meta itemprop="address" content="0000 0001 0526 7079, grid.1021.2, School of Information Technology, Deakin University, Melbourne, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Paul-Richard" data-author-popup="auth-Paul-Richard">Paul Richard</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Angers" /><meta itemprop="address" content="0000 0001 2248 3363, grid.7252.2, LARIS, University of Angers, Angers, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Eulalie-Verhulst" data-author-popup="auth-Eulalie-Verhulst">Eulalie Verhulst</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Angers" /><meta itemprop="address" content="0000 0001 2248 3363, grid.7252.2, LARIS, University of Angers, Angers, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">339</span>–<span itemprop="pageEnd">351</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">382 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0337-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our method uses an open-source constraint solver integrated in a popular 3D game engine. We designed multimodal interaction techniques for the system, based on gesture and voice input. We conducted a user study with an interactive task of laying out room furniture to compare and evaluate the mono- and multimodal interaction techniques. Results showed that voice command provided the best performance and was most preferred by participants, based on the analysis of both objective and subjective data. Results also revealed that there was no significant difference between the voice and multimodal input (voice and gesture). Our original approach opens the way to multidisciplinary theoretical work and promotes the development of high-level applications for the VR applications.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Constraint programming (CP) is employed to model constraints, propose possible solutions of complex problems, and assist the user in the layout task. CP is often applied to solve constraint satisfaction problems (CSPs) and provides resolution methods and techniques for problems defined with discrete and continuous variables. CP has been successfully used in several areas such as planning, scheduling, and resources allocation (Régin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Régin JC (2004) Modlisation et Contraintes Globales en Programmation par Contraintes. Habilitation diriger des recherches, Universit de Nice" href="/article/10.1007/s10055-018-0337-4#ref-CR35" id="ref-link-section-d53257e363">2004</a>; Rossi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rossi F, Van Beek P, Walsh T (eds) (2006) Handbook of constraint programming. Elsevier, Amsterdam" href="/article/10.1007/s10055-018-0337-4#ref-CR39" id="ref-link-section-d53257e366">2006</a>; Goel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Goel V, Slusky M, van Hoeve WJ, Furman KC, Shao Y (2015) Constraint programming for LNG ship scheduling and inventory management. Eur J Oper Res 241(3):662–673" href="/article/10.1007/s10055-018-0337-4#ref-CR13" id="ref-link-section-d53257e369">2015</a>).</p><p>A layout problem generally involves the task of arranging a set of components, such as furniture or devices, while respecting geometrical and functional constraints. An example is the layout of electronic components with such functional constraints as heating, consumption, and electronic compatibility. The 3D layout problems are usually solved manually without any user’s assistance and using tools that are limited in terms of 3D visualization and interaction. Therefore, expertise of the designers is required to assess and validate the final design.</p><p>Virtual reality (VR) is a powerful tool for interactive visualization and immersion of the user in 3D virtual spaces (Cal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Cal C, Baghabra J, Boges DJ, Holst GR, Kreshuk A, Hamprecht FA, Magistretti PJ (2016) Three-dimensional immersive virtual reality for studying cellular compartments in 3D models from EM preparations of neural tissues. J Comp Neurol 524(1):23–38" href="/article/10.1007/s10055-018-0337-4#ref-CR5" id="ref-link-section-d53257e378">2016</a>; Seth et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Seth A, Vance JM, Oliver JH (2011) Virtual reality for assembly methods prototyping: a review. Virtual Real 15(1):5–20" href="/article/10.1007/s10055-018-0337-4#ref-CR42" id="ref-link-section-d53257e381">2011</a>). In the context of a layout problem, VR can be efficiently used to visualize the 3D space and to allow users to interact with the objects. However, VR systems do generally not provide user with assistance for 3D layout tasks and the objects have to be selected, picked, and placed manually, which is generally tedious.</p><p>In order to provide more intuitive and natural interaction techniques for selection and manipulation of virtual objects, multimodal approaches using speech and gestures have been proposed (Bolt and Herranz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on user interface software and technology, pp 7–14" href="/article/10.1007/s10055-018-0337-4#ref-CR3" id="ref-link-section-d53257e387">1992</a>; Zhao and Madhavan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Zhao W, Madhavan V (2005) Integration of voice commands into a virtual reality environment for assembly design. In: Proceedings of the 10th annual international conference on industrial engineering theory, applications &amp; practice, Clearwater Beach, FL, USA" href="/article/10.1007/s10055-018-0337-4#ref-CR50" id="ref-link-section-d53257e390">2005</a>; Chun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chun LM, Arshad H, Piumsomboon T, Billinghurst M (2015) A combination of static and stroke gesture with speech for multimodal interaction in a virtual environment. In: 2015 International conference on electrical engineering and informatics (ICEEI). IEEE, pp 59–64" href="/article/10.1007/s10055-018-0337-4#ref-CR7" id="ref-link-section-d53257e393">2015</a>). However, multimodal interaction is underexplored in the context of user assistance systems such as constraint solver.</p><p>In this paper, we propose a new approach to solve 3D layout problems by integrating desktop VR techniques and constraint programming. We aim toward the development of a decision-making system that proposes feasible configurations of 3D spaces given a set of objects and constraints. Our approach contains an intelligent module to model and solve constraint-based problems. The contribution of our paper is the empirical evaluation of multimodal interaction techniques in a virtual environment with digital user assistance compared to a single modal approach.</p><p>The remainder of the paper is organized as follows: The next section provides a survey of the related work concerning 3D layout problems and constraint solvers, interaction techniques, and multimodality. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0337-4#Sec5">3</a> presents an overview of the proposed system, including description of the supported constraints that can be used to layout 3D scenes. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0337-4#Sec10">4</a>, we describe our user experiment comparing two monomodal and multimodal interaction techniques. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0337-4#Sec12">5</a> concludes the paper and discusses directions for future work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">3D layout problems and constraint solvers</h3><p>Different approaches involving layout problems have been developed. For example, Xu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics interface proceedings, University of Calgary, pp 25–34" href="/article/10.1007/s10055-018-0337-4#ref-CR48" id="ref-link-section-d53257e424">2002</a>) addressed the combination of physics, semantics, and placement constraints for scene layout The authors proposed a new modeling technique where users can create scenes by specifying the number and distribution of each class of object to be included in the scene. In a similar way, Sanchez et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA" href="/article/10.1007/s10055-018-0337-4#ref-CR40" id="ref-link-section-d53257e427">2003</a>) presented a general-purpose constraint-based system for non-isothetic 3D-object layout built on a genetic algorithm. This system is able to process a complex set of constraints, including geometric and pseudo-physics ones. The paper focuses mainly on the algorithmic contribution with textual constraints, without the ability for interactive user intervention.</p><p>
Calderon et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. Lect Notes Comput Sci 2733:112–122" href="/article/10.1007/s10055-018-0337-4#ref-CR6" id="ref-link-section-d53257e433">2003</a>) presented a novel framework for the use of VEs in interactive problem solving. This framework extends visualization to serve as a natural interface for the exploration of configuration space and enables the implementation of reactive VEs. The implementation is based on a fully interactive mechanism where both visualization and the generation of a new solution are under the control of the user. Tutenel et al. introduced a novel rule-based layout solving approach which is suited for use in conjunction with procedural generation methods (Tim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Tim T, Rafael B, Ruben MS, Klaas JK (2009) Rule-based layout solving and its application to procedural interior generation. In: Proceedings of the CASA workshop on 3D advanced media in gaming and simulation (3AMIGAS), pp 212–227" href="/article/10.1007/s10055-018-0337-4#ref-CR43" id="ref-link-section-d53257e436">2009</a>). Authors showed how this solving approach can be used for procedural generation by providing the solver with a user-defined plan. In this plan, users can specify objects to be placed as instances of classes, which in turn contain rules about how instances should be placed. Being procedurally generated, user intervention in the system is minimal. The initial user-defined plan is two-dimensional, which does not provide visual feedback for the layout solutions. There is no mechanism for the user to adjust the layout afterward.</p><p>Medjdoub proposed an interactive system for ceiling mounted fan coil system in a building ceiling void (Medjdoub <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Medjdoub B (2004) Constraint-based adaptation for complex space configuration in building services. J Inf Technol Constr 243:627–636" href="/article/10.1007/s10055-018-0337-4#ref-CR30" id="ref-link-section-d53257e442">2004</a>). He used a hybrid approach combining case-based reasoning and CP techniques. The system is used with interactive modification of the 3D parametric model. The author has shown the potential to significantly reduce design costs by reducing design time by 50%, improve the quality of the solution, and produce additional benefits elsewhere in the supply chain.</p><p>The user interaction in the approaches mentioned above lacks an intuitive and suitable interaction interface. They all focused on solving the layout problem without proper investigation of the interactivity between the user and the developed system. The use of other input modalities, such as speech interaction, has not been considered in previous approaches (Sanchez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA" href="/article/10.1007/s10055-018-0337-4#ref-CR40" id="ref-link-section-d53257e448">2003</a>; Calderon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. Lect Notes Comput Sci 2733:112–122" href="/article/10.1007/s10055-018-0337-4#ref-CR6" id="ref-link-section-d53257e451">2003</a>; Fages et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fages F, Soliman S, Coolenand R (2004) CLPGUI: a generic graphical user interface for constraint logic programming. Constraints 9:241–262" href="/article/10.1007/s10055-018-0337-4#ref-CR9" id="ref-link-section-d53257e454">2004</a>; Jacquenot <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Jacquenot G (2009) Méthode générique pour l’optimisation d’agencement géometrique et fonctionnel. Thése de Doctorat, Ecole Centrale de Nantes" href="/article/10.1007/s10055-018-0337-4#ref-CR20" id="ref-link-section-d53257e457">2009</a>). Our research investigates the use of voice command for 3D layout assistance in a virtual environment.</p><h3 class="c-article__sub-heading" id="Sec4">Voice-based interaction</h3><p>In VR applications, the interaction technique and the proposed protocol must be appropriate for the task to be performed (Jacob et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Jacob R, Sibert L, MacFarlane D, Mullen P Jr (1994) Integrality and separability of input devices. Comput Hum Interact 1:3–26" href="/article/10.1007/s10055-018-0337-4#ref-CR19" id="ref-link-section-d53257e468">1994</a>) and utilize multiple modalities that is considered natural interaction for the user.</p><p>Voice-based interaction techniques were introduced for the first time by Bolt and Herranz (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on user interface software and technology, pp 7–14" href="/article/10.1007/s10055-018-0337-4#ref-CR3" id="ref-link-section-d53257e474">1992</a>) through the <i>Put-That-There</i> application. Voice-based interaction is mainly used in multimodal interfaces in combination with other interaction modalities (Turk <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Turk M (2014) Multimodal interaction: a review. Pattern Recogn Lett 36:189–195" href="/article/10.1007/s10055-018-0337-4#ref-CR44" id="ref-link-section-d53257e480">2014</a>).</p><p>
Kulyukin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kulyukin VA (2004) Human–robot interaction through gesture-free spoken dialogue" href="/article/10.1007/s10055-018-0337-4#ref-CR26" id="ref-link-section-d53257e486">2004</a>) presents both practical and scientific arguments for human-machine voice communication. Pires (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Pires JN (2006) Industrial robots programming: building applications for the factories of the future. Springer, Berlin" href="/article/10.1007/s10055-018-0337-4#ref-CR34" id="ref-link-section-d53257e489">2006</a>) outlines the usefulness of voice communication in industrial robotic cells, where humans and robots safely share the workspace.</p><p>Voice interface is one of the most natural and intuitive ways to facilitate interaction between human and machines (Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput. Graph. 32(4):60–63" href="/article/10.1007/s10055-018-0337-4#ref-CR2" id="ref-link-section-d53257e495">1998</a>). It has been embedded into virtual environments and CAD systems to support free-hand interaction and mobility, which are major advantages over classic interaction devices.</p><p>Research on voice-based interactive devices or interfaces has covered many application domains, including industrial applications, manufacturing, and education. Vacher et al. explored the usage of voice-based interaction for home automation (Vacher et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Vacher M, Lecouteux B, Istrate D, Joubert T, Portet F et al (2013) Experimental evaluation of speech recognition technologies for voice-based home automation control in a smart home. In: 4th Workshop on speech and language processing for assistive technologies, Grenoble, France, pp 99–105" href="/article/10.1007/s10055-018-0337-4#ref-CR46" id="ref-link-section-d53257e502">2013</a>). The system uses speech recognition to target a range of user population, including seniors, visually impaired users, and users with no special needs. Rogalski and Wielgat (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Rogalski T, Wielgat R (2010) A concept of voice guided general aviation aircraft. Aerosp Sci Technol 14:321–328" href="/article/10.1007/s10055-018-0337-4#ref-CR36" id="ref-link-section-d53257e505">2010</a>) detailed the realization of a speech recognition-based aircraft control system for general aviation aircraft. Through a set of voice commands, the proposed avionic system supports direct control of the flight of the aircraft. Ku et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ku P-Y et al (2013) Ergonomics design on expert convenience of voice-based interface for vehicles AV systems. In: Kurosu M (ed) Human–computer interaction. Applications and services: 15th international conference, HCI international 2013, Las Vegas, NV, USA, July 21–26, 2013, Proceedings, Part II. Springer, Berlin, pp 606–611" href="/article/10.1007/s10055-018-0337-4#ref-CR25" id="ref-link-section-d53257e508">2013</a>) developed a voice-based interaction for in-vehicle interface. Their study suggested that speech interaction modality reduces mental workload for drivers behind the wheels.</p><p>
Rogowski (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012a" title="Rogowski A (2012a) A industrially oriented voice control system. Robot Comput Integr Manuf 28:303–315" href="/article/10.1007/s10055-018-0337-4#ref-CR37" id="ref-link-section-d53257e514">2012a</a>) combined automatic speech recognition and web-based remote control to develop an effective tool for remote voice control of industrial robotized cells. The author discussed the prerequisite conditions to provide an effective and faultless voice communication.</p><p>In manufacturing, Weyrich and Drews (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Weyrich M, Drews P (1999) An interactive environment for virtual manufacturing: the virtual workbench. Comput Ind 38:5–15" href="/article/10.1007/s10055-018-0337-4#ref-CR47" id="ref-link-section-d53257e520">1999</a>) integrated speech recognition, synthesis, 6D pointing pens, and data gloves in their virtual manufacturing environment. The speech-based module was defined by a 50-word vocabulary. Garcia et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Garcia VM, Ruíz MP, Pérez JR (2010) Voice interactive classroom, a service-oriented software architecture for speech-enabled learning. J Netw Comput Appl 33:603–610" href="/article/10.1007/s10055-018-0337-4#ref-CR12" id="ref-link-section-d53257e523">2010</a>) introduced the “Voice Interactive Classroom,” a software solution that proposes a middleware approach to provide cross-platform multichannel access to internet-based learning. Their research showed that visual and auditory e-learning can be achieved by adapting visual-only learning into naturalistic voice dialogs. Using a service-oriented middleware, voice dialogs can easily be reused and integrated within a heterogeneous set of e-learning platforms.</p><p>Voice-based interfaces are also found in modeling applications. Gao et al combine voice-based command and direct 3D manipulations using 3D mouse and menu in a semi-immersive environment (Gao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Gao S, Wan H, Peng Q (2000) An approach to solid modeling in a semi-immersive virtual environment. Comput Graph 24:191–202" href="/article/10.1007/s10055-018-0337-4#ref-CR11" id="ref-link-section-d53257e529">2000</a>). A voice command module was used to activate different 3D actions, such as translation and scaling. Bolt and Herranz employ a combination of voice input, bimanual gesture, and eye tracking approaches to manipulate 3D solids (Bolt and Herranz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on user interface software and technology, pp 7–14" href="/article/10.1007/s10055-018-0337-4#ref-CR3" id="ref-link-section-d53257e532">1992</a>). Object manipulations are supported through simple expressions like “turn the block” to activate the rotation of the object that the eyes are looking at.</p><p>In many application domains, especially for industrial systems, a high level of accuracy from voice command is required. A poor speech recognition accuracy can lead to erroneous actions to be performed by robots and/or machines. Rogowski (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012b" title="Rogowski A (2012b) Web-based remote voice control of robotized cells. Robot Comput Integr Manuf 4:77–89" href="/article/10.1007/s10055-018-0337-4#ref-CR38" id="ref-link-section-d53257e538">2012b</a>) defined a set of requirements to ensure reliability and effectiveness of speech communication.</p><p>There are some limitations to voice communication. Users are required to familiarize themselves with the voice commands that must be correctly uttered to activate a specific action. Therefore, voice-based interaction could impose a high cognitive load on users, limiting the freedom and ease of use of the system. Other practical issues include external noise and recognition latency (Laviola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Laviola JJ Jr (1999) Whole-hand and speech input in virtual environments (Doctoral dissertation, Brown University)" href="/article/10.1007/s10055-018-0337-4#ref-CR27" id="ref-link-section-d53257e544">1999</a>) or recognition error (Azenkot and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Azenkot S, Lee NB (2013) Exploring the use of speech input by blind people on mobile devices. In: Proceedings of the 15th international ACM SIGACCESS conference on computers and accessibility. ACM, pp 1–8" href="/article/10.1007/s10055-018-0337-4#ref-CR1" id="ref-link-section-d53257e547">2013</a>) that could cause further frustration for users. We aim to compare and evaluate voice command-based interactions in order to identify the more intuitive and effective control of our constraint solver-based system, especially with regard to cognitive load.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">System overview</h2><div class="c-article-section__content" id="Sec5-content"><p>We propose a real-time 3D environment developed using Unity3D game engine (version 3.2.0f4), commonly used for the development of 3D games and VEs (Unity <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Unity 3D Game Engine (2013) &#xA;                    http://Unity3d.com&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0337-4#ref-CR45" id="ref-link-section-d53257e559">2013</a>). The system supports data exchange with the open-sourced Gecode constraints solver (Schulte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Schulte C, Tack G, Lagerkvist M (2013) Modeling with Gecode" href="/article/10.1007/s10055-018-0337-4#ref-CR41" id="ref-link-section-d53257e562">2013</a>) using C# scripts.</p><h3 class="c-article__sub-heading" id="Sec6">Problem formulation</h3><p>The modeling of 3D layout problems requires an efficient and well-structured formalism. CP is particularly appropriate for the resolution of planning problems, as demonstrated by Honda and Mizoguchi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Honda K, Mizoguchi F (1995) Constraint-based approach for automatic spatial layout planning. Conference on artificial intelligence for applications. IEEE Press, Ne York" href="/article/10.1007/s10055-018-0337-4#ref-CR17" id="ref-link-section-d53257e572">1995</a>) and Pfefferkorn (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Pfefferkorn C (1975) A heuristic problem solving design system for equipment or furniture layouts. Commun ACM 18(5):286–297" href="/article/10.1007/s10055-018-0337-4#ref-CR32" id="ref-link-section-d53257e575">1975</a>). Moreover, CP techniques allow modeling with respect to physical constraints (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0337-4#Sec7">3.2</a>) for which the manual satisfaction without a solving engine is very difficult. We use CSP formalism (constraint satisfaction problem), which is a simple and formal framework for representing and solving a constraint satisfaction problem. In our problem domain, the unknowns are the 3D positions of objects (such as room furniture) and the layout constraints are relations or restrictions among the objects. The resolution of a such CSP consists in assigning values to the variables (unknowns) while satisfying all the constraints. Algorithms making it possible to solve a CSP are called constraints solvers. The process of solving a CSP is to specify the variables and constraints to the solver, which will provide the solutions to the user.</p><p>To formulate our problem in CSP form, we suppose that a virtual environment of certain dimensions (<i>w</i>, <i>h</i>, <i>d</i>) is composed of <i>n</i> objects related by <i>m</i> constraints. Let <i>X</i> be the set of unknowns of the problem (3D positions of objects), <span class="mathjax-tex">\({\mathcal {D}}\)</span> be a function that associates a domain (authorized values) to each variable (<span class="mathjax-tex">\(x_{i},y_{i},z_{i}\)</span>), and <i>C</i> be the set of layout constraints. Thus, the problem can be defined by the triplet (<span class="mathjax-tex">\(X,{\mathcal {D}},C\)</span>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p><span class="mathjax-tex">\(X=\{x_{1},y_{1},z_{1},\ldots ,x_{n},y_{n},z_{n}\}\)</span>, <span class="mathjax-tex">\((x_{i},y_{i},z_{i} /i\in {[1,n]})\)</span> position (center) of <span class="mathjax-tex">\(object_{i}\)</span></p>
                    </li>
                    <li>
                      <p><span class="mathjax-tex">\({\mathcal {D}}(x_{i})=[w_{i}/2,w - w_{i}/2], w_{i}/i\in {[1,n]}\)</span> is the width of <span class="mathjax-tex">\(object_{i}\)</span></p>
                      <p><span class="mathjax-tex">\({\mathcal {D}}(y_{i})=[h_{i}/2,h - h_{i}/2]\)</span>, <span class="mathjax-tex">\(h_{i}/i\in {[1,n]}\)</span> is the height of <span class="mathjax-tex">\(object_{i}\)</span></p>
                      <p><span class="mathjax-tex">\({\mathcal {D}}(z_{i})=[d_{i}/2,d - d_{i}/2], d_{i}/i\in {[1,n]}\)</span> is the depth of <span class="mathjax-tex">\(object_{i}\)</span></p>
                    </li>
                    <li>
                      <p><span class="mathjax-tex">\(C=\{c_{1}^{i,j},c_{2}^{i,j},\ldots ,c_{m}^{i,j}\}, c_{k}^{i,j}/(k\in {[1,m]}\)</span> and <span class="mathjax-tex">\(i,j\in {[1,n]})\)</span> is a constraint between <span class="mathjax-tex">\(object_{i}\)</span> and <span class="mathjax-tex">\(object_{j}\)</span>.</p>
                    </li>
                  </ul>
<h3 class="c-article__sub-heading" id="Sec7">Supported constraints</h3><p>In our approach, we developed specific constraints related to a layout problem proposed by an industrial partner. Physical constraints have been developed to take into account for the possible effect of certain objects on some others.</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Geometric constraints</b>
                      </p>
                      <p>-<i>No_overlapping constraint</i>: Objects should not overlap.</p>
                      <p>-<i>Min_distance constraint</i>: This constraint can be applied to at least two selected objects. The solver ensures that the selected objects are separated by a minimum distance which can be set by the user. The same principle is used for the <i>Max_distance and Fixed_distance constraints</i>.</p>
                      <p>-<i>Surround constraint</i>: The solver looks at positioning <i>n</i> objects around a central one (<span class="mathjax-tex">\(obj_{0}\)</span>: the first one selected) through an association of the <i>Min_distance</i> and <i>Max_distance constraints</i> (a default distance is set for each constraint): <span class="mathjax-tex">\(Max\_distance(obj_{0}, obj_{i}, \_dmax)\)</span> and <span class="mathjax-tex">\(Min\_distance(obj_{i}, obj_{j}, \_dmin) /i,j\in {[1,n-1]}\)</span></p>
                      <p>-<i>Right constraint</i>: It can be used to place the first selected object at the right of the second one. The same principle is used for <i>Left</i>, <i>Front</i> and <i>Back constraints</i>.</p>
                      <p>-<i>Object_on_object constraint</i>: It can be applied to two objects; the second selected object being placed on top of the first one.</p>
                    </li>
                    <li>
                      <p>
                        <b>Physical constraints</b>
                      </p>
                      <p>They are based on the laws of physics and have been developed to minimize the effect of electric and magnetic fields on electro- or magneto-sensitive objects. After adding an object in the 3D scene, the user should specify to what extend the added object emits an electric or magnetic field by entering the value of the electric charge <b><i>q</i></b>. For the magnetic field, the user should also specify the speed <b><i>v</i></b> of <b><i>q</i></b>.</p>
                    </li>
                  </ul>
<h3 class="c-article__sub-heading" id="Sec8">User interaction</h3><p>The user interacts with the resolution system by selecting 3D objects (such as chairs, desks, or sofas) via a menu (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig1">1</a>). Another menu allows them to select geometric and/or physical constraints to be applied to the selected objects. Each selection is a toggle button to indicate which constraints are currently being applied. At any time, the user can trigger the resolution of the layout problem. A CSP is created, in which the variables are the 3D positions of the centers of the objects and the constraints are those selected by the user. The solver calculates feasible configurations within a timeout period (5 s) to solve the CSP. The complexity of the problem is based on the number of objects and the nature of the constraints. Once the solving of the CSP is completed, the list of found solutions is transmitted to the VE, which updates the 3D object positions according to the new values of object centers. The user can step through the solutions on the list by pressing the left and right arrows on the interface. The system will reorganize the objects on the screen based on the current solution.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>User interface of the application: list of constraints on the left and the list of objects on the right. The virtual scene is in the center of the interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Contrary to some previous works (Sanchez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA" href="/article/10.1007/s10055-018-0337-4#ref-CR40" id="ref-link-section-d53257e2030">2003</a>; Xu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics interface proceedings, University of Calgary, pp 25–34" href="/article/10.1007/s10055-018-0337-4#ref-CR48" id="ref-link-section-d53257e2033">2002</a>; Fernando et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 147–154" href="/article/10.1007/s10055-018-0337-4#ref-CR10" id="ref-link-section-d53257e2036">1999</a>; Fages et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fages F, Soliman S, Coolenand R (2004) CLPGUI: a generic graphical user interface for constraint logic programming. Constraints 9:241–262" href="/article/10.1007/s10055-018-0337-4#ref-CR9" id="ref-link-section-d53257e2039">2004</a>), our system enables the user to intervene and change the proposed 3D configuration according to his/her preferences. Two cases are possible:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>The user can increase the problem size by inserting new 3D objects and selecting new constraints. In this case, the system creates a new CSP by adding new variables (positions of the new objects) and constraints. The solver deals this new CSP to find a new solution;</p>
                    </li>
                    <li>
                      <p>The user can manually modify the location of certain objects. In this case, the solver adds new constraints to the moved objects and updates the already existing CSP by setting the new 3D positions of the displaced object. If the CSP failed to find a solution, the system cancels the user action and replace the object in their last correct positions.</p>
                    </li>
                  </ul><p>Our system is based on a set of algorithms, which are defined and illustrated in Kefi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kefi M, Barichard V, Richard P (2012) A constraint-solver based tool for user-assisted interactive 3D layout. In: ICTAI, pp 199–206" href="/article/10.1007/s10055-018-0337-4#ref-CR23" id="ref-link-section-d53257e2058">2012</a>), to accomplish these functionalities. In this paper, we focus on the identification of a suitable interaction for the user to communicate with our system.</p><p>The communication between the user and the solver is completed through the VE. Each user action, such as objects manipulation and launch of resolution, generates an event in the form of input queries sent to the solver to update the current CSP. Calculations of the solver are transmitted to the VE to update or rearrange the 3D scene. The <i>Manager class</i> manages the creation, transmission, and execution of the events and queries.</p><p>Unity3D physics engine is used with collision detection of 3D objects to enhance realism. This application of a physics engine is to improve the user interaction and has no effect on the resolution mechanism.</p><h3 class="c-article__sub-heading" id="Sec9">User assistance</h3><p>Although previous works proposed interesting approaches to solve layout problems using a constraint solving engine, most of them (Sanchez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA" href="/article/10.1007/s10055-018-0337-4#ref-CR40" id="ref-link-section-d53257e2079">2003</a>; Xu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics interface proceedings, University of Calgary, pp 25–34" href="/article/10.1007/s10055-018-0337-4#ref-CR48" id="ref-link-section-d53257e2082">2002</a>; Fernando et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 147–154" href="/article/10.1007/s10055-018-0337-4#ref-CR10" id="ref-link-section-d53257e2085">1999</a>; Jacquenot <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Jacquenot G (2009) Méthode générique pour l’optimisation d’agencement géometrique et fonctionnel. Thése de Doctorat, Ecole Centrale de Nantes" href="/article/10.1007/s10055-018-0337-4#ref-CR20" id="ref-link-section-d53257e2088">2009</a>; Fages et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fages F, Soliman S, Coolenand R (2004) CLPGUI: a generic graphical user interface for constraint logic programming. Constraints 9:241–262" href="/article/10.1007/s10055-018-0337-4#ref-CR9" id="ref-link-section-d53257e2091">2004</a>) did not focus on user assistance during interaction with the 3D scene to be laid out. As described before, our system allows resolution of a 3D layout problem by proposing feasible configurations to the user. In addition, the system is able to cancel any manual object’s displacement if at least one constraint is not satisfied. In most approaches, the provided assistance generally occurs once the objects placement is achieved. In order to have a more efficient system, we decided to provide a predictive real-time visual assistance (similar to the <b><i>look ahead</i></b> mechanism) which occurs before the manual placement. The user selects a desired object <span class="mathjax-tex">\(obj_{i}\)</span> and enables the visualization of impossible 3D zones for this object. By splitting the 3D scene in many zones, the solver identifies and transmits the impossible ones to the VE for visualization in form of 3D red cubes. Impossible zones for <span class="mathjax-tex">\(obj_{i}\)</span> are the areas in which <span class="mathjax-tex">\(obj_{i}\)</span> cannot be placed because of constraints violation. More details of this user assistance are described in Kefi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kefi M, Barichard V, Richard P (2012) A constraint-solver based tool for user-assisted interactive 3D layout. In: ICTAI, pp 199–206" href="/article/10.1007/s10055-018-0337-4#ref-CR23" id="ref-link-section-d53257e2191">2012</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Impossible zone highlighted as red (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">User study</h2><div class="c-article-section__content" id="Sec10-content"><h3 class="c-article__sub-heading" id="Sec11">Motivation and objective</h3><p>The integration of 3D models in a VE allows simulating and assessing different layouts problems. A suitable hardware configuration consisting of interaction interfaces and devices is required to support user interaction. Through these interfaces and devices, the user is able to control the constraints solver and interact with virtual entities in a natural, fast, and efficient manner.</p><p>An interaction interface greatly influences the perception of the VE, which subsequently affects users performance in the layout task in terms of time and accuracy. Moreover, users’ appreciation and satisfaction of the proposed system are highly dependent on the relevance of the proposed interface. Therefore, we are interested in evaluating the interaction interfaces across a 3D layout task that required selection, manipulation, and pointing, using our proposed system.</p><p>The study investigates the usability and the effectiveness of the three interaction devices currently proposed to interact and control the resolution system: a classic interaction device using a standard mouse, a voice command interface, and a combined input modality of voice and mouse. We define usability as a subset of quality in use consisting of effectiveness, efficiency, and satisfaction by the users of the application (ISO/IEC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="ISO/IEC (2011) 25010:2011 Systems and software engineering—systems and software quality requirements and evaluation (SQuaRE)—system and software quality models, ISO" href="/article/10.1007/s10055-018-0337-4#ref-CR18" id="ref-link-section-d53257e2231">2011</a>).</p><p>The focus of our comparison is user performance in terms of task completion time, workload and their influence on the learning process, subjective users assessment, and user preference.</p><p>Our hypotheses are: H1. There is a measurable difference in task completion time among the three interaction mechanisms. H2. There is a measurable difference in work load among the three interaction mechanisms. H3. There is a measurable difference in user preference among the three interaction mechanisms.</p><p>The outcomes of this study will inform the design of the appropriate interaction technique that is intuitive and easy to use for 3D layout problems. We explored both monomodal and multimodal approaches. For monomodal interaction, we are interested in voice-based interaction for a natural and intuitive interface. The proposed experiment compares voice-based interaction with the mouse as the most common monomodal technique for desktop application. The mouse is also a representative of handheld input devices for virtual environment. One of our hypotheses is that the mouse can be tedious and time-consuming when dealing with a large number of objects and various constraints in the layout system, due to increased movements. On the other hand, there are several drawbacks affecting the efficiency of speech recognition, such as environmental noise. Users are also required to familiarize themselves with the voice commands that need to be correctly uttered to activate a specific action. For these reasons, we decided to combine both the mouse and the voice interface to interact with the VE, representing a multimodal approach.</p><p>Our study uses projection screen instead of a head-mounted display system. Previous research has proven that the usage of projection screen is effective as an alternative to head-mounted display for virtual environments (Patrick et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Patrick E et al (2000) Using a large projection screen as an alternative to head-mounted displays for virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM" href="/article/10.1007/s10055-018-0337-4#ref-CR31" id="ref-link-section-d53257e2247">2000</a>). Therefore, our study findings can be applicable in a VR context.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Methods</h2><div class="c-article-section__content" id="Sec12-content"><h3 class="c-article__sub-heading" id="Sec13">Population</h3><p>Twelve participants (6 males and 6 females) aged between 24 and 49 years took part in this experiment. All of the participants were students at the University of Angers and were familiar with a classic interaction technique of mouse and keyboard on a daily basis, but had not previously used speech interface or other 3D manipulation techniques. They all had similar experience of using virtual reality applications, as part of their course work.</p><h3 class="c-article__sub-heading" id="Sec14">Task</h3><p>The participants were required to complete a 3D layout task, consisting of laying out a set of five workstations and two cabinets in a 3D space by controlling the constraints solver in accordance with a set of predefined constraints.</p><p>For <span class="mathjax-tex">\(i\in {[1,5]}\)</span>:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Put the computer<span class="mathjax-tex">\(_{i}\)</span> on the desktop<span class="mathjax-tex">\(_{i} \Longrightarrow\)</span> <i>constraint Object_on_object</i>;</p>
                    </li>
                    <li>
                      <p>Place the chair<span class="mathjax-tex">\(_{i}\)</span> behind the desk<span class="mathjax-tex">\(_{i}\)</span> at a fixed distance <span class="mathjax-tex">\({{{\varvec{d}}_fixed}}\)</span> <span class="mathjax-tex">\(\Longrightarrow\)</span> <i>constraint Distance_fixed + constraint Behind</i>;</p>
                    </li>
                    <li>
                      <p>Place the cabinet<span class="mathjax-tex">\(_{i}\)</span> in the corner between the left and the front walls <span class="mathjax-tex">\(\Longrightarrow\)</span> <i>constraint Left + constraint Front</i>;</p>
                    </li>
                    <li>
                      <p>Place the cabinet<span class="mathjax-tex">\(_{i}\)</span> in the corner between the right and the front walls<span class="mathjax-tex">\(\Longrightarrow\)</span> <i>constraint Right + constraint Front</i>;</p>
                    </li>
                    <li>
                      <p>Place the office<span class="mathjax-tex">\(_{1}\)</span> and office<span class="mathjax-tex">\(_{2}\)</span> against the right wall <span class="mathjax-tex">\(\Longrightarrow\)</span><i>constraint Right</i>;</p>
                    </li>
                    <li>
                      <p>Place office<span class="mathjax-tex">\(_{3}\)</span> and office<span class="mathjax-tex">\(_{4}\)</span> against the left wall <span class="mathjax-tex">\(\Longrightarrow\)</span><i>constraint Left</i>;</p>
                    </li>
                    <li>
                      <p>Place office<span class="mathjax-tex">\(_{5}\)</span> against the front wall <span class="mathjax-tex">\(\Longrightarrow\)</span> <i>constraint Front</i>;</p>
                    </li>
                    <li>
                      <p>Separate all offices by a minimum distance <span class="mathjax-tex">\({{{\varvec{d}}_min}}\)</span> <span class="mathjax-tex">\(\Longrightarrow\)</span> <i>constraint Distance_minimum</i>.</p>
                    </li>
                  </ul><p>Each workstation was composed of a desk, a chair, and a computer (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig2">2</a>b). An number ID was given to each object to facilitate manipulation. For example, <span class="mathjax-tex">\(\hbox {office}_{i}\)</span> was associated with <span class="mathjax-tex">\(\hbox {chair}_{i}\)</span> and the computer. All the 3D objects were randomly placed in the 3D scene at the beginning of the task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig2">2</a>a). The goal was to layout the scene as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig2">2</a>b, and the task was performed using three different interaction techniques (two monomodal and one multimodal techniques).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Initial configuration of the scene (<b>a</b>) and the arrangement to achieve (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec15">Questionnaires</h3><p>To measure the workload during the experiment in VR, the NASA Task Load Index (NASA-TLX) (Hart and Staveland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Hart S, Staveland L (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research, pp 139–183" href="/article/10.1007/s10055-018-0337-4#ref-CR14" id="ref-link-section-d53257e2961">1988</a>) questionnaire was used. The assessment of workload level caused by the 3D layout task is measured through six indices without weighting: mental demands (MD), physical demands (PD), temporal demands (TD), own performance (OP), effort (EF), and frustration (FR).</p><p>The participants were also asked to rate the intuitiveness of the interaction techniques, based on a seven-point Likert scale, with 1 for “very easy” and 7 for “very complex” to assess the interaction technique usability.</p><p>The questionnaires aimed to evaluate the benefits of the interaction techniques and the user preferences.</p><h3 class="c-article__sub-heading" id="Sec16">Procedure</h3><p>The experiment was performed on a computer with <i>Intel</i><sup>TM</sup> i7-frequency 2.00 GHz processor. The refresh rate of the projector was 120 Hz. The VR interface is projected on a wall in front of the participants at 1.5 m to increase immersiveness.</p><p>After a training session, the participant performed the task three successive times with a rest period of 30 s between each experiment condition. The order of the conditions is randomized. After each experimental condition, the participants were asked to complete subjective questionnaires and were asked to indicate their preferred technique, based on their personal preferences, and to justify their choice.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">3D layout using WIMP interaction technique: mouse condition</h4><p>The participants used a standard desktop mouse to select the objects in the 3D scene and chose the constraints from the menu displayed on the left of the screen (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig3">3</a>a). The mouse could also be used to change the view of the scene during the layout, which can be necessary to select objects that may be occluded in the scene. View changing was completed by pressing and holding down the right button to pan around the scene and the middle button to rotate the viewpoint. This was introduced to the participants during training. This process was repeated for each association of object(s)–constraint(s). The launch of the resolution (or the solver call) could be triggered by a button on the constraints menu. For this experiment condition, the participants were sitting on a chair in front of the projected wall (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig4">4</a>). Other interaction techniques and/or input devices for virtual reality, such as pointer, joystick, wand, hand tracking system, use similar gestures, and modality as the mouse. Jaimes and Sebe (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Jaimes A, Sebe N (2007) Multimodal human–computer interaction: a survey. Comput Vis Image Underst 108(1):116–134" href="/article/10.1007/s10055-018-0337-4#ref-CR21" id="ref-link-section-d53257e2998">2007</a>) define pointing device as a separate category to human senses in the classification of multimodal interaction, which justified our approach of adding voice as an additional modality. Therefore, we use this condition as a representative of pointer-based handheld input techniques/devices for virtual environment. The experiment focuses on the different modalities of input.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A subject selecting objects and constraints using the mouse</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">3D layout using voice commands</h4><p>For this condition, the participants used voice command to select objects and constraints and to call the solver (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig5">5</a>). This condition represents the second monomodal interaction. While standing, the participants spoke the name and ID of an object to select and then spoke the name of the constraints to apply on the selected object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>A subject selecting objects and constraints using voice commands</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To facilitate objects selection, participants could identify an object simply by speaking the first letter of its name followed by its ID. For example, “C1” for the <span class="mathjax-tex">\(\hbox {chair}_{1}\)</span> and “O<span class="mathjax-tex">\(_2\)</span>” for the <span class="mathjax-tex">\(\hbox {office}_{2}\)</span>. We implemented the voice input interface using Microsoft Speech Application Programming Interface (SAPI) 5.1. A small set of vocabulary was implemented to ensure an efficient recognition rate, since it is faster to recognize two letters than a word or phrase. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab1">1</a> outlines the vocabulary used to interact with the VE.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 The vocabulary words used to select objects and constraints</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0337-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">3D layout using both mouse and voice command</h4><p>For this condition, the participants were seated in front of the projected wall and selected objects by voice commands and chose the constraints using the mouse. The selection of objects was based on the vocabulary given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab1">1</a>. This implementation is based on our observation that it seems easier to memorize the name of an object than that of a constraint. In this study, we focused on this assumption, but we plan to test the other assumptions in which objects are selected by the mouse and constraints by voice commands.</p><p>For both conditions, the system only provided feedback upon the calling of the constraint solver. The feedback was display textually at the bottom of the screen, to indicate whether the participants had satisfied all the constraints; if not, the system listed the constraints that were not satisfied or placed incorrectly. The participants performed corrections and called the solver again, until all constraints were met.</p><h3 class="c-article__sub-heading" id="Sec20">Data collection</h3><p>We measured the task completion time as an indication of user performance. The task completion time is the time taken by the user to layout the 3D scene according to the configuration given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig2">2</a>. The start of the task was triggered when the participants selected the first object and the task ended when the solver was launched.</p><h3 class="c-article__sub-heading" id="Sec21">Results and discussion</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Objective analysis</h4><p>We performed a repeated-measures ANOVA <i>within subjects</i> to investigate the effect of interaction techniques on the performance. The only independent variable was the type of interaction (<b><i>I</i></b>), while the task completion time was the only dependent variable. The <b><i>I</i></b> factor is defined by three conditions: interaction with mouse (<b><i>M</i></b>), interaction with voice (<b><i>V</i></b>), and multimodal interaction (<b><i>M&amp;V</i></b>) with both mouse and voice commands. In addition, pair-wise <i>t</i> tests (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab2">2</a>) were carried out to determine whether there is a significant difference between each pair of obtained data (<b><i>M</i></b> with <b><i>V</i></b>, <b><i>M</i></b> with <b><i>M&amp;V</i></b>, and <b><i>V</i></b> with <b><i>M&amp;V</i></b>).</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>
                            <i>Task completion time</i>
                          </p>
                          <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig6">6</a>, there was a significant effect of <b><i>I</i></b> on the task completion time (<span class="mathjax-tex">\(F_{2.11}=21.88, p &lt; 0.05\)</span>). It can be concluded that when the voice interface was used, performance was increased approximately 21.1% for the exclusive use (<b><i>V</i></b>), and about 18.8% for partial use (<b><i>M&amp;V</i></b>), as compared to the mouse only condition (<b><i>M</i></b>) (see Table  <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab2">2</a>). <i>H1 was supported.</i></p>
                          <p>The difference of performance between the interaction <b><i>M</i></b> and the two others can be explained by the relatively long and tedious selection cycle of 3D objects. During the experiment, the participants were required to: (1) identify the object to select in the 3D scene, (2) change the point of view of the camera for selection, if required, and (3) manually move the mouse cursor to the desired object. Changing viewpoints to select occluded objects can add more time to the task, as compared to the voice option where the user can select occluded objects with voice commands. Moreover, selecting the constraints menu took more time since the participants needed to visually identify the names of the correct constraints. The use of the voice interface (<b><i>V</i></b> and <b><i>M&amp;V</i></b>) significantly reduced object selection time because the participants only needed to pronounce the objects’ names without changing the point of view of the camera and without hand movements (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab3">3</a>).</p>
                          <p>Although the exclusive use of the voice command interface seemed the easiest and most intuitive way to layout the VE, a further ANOVA analysis between the <b><i>V</i></b> and <b><i>M&amp;V</i></b> techniques showed no significant difference in performance (<span class="mathjax-tex">\(F_{1.11} =0658, p =0.49\)</span>). The selection of constraints, which differentiated the techniques, took almost the same time. The results indicated that there was little difference between remembering the name of a constraint to be applied (<b><i>V</i></b>) and visually identifying constraints in the menu (<b><i>M&amp;V</i></b>).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>
                            <i>Learning effect</i>
                          </p>
                          <p>The concept of learning effect allows the identification of performance issues during repetitive testing of a given task. A lack of learning or a degradation in performance can also indicate the difficulties in the usage of the techniques.</p>
                          <p>We analyzed the evolution of the task completion time during the repetitive sessions of the layout task, to further investigate the learning effect. An important performance gain of 73, 63, and 67% was observed, respectively, for <b><i>M</i></b>, <b><i>V</i></b>, and <b><i>M&amp;V</i></b> (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig7">7</a>).</p>
                        
                      </li>
                    </ol>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Box plot of the task completion time for each type of interaction (mouse M, voice V, mouse and voice M&amp;V)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Means and standard deviations of the task completion time for each type of interaction</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0337-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The <i>p</i> value of the <i>t</i> test for the three experimental conditions [mouse (M), voice (V), mouse&amp;voice (M&amp;V)]</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0337-4/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>

                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The learning effect on the task completion time for each interaction technique. M (mouse), V (voice interface), and M&amp;V</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Subjective aspects</h4><p>In this subsection, we evaluated the perceived level of workload based on the NASA-TLX questionnaire (Hart and Staveland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Hart S, Staveland L (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research, pp 139–183" href="/article/10.1007/s10055-018-0337-4#ref-CR14" id="ref-link-section-d53257e4270">1988</a>) and analyzed participants’ feedbacks observed during the experiment, including task difficulty and user preference. As for the objective data, a one-way analysis of variance (ANOVA) was performed. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0337-4#Tab4">4</a> summarizes the main effect of the independent variable on the workload indices. Except for OP, <b><i>I</i></b> significantly affected MD, PD, TD, EF, and FR indices. We are particularly interested in the effects of the multimodal interactions; therefore, we performed a post hoc <i>t</i> test between the two conditions <b><i>M</i></b> and <b><i>V</i></b> and found a significant difference in MD, PD, TD, EF, and FR indices (all tests <span class="mathjax-tex">\(p&lt;0.001\)</span>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 The main effect of the interaction type (I) on the workload indices (mental demand (MD), physical demand (PD), temporal demands (TD), own performance (OP), effort (EF), frustration (FR)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0337-4/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>

                    <ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>
                            <i>Workload</i>
                          </p>
                          <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig8">8</a> shows the average score for each index involved in the workload evaluation. The results showed that the participants associated the use of mouse for VE interaction with higher mental, physical, and temporal demands (<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{MD}\}}=4.41, \hbox {Mean}_{\{\mathrm{PD}\}}=5.08\)</span> and <span class="mathjax-tex">\(\hbox {Mean}_{\{\mathrm{TD}\}}=4.75\)</span>). This condition also led to more frustration (<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{FR}\}}=5.66\)</span>) compared to the other techniques. Voice interaction also caused a high level of mental demand (<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{MD}\}}=4\)</span>) as compared to the multimodal technique (<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{MD}\}}=3.25\)</span>). This may be because the participants were asked to remember the objects and constraints names. For a large number of objects and constraints, we speculate that this technique can quickly reach a cognitive limit. For the combined technique (<b><i>M&amp;V</i></b>), a higher physical demand ((<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{PD}\}}=3.16\)</span>) was caused compared with the voice-based interaction (<span class="mathjax-tex">\(\mathrm{Mean}_{\{\mathrm{PD}\}}=2.5\)</span>). Similar results were obtained for the TD, EF, and FR. This was possibly due to the use, even partial, of the mouse.</p>
                          <p>For the own performance (OP) index, a higher score is observed in the voice-based tasks and also in the mouse-based one (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig8">8</a>). This is due to the fact that the OP index was presented as a personal estimation of successful task accomplishment. Participants were aware of the existence of a solver assistance.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>
                            <i>Evaluation of interaction techniques</i>
                          </p>
                          <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0337-4#Fig9">9</a>, more than 66% of the participants found that the mouse interaction (<b><i>M</i></b>) was complex or very complex to use. In contrast, nearly 92% of the participants found that the use of the voice-based interaction was easy or very easy. Similar opinions were expressed concerning the multimodal technique.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>
                            <i>Users preferences</i>
                          </p>
                          <p>The descriptive statistics analysis of the user preference confirmed the results described in the previous paragraph. The participants’ preferences were shared between the interactions <b><i>V</i></b> and <b><i>M&amp;V</i></b> in which the voice interface was both used. Only one participant expressed a preference for the mouse interaction. Most of participants justified their choice by pointing out that the use of the voice interface for a 3D layout task was easier and more intuitive than the mouse interaction. They reported that, unlike the interaction using the mouse, they did not need getting accustomed to the system interface to interact with the VE.</p>
                        
                      </li>
                    </ol>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Scores of workload index for each type of interaction (mouse M, voice V, mouse and voice M&amp;V</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0337-4/MediaObjects/10055_2018_337_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Subjective evaluation of each type of interaction (mouse M, voice V, mouse and voice M&amp;V</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0337-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The study is focused on evaluating different modalities of interaction with a solver-assisted 3D layout system, or any other similar virtual reality system with an intelligent or digital assistant or agent. The results of the study confirm that the use of the speech modality has significantly helped the participants to accomplish the layout task, as compared to a pointer-based handheld input device, represented by the mouse condition. Even the partial use of this technique (associated with the mouse) allowed the improvement of the performance and has proven itself in terms of performance and participants’ satisfaction. However, we suspected that the exclusive use of this technique in a more complex 3D scene involving a large number of objects and constraints is not necessarily the most efficient and suitable one. The user will be required to memorize all the words identifying constraints, which leads us to believe that a multimodality technique would be more appropriate.</p><p>Our findings provide design guidelines for multimodal applications with focus on designing, building, and modifying the virtual environment, i.e., VR world editor. The consideration of the speech modality has a positive impact on task completion.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusion</h2><div class="c-article-section__content" id="Sec24-content"><p>In this paper, we propose an interactive approach to solve 3D layout problems by using a constraint programming techniques. We specifically aim toward the development of a decision-making system that proposes feasible 3D configurations to the user, given a set of selected objects and constraints to be applied. We developed several interaction techniques (monomodal and multimodal) so that users can experiment naturally with the 3D prototype and the proposed system. We presented a usability study comparing the proposed interaction interfaces across a 3D layout tasks. Both objective and subjective data were collected and analyzed. The layout task consisted of an interactive arrangement of a room with furniture (armoires, desk, and computers).</p><p>The results confirm that the use of the voice interface has significantly helped the subjects to accomplish the layout task. One of the limitations of our study is the limited number of constraints and objects. For more complex 3D scene, there may be a consideration for the trade-off between remembering the names of the constraints for voice commands versus visually identifying the constraints for mouse selection, with regard to the usage of voice commands. The contribution of the paper is an empirical evaluation of multimodal interaction technique, leading to the conclusion that <i>multimodality does not improve the interaction if an appropriate single modality is found</i>. The choice of the most appropriate interaction strongly depends on the size of the problem of 3D arrangement. Due to the limitation in the number of objects in our study, further evaluation with multimodal interaction for a larger number of objects will be required to be conducted in the future.</p><p>Our approach contributes to the multidisciplinary work by combining constraint programming and virtual reality interaction to solve a layout task. Our work contributes to potential future work in many related disciplines that will benefit from immersive VR interactions, such as virtual assistance using artificial intelligence, industrial, and architectural design.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Azenkot S, Lee NB (2013) Exploring the use of speech input by blind people on mobile devices. In: Proceedings " /><p class="c-article-references__text" id="ref-CR1">Azenkot S, Lee NB (2013) Exploring the use of speech input by blind people on mobile devices. In: Proceedings of the 15th international ACM SIGACCESS conference on computers and accessibility. ACM, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput. Graph. 32(" /><p class="c-article-references__text" id="ref-CR2">Billinghurst M (1998) Put that where? Voice and gesture at the graphics interface. SIGGRAPH Comput. Graph. 32(4):60–63</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F307710.307730" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Put%20that%20where%3F%20Voice%20and%20gesture%20at%20the%20graphics%20interface&amp;journal=SIGGRAPH%20Comput.%20Graph.&amp;volume=32&amp;issue=4&amp;pages=60-63&amp;publication_year=1998&amp;author=Billinghurst%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on us" /><p class="c-article-references__text" id="ref-CR3">Bolt RA, Herranz E (1992) Two-handed gesture in multi-modal natural dialog. In: 5th Annual ACM symposium on user interface software and technology, pp 7–14</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cal, J. Baghabra, DJ. Boges, GR. Holst, A. Kreshuk, FA. Hamprecht, PJ. Magistretti, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Cal C, Baghabra J, Boges DJ, Holst GR, Kreshuk A, Hamprecht FA, Magistretti PJ (2016) Three-dimensional immers" /><p class="c-article-references__text" id="ref-CR5">Cal C, Baghabra J, Boges DJ, Holst GR, Kreshuk A, Hamprecht FA, Magistretti PJ (2016) Three-dimensional immersive virtual reality for studying cellular compartments in 3D models from EM preparations of neural tissues. J Comp Neurol 524(1):23–38</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fcne.23852" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Threedimensional%20immersive%20virtual%20reality%20for%20studying%20cellular%20compartments%20in%203D%20models%20from%20EM%20preparations%20of%20neural%20tissues&amp;journal=J%20Comp%20Neurol&amp;volume=524&amp;issue=1&amp;pages=23-38&amp;publication_year=2016&amp;author=Cal%2CC&amp;author=Baghabra%2CJ&amp;author=Boges%2CDJ&amp;author=Holst%2CGR&amp;author=Kreshuk%2CA&amp;author=Hamprecht%2CFA&amp;author=Magistretti%2CPJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Calderon, M. Cavazza, D. Diaz, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in" /><p class="c-article-references__text" id="ref-CR6">Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. Lect Notes Comput Sci 2733:112–122</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F3-540-37620-8_11" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20approach%20to%20the%20interactive%20resolution%20of%20configuration%20problems%20in%20virtual%20environments&amp;journal=Lect%20Notes%20Comput%20Sci&amp;volume=2733&amp;pages=112-122&amp;publication_year=2003&amp;author=Calderon%2CC&amp;author=Cavazza%2CM&amp;author=Diaz%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chun LM, Arshad H, Piumsomboon T, Billinghurst M (2015) A combination of static and stroke gesture with speech" /><p class="c-article-references__text" id="ref-CR7">Chun LM, Arshad H, Piumsomboon T, Billinghurst M (2015) A combination of static and stroke gesture with speech for multimodal interaction in a virtual environment. In: 2015 International conference on electrical engineering and informatics (ICEEI). IEEE, pp 59–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Fages, S. Soliman, R. Coolenand, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Fages F, Soliman S, Coolenand R (2004) CLPGUI: a generic graphical user interface for constraint logic program" /><p class="c-article-references__text" id="ref-CR9">Fages F, Soliman S, Coolenand R (2004) CLPGUI: a generic graphical user interface for constraint logic programming. Constraints 9:241–262</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3ACONS.0000049203.53383.c1" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=CLPGUI%3A%20a%20generic%20graphical%20user%20interface%20for%20constraint%20logic%20programming&amp;journal=Constraints&amp;volume=9&amp;pages=241-262&amp;publication_year=2004&amp;author=Fages%2CF&amp;author=Soliman%2CS&amp;author=Coolenand%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environ" /><p class="c-article-references__text" id="ref-CR10">Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 147–154</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Gao, H. Wan, Q. Peng, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Gao S, Wan H, Peng Q (2000) An approach to solid modeling in a semi-immersive virtual environment. Comput Grap" /><p class="c-article-references__text" id="ref-CR11">Gao S, Wan H, Peng Q (2000) An approach to solid modeling in a semi-immersive virtual environment. Comput Graph 24:191–202</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2899%2900154-5" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20approach%20to%20solid%20modeling%20in%20a%20semi-immersive%20virtual%20environment&amp;journal=Comput%20Graph&amp;volume=24&amp;pages=191-202&amp;publication_year=2000&amp;author=Gao%2CS&amp;author=Wan%2CH&amp;author=Peng%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VM. Garcia, MP. Ruíz, JR. Pérez, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Garcia VM, Ruíz MP, Pérez JR (2010) Voice interactive classroom, a service-oriented software architecture for " /><p class="c-article-references__text" id="ref-CR12">Garcia VM, Ruíz MP, Pérez JR (2010) Voice interactive classroom, a service-oriented software architecture for speech-enabled learning. J Netw Comput Appl 33:603–610</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jnca.2010.03.005" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Voice%20interactive%20classroom%2C%20a%20service-oriented%20software%20architecture%20for%20speech-enabled%20learning&amp;journal=J%20Netw%20Comput%20Appl&amp;volume=33&amp;pages=603-610&amp;publication_year=2010&amp;author=Garcia%2CVM&amp;author=Ru%C3%ADz%2CMP&amp;author=P%C3%A9rez%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Goel, M. Slusky, WJ. Hoeve, KC. Furman, Y. Shao, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Goel V, Slusky M, van Hoeve WJ, Furman KC, Shao Y (2015) Constraint programming for LNG ship scheduling and in" /><p class="c-article-references__text" id="ref-CR13">Goel V, Slusky M, van Hoeve WJ, Furman KC, Shao Y (2015) Constraint programming for LNG ship scheduling and inventory management. Eur J Oper Res 241(3):662–673</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3282281" aria-label="View reference 11 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ejor.2014.09.048" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Constraint%20programming%20for%20LNG%20ship%20scheduling%20and%20inventory%20management&amp;journal=Eur%20J%20Oper%20Res&amp;volume=241&amp;issue=3&amp;pages=662-673&amp;publication_year=2015&amp;author=Goel%2CV&amp;author=Slusky%2CM&amp;author=Hoeve%2CWJ&amp;author=Furman%2CKC&amp;author=Shao%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Sandra G.. Hart, Lowell E.. Staveland, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Hart S, Staveland L (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical res" /><p class="c-article-references__text" id="ref-CR14">Hart S, Staveland L (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research, pp 139–183</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20Psychology&amp;pages=139-183&amp;publication_year=1988&amp;author=Hart%2CSandra%20G.&amp;author=Staveland%2CLowell%20E.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Honda K, Mizoguchi F (1995) Constraint-based approach for automatic spatial layout planning. Conference on art" /><p class="c-article-references__text" id="ref-CR17">Honda K, Mizoguchi F (1995) Constraint-based approach for automatic spatial layout planning. Conference on artificial intelligence for applications. IEEE Press, Ne York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ISO/IEC (2011) 25010:2011 Systems and software engineering—systems and software quality requirements and evalu" /><p class="c-article-references__text" id="ref-CR18">ISO/IEC (2011) 25010:2011 Systems and software engineering—systems and software quality requirements and evaluation (SQuaRE)—system and software quality models, ISO</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Jacob, L. Sibert, D. MacFarlane, P. Mullen, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Jacob R, Sibert L, MacFarlane D, Mullen P Jr (1994) Integrality and separability of input devices. Comput Hum " /><p class="c-article-references__text" id="ref-CR19">Jacob R, Sibert L, MacFarlane D, Mullen P Jr (1994) Integrality and separability of input devices. Comput Hum Interact 1:3–26</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F174630.174631" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrality%20and%20separability%20of%20input%20devices&amp;journal=Comput%20Hum%20Interact&amp;volume=1&amp;pages=3-26&amp;publication_year=1994&amp;author=Jacob%2CR&amp;author=Sibert%2CL&amp;author=MacFarlane%2CD&amp;author=Mullen%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacquenot G (2009) Méthode générique pour l’optimisation d’agencement géometrique et fonctionnel. Thése de Doc" /><p class="c-article-references__text" id="ref-CR20">Jacquenot G (2009) Méthode générique pour l’optimisation d’agencement géometrique et fonctionnel. Thése de Doctorat, Ecole Centrale de Nantes</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Jaimes, N. Sebe, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Jaimes A, Sebe N (2007) Multimodal human–computer interaction: a survey. Comput Vis Image Underst 108(1):116–1" /><p class="c-article-references__text" id="ref-CR21">Jaimes A, Sebe N (2007) Multimodal human–computer interaction: a survey. Comput Vis Image Underst 108(1):116–134</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2006.10.019" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20humancomputer%20interaction%3A%20a%20survey&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=108&amp;issue=1&amp;pages=116-134&amp;publication_year=2007&amp;author=Jaimes%2CA&amp;author=Sebe%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kefi M, Barichard V, Richard P (2012) A constraint-solver based tool for user-assisted interactive 3D layout. " /><p class="c-article-references__text" id="ref-CR23">Kefi M, Barichard V, Richard P (2012) A constraint-solver based tool for user-assisted interactive 3D layout. In: ICTAI, pp 199–206</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Pei-Ying. Ku, Sheue-Ling. Hwang, Hsin-Chang. Chang, Jian-Yung. Hung, Chih-Chung. Kuo, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Ku P-Y et al (2013) Ergonomics design on expert convenience of voice-based interface for vehicles AV systems. " /><p class="c-article-references__text" id="ref-CR25">Ku P-Y et al (2013) Ergonomics design on expert convenience of voice-based interface for vehicles AV systems. In: Kurosu M (ed) Human–computer interaction. Applications and services: 15th international conference, HCI international 2013, Las Vegas, NV, USA, July 21–26, 2013, Proceedings, Part II. Springer, Berlin, pp 606–611</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human-Computer%20Interaction.%20Applications%20and%20Services&amp;pages=606-611&amp;publication_year=2013&amp;author=Ku%2CPei-Ying&amp;author=Hwang%2CSheue-Ling&amp;author=Chang%2CHsin-Chang&amp;author=Hung%2CJian-Yung&amp;author=Kuo%2CChih-Chung">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Vladimir. Kulyukin, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kulyukin VA (2004) Human–robot interaction through gesture-free spoken dialogue" /><p class="c-article-references__text" id="ref-CR26">Kulyukin VA (2004) Human–robot interaction through gesture-free spoken dialogue</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AAURO.0000025789.33843.6d" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human-Robot%20Interaction%20Through%20Gesture-Free%20Spoken%20Dialogue&amp;journal=Autonomous%20Robots&amp;volume=16&amp;issue=3&amp;pages=239-257&amp;publication_year=2004&amp;author=Kulyukin%2CVladimir">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Laviola JJ Jr (1999) Whole-hand and speech input in virtual environments (Doctoral dissertation, Brown Univers" /><p class="c-article-references__text" id="ref-CR27">Laviola JJ Jr (1999) Whole-hand and speech input in virtual environments (Doctoral dissertation, Brown University)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Medjdoub, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Medjdoub B (2004) Constraint-based adaptation for complex space configuration in building services. J Inf Tech" /><p class="c-article-references__text" id="ref-CR30">Medjdoub B (2004) Constraint-based adaptation for complex space configuration in building services. J Inf Technol Constr 243:627–636</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Constraint-based%20adaptation%20for%20complew%20space%20configuration%20in%20building%20services&amp;journal=J%20Inf%20Technol%20Constr&amp;volume=243&amp;pages=627-636&amp;publication_year=2004&amp;author=Medjdoub%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Patrick E et al (2000) Using a large projection screen as an alternative to head-mounted displays for virtual " /><p class="c-article-references__text" id="ref-CR31">Patrick E et al (2000) Using a large projection screen as an alternative to head-mounted displays for virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Pfefferkorn, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Pfefferkorn C (1975) A heuristic problem solving design system for equipment or furniture layouts. Commun ACM " /><p class="c-article-references__text" id="ref-CR32">Pfefferkorn C (1975) A heuristic problem solving design system for equipment or furniture layouts. Commun ACM 18(5):286–297</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F360762.360817" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20heuristic%20problem%20solving%20design%20system%20for%20equipment%20or%20furniture%20layouts&amp;journal=Commun%20ACM&amp;volume=18&amp;issue=5&amp;pages=286-297&amp;publication_year=1975&amp;author=Pfefferkorn%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JN. Pires, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Pires JN (2006) Industrial robots programming: building applications for the factories of the future. Springer" /><p class="c-article-references__text" id="ref-CR34">Pires JN (2006) Industrial robots programming: building applications for the factories of the future. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Industrial%20robots%20programming%3A%20building%20applications%20for%20the%20factories%20of%20the%20future&amp;publication_year=2006&amp;author=Pires%2CJN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Régin JC (2004) Modlisation et Contraintes Globales en Programmation par Contraintes. Habilitation diriger des" /><p class="c-article-references__text" id="ref-CR35">Régin JC (2004) Modlisation et Contraintes Globales en Programmation par Contraintes. Habilitation diriger des recherches, Universit de Nice</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Rogalski, R. Wielgat, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Rogalski T, Wielgat R (2010) A concept of voice guided general aviation aircraft. Aerosp Sci Technol 14:321–32" /><p class="c-article-references__text" id="ref-CR36">Rogalski T, Wielgat R (2010) A concept of voice guided general aviation aircraft. Aerosp Sci Technol 14:321–328</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ast.2010.02.006" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20concept%20of%20voice%20guided%20general%20aviation%20aircraft&amp;journal=Aerosp%20Sci%20Technol&amp;volume=14&amp;pages=321-328&amp;publication_year=2010&amp;author=Rogalski%2CT&amp;author=Wielgat%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rogowski, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Rogowski A (2012a) A industrially oriented voice control system. Robot Comput Integr Manuf 28:303–315" /><p class="c-article-references__text" id="ref-CR37">Rogowski A (2012a) A industrially oriented voice control system. Robot Comput Integr Manuf 28:303–315</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.rcim.2011.09.010" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20industrially%20oriented%20voice%20control%20system&amp;journal=Robot%20Comput%20Integr%20Manuf&amp;volume=28&amp;pages=303-315&amp;publication_year=2012&amp;author=Rogowski%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rogowski, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Rogowski A (2012b) Web-based remote voice control of robotized cells. Robot Comput Integr Manuf 4:77–89" /><p class="c-article-references__text" id="ref-CR38">Rogowski A (2012b) Web-based remote voice control of robotized cells. Robot Comput Integr Manuf 4:77–89</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Web-based%20remote%20voice%20control%20of%20robotized%20cells&amp;journal=Robot%20Comput%20Integr%20Manuf&amp;volume=4&amp;pages=77-89&amp;publication_year=2012&amp;author=Rogowski%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Rossi F, Van Beek P, Walsh T (eds) (2006) Handbook of constraint programming. Elsevier, Amsterdam" /><p class="c-article-references__text" id="ref-CR39">Rossi F, Van Beek P, Walsh T (eds) (2006) Handbook of constraint programming. Elsevier, Amsterdam</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20constraint%20programming&amp;publication_year=2006">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic a" /><p class="c-article-references__text" id="ref-CR40">Sanchez S, Le Roux O, Inglese F, Luga H, Gaildart V (2003) Constraint-based 3D-object layout using a genetic algorithm. 3IA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schulte C, Tack G, Lagerkvist M (2013) Modeling with Gecode" /><p class="c-article-references__text" id="ref-CR41">Schulte C, Tack G, Lagerkvist M (2013) Modeling with Gecode</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Seth, JM. Vance, JH. Oliver, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Seth A, Vance JM, Oliver JH (2011) Virtual reality for assembly methods prototyping: a review. Virtual Real 15" /><p class="c-article-references__text" id="ref-CR42">Seth A, Vance JM, Oliver JH (2011) Virtual reality for assembly methods prototyping: a review. Virtual Real 15(1):5–20</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0153-y" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20for%20assembly%20methods%20prototyping%3A%20a%20review&amp;journal=Virtual%20Real&amp;volume=15&amp;issue=1&amp;pages=5-20&amp;publication_year=2011&amp;author=Seth%2CA&amp;author=Vance%2CJM&amp;author=Oliver%2CJH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tim T, Rafael B, Ruben MS, Klaas JK (2009) Rule-based layout solving and its application to procedural interio" /><p class="c-article-references__text" id="ref-CR43">Tim T, Rafael B, Ruben MS, Klaas JK (2009) Rule-based layout solving and its application to procedural interior generation. In: Proceedings of the CASA workshop on 3D advanced media in gaming and simulation (3AMIGAS), pp 212–227</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Turk, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Turk M (2014) Multimodal interaction: a review. Pattern Recogn Lett 36:189–195" /><p class="c-article-references__text" id="ref-CR44">Turk M (2014) Multimodal interaction: a review. Pattern Recogn Lett 36:189–195</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patrec.2013.07.003" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20interaction%3A%20a%20review&amp;journal=Pattern%20Recogn%20Lett&amp;volume=36&amp;pages=189-195&amp;publication_year=2014&amp;author=Turk%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Unity 3D Game Engine (2013) http://Unity3d.com&#xA;                        " /><p class="c-article-references__text" id="ref-CR45">Unity 3D Game Engine (2013) <a href="http://Unity3d.com">http://Unity3d.com</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vacher M, Lecouteux B, Istrate D, Joubert T, Portet F et al (2013) Experimental evaluation of speech recogniti" /><p class="c-article-references__text" id="ref-CR46">Vacher M, Lecouteux B, Istrate D, Joubert T, Portet F et al (2013) Experimental evaluation of speech recognition technologies for voice-based home automation control in a smart home. In: 4th Workshop on speech and language processing for assistive technologies, Grenoble, France, pp 99–105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Weyrich, P. Drews, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Weyrich M, Drews P (1999) An interactive environment for virtual manufacturing: the virtual workbench. Comput " /><p class="c-article-references__text" id="ref-CR47">Weyrich M, Drews P (1999) An interactive environment for virtual manufacturing: the virtual workbench. Comput Ind 38:5–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0166-3615%2898%2900104-3" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20interactive%20environment%20for%20virtual%20manufacturing%3A%20the%20virtual%20workbench&amp;journal=Comput%20Ind&amp;volume=38&amp;pages=5-15&amp;publication_year=1999&amp;author=Weyrich%2CM&amp;author=Drews%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics inter" /><p class="c-article-references__text" id="ref-CR48">Xu K, Stewart J, Fiume E (2002) Constraint-based automatic placement for scene composition. In: Graphics interface proceedings, University of Calgary, pp 25–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhao W, Madhavan V (2005) Integration of voice commands into a virtual reality environment for assembly design" /><p class="c-article-references__text" id="ref-CR50">Zhao W, Madhavan V (2005) Integration of voice commands into a virtual reality environment for assembly design. In: Proceedings of the 10th annual international conference on industrial engineering theory, applications &amp; practice, Clearwater Beach, FL, USA</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0337-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LARIS, University of Angers, Angers, France</p><p class="c-article-author-affiliation__authors-list">Marounene Kefi, Paul Richard &amp; Eulalie Verhulst</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">School of Information Technology, Deakin University, Melbourne, Australia</p><p class="c-article-author-affiliation__authors-list">Thuong N. Hoang</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Marounene-Kefi"><span class="c-article-authors-search__title u-h3 js-search-name">Marounene Kefi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marounene+Kefi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marounene+Kefi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marounene+Kefi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Thuong_N_-Hoang"><span class="c-article-authors-search__title u-h3 js-search-name">Thuong N. Hoang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Thuong N.+Hoang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thuong N.+Hoang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thuong N.+Hoang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Paul-Richard"><span class="c-article-authors-search__title u-h3 js-search-name">Paul Richard</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Paul+Richard&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Paul+Richard" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Paul+Richard%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Eulalie-Verhulst"><span class="c-article-authors-search__title u-h3 js-search-name">Eulalie Verhulst</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Eulalie+Verhulst&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Eulalie+Verhulst" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Eulalie+Verhulst%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0337-4/email/correspondent/c1/new">Thuong N. Hoang</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20evaluation%20of%20multimodal%20interaction%20techniques%20for%203D%20layout%20constraint%20solver%20in%20a%20desktop-based%20virtual%20environment&amp;author=Marounene%20Kefi%20et%20al&amp;contentID=10.1007%2Fs10055-018-0337-4&amp;publication=1359-4338&amp;publicationDate=2018-02-24&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0337-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0337-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kefi, M., Hoang, T.N., Richard, P. <i>et al.</i> An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment.
                    <i>Virtual Reality</i> <b>22, </b>339–351 (2018). https://doi.org/10.1007/s10055-018-0337-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0337-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-09-25">25 September 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-13">13 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-24">24 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11">November 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0337-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0337-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Interaction techniques</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Constraint solver</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D layout</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0337-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=337;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

