<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Wii remote&#8211;based low-cost motion capture for automated assembly "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper describes the development of a Wii remote (Wiimote)&#8211;based low-cost motion capture system and demonstrates its application for automated assembly simulation. Multiple Wiimotes are..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Wii remote&#8211;based low-cost motion capture for automated assembly simulation"/>

    <meta name="dc.source" content="Virtual Reality 2011 17:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-12-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper describes the development of a Wii remote (Wiimote)&#8211;based low-cost motion capture system and demonstrates its application for automated assembly simulation. Multiple Wiimotes are used to form a vision system to perform motion capture in 3D space. A hybrid algorithm for calibrating a multi-camera stereo vision system has been developed based on Zhang&#8217;s and Svoboda&#8217;s calibration algorithms. This hybrid algorithm has been evaluated and shown accuracy improvement over Svoboda&#8217;s algorithm for motion capture with multiple cameras. The captured motion data are used to automatically generate an assembly simulation of objects represented by CAD models in real time. The Wiimote-based motion capture system is practically attractive because it is inexpensive, wireless, and easily portable. Application examples have been developed for a single vision system with two Wiimotes to track the assembly of a microsatellite prototype frame and for an integrated vision system with four Wiimotes to track the assembly of a bookshelf."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-12-23"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="125"/>

    <meta name="prism.endingPage" content="136"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0204-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0204-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0204-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0204-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Wii remote&#8211;based low-cost motion capture for automated assembly simulation"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2013/06"/>

    <meta name="citation_online_date" content="2011/12/23"/>

    <meta name="citation_firstpage" content="125"/>

    <meta name="citation_lastpage" content="136"/>

    <meta name="citation_article_type" content="SI: Mixed and Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0204-z"/>

    <meta name="DOI" content="10.1007/s10055-011-0204-z"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0204-z"/>

    <meta name="description" content="This paper describes the development of a Wii remote (Wiimote)&#8211;based low-cost motion capture system and demonstrates its application for automated as"/>

    <meta name="dc.creator" content="Wenjuan Zhu"/>

    <meta name="dc.creator" content="Anup M. Vader"/>

    <meta name="dc.creator" content="Abhinav Chadda"/>

    <meta name="dc.creator" content="Ming C. Leu"/>

    <meta name="dc.creator" content="Xiaoqing F. Liu"/>

    <meta name="dc.creator" content="Jonathan B. Vance"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="ART GmbH (2011) Advanced realtime tracking GmbH. 
                    http://www.ar-tracking.de/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Ind Ergon; citation_title=Building physical and mental models in assembly tasks; citation_author=P Baggett, A Ehrefeucht; citation_volume=7; citation_issue=3; citation_publication_date=1991; citation_pages=217-227; citation_doi=10.1016/0169-8141(91)90005-7; citation_id=CR1"/>

    <meta name="citation_reference" content="Bouguet JY (2010) Camera calibration toolbox for Matlab. 
                    http://www.vision.caltech.edu/bouguetj/calib_doc/index.html
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="Chen X, Davis J, Sluallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), pp 520&#8211;527"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography; citation_author=M Fischler, R Bolles; citation_volume=24; citation_publication_date=1981; citation_pages=381-395; citation_doi=10.1145/358669.358692; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Multiple view geometry in computer vision; citation_publication_date=2003; citation_id=CR5; citation_author=R Hartley; citation_author=A Zisserman; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="Hay S, Newman J, Harley R (2008) Optical tracking using commodity hardware. In: Proceedings of IEEE and ACM international symposium on mixed and augmented reality (ISMAR)"/>

    <meta name="citation_reference" content="Heikkila J, Silven O (1997) A four-step camera calibration procedure with implicit image correction. In: IEEE computer society conference on computer vision and pattern"/>

    <meta name="citation_reference" content="citation_journal_title=J Opt Soc Am; citation_title=Closed-form solution of absolute orientation using unit quaternions; citation_author=BKP Horn; citation_volume=A4; citation_publication_date=1987; citation_pages=629-642; citation_doi=10.1364/JOSAA.4.000629; citation_id=CR8"/>

    <meta name="citation_reference" content="iotracker (2011) Advanced optical motion tracking. 
                    http://www.iotracker.com/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="Kibira D, McLean C (2002) Virtual reality simulation of a mechanical assembly production line. In: Proceedings of winter simulation conference, pp 1130&#8211;1137"/>

    <meta name="citation_reference" content="Lee J (2007) Head tracking for desktop VR displays using the Wii remote. 
                    http://johnnylee.net/projects/wii/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="Martinec D, Pajdla T (2002) Structure from many perspective images with occlusions. In: Proceedings of the European conference on computer vision, pp 355&#8211;369"/>

    <meta name="citation_reference" content="Melen T (1994) Geometrical modeling and calibration of video cameras for underwater navigation. Dissertation, Institute for teknisk kybernetikk, Norges tekniske hogskole"/>

    <meta name="citation_reference" content="NaturalPoint (2011) OptiTrack. 
                    http://www.naturalpoint.com/optitrack/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="PhaseSpace (2011) PhaseSpace optical motion capture. 
                    http://www.phasespace.com/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="Pintaric T, Kaufmann H (2007) Affordable infrared-optical pose-tracking for virtual and augmented reality. In: Proceedings of trends and issues in tracking for virtual environments workshop"/>

    <meta name="citation_reference" content="Sturm P, Triggs B (1996) A factorization based algorithm for multi-image projective structure and motion. In: European conference on computer vision, pp 709&#8211;720"/>

    <meta name="citation_reference" content="citation_journal_title=Teleoperators Virtual Environ; citation_title=A convenient multi-camera self-calibration for virtual environments; citation_author=T Svoboda, D Martinec, T Pajdla; citation_volume=14; citation_issue=4; citation_publication_date=2005; citation_pages=407-422; citation_doi=10.1162/105474605774785325; citation_id=CR16"/>

    <meta name="citation_reference" content="Tsai RY (1986) An efficient and accurate camera calibration technique for 3d-machine vision. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 364&#8211;374"/>

    <meta name="citation_reference" content="Vicon (2011) Vicon motion system. 
                    http://www.vicon.com/company/
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="Wang W, Guo B, Li X, Cao J (2008) Influence factors evaluation on high-precision planar calibration of non-metric digital camera. In: The international archives of the photogrammetry, remote sensing and spatial information science, Vol. XXXVII (B1)"/>

    <meta name="citation_reference" content="Wengert C, Bianchi G (2008) Implementation of closed-form solution of absolute orientation using unit quaternions. 
                    http://www.mathworks.com/matlabcentral/fileexchange/22422-absolute-orientation
                    
                  . Accessed 8 Feb 2011"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A flexible new technique for camera calibration; citation_author=Z Zhang; citation_volume=22; citation_issue=11; citation_publication_date=2000; citation_pages=1330-1334; citation_doi=10.1109/34.888718; citation_id=CR21"/>

    <meta name="citation_reference" content="Zollner H, Sablatnig R (2004) Comparison of methods for geometric camera calibration using planar calibration targets. In: Proceedings of 28th workshop Austrian association of pattern recognition, pp 234&#8211;224"/>

    <meta name="citation_author" content="Wenjuan Zhu"/>

    <meta name="citation_author_email" content="zhuwe@mst.edu"/>

    <meta name="citation_author_institution" content="Missouri University of Science and Technology, Rolla, USA"/>

    <meta name="citation_author" content="Anup M. Vader"/>

    <meta name="citation_author_email" content="Vader_Anup@cat.com"/>

    <meta name="citation_author_institution" content="Caterpillar Inc., Mossville, USA"/>

    <meta name="citation_author" content="Abhinav Chadda"/>

    <meta name="citation_author_email" content="abhinavchadda@gmail.com"/>

    <meta name="citation_author_institution" content="Salesforce.com, San Francisco, USA"/>

    <meta name="citation_author" content="Ming C. Leu"/>

    <meta name="citation_author_email" content="mleu@mst.edu"/>

    <meta name="citation_author_institution" content="Missouri University of Science and Technology, Rolla, USA"/>

    <meta name="citation_author" content="Xiaoqing F. Liu"/>

    <meta name="citation_author_email" content="fliu@mst.edu"/>

    <meta name="citation_author_institution" content="Missouri University of Science and Technology, Rolla, USA"/>

    <meta name="citation_author" content="Jonathan B. Vance"/>

    <meta name="citation_author_email" content="Jonathan.B.Vance@boeing.com"/>

    <meta name="citation_author_institution" content="Boeing Research and Technology, St. Louis, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0204-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0204-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Wii remote–based low-cost motion capture for automated assembly simulation"/>
        <meta property="og:description" content="This paper describes the development of a Wii remote (Wiimote)–based low-cost motion capture system and demonstrates its application for automated assembly simulation. Multiple Wiimotes are used to form a vision system to perform motion capture in 3D space. A hybrid algorithm for calibrating a multi-camera stereo vision system has been developed based on Zhang’s and Svoboda’s calibration algorithms. This hybrid algorithm has been evaluated and shown accuracy improvement over Svoboda’s algorithm for motion capture with multiple cameras. The captured motion data are used to automatically generate an assembly simulation of objects represented by CAD models in real time. The Wiimote-based motion capture system is practically attractive because it is inexpensive, wireless, and easily portable. Application examples have been developed for a single vision system with two Wiimotes to track the assembly of a microsatellite prototype frame and for an integrated vision system with four Wiimotes to track the assembly of a bookshelf."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Wii remote–based low-cost motion capture for automated assembly simulation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0204-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Wii remote, Low cost, Motion capture, Hybrid camera calibration algorithm, Assembly simulation","kwrd":["Wii_remote","Low_cost","Motion_capture","Hybrid_camera_calibration_algorithm","Assembly_simulation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0204-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0204-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=204;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0204-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Wii remote–based low-cost motion capture for automated assembly simulation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0204-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0204-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Mixed and Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-12-23" itemprop="datePublished">23 December 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Wii remote–based low-cost motion capture for automated assembly simulation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Wenjuan-Zhu" data-author-popup="auth-Wenjuan-Zhu" data-corresp-id="c1">Wenjuan Zhu<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Missouri University of Science and Technology" /><meta itemprop="address" content="grid.260128.f, 0000000093646281, Missouri University of Science and Technology, Rolla, MO, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Anup_M_-Vader" data-author-popup="auth-Anup_M_-Vader">Anup M. Vader</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Caterpillar Inc." /><meta itemprop="address" content="grid.418702.8, 0000000106986083, Caterpillar Inc., Mossville, IL, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Abhinav-Chadda" data-author-popup="auth-Abhinav-Chadda">Abhinav Chadda</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Salesforce.com" /><meta itemprop="address" content="grid.431504.7, Salesforce.com, San Francisco, CA, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ming_C_-Leu" data-author-popup="auth-Ming_C_-Leu">Ming C. Leu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Missouri University of Science and Technology" /><meta itemprop="address" content="grid.260128.f, 0000000093646281, Missouri University of Science and Technology, Rolla, MO, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xiaoqing_F_-Liu" data-author-popup="auth-Xiaoqing_F_-Liu">Xiaoqing F. Liu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Missouri University of Science and Technology" /><meta itemprop="address" content="grid.260128.f, 0000000093646281, Missouri University of Science and Technology, Rolla, MO, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jonathan_B_-Vance" data-author-popup="auth-Jonathan_B_-Vance">Jonathan B. Vance</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Boeing Research and Technology" /><meta itemprop="address" content="Boeing Research and Technology, St. Louis, MO, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">125</span>–<span itemprop="pageEnd">136</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">503 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0204-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper describes the development of a Wii remote (Wiimote)–based low-cost motion capture system and demonstrates its application for automated assembly simulation. Multiple Wiimotes are used to form a vision system to perform motion capture in 3D space. A hybrid algorithm for calibrating a multi-camera stereo vision system has been developed based on Zhang’s and Svoboda’s calibration algorithms. This hybrid algorithm has been evaluated and shown accuracy improvement over Svoboda’s algorithm for motion capture with multiple cameras. The captured motion data are used to automatically generate an assembly simulation of objects represented by CAD models in real time. The Wiimote-based motion capture system is practically attractive because it is inexpensive, wireless, and easily portable. Application examples have been developed for a single vision system with two Wiimotes to track the assembly of a microsatellite prototype frame and for an integrated vision system with four Wiimotes to track the assembly of a bookshelf.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Assembling a product or its subsystems often involves fairly sophisticated sequences of operations performed by one or more operators with various tools and parts. It is necessary to provide effective training of the assembly process for new operators. Dynamic presentation of an assembly process using video is superior to static presentation using text (Baggett and Ehrefeucht <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Baggett P, Ehrefeucht A (1991) Building physical and mental models in assembly tasks. Int J Ind Ergon 7(3):217–227" href="/article/10.1007/s10055-011-0204-z#ref-CR1" id="ref-link-section-d17226e429">1991</a>). However, in the video format, crucial movements of objects could be obscured by other objects that are in the view of camera. Moreover, video information cannot be modified easily to show alternative ways for improving existing assembly operations or for planning new operations for variant assembly tasks. Thus, there is a great need for CAD model–based assembly simulation.</p><p>A major problem in assembly simulation is to construct an assembly sequence, including timing of part movement, which is often laborious and costly if done manually (Kibira and McLean <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kibira D, McLean C (2002) Virtual reality simulation of a mechanical assembly production line. In: Proceedings of winter simulation conference, pp 1130–1137" href="/article/10.1007/s10055-011-0204-z#ref-CR9" id="ref-link-section-d17226e435">2002</a>). One approach to address this issue is to capture movements of parts, tools, and/or operators with attached sensors that can provide their positions and orientations in real time during an actual assembly process. Then, the assembly simulation with CAD models can be generated using the obtained motion data from sensors. With the CAD model–based assembly simulation, a new operator can view the assembly process from different angles and with different speeds. Moreover, the assembly sequence can be modified easily to investigate alternative assembly sequences for purposes of reduction in cycle time, improvement in safety, etc.</p><p>Motion capture systems based on optical, magnetic, acoustic, inertial, and other technologies for interactive computer graphic simulation applications have been explored by researchers for many years, and commercial products are continuously evolving. In particular, multi-camera motion capture systems continue to emerge because of decreasing prices of powerful computers and cameras. Among them, the infrared-based optical motion capture systems (e.g., iotracker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="iotracker (2011) Advanced optical motion tracking. &#xA;                    http://www.iotracker.com/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR202" id="ref-link-section-d17226e441">2011</a>; PhaseSpace <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="PhaseSpace (2011) PhaseSpace optical motion capture. &#xA;                    http://www.phasespace.com/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR204" id="ref-link-section-d17226e444">2011</a>; Vicon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vicon (2011) Vicon motion system. &#xA;                    http://www.vicon.com/company/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR205" id="ref-link-section-d17226e447">2011</a>; ART GmbH <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="ART GmbH (2011) Advanced realtime tracking GmbH. &#xA;                    http://www.ar-tracking.de/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR201" id="ref-link-section-d17226e450">2011</a>; NaturalPoint <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="NaturalPoint (2011) OptiTrack. &#xA;                    http://www.naturalpoint.com/optitrack/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR203" id="ref-link-section-d17226e453">2011</a>) are becoming increasingly popular because of their high precision and flexibility. Also, the infrared motion tracking system can achieve a higher frame rate than the normal camera motion tracking system with the simple image or the colored markers since the IR image only contains the IR markers with less background noise, which can speed up the image processing accordingly. However, most of these systems are priced in a range of tens of thousands of dollars. The lack of affordability has prohibited their wide applications. Since the debut of Nintendo’s Wii games, a vibrant internet community has emerged to take advantage of capabilities provided by the Wii remote (Wiimote), which is extremely inexpensive as a result of mass production. The Wiimote includes inside an infrared camera that can detect the infrared light emitted from IR LEDs mounted on a sensor bar. Wiimote-based motion tracking systems as described in this paper can be developed very inexpensively because a Wiimote costs only $40. A system including four Wiimotes, one Bluetooth card, software for the Bluetooth card, one PC, and infrared LEDs used as markers costs less than $600. The reliable software capable of real-time processing has been developed and will not add much cost to the whole system in its deployment.</p><p>Lee (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lee J (2007) Head tracking for desktop VR displays using the Wii remote. &#xA;                    http://johnnylee.net/projects/wii/&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR10" id="ref-link-section-d17226e459">2007</a>) used Wiimotes to create several excellent demonstrations, such as head tracking for desktop display in virtual reality, and thus has shown the potential of the Wiimote beyond gaming and brought it to the attention of many researchers. He used one Wiimote and two infrared LEDs, and the LEDs were apart at a known distance for the head tracking in his demonstration. An impression of parallax was displayed when the line between the two LEDs and the image plane of the Wiimote camera were approximately parallel. However, the use of only one camera cannot provide data for obtaining the six-degree-of-freedom pose (i.e., both position and orientation) of an object. Hay et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Hay S, Newman J, Harley R (2008) Optical tracking using commodity hardware. In: Proceedings of IEEE and ACM international symposium on mixed and augmented reality (ISMAR)" href="/article/10.1007/s10055-011-0204-z#ref-CR6" id="ref-link-section-d17226e462">2008</a>) used two Wiimotes in a stereo vision to perform 3D motion capture with good accuracy, but they did not investigate how to increase the volume of motion capture with multiple Wiimotes. Furthermore, to our knowledge, Wiimote-based motion capture has never been applied for automated assembly simulation as described in this paper.</p><p>Good calibration is key to the effective use of a multi-camera stereo vision system. Many researchers have dealt with the problem of camera calibration by taking images from a 2D object in the form of a planar pattern (Heikkila and Silven <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Heikkila J, Silven O (1997) A four-step camera calibration procedure with implicit image correction. In: IEEE computer society conference on computer vision and pattern" href="/article/10.1007/s10055-011-0204-z#ref-CR7" id="ref-link-section-d17226e469">1997</a>; Tsai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Tsai RY (1986) An efficient and accurate camera calibration technique for 3d-machine vision. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 364–374" href="/article/10.1007/s10055-011-0204-z#ref-CR17" id="ref-link-section-d17226e472">1986</a>; Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334" href="/article/10.1007/s10055-011-0204-z#ref-CR21" id="ref-link-section-d17226e475">2000</a>). Tsai’s method of camera calibration is a classic one that is still widely used in computer vision, and there have been implementations of this method in C/C++ and other programming languages. The DLT-based calibration model of Heikkilla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Heikkila J, Silven O (1997) A four-step camera calibration procedure with implicit image correction. In: IEEE computer society conference on computer vision and pattern" href="/article/10.1007/s10055-011-0204-z#ref-CR7" id="ref-link-section-d17226e478">1997</a>) uses the concepts and techniques of Melen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Melen T (1994) Geometrical modeling and calibration of video cameras for underwater navigation. Dissertation, Institute for teknisk kybernetikk, Norges tekniske hogskole" href="/article/10.1007/s10055-011-0204-z#ref-CR12" id="ref-link-section-d17226e481">1994</a>) in photogrammetry, and its implementation is available from the MATLAB software package. Zhang’s method (Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334" href="/article/10.1007/s10055-011-0204-z#ref-CR21" id="ref-link-section-d17226e485">2000</a>) makes use of advanced concepts in projective geometry, and the implementation of his method is also available in a MATLAB toolbox. Zollner and Sablatnig (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zollner H, Sablatnig R (2004) Comparison of methods for geometric camera calibration using planar calibration targets. In: Proceedings of 28th workshop Austrian association of pattern recognition, pp 234–224" href="/article/10.1007/s10055-011-0204-z#ref-CR22" id="ref-link-section-d17226e488">2004</a>) carried out a detailed comparative study of the above three methods; their experimental evaluations indicated that the overall error of the DLT-based estimation is significantly smaller than that of Tsai’s method in the mono-view case and that the DLT-based method generates larger errors than Zhang’s method in the multi-view case.</p><p>The motion capture plays a very important role in the VR system. Developing a low-cost motion capture system with relatively high accuracy can make VR applications more accessible and affordable. In this paper, we present our investigation into Wiimote-based low-cost motion capture for automated CAD model–based assembly simulation. We describe the calibration techniques for Wiimote cameras and vision systems, including a hybrid algorithm for a stereo vision system with multiple Wiimotes that we have developed based on Zhang’s and Svoboda’s calibration algorithms. With the calibration result of camera intrinsic and extrinsic parameters, the image data of IR sensors captured by Wiimotes are used to compute the six-degree-of-freedom motion of an object attached with IR sensors. Simultaneously, the motion data are used to automatically generate an assembly simulation with CAD models.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Principles of stereo vision and distributed vision</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Stereo vision principle</h3><p>A Wiimote contains a CMOS sensor with 128 × 96 pixels, but it can give a 1,024 × 768 pixel resolution after sub-pixel image processing. With an IR filter, a Wiimote can detect up to four IR hotspots and transmit their pixel coordinates via the wireless Bluetooth at a rate of 120 Hz. A Wiimote has a measurement range of 7 m. It is very inexpensive and easily portable.</p><p>Two Wiimotes can be set up as a stereo vision system to track the motion of infrared LEDs attached to the parts, tools, and operators in an assembly operation. The basic principle of stereo vision is as follows. Assume the simplified configuration of two parallel cameras with identical intrinsic parameters as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig1">1</a>. Also assume the straight line connecting the two cameras’ optical centers is coincident with the <i>x</i>
                  <sub>
                    <i>c</i>
                  </sub>-axis.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Principle of stereo vision</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In the figure, point <i>P</i> has the coordinates (<i>X</i>
                  <sub>
                    <i>C</i>
                  </sub>, <i>Y</i>
                  <sub>
                    <i>C</i>
                  </sub>, <i>Z</i>
                  <sub>
                    <i>C</i>
                  </sub>) and projects to both camera sensors. The pair of image points <i>u</i>
                  <sub>1</sub> and <i>u</i>
                  <sub>2</sub> of point <i>P</i> is referred to as conjugate points. The difference between the image locations of the two conjugate points is the disparity <span class="mathjax-tex">\( d = u_{ 1} - u_{ 2} . \)</span> The <i>Z</i>
                  <sub>
                    <i>C</i>
                  </sub> coordinate, which is the distance of P from the stereo vision system, can be calculated as</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ Z_{C} = \frac{fb}{d} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>f</i> is the focal length of the Wiimote and <i>b</i> is the basis length (distance between the two cameras). Thus, the position of the point can be calculated using the image data from the two cameras based on the triangulation principle. Note that it is necessary to determine the intrinsic parameters of both cameras and the transformation between the two coordinate frames.</p><h3 class="c-article__sub-heading" id="Sec4">Coverage of a single stereo vision system with two Wiimotes</h3><p>An important parameter in the stereo system is the measurement volume, which is the overlapping region of the two Wiimotes’ coverage. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig2">2</a>, two Wiimotes are placed such that their centers are connected by a horizontal line. The overlapping area at the distance of H from this line is represented by the length <i>L</i> and width <i>W</i>. The length <i>L</i> can be calculated as</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Stereo system coverage</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ L = 2H\tan \frac{\theta }{2} - b $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                <p>For the Wiimote, θ = 41° is the horizontal view angle, <i>b</i> = 10 cm is the assumed basis length, and <i>H</i> = 7 m is the height. Note that 7 m is the maximum distance that the Wiimote can sense in a direction perpendicular to the image chip. Substituting these parameter values into (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ2">2</a>), the overlapping length <i>L</i> between the two cameras can be determined to be 5.1 m (at the distance of 7 m). From Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig2">2</a>, we also get the coverage width as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ W = 2H\tan \frac{\alpha }{2} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where α = 30° and <i>H</i> = 7 m. Substituting these values into (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ3">3</a>), we find the coverage width as 3.75 m. Therefore, for a single stereo system, the tracking area is 5.1 m × 3.75 m at the height of 7 m.</p><h3 class="c-article__sub-heading" id="Sec5">Distributed vision principle</h3><p>Because of the limited coverage of a single vision system, multiple vision systems can be integrated into one system so as to track a wide variety of assemblies on a typical shop floor. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig3">3</a> illustrates the working principle of distributed vision. As illustrated in the figure, system 1 and system 2 represent two vision systems, with system 1 designated as the “Master.” Consider the position of an LED at time instants T1 and T2 as shown in the figure. Only system 1 can see the marker at these positions; thus, system 1 provides its position information. For LED positions at T3 and T5, the marker can be seen only by system 2, and thus, its position is provided by system 2. At T4, the marker lies in the overlapped region of system 1 and system 2 and can be tracked by both stereo vision systems. In this case, the master system (system 1) uses the data obtained by itself and the data it receives from the other system (system 2) to calculate the average, thus providing a smooth data transition.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Integration of 2 vision systems</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Each Bluetooth adapter is capable of supporting a maximum of 7 devices in a Bluetooth stack, which allows the Bluetooth protocol to communicate with the PC. More than one Bluetooth stack can be deployed to increase the number of devices (Wiimotes) to 14, 21, etc. Given the size of a typical shop floor, it is necessary to increase the measurement volume in many applications. A distributed motion capture system can be developed where multiple computers (each with a set of Wiimote-based stereo vision systems) communicate with each other to form a distributed vision network. The application runs with one of the systems filling the role of the “Master.” Each system sends the data it receives to the “Master,” which is responsible for the storage of all the captured motion data and also provides the positional information of the part being tracked to a simulation system.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Calibration of camera and vision system</h2><div class="c-article-section__content" id="Sec6-content"><p>The Wiimote provides information in terms of the 2D image coordinates (<i>u</i>, <i>v</i>) for every image infused on its CCD chip. A mathematical model can be developed to link the 2D data from two Wiimotes for an IR source in 3D space, so as to obtain the 3D coordinates of the marker. This process requires determining the focal length and the principal point of each Wiimote. It is also useful to build a distortion model to allow image correction and accuracy improvement. Furthermore, the positions and orientations of the Wiimotes relative to each other must be determined. Although there exist multi-camera calibration methods that can be used to determine the intrinsic and extrinsic parameters simultaneously (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Chen X, Davis J, Sluallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), pp 520–527" href="/article/10.1007/s10055-011-0204-z#ref-CR3" id="ref-link-section-d17226e761">2000</a>; Svoboda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Svoboda T, Martinec D, Pajdla T (2005) A convenient multi-camera self-calibration for virtual environments. Teleoperators Virtual Environ 14(4):407–422" href="/article/10.1007/s10055-011-0204-z#ref-CR16" id="ref-link-section-d17226e764">2005</a>), the two problems can be treated separately because a camera’s intrinsic parameters are determined far less frequently (ideally, only once) than its extrinsic (position and orientation) parameters.</p><h3 class="c-article__sub-heading" id="Sec7">Camera model</h3><p>During camera calibration, the transformation between 3D world coordinates and 2D image coordinates is determined by solving the unknown parameters of the camera model. Here, we use the perspective projection (i.e., pinhole) camera model as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig4">4</a>. The center of projection is at the origin <i>O</i> of the camera coordinate system. The image coordinate system is parallel to the camera coordinate system, with a distance <i>f</i> (focal length) from <i>O</i> along the <i>z</i>
                  <sub>
                    <i>c</i>
                  </sub> axis. The <i>z</i>
                  <sub>
                    <i>c</i>
                  </sub> axis is the optical axis, also called the principal axis. The intersection between the image plane and the optical axis is the principal point <i>o</i>. The <i>u</i> and <i>v</i> axes of the image plane coordinate system lie parallel to the <i>x</i> and <i>y</i> axes, respectively. The coordinates of the principal point in the image plane coordinate system are (<i>u</i>
                  <sub>0</sub>, <i>v</i>
                  <sub>0</sub>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Pinhole camera model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig4">4</a>, let <i>P</i> be an arbitrary point located on the positive side of the <i>z</i>
                  <sub>c</sub> axis and <i>p</i> be its projection on the image plane. The coordinates of <i>P</i> in the camera coordinate system are (<i>x</i>
                  <sub>c</sub>, <i>y</i>
                  <sub>c</sub>, <i>z</i>
                  <sub>c</sub>) and in the world coordinates system are (<i>X</i>, <i>Y</i>, <i>Z</i>). The coordinates of <i>p</i> in the image plane coordinate system are (<i>u</i>, <i>v</i>), which are related to (<i>X</i>, <i>Y</i>, <i>Z</i>) by the following equation:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \lambda \left[ \begin{gathered} u \hfill \\ v \hfill \\ 1 \hfill \\ \end{gathered} \right] = A\left[ {R\;\;t} \right]\left[ \begin{gathered} X \hfill \\ Y \hfill \\ Z \hfill \\ 1 \hfill \\ \end{gathered} \right] $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\text{A}} = \left[ \begin{gathered} \alpha_{x} \;\;s\;\;\;u_{0} \hfill \\ 0\;\;\;\alpha_{y} \;\;v_{0} \hfill \\ 0\;\;\;\;0\;\;\;1 \hfill \\ \end{gathered} \right]. $$</span></div></div>
                <p>In the above equation <i>R</i> and <i>t</i> are the rotation matrix and translation vector, which relate the world coordinate system to the camera coordinate system, and A is the intrinsic parameter matrix. The parameter <i>s</i> represents the skewness of the image in terms of the two image axes, <i>α</i>
                  <sub>
                    <i>x</i>
                  </sub> = <i>f/d</i>
                  <sub>
                    <i>x</i>
                  </sub> and <i>α</i>
                  <sub>
                    <i>y</i>
                  </sub> = <i>f/d</i>
                  <sub>
                    <i>y</i>
                  </sub> are scaling factors in the image <i>u</i> and <i>v</i> axes, respectively, <i>f</i> is camera focal length, and <i>d</i>
                  <sub>
                    <i>x</i>
                  </sub> and <i>d</i>
                  <sub>
                    <i>y</i>
                  </sub> are the pixel dimensions in the <i>x</i> and <i>y</i> directions, respectively. The parameter <i>λ</i> is a scale factor.</p><p>For a stereo vision system, the relationship between the camera coordinate systems must be determined. In a two-camera vision system, for a point P in 3D space, its two coordinates <i>P</i>
                  <sub>
                    <i>L</i>
                  </sub> and <i>P</i>
                  <sub>
                    <i>R</i>
                  </sub> in the left and right camera coordinate systems have the following relationship:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P_{R} = R_{S} P_{L} + T_{S} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>R</i>
                  <sub>s</sub> is the rotation matrix and <i>T</i>
                  <sub>s</sub> is the translation vector between the two coordinate frames of the vision system. <i>R</i>
                  <sub>s</sub> and <i>T</i>
                  <sub>s</sub> each consist of three independent parameters, which are the extrinsic parameters of the vision system.</p><p>Determining the intrinsic and extrinsic parameters is essential to the calibration of a stereo vision system with digital cameras. This will be discussed below.</p><h3 class="c-article__sub-heading" id="Sec8">Zhang’s algorithm for a two-camera vision system</h3><p>To calibrate a Wiimote, a calibration plate can be used. We used a plate that could hold 36 LEDs as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig5">5</a>. In the calibration process, the plate was moved to different locations in the Wiimote’s field of view. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig6">6</a> indicates a pictorial representation of the 36 LEDs at two different locations as seen by a Wiimote in the camera coordinate system. The Wiimote obtained pixel coordinates of each LED at each fixed location. We determined the intrinsic parameters of the Wiimote using a calibration algorithm developed by Zhang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334" href="/article/10.1007/s10055-011-0204-z#ref-CR21" id="ref-link-section-d17226e1099">2000</a>), which has been coded by Bouguet (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bouguet JY (2010) Camera calibration toolbox for Matlab. &#xA;                    http://www.vision.caltech.edu/bouguetj/calib_doc/index.html&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR2" id="ref-link-section-d17226e1102">2010</a>) for the Camera Calibration Toolbox of the MATLAB software.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The 36-LED calibration plate</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Data collected from a calibration plate</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>After the calibration of each Wiimote, the calibration results can be used to calibrate the stereo system. There exists a relationship between the world coordinate system and the camera coordinate system through the extrinsic parameters, i.e., the rotation matrix <i>R</i> and translation vector <i>T</i>, as follows:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P{}_{\text{CL}} = R_{1} P_{1} + T_{1} ,\;\;\;\;\;P_{\text{CR}} = R_{2} P_{2} + T_{2} $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>P</i>
                  <sub>1</sub> and <i>P</i>
                  <sub>2</sub> are two points expressed in the world coordinate system; <i>P</i>
                  <sub>CL</sub> and <i>P</i>
                  <sub>CR</sub> represent the 3D coordinates of these two points in the left camera frame and in the right camera frame, respectively; <i>R</i>
                  <sub>1</sub> and <i>T</i>
                  <sub>1</sub> are the extrinsic parameter calibration results for the left camera; and R<sub>2</sub> and T<sub>2</sub> are the extrinsic parameter calibration results for the right camera. The two points <i>P</i>
                  <sub>1</sub> and <i>P</i>
                  <sub>2</sub> in the calibration are the same point; thus, <i>P</i>
                  <sub>1</sub> = <i>P</i>
                  <sub>2</sub>. From (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ6">6</a>), we obtain the following relationship:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P_{\text{CR}} = R_{2} R_{1}^{ - 1} P_{\text{CL}} + (T_{2} - R_{2} R_{1}^{ - 1} T_{1} ) $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                <p>The above equation can be written in the form of</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P_{\text{CR}} = R_{s} P_{\text{CL}} + T_{s} $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <i>R</i>
                  <sub>
                    <i>S</i>
                  </sub> is the rotation matrix and <i>T</i>
                  <sub>
                    <i>S</i>
                  </sub> is the translation vector for the transformation between the two-camera coordinate systems.</p><h3 class="c-article__sub-heading" id="Sec9">Svoboda’s algorithm for multi-camera vision system calibration</h3><p>Svoboda et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Svoboda T, Martinec D, Pajdla T (2005) A convenient multi-camera self-calibration for virtual environments. Teleoperators Virtual Environ 14(4):407–422" href="/article/10.1007/s10055-011-0204-z#ref-CR16" id="ref-link-section-d17226e1280">2005</a>) proposed a method for calibrating multiple cameras together. A minimum of 3 cameras can be calibrated together using this method, and there is no upper limit. The only calibration object required is an IR point source. Several IR LEDs can be mounted closely together on a wand such that together they form a uniform IR light source that can be seen practically by all cameras in all directions. The calibration can be achieved by moving an IR LED through the work volume; the cameras being calibrated do not have to see all of the points where the data are recorded, i.e., only sufficient overlap among the cameras being calibrated is necessary. This property is beneficial in terms of obtaining more coverage volume in the vision system.</p><p>Svoboda’s algorithm is given briefly as follows. Let <i>m</i> be the number of cameras to be calibrated together, <i>n</i> be the number of calibration points recorded, and [<i>u</i>
                  <span class="c-stack">
                    <sup>i</sup><sub>j</sub>
                    
                  </span>
                  <i>v</i>
                  <span class="c-stack">
                    <sup>i</sup><sub>j</sub>
                    
                  </span> 1] be the image pixel coordinates for the number <i>j</i> point from the number <i>i</i> camera. Also, let X<sub>j</sub> be a calibration point with homogeneous coordinates [<i>x</i>
                  <sub>
                    <i>j</i>
                  </sub>
                  <i>, y</i>
                  <sub>
                    <i>j</i>
                  </sub>
                  <i>, z</i>
                  <sub>
                    <i>j</i>
                  </sub>
                  <i>,</i> 1]<sup>T</sup> in the world coordinates, where j = 1,…,n. The pinhole camera model in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ4">4</a>) can be written for <i>m</i> cameras and <i>n</i> calibration points as</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{gathered} \left[ {\begin{array}{*{20}c} {\lambda_{1}^{1} \left[ {\begin{array}{*{20}c} {u_{1}^{1} } \\ {v_{1}^{1} } \\ 1 \\ \end{array} } \right]} &amp; { \ldots \, \ldots } &amp; {\lambda_{n}^{1} \left[ {\begin{array}{*{20}c} {u_{n}^{1} } \\ {v_{n}^{1} } \\ 1 \\ \end{array} } \right]} \\ {} &amp; { \ldots \, \ldots } &amp; {} \\ {\lambda_{1}^{m} \left[ {\begin{array}{*{20}c} {u_{1}^{m} } \\ {v_{1}^{m} } \\ 1 \\ \end{array} } \right]} &amp; { \ldots \, \ldots } &amp; {\lambda_{n}^{m} \left[ {\begin{array}{*{20}c} {{\text{u}}_{n}^{m} } \\ {{\text{v}}_{n}^{m} } \\ 1 \\ \end{array} } \right]} \\ \end{array} } \right] \hfill \\ \begin{array}{*{20}c} { = \left[ {\begin{array}{*{20}c} {{\text{P}}^{1} } \\ \vdots \\ {{\text{P}}^{m} } \\ \end{array} } \right]_{3m \times 4} } &amp; {\left[ {\begin{array}{*{20}c} {{\text{X}}_{1} } &amp; \ldots &amp; {{\text{X}}_{n} } \\ \end{array} } \right]} \\ \end{array}_{4 \times n} \hfill \\ \end{gathered} $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>where <i>P</i>
                  <sup><i>i</i></sup> is a 3 × 4 matrix for camera <i>i</i>. It contains all 11 camera parameters (5 intrinsic and 6 extrinsic). Thus, the calibration here involves finding the camera projection matrices P<sup>i</sup> and the scaling factors<span class="mathjax-tex">\( \lambda_{j}^{i} \)</span>. In the calibration process, the 2D projection coordinates (u,v) of the image of a 3D marker can be obtained first. Then, the problematic data points (outliers) can be detected using the RANSAC analysis (Fischler and Bolles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24:381–395" href="/article/10.1007/s10055-011-0204-z#ref-CR4" id="ref-link-section-d17226e1401">1981</a>). The scaling factors <span class="mathjax-tex">\( \lambda_{j}^{i} \)</span> then can be determined, and missing points can be estimated by using the method described by Martinec and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Martinec D, Pajdla T (2002) Structure from many perspective images with occlusions. In: Proceedings of the European conference on computer vision, pp 355–369" href="/article/10.1007/s10055-011-0204-z#ref-CR11" id="ref-link-section-d17226e1415">2002</a>). The projective structures can be optimized further using the bundle adjustment (Sturm and Triggs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Sturm P, Triggs B (1996) A factorization based algorithm for multi-image projective structure and motion. In: European conference on computer vision, pp 709–720" href="/article/10.1007/s10055-011-0204-z#ref-CR15" id="ref-link-section-d17226e1419">1996</a>), and the overall matrix in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ9">9</a>) can be factorized further to get matrices P and X (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-011-0204-z#ref-CR5" id="ref-link-section-d17226e1425">2003</a>).</p><h3 class="c-article__sub-heading" id="Sec10">The hybrid camera calibration algorithm</h3><p>As discussed above, Svoboda’s algorithm can be used to obtain intrinsic and extrinsic parameters simultaneously for the cameras in a vision system. However, our experimental tests have indicated that the values of intrinsic parameters obtained using Svoboda’s algorithm deviate more from the known values of Wiimotes’ intrinsic parameters compared with Zhang’s algorithm, especially when the overlapping volume of coverage among the Wiimotes is small. Theoretically, when Svoboda’s algorithm is used to calibrate a multi-camera system, the points used for the calibration do not need to be seen by all the cameras. However, there are many unknowns in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ9">9</a>) including <i>m</i> × <i>n</i> for <span class="mathjax-tex">\( \lambda_{j}^{i} \)</span>, 11 × <i>m</i> for the intrinsic and extrinsic parameters of m cameras and 3 × <i>n</i> for the <i>n</i> calibration points. In addition, many <i>u</i>
                  <span class="c-stack">
                    <sup>i</sup><sub>j</sub>
                    
                  </span> and <i>v</i>
                  <span class="c-stack">
                    <sup>i</sup><sub>j</sub>
                    
                  </span> are also unknown because each of the cameras does not see all of the calibration points. With so many unknowns, it is expected that the intrinsic parameters of cameras obtained using Svoboda’s algorithm are not as accurate as those obtained using Zhang’s algorithm, which consists of only 11 parameters in calibrating the intrinsic and extrinsic parameters of each camera.</p><p>Here, we use only the intrinsic parameters from Zhang’s algorithm because these parameters are invariant. On the other hand, the extrinsic parameters depend on the position and orientation of a camera. Because Zhang’s algorithm is for calibrating two cameras, the errors of extrinsic parameters obtained from his algorithm will propagate in the case of multiple cameras. Based on the above reasoning, a separate calibration process for the intrinsic and extrinsic parameters of a multi-camera vision system is desired. Furthermore, the study by Wang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wang W, Guo B, Li X, Cao J (2008) Influence factors evaluation on high-precision planar calibration of non-metric digital camera. In: The international archives of the photogrammetry, remote sensing and spatial information science, Vol. XXXVII (B1)" href="/article/10.1007/s10055-011-0204-z#ref-CR19" id="ref-link-section-d17226e1490">2008</a>) showed that to get good values of intrinsic parameters, the calibration points in the image must be even and cover the whole image or at least around the image center, which cannot be obtained when Svobada’s algorithm is employed. The separation for calibrating the intrinsic and extrinsic parameters also makes sense from a practical view point because during the application, a camera’s intrinsic parameters are determined far less frequently (ideally, only once) than its extrinsic (position and orientation) parameters.</p><p>Thus, we devise a <i>hybrid calibration algorithm</i> for a multi-camera vision system using Zhang’s algorithm for the calibration of intrinsic parameters and Svoboda’s algorithm for the calibration of extrinsic parameters. The basic idea is to decompose the camera calibration matrix P<sup>i</sup> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ9">9</a>) into a matrix of intrinsic parameters and a matrix of extrinsic parameters and using Zhang’s algorithm to determine the intrinsic parameters and then using Svoboda’s algorithm to determine the extrinsic parameters.</p><p>It can be shown that each matrix <i>P</i>
                  <sup><i>i</i></sup> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ9">9</a>) can be decomposed into two separate matrices of intrinsic and extrinsic parameters in the following form (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-011-0204-z#ref-CR5" id="ref-link-section-d17226e1517">2003</a>):</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P{ = }\left[ {\begin{array}{*{20}c} {\text{A}} &amp; 0 \\ 0 &amp; 1 \\ \end{array} } \right]\left[ {\begin{array}{*{20}c} {\text{R}} &amp; {\text{t}} \\ 0 &amp; 1 \\ \end{array} } \right] $$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>where the two matrices on the right-hand side of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ10">10</a>) represent the intrinsic parameters and the extrinsic parameters, respectively. Thus, the intrinsic parameters can be determined using Zhang’s algorithm. Then, using these values of intrinsic parameters (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0204-z#Equ9">9</a>), the extrinsic parameters of a system of cameras can be determined using Svoboda’s algorithm.</p><h3 class="c-article__sub-heading" id="Sec11">Calibration of an integrated vision system</h3><p>When one multi-camera vision system cannot cover a sufficiently large measurement volume as required, more than one stereo vision system can be adopted. Each stereo vision system generates the 3D coordinates of measured hotspots with respect to its own coordinate system. In order to integrate them, it is necessary to determine the relative position and orientation between the various vision systems.</p><p>Consider a set of 3D points measured by two vision systems, each having its local coordinate frame, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig7">7</a>. These points represent the locations of a marker positioned randomly in 3D space with the requirement that they can be seen by both stereo vision systems each time the data are recorded.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Calibration of an integrated vision system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Suppose there are n different positions of the LED in 3D space. For the <i>i</i>th position, let the data read by the left vision system be represented as <i>r</i>
                  <sub>
                    <i>l,i</i>
                  </sub> in the left camera coordinate system, and let the data read by the right vision system be represented as <i>r</i>
                  <sub>
                    <i>r,i</i>
                  </sub> in the right camera coordinate system. The centroids <span class="mathjax-tex">\( \bar{r}_{l} \)</span> and <span class="mathjax-tex">\( \bar{r}_{r} \)</span>of the two sets of points can be calculated in the left and the right coordinate systems, respectively, as follows:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \bar{r}_{l} = \frac{1}{n}\sum\limits_{i = 1}^{n} {(r_{l,i} } ),\quad \bar{r}_{r} = \frac{1}{n}\sum\limits_{i = 1}^{n} {(r_{r,i} )} $$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div>
                <p>The transformation from the left to the right coordinate system has the form</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \bar{r}_{r} = sR(\bar{r}_{l} ) + T_{o} $$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where <i>s</i> is a scale factor, <i>T</i>
                  <sub>o</sub> denotes the translation, and <i>R</i>(<i>r</i>
                  <sub>l</sub>) denotes the rotation of vector <span class="mathjax-tex">\( \bar{r}_{l} \)</span>.</p><p>It is practically impossible to find a transformation that maps the measured coordinates of a set of points in one coordinate system exactly into the measured coordinates of these points in the other coordinate system. The residual error can be written as</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ e_{i} = r_{r,i} - sR(r_{l,i} ) - T_{o} $$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
                <p>The sum of squares of errors for n data points can be calculated as</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \sum\limits_{i = 1}^{n} {\;\left\| {e_{i} } \right\|}^{2} . $$</span></div></div>
                <p>The classical approach to finding the transformation parameters <i>s</i>, <i>R</i>, and <i>T</i>
                  <sub>o</sub> is to minimize the sum of squares of errors numerically. However, Horn (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Horn BKP (1987) Closed-form solution of absolute orientation using unit quaternions. J Opt Soc Am A4:629–642" href="/article/10.1007/s10055-011-0204-z#ref-CR8" id="ref-link-section-d17226e1720">1987</a>) derived a closed-form solution to the least-square problem by the use of unit quaternions to represent rotation. This solution has been coded into a software routine by Wengert and Bianchi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wengert C, Bianchi G (2008) Implementation of closed-form solution of absolute orientation using unit quaternions. &#xA;                    http://www.mathworks.com/matlabcentral/fileexchange/22422-absolute-orientation&#xA;                    &#xA;                  . Accessed 8 Feb 2011" href="/article/10.1007/s10055-011-0204-z#ref-CR20" id="ref-link-section-d17226e1724">2008</a>). We used this routine to calculate the transformation parameters for calibrating two stereo vision systems with respect to each other.</p><h3 class="c-article__sub-heading" id="Sec12">Defining the world coordinate system</h3><p>From the application point of view, it is convenient to locate the World Coordinate System at a known position. The procedure we used involves locating an L-shaped wand within the measurement volume as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig8">8</a>. The system calculates the coordinate transformation parameters that relate the Local Coordinate System of each stereo system to the World Coordinate System defined by the wand.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Defining the world coordinate system. <b>a</b> Coordinate transformation between two stereo vision systems. <b>b</b> A wand placed in a workspace where motion capture is performed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Measurement accuracy</h2><div class="c-article-section__content" id="Sec13-content"><p>Based on the principles and different calibration algorithms described above, we have developed vision systems with Wiimotes for 3D motion capture and have assessed their measurement accuracy. The following discusses our experiments with these systems.</p><h3 class="c-article__sub-heading" id="Sec14">Experiment with a two-camera vision system</h3><p>Two Wiimotes were set up as a vision system, and Zhang’s calibration algorithm was used to get the intrinsic and extrinsic parameters of the Wiimotes. We set two LEDs 19.5 cm apart in a straight wand. We moved the wand randomly in 3D space at the distance of 2–2.5 m from the two Wiimotes and calculate the distance between the two markers from the obtained image data for more than 150 locations of the wand. The RMS distance error observed was 2.7 mm. This result was comparable to that obtained by a much more expensive system (Pintaric and Kaufmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Pintaric T, Kaufmann H (2007) Affordable infrared-optical pose-tracking for virtual and augmented reality. In: Proceedings of trends and issues in tracking for virtual environments workshop" href="/article/10.1007/s10055-011-0204-z#ref-CR13" id="ref-link-section-d17226e1778">2007</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig9">9</a> shows the measurement accuracy for a typical single stereo vision system with 2 Wiimotes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Measurement accuracy with a single stereo vision system with 2 Wiimotes using Zhang’s calibration algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec15">Experiments with a four-camera vision system</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Using Svoboda’s calibration algorithm</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig10">10</a> shows our experimental setup with 4 Wiimotes mounted on a computer-assisted virtual environment (CAVE) frame for the evaluation of Svoboda’s calibration algorithm. Two IR LEDs set at a known distance apart were waved randomly inside the CAVE, and their image data were collected. The intrinsic and extrinsic parameters then were determined and used to compute the position of each LED at each calibration point. Points were recorded when the LED was detected by at least three of the four Wiimotes. Then, the scaling factor was determined with a sufficient number of observations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Calibration of 4 Wiimotes based on Svoboda’s calibration algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The measurement accuracy is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig11">11</a>, which depicts the difference between the actual distance of the two LEDs and the distance obtained by the motion capture system employing Svoboda’s calibration algorithm. As indicated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig11">11</a>, the average measurement error for almost 200 locations was 2.37 mm.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Measurement accuracy of a single vision system with 4 Wiimotes using Svoboda’s calibration algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Using the hybrid calibration algorithm</h4><p>In this experiment, the four Wiimotes were calibrated using the hybrid calibration algorithm described above. A wand with two IR LEDs mounted at a known distance apart was moved randomly inside the CAVE, and the two markers’ positions and the distance between them were tracked in real time. The measurement accuracy is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig12">12</a>, wherein the image data used are the same as that in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig11">11</a>. The hybrid calibration algorithm has improved the measurement accuracy from 2.37 mm average error (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig11">11</a>) to 1.07 mm average error (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Measurement accuracy of a single vision system with 4 Wiimotes using the hybrid calibration algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec18">Experiment with an eight-camera vision system</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Result</h4><p>To cover a larger measurement volume, 8 Wiimotes were arranged such that each set of 4 Wiimotes forms a stereo vision system. The two vision systems were integrated using Horn’s algorithm to determine their relative position and orientation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig13">13</a> shows the 8 Wiimotes mounted in the CAVE, whose ceiling is mounted with 2 stereo vision systems each consisting of 4 Wiimotes. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig14">14</a> shows the measurement accuracy of the integrated stereo vision system with 8 Wiimotes, where each subsystem of 4 Wiimotes is calibrated using the hybrid calibration algorithm. The average measurement error for over 1,000 locations is 8.7 mm.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>An integrated vision system with 8 Wiimotes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Measurement accuracy of an integrated vision system with 8 Wiimotes using the hybrid calibration algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Discussion</h4><p>The eight Wiimotes deployed inside the CAVE shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig13">13</a> can be calibrated as one stereo vision system using the hybrid calibration algorithm developed in this paper. The reason we first develop two stereo vision systems, each of which having 4 Wiimotes, and then integrate them together using Horn’s algorithm is that the integrated system provides a significantly larger tracking volume than the one stereo vision system with eight Wiimotes, even though the accuracy of the integrated system is slightly lower. The larger tracking volume is often useful for assembly simulation, which usually involves a large tracking volume on a factory floor.</p><h3 class="c-article__sub-heading" id="Sec21">Data smoothing</h3><p>Because the measured data sometimes fluctuated during the motion capture, different techniques, including the moving average, the B-spline curve, and the Bezier curve, were considered for smoothing the captured data. With consideration of both accuracy and need for real-time animation in the assembly simulation, it was decided to implement the moving average technique. The implementation of this technique in the computer code revealed a considerable improvement in the generated dynamic simulation of CAD models. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig15">15</a> displays the data before and after smoothing using a five-point moving average. It can be seen clearly that the data after taking the moving average is substantially smoother than the original data.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Data before and after smoothing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Application to assembly simulation</h2><div class="c-article-section__content" id="Sec22-content"><p>For automated assembly simulation, we have developed an interface between our motion capture system and the NX software from Siemens. NX provides an application programming interface (API) named NXOpen, which provides interfaces for the user to communicate with CAD models in NX and to perform various tasks through Microsoft’s.NET framework. We have utilized this API to develop assembly simulations, where the user can perform play/pause, zoom, pan, rotate, and other operations and perform analysis on the simulated assembly process.</p><p>During the motion capture, the data from the Wiimote stereo system was stored in an XML file, which contained information about the part being tracked, the timestamp, and the coordinate details of the part. We implemented a speed setting in which the timestamp of the motion capture data and the animation speed can be adjusted according to the time elapsed in the simulation and the current timestamp of the data. The XML data file and the CAD models of the assembly acted as the inputs to the simulation engine we developed. Once the data have been loaded, the user has various options, such as to hide or display unused CAD models, select animation speed, and add more XML data files.</p><h3 class="c-article__sub-heading" id="Sec23">Implementation with a single stereo vision system</h3><p>For demonstration, a Wiimote-based single stereo vision system has been implemented for tracking the assembly operation of a prototype MSAT (microsatellite) frame shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig16">16</a>. CAD models were generated in NX for the various parts of the MSAT frame.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>The MSAT prototype frame. <b>a</b> Physical prototype. <b>b</b> CAD assembly model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The locations for the LEDs mounted on the various MSAT frame parts were decided beforehand. The local coordinate system in the virtual world was set at the same location as that of the physical LED marker. Points of symmetry were normally chosen in mounting the LEDs for ease of locating them physically and specifying the corresponding coordinate frames on the CAD models. Attention was paid to ensure that the marker was exposed to the Wiimotes continuously while carrying out the assembly process. The LEDs used in the motion tracking were powered by button cells. The small sizes of these cells allowed them to attach the LEDs easily on the physical parts. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig17">17</a> shows the corresponding sensor locations on the CAD models for total 14 parts. For each part, one LED was attached to get the part’s 3D position; also, four LEDs can be attached if the part’s orientation is desired. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig18">18</a> depicts the motion capture and the automated generation of CAD model–based simulation in real time during the MSAT assembly process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>LED locations for several parts of the MSAT prototype frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig18_HTML.jpg?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig18_HTML.jpg" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Real-time generation of CAD model simulation with motion capture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec24">Implementation with an integrated stereo vision system</h3><p>This implementation example involves the use of two stereo vision systems, each with two Wiimotes, integrated together into one system for tracking the assembly of a bookshelf (1 m × 1.5 m × 0.5 m). CAD models were generated after taking measurements on an actual bookshelf.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig19">19</a> shows the motion capture experimental setup, where four Wiimotes were mounted on the ceiling. The four Wiimotes were so located as to ensure coverage of the entire volume within which the assembly operation took place. The world coordinate system was defined with the help of an L-shaped wand as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig8">8</a>. For assembly simulation, the locations for mounting LEDs on the total five parts of the bookshelf for motion tracking were decided beforehand and are shown as Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig20">20</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0204-z#Fig21">21</a> displays four snapshots taken from the generated animation of the bookshelf assembly with CAD models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig19_HTML.jpg?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig19_HTML.jpg" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Experimental setup for motion capture for a bookshelf assembly</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig20_HTML.gif?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig20_HTML.gif" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>LED locations for the parts of the bookshelf</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig21_HTML.gif?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0204-z/MediaObjects/10055_2011_204_Fig21_HTML.gif" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Snapshots of automated simulation of bookshelf assembly</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0204-z/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusions</h2><div class="c-article-section__content" id="Sec25-content"><p>This paper describes the development of stereo vision systems with Wiimotes for 3D motion capture and their application to automated simulation of assembly operations. For accuracy improvement, a hybrid camera calibration algorithm based on Zhang’s and Svoboda’s calibration algorithms has been devised and evaluated for a multi-camera vision system. This hybrid calibration method uses Zhang’s algorithm for calibration of intrinsic parameters and Svoboda’s algorithm for calibration of extrinsic parameters. Such stereo vision systems with Wiimotes are much cheaper than commercially available motion capture systems, thus providing great economic benefits for many practical applications. They are wireless, and multiple Wiimote-based systems can be integrated together into a distributed vision system to increase the coverage volume for motion capture. With the Wiimote-based systems, we have demonstrated that the simulation of an assembly operation can be generated automatically in real time with fairly good accuracy. The CAD model–based simulation with motion capture can provide detailed simulations of assembly operations with animated movements of tools, parts, and operators. It can be used to train assembly operations, improve existing assembly tasks, and plan new assembly tasks.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ART GmbH (2011) Advanced realtime tracking GmbH. http://www.ar-tracking.de/. Accessed 8 Feb 2011" /><p class="c-article-references__text" id="ref-CR201">ART GmbH (2011) Advanced realtime tracking GmbH. <a href="http://www.ar-tracking.de/">http://www.ar-tracking.de/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Baggett, A. Ehrefeucht, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Baggett P, Ehrefeucht A (1991) Building physical and mental models in assembly tasks. Int J Ind Ergon 7(3):217" /><p class="c-article-references__text" id="ref-CR1">Baggett P, Ehrefeucht A (1991) Building physical and mental models in assembly tasks. Int J Ind Ergon 7(3):217–227</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0169-8141%2891%2990005-7" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Building%20physical%20and%20mental%20models%20in%20assembly%20tasks&amp;journal=Int%20J%20Ind%20Ergon&amp;volume=7&amp;issue=3&amp;pages=217-227&amp;publication_year=1991&amp;author=Baggett%2CP&amp;author=Ehrefeucht%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bouguet JY (2010) Camera calibration toolbox for Matlab. http://www.vision.caltech.edu/bouguetj/calib_doc/inde" /><p class="c-article-references__text" id="ref-CR2">Bouguet JY (2010) Camera calibration toolbox for Matlab. <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/index.html">http://www.vision.caltech.edu/bouguetj/calib_doc/index.html</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen X, Davis J, Sluallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceed" /><p class="c-article-references__text" id="ref-CR3">Chen X, Davis J, Sluallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), pp 520–527</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Fischler, R. Bolles, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image a" /><p class="c-article-references__text" id="ref-CR4">Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24:381–395</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=618158" aria-label="View reference 5 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F358669.358692" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20sample%20consensus%3A%20a%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography&amp;journal=Commun%20ACM&amp;volume=24&amp;pages=381-395&amp;publication_year=1981&amp;author=Fischler%2CM&amp;author=Bolles%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Hartley, A. Zisserman, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR5">Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple%20view%20geometry%20in%20computer%20vision&amp;publication_year=2003&amp;author=Hartley%2CR&amp;author=Zisserman%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hay S, Newman J, Harley R (2008) Optical tracking using commodity hardware. In: Proceedings of IEEE and ACM in" /><p class="c-article-references__text" id="ref-CR6">Hay S, Newman J, Harley R (2008) Optical tracking using commodity hardware. In: Proceedings of IEEE and ACM international symposium on mixed and augmented reality (ISMAR)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heikkila J, Silven O (1997) A four-step camera calibration procedure with implicit image correction. In: IEEE " /><p class="c-article-references__text" id="ref-CR7">Heikkila J, Silven O (1997) A four-step camera calibration procedure with implicit image correction. In: IEEE computer society conference on computer vision and pattern</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BKP. Horn, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Horn BKP (1987) Closed-form solution of absolute orientation using unit quaternions. J Opt Soc Am A4:629–642" /><p class="c-article-references__text" id="ref-CR8">Horn BKP (1987) Closed-form solution of absolute orientation using unit quaternions. J Opt Soc Am A4:629–642</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1364%2FJOSAA.4.000629" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Closed-form%20solution%20of%20absolute%20orientation%20using%20unit%20quaternions&amp;journal=J%20Opt%20Soc%20Am&amp;volume=A4&amp;pages=629-642&amp;publication_year=1987&amp;author=Horn%2CBKP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="iotracker (2011) Advanced optical motion tracking. http://www.iotracker.com/. Accessed 8 Feb 2011" /><p class="c-article-references__text" id="ref-CR202">iotracker (2011) Advanced optical motion tracking. <a href="http://www.iotracker.com/">http://www.iotracker.com/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kibira D, McLean C (2002) Virtual reality simulation of a mechanical assembly production line. In: Proceedings" /><p class="c-article-references__text" id="ref-CR9">Kibira D, McLean C (2002) Virtual reality simulation of a mechanical assembly production line. In: Proceedings of winter simulation conference, pp 1130–1137</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee J (2007) Head tracking for desktop VR displays using the Wii remote. http://johnnylee.net/projects/wii/. A" /><p class="c-article-references__text" id="ref-CR10">Lee J (2007) Head tracking for desktop VR displays using the Wii remote. <a href="http://johnnylee.net/projects/wii/">http://johnnylee.net/projects/wii/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martinec D, Pajdla T (2002) Structure from many perspective images with occlusions. In: Proceedings of the Eur" /><p class="c-article-references__text" id="ref-CR11">Martinec D, Pajdla T (2002) Structure from many perspective images with occlusions. In: Proceedings of the European conference on computer vision, pp 355–369</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Melen T (1994) Geometrical modeling and calibration of video cameras for underwater navigation. Dissertation, " /><p class="c-article-references__text" id="ref-CR12">Melen T (1994) Geometrical modeling and calibration of video cameras for underwater navigation. Dissertation, Institute for teknisk kybernetikk, Norges tekniske hogskole</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="NaturalPoint (2011) OptiTrack. http://www.naturalpoint.com/optitrack/. Accessed 8 Feb 2011" /><p class="c-article-references__text" id="ref-CR203">NaturalPoint (2011) OptiTrack. <a href="http://www.naturalpoint.com/optitrack/">http://www.naturalpoint.com/optitrack/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="PhaseSpace (2011) PhaseSpace optical motion capture. http://www.phasespace.com/. Accessed 8 Feb 2011" /><p class="c-article-references__text" id="ref-CR204">PhaseSpace (2011) PhaseSpace optical motion capture. <a href="http://www.phasespace.com/">http://www.phasespace.com/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pintaric T, Kaufmann H (2007) Affordable infrared-optical pose-tracking for virtual and augmented reality. In:" /><p class="c-article-references__text" id="ref-CR13">Pintaric T, Kaufmann H (2007) Affordable infrared-optical pose-tracking for virtual and augmented reality. In: Proceedings of trends and issues in tracking for virtual environments workshop</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sturm P, Triggs B (1996) A factorization based algorithm for multi-image projective structure and motion. In: " /><p class="c-article-references__text" id="ref-CR15">Sturm P, Triggs B (1996) A factorization based algorithm for multi-image projective structure and motion. In: European conference on computer vision, pp 709–720</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Svoboda, D. Martinec, T. Pajdla, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Svoboda T, Martinec D, Pajdla T (2005) A convenient multi-camera self-calibration for virtual environments. Te" /><p class="c-article-references__text" id="ref-CR16">Svoboda T, Martinec D, Pajdla T (2005) A convenient multi-camera self-calibration for virtual environments. Teleoperators Virtual Environ 14(4):407–422</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474605774785325" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20convenient%20multi-camera%20self-calibration%20for%20virtual%20environments&amp;journal=Teleoperators%20Virtual%20Environ&amp;volume=14&amp;issue=4&amp;pages=407-422&amp;publication_year=2005&amp;author=Svoboda%2CT&amp;author=Martinec%2CD&amp;author=Pajdla%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tsai RY (1986) An efficient and accurate camera calibration technique for 3d-machine vision. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR17">Tsai RY (1986) An efficient and accurate camera calibration technique for 3d-machine vision. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 364–374</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vicon (2011) Vicon motion system. http://www.vicon.com/company/. Accessed 8 Feb 2011" /><p class="c-article-references__text" id="ref-CR205">Vicon (2011) Vicon motion system. <a href="http://www.vicon.com/company/">http://www.vicon.com/company/</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang W, Guo B, Li X, Cao J (2008) Influence factors evaluation on high-precision planar calibration of non-met" /><p class="c-article-references__text" id="ref-CR19">Wang W, Guo B, Li X, Cao J (2008) Influence factors evaluation on high-precision planar calibration of non-metric digital camera. In: The international archives of the photogrammetry, remote sensing and spatial information science, Vol. XXXVII (B1)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wengert C, Bianchi G (2008) Implementation of closed-form solution of absolute orientation using unit quaterni" /><p class="c-article-references__text" id="ref-CR20">Wengert C, Bianchi G (2008) Implementation of closed-form solution of absolute orientation using unit quaternions. <a href="http://www.mathworks.com/matlabcentral/fileexchange/22422-absolute-orientation">http://www.mathworks.com/matlabcentral/fileexchange/22422-absolute-orientation</a>. Accessed 8 Feb 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhang, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):133" /><p class="c-article-references__text" id="ref-CR21">Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.888718" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20flexible%20new%20technique%20for%20camera%20calibration&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=11&amp;pages=1330-1334&amp;publication_year=2000&amp;author=Zhang%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zollner H, Sablatnig R (2004) Comparison of methods for geometric camera calibration using planar calibration " /><p class="c-article-references__text" id="ref-CR22">Zollner H, Sablatnig R (2004) Comparison of methods for geometric camera calibration using planar calibration targets. In: Proceedings of 28th workshop Austrian association of pattern recognition, pp 234–224</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0204-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to acknowledge the financial support for this research from the Center for Aerospace Manufacturing Technologies (CAMT) through Air Force Research Laboratory contract FA8650–04–C–5704 and the CAMT Industrial Consortium whose members include Boeing, Spirit AeroSystems, GKN Aerospace, Bell Helicopters, Rolls Royce, Siemens, KMT Waterjet, Product Innovation and Engineering, and Steelville Manufacturing.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Missouri University of Science and Technology, Rolla, MO, USA</p><p class="c-article-author-affiliation__authors-list">Wenjuan Zhu, Ming C. Leu &amp; Xiaoqing F. Liu</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Caterpillar Inc., Mossville, IL, USA</p><p class="c-article-author-affiliation__authors-list">Anup M. Vader</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Salesforce.com, San Francisco, CA, USA</p><p class="c-article-author-affiliation__authors-list">Abhinav Chadda</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Boeing Research and Technology, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Jonathan B. Vance</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Wenjuan-Zhu"><span class="c-article-authors-search__title u-h3 js-search-name">Wenjuan Zhu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Wenjuan+Zhu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wenjuan+Zhu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wenjuan+Zhu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Anup_M_-Vader"><span class="c-article-authors-search__title u-h3 js-search-name">Anup M. Vader</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Anup M.+Vader&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Anup M.+Vader" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Anup M.+Vader%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Abhinav-Chadda"><span class="c-article-authors-search__title u-h3 js-search-name">Abhinav Chadda</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Abhinav+Chadda&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Abhinav+Chadda" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Abhinav+Chadda%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ming_C_-Leu"><span class="c-article-authors-search__title u-h3 js-search-name">Ming C. Leu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ming C.+Leu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ming C.+Leu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ming C.+Leu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Xiaoqing_F_-Liu"><span class="c-article-authors-search__title u-h3 js-search-name">Xiaoqing F. Liu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Xiaoqing F.+Liu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xiaoqing F.+Liu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xiaoqing F.+Liu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jonathan_B_-Vance"><span class="c-article-authors-search__title u-h3 js-search-name">Jonathan B. Vance</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jonathan B.+Vance&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jonathan B.+Vance" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jonathan B.+Vance%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0204-z/email/correspondent/c1/new">Wenjuan Zhu</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>This work was done while the authors Anup M. Vader and Abhinav Chadda were with Missouri University of Science and Technology.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Wii%20remote%E2%80%93based%20low-cost%20motion%20capture%20for%20automated%20assembly%20simulation&amp;author=Wenjuan%20Zhu%20et%20al&amp;contentID=10.1007%2Fs10055-011-0204-z&amp;publication=1359-4338&amp;publicationDate=2011-12-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Zhu, W., Vader, A.M., Chadda, A. <i>et al.</i> Wii remote–based low-cost motion capture for automated assembly simulation.
                    <i>Virtual Reality</i> <b>17, </b>125–136 (2013). https://doi.org/10.1007/s10055-011-0204-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0204-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-03-17">17 March 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-02">02 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-23">23 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06">June 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0204-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0204-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Wii remote</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Low cost</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Motion capture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hybrid camera calibration algorithm</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Assembly simulation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0204-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=204;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

