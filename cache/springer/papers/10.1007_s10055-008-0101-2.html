<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Design and implementation of medical training simulators"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper discusses the design issues and implementation details of building a medical training simulator. Example projects that have been undertaken by the Visualization and Medical Graphics..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/12/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Design and implementation of medical training simulators"/>

    <meta name="dc.source" content="Virtual Reality 2008 12:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2008-10-22"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper discusses the design issues and implementation details of building a medical training simulator. Example projects that have been undertaken by the Visualization and Medical Graphics group at Bangor University and our collaborators are used to illustrate the points made. A detailed case study is then presented of a virtual environment designed to train the Seldinger Technique, a common procedure in interventional radiology. The paper will introduce a medical practitioner to the technology behind a medical virtual environment. It will also provide an engineer with an overview of many of the issues that need to be considered when undertaking to build such an application. The paper ends with the author&#8217;s views on future developments in this exciting domain."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2008-10-22"/>

    <meta name="prism.volume" content="12"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="269"/>

    <meta name="prism.endingPage" content="279"/>

    <meta name="prism.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-008-0101-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-008-0101-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-008-0101-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-008-0101-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Design and implementation of medical training simulators"/>

    <meta name="citation_volume" content="12"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2008/12"/>

    <meta name="citation_online_date" content="2008/10/22"/>

    <meta name="citation_firstpage" content="269"/>

    <meta name="citation_lastpage" content="279"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-008-0101-2"/>

    <meta name="DOI" content="10.1007/s10055-008-0101-2"/>

    <meta name="citation_doi" content="10.1007/s10055-008-0101-2"/>

    <meta name="description" content="This paper discusses the design issues and implementation details of building a medical training simulator. Example projects that have been undertaken by t"/>

    <meta name="dc.creator" content="Nigel W. John"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=The visible human project; citation_author=MJ Ackerman; citation_volume=86; citation_issue=3; citation_publication_date=1998; citation_pages=504-511; citation_doi=10.1109/5.662875; citation_id=CR1"/>

    <meta name="citation_reference" content="Allard J, Cotin S, Faure F, Bensoussan PJ, Poyer F, Duriez C, Delingette H, Grisoni L (2007) SOFA&#8212;an open source framework for medical simulation. In: Medicine meets virtual reality 15. Studies in health technology and informatics, vol 125. IOS Press, pp 13&#8211;18"/>

    <meta name="citation_reference" content="Azar T, Hayward V (2008) Estimation of the fracture toughness of soft tissue from needle insertion. Springer lecture notes in computer science, vol 5104, pp 166&#8211;175. doi: 
                    10.1007/978-3-540-70521-5
                    
                  
                        "/>

    <meta name="citation_reference" content="Buckley B, John NW (2008) Efficient soft tissue modelling using charged particle control points, Eurographics 2008 short paper, Crete, pp 191&#8211;194. ISSN 1017-4656"/>

    <meta name="citation_reference" content="Cavusoglu MC, Goktekin TG, Tendick F, Sastry SS (2004) GiPSi: an open source/open architecture software development framework for surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 98. IOS Press, pp 46&#8211;48"/>

    <meta name="citation_reference" content="citation_journal_title=Endosc Rev; citation_title=Augmented reality in surgery ARIS*ER, research training network for minimally invasive therapy technologies; citation_author=A Freudenthal, E Samset, B Gersak, J Declerck, D Schmalsieg, S Casciaro, O Rident, J Vander Sloten; citation_volume=10; citation_issue=23; citation_publication_date=2005; citation_pages=5-10; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Efficient collision culling among deformable objects using graphics processors; citation_author=NK Govindaraju, MC Lin, D Manocha; citation_volume=15; citation_issue=1; citation_publication_date=2006; citation_pages=62-76; citation_doi=10.1162/pres.2006.15.1.62; citation_id=CR8"/>

    <meta name="citation_reference" content="Gustafsson J, Lindfors C, Mattsson L, Kjellberg T. (2005) Large-format 3D interaction table In: Woods AJ, Bolas MT, Merritt JO, McDowall IE (eds) Stereoscopic display and virtual reality systems XII: SPIE 5664, pp 589&#8211;595"/>

    <meta name="citation_reference" content="Healey AE, Evans JC, Murphy MG, Powell S et al (2005) In vivo force during arterial interventional radiology needle puncture procedures. In: Medicine meets virtual reality 13. Studies in health technology and informatics, vol 111. IOS Press, pp 178&#8211;184"/>

    <meta name="citation_reference" content="Hughes CJ, John NW (2006) A flexible infrastructure for delivering augmented reality enabled transcranial magnetic stimulation. In: Proceedings of medicine meets virtual reality 14, 24&#8211;27 January 2006, Long Beach, CA, pp 219&#8211;114"/>

    <meta name="citation_reference" content="citation_journal_title=Otol Neurotol; citation_title=Developing a virtual reality environment in petrous bone surgery; citation_author=A Jackson, NW John, NA Thacker, RT Ramsden; citation_volume=23; citation_publication_date=2002; citation_pages=111-121; citation_doi=10.1097/00129492-200203000-00001; citation_id=CR12"/>

    <meta name="citation_reference" content="John NW (2002) Using stereoscopy for medical VR, In: Medicine meets virtual reality 02/10. Studies in health technology and informatics, IOS Press, ISSN: 0926-9630, ISBN 1-58603-203-8, pp 214&#8211;220"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Educ; citation_title=The impact of Web3D technologies on medical education and training; citation_author=NW John; citation_volume=49; citation_issue=1; citation_publication_date=2007; citation_pages=19-31; citation_doi=10.1016/j.compedu.2005.06.003; citation_id=CR14"/>

    <meta name="citation_reference" content="John NW, Riding M, Phillips NI, Mackay S, Steineke L, Fontaine B et al (2001) Web-based surgical educational tools. Medicine meets virtual reality 9. Studies in health technology and informatics. IOS Press, pp 212&#8211;217. ISBN 1-58603-143-0"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Aided Surg; citation_title=Interrogation of patient data delivered to the operating theatre during hepato-pancreatic surgery using high performance computing; citation_author=NW John, RF McCloy, S Herrman; citation_volume=9; citation_issue=6; citation_publication_date=2004; citation_pages=235-242; citation_doi=10.1080/10929080500163430; citation_id=CR16"/>

    <meta name="citation_reference" content="John NW, Aratow M, Couch J, Evestedt D, Hudson AD, Polys N et al (2008) MedX3D: standards enabled desktop medical 3D, Stud Health Technol Inform. In: Proceedings of medicine meets virtual reality 2008, vol 132. pp 189&#8211;194. ISBN 978-1-58603-822-9"/>

    <meta name="citation_reference" content="citation_journal_title=J Clin Radiol; citation_title=Physical and cognitive task analysis in interventional radiology; citation_author=SJ Johnson, AE Healey, JC Evans, MG Murphy, M Crawshaw, DA Gould; citation_volume=61; citation_issue=1; citation_publication_date=2005; citation_pages=97-103; citation_doi=10.1016/j.crad.2005.09.003; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=A survey of surgical simulation: applications, technology, and education; citation_author=A Liu, F Tendick, K Cleary, C Kaufmann; citation_volume=12; citation_issue=6; citation_publication_date=2003; citation_pages=599-614; citation_doi=10.1162/105474603322955905; citation_id=CR19"/>

    <meta name="citation_reference" content="Montgomery K, Bruyns C, Brown J, Sorkin S, Mazzella F, Thonier G, Tellier A, Lerman B, Menon A (2002) Spring: a general framework for collaborative, real-time surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 85. IOS Press, pp 296&#8211;303"/>

    <meta name="citation_reference" content="Moorthy K, Mansoori M, Bello F, Hance J, Undre S, Munz Y et al (2004) Evaluation of the benefit of VR simulation in a multi-media web-based educational tool. In: Medicine meets virtual reality 12. Studies in health technology and informatics. IOS Press, pp 247&#8211;252"/>

    <meta name="citation_reference" content="National Capital Area Medical Simulator Center Surgical Simulation Laboratory (2008) Web Site, 
                    http://simcen.org/surgery/index.html
                    
                   Last visited August 2008"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Biomed Eng; citation_title=Force modeling for needle insertion into soft tissue; citation_author=AM Okamura, C Simone, MD O&#8217;Leary; citation_volume=51; citation_issue=10; citation_publication_date=2004; citation_pages=1707-1716; citation_doi=10.1109/TBME.2004.831542; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=A survey of general-purpose computation on graphics hardware; citation_author=JD Owens, D Luebke, N Govindaraju, M Harris; citation_volume=26; citation_issue=1; citation_publication_date=2007; citation_pages=80-113; citation_doi=10.1111/j.1467-8659.2007.01012.x; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Dextrous virtual work; citation_author=T Poston, L Serra; citation_volume=39; citation_issue=5; citation_publication_date=1996; citation_pages=37-45; citation_doi=10.1145/229459.229464; citation_id=CR25"/>

    <meta name="citation_reference" content="Ruspini DC, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of SIGGRAPH 1997, pp 345&#8211;352"/>

    <meta name="citation_reference" content="citation_journal_title=BMC Bioinformatics; citation_title=High-throughput sequence alignment using graphics processing units; citation_author=MC Schatz, C Trapnell, AL Delcher, A Varshney; citation_volume=8; citation_publication_date=2007; citation_pages=474; citation_doi=10.1186/1471-2105-8-474; citation_id=CR27"/>

    <meta name="citation_reference" content="Schroeder W, Martin K, Lorensen B (2006) The visualization toolkit an object-oriented approach to 3D graphics, 4th edn. ISBN: 1-930934-19-X"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Radiologica; citation_title=Catheter replacement of the needle in percutaneous arteriography; a new technique; citation_author=SI Seldinger; citation_volume=39; citation_issue=5; citation_publication_date=1953; citation_pages=368-376; citation_doi=10.3109/00016925309136722; citation_id=CR29"/>

    <meta name="citation_reference" content="Shahabi C, Ortega A, Kolahdouzan MR (2002) A comparison of different haptic compression techniques. IEEE international conference on multimedia and expo, vol 1, pp 657&#8211;660"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Advanced graphics behind medical virtual reality: evolution of algorithms, hardware, and software interfaces; citation_author=Z Soferman, D Blythe, NW John; citation_volume=86; citation_issue=3; citation_publication_date=1998; citation_pages=531-554; citation_doi=10.1109/5.662878; citation_id=CR30"/>

    <meta name="citation_reference" content="Thomas RG, John NW, Lim IS (2006) A mixed reality anatomy teaching tool. In: Proceedings of theory and practice of computer graphics 2006, Teesside, 165&#8211;170"/>

    <meta name="citation_reference" content="Vidal, F.P., John, N.W. (2007) Interactive Physically-Based X-Ray Simulation: CPU or GPU? In: Proceedings of medicine meets virtual reality, Stud Health Technol Inform vol 125, pp 479&#8211;481"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Principles and applications of computer graphics in medicine; citation_author=FP Vidal, F Bello, K Brodlie, NW John, DA Gould, R Phillips, N Avis; citation_volume=25; citation_issue=1; citation_publication_date=2006; citation_pages=113-137; citation_doi=10.1111/j.1467-8659.2006.00822.x; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Animat Virtual Worlds; citation_title=Simulation of ultrasound guided needle puncture using patient specific data with 3D textures and volume haptics; citation_author=FP Vidal, NW John, AE Healey, DA Gould; citation_volume=19; citation_issue=2; citation_publication_date=2008; citation_pages=111-127; citation_doi=10.1002/cav.217; citation_id=CR34"/>

    <meta name="citation_reference" content="Vidal FP, Healy AE, John NW, Gould DA (2008) Force penetration of chiba needles for haptic rendering in ultrasound guided needle puncture training simulator. In: Workshop on needle steering: recent results and future opportunities, MICCAI 2008, New York"/>

    <meta name="citation_author" content="Nigel W. John"/>

    <meta name="citation_author_email" content="n.w.john@bangor.ac.uk"/>

    <meta name="citation_author_institution" content="Bangor University, Bangor, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-008-0101-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2008/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-008-0101-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Design and implementation of medical training simulators"/>
        <meta property="og:description" content="This paper discusses the design issues and implementation details of building a medical training simulator. Example projects that have been undertaken by the Visualization and Medical Graphics group at Bangor University and our collaborators are used to illustrate the points made. A detailed case study is then presented of a virtual environment designed to train the Seldinger Technique, a common procedure in interventional radiology. The paper will introduce a medical practitioner to the technology behind a medical virtual environment. It will also provide an engineer with an overview of many of the issues that need to be considered when undertaking to build such an application. The paper ends with the author’s views on future developments in this exciting domain."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Design and implementation of medical training simulators | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-008-0101-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Medical virtual environments, Haptics, 3D displays, Graphics hardware, Seldinger technique, Augmented reality","kwrd":["Medical_virtual_environments","Haptics","3D_displays","Graphics_hardware","Seldinger_technique","Augmented_reality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-008-0101-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-008-0101-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=101;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-008-0101-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Design and implementation of medical training simulators
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0101-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0101-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2008-10-22" itemprop="datePublished">22 October 2008</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Design and implementation of medical training simulators</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nigel_W_-John" data-author-popup="auth-Nigel_W_-John" data-corresp-id="c1">Nigel W. John<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Bangor University" /><meta itemprop="address" content="grid.7362.0, 0000000118820937, Bangor University, Bangor, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 12</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">269</span>–<span itemprop="pageEnd">279</span>(<span data-test="article-publication-year">2008</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">425 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">12 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-008-0101-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper discusses the design issues and implementation details of building a medical training simulator. Example projects that have been undertaken by the Visualization and Medical Graphics group at Bangor University and our collaborators are used to illustrate the points made. A detailed case study is then presented of a virtual environment designed to train the Seldinger Technique, a common procedure in interventional radiology. The paper will introduce a medical practitioner to the technology behind a medical virtual environment. It will also provide an engineer with an overview of many of the issues that need to be considered when undertaking to build such an application. The paper ends with the author’s views on future developments in this exciting domain.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Training based on an apprenticeship model has been used effectively by the medical profession for centuries. As technology has progressed many different tools and techniques have been deployed to provide added value to the training process. In recent years one of the most exciting developments has been the use of immersive virtual environments to provide structured and flexible training scenarios. Great efforts have been made in the last decade to try to increase the fidelity of medical simulation to approach that of training on real patients, as documented in survey papers (Soferman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Soferman Z, Blythe D, John NW (1998) Advanced graphics behind medical virtual reality: evolution of algorithms, hardware, and software interfaces. Proc IEEE 86(3):531–554" href="/article/10.1007/s10055-008-0101-2#ref-CR30" id="ref-link-section-d19324e287">1998</a>; Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Liu A, Tendick F, Cleary K, Kaufmann C (2003) A survey of surgical simulation: applications, technology, and education. Presence Teleoper Virtual Environ 12(6):599–614" href="/article/10.1007/s10055-008-0101-2#ref-CR19" id="ref-link-section-d19324e290">2003</a>; Vidal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Vidal FP, Bello F, Brodlie K, John NW, Gould DA, Phillips R, Avis N (2006) Principles and applications of computer graphics in medicine. Comput Graph Forum 25(1):113–137" href="/article/10.1007/s10055-008-0101-2#ref-CR33" id="ref-link-section-d19324e293">2006</a>). However, we are still a long way from replicating the real life environment. This paper provides an update on the latest technology being used to create a medical training simulator, and highlights issues and trends identified by the research team at Bangor and our collaborators. The impact of advances in software algorithms will be discussed, together with the constraints and advantages of using off-the-shelf hardware components. It is important to note that building such a simulator is a multi-disciplinary process. This paper has a technology focus, but equally important for success are contributions from experts in task analysis, validation studies and guidance from subject matter experts. The case study presented in section three will include a discussion of all of these important aspects. The paper ends with a forward look at the next generation of medical training simulators.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Components of a medical training simulator</h2><div class="c-article-section__content" id="Sec2-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig1">1</a> summarises the typical components that make up a medical virtual environment (VE). The input is usually patient specific medical scan data such as CT, MRI or other imaging modality, in the digital imaging and communications in medicine (DICOM) format. Stock anatomical models may also be used, plus CAD models of any instruments or other equipment needed. The computer processor has much work to do. Segmentation, volume and/or surface rendering, collision detection, soft body deformation, cutting and other effects must be carried out, often in real time. The user interacts with the three-dimensional (3D) models in the simulator using haptics, stereoscopy, audio, etc. Often a mannequin will also be integrated into the environment, although this does limit the variations possible in the simulation. The following sections discuss many of the different options available for building a simulator, illustrated using results from our own research activities.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Typical components of a medical training simulator</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec3">Data sources</h3><p>Not withstanding the requirements for patient privacy and ethical constraints, access to patient image data has never been more straightforward. The DICOM standard has ensured interoperability between medical scanners and the hospital databases and picture archiving and communication systems (PACS). Many hospitals today also provide secure access to image data over the World Wide Web or via a bespoke telemedicine application. Whereas maximum flexibility can be provided within a medical simulator if patient specific data is used on demand, there are also many other sources of high quality data sets available for training and education purposes. The most well known is the Visible Human data set (Ackerman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ackerman MJ (1998) The visible human project. Proc IEEE 86(3):504–511" href="/article/10.1007/s10055-008-0101-2#ref-CR1" id="ref-link-section-d19324e332">1998</a>). The detailed scans and images from this and other similar sources have been used to generate anatomical objects for many simulators.</p><p>Support for 3D data is important for an anatomical education tool or medical VE. It has been shown that the Web can be used as an infrastructure for delivery of 3D medical applications—see (John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="John NW (2007) The impact of Web3D technologies on medical education and training. Comput Educ 49(1):19–31" href="/article/10.1007/s10055-008-0101-2#ref-CR14" id="ref-link-section-d19324e338">2007</a>) for a full survey. One example produced using the ISO VRML97 standard for 3D graphics is a web-based simulation of a lumbar puncture procedure (John et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="John NW, Riding M, Phillips NI, Mackay S, Steineke L, Fontaine B et al (2001) Web-based surgical educational tools. Medicine meets virtual reality 9. Studies in health technology and informatics. IOS Press, pp 212–217. ISBN 1-58603-143-0" href="/article/10.1007/s10055-008-0101-2#ref-CR15" id="ref-link-section-d19324e341">2001</a>), see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig2">2</a>a. The VRML anatomy models were created by segmenting CT data, and the application allows the user to interact with these models in a web browser using a computer mouse. Each step in the lumbar puncture procedure is simulated, from positioning the patient, identifying landmarks and inserting the needle. Despite the limitations of just using web-based delivery, validation studies show that this web-based tool does improve the training of students in performing this procedure (Moorthy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Moorthy K, Mansoori M, Bello F, Hance J, Undre S, Munz Y et al (2004) Evaluation of the benefit of VR simulation in a multi-media web-based educational tool. In: Medicine meets virtual reality 12. Studies in health technology and informatics. IOS Press, pp 247–252" href="/article/10.1007/s10055-008-0101-2#ref-CR21" id="ref-link-section-d19324e347">2004</a>). It also has the advantage that the application can be accessed from any network connected PC and at any time. Recently, the Web 3D Consortium has begun an initiative to make an extension to the ISO X3D standard (the successor to VRML) that will better cater for 3D medical applications. The prototype of this extension—MedX3D (John et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="John NW, Aratow M, Couch J, Evestedt D, Hudson AD, Polys N et al (2008) MedX3D: standards enabled desktop medical 3D, Stud Health Technol Inform. In: Proceedings of medicine meets virtual reality 2008, vol 132. pp 189–194. ISBN 978-1-58603-822-9" href="/article/10.1007/s10055-008-0101-2#ref-CR17" id="ref-link-section-d19324e350">2008</a>)—provides support for importing DICOM image data, volume rendering in a web browser (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig2">2</a>b) and integration with medical ontologies. Discussions are taking place with the DICOM Working Group 17 (3D) members who are responsible for extending the DICOM standard with respect to 3D and other multi-dimensional data sets.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Web-based medical tools (<b>a</b>). VRML lumbar puncture simulator (<b>b</b>). MedX3D volume rendering in Firefox web browser</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Data descriptions of all of the instruments (needles, guidewires, scalpel, trocars, etc.) used in a particular medical procedure are also required. It is relatively straightforward to create a geometric model of an instrument/tool using your favourite CAD package. However, access to data on the physical properties of instruments is more difficult to obtain. Medical manufacturers of catheters and guidewires, for example, will not disclose such information as they regard this as their intellectual property. Also, obtaining information about the physical properties of human soft tissue is non trivial as it is a viscoelastic material with complex behaviour that varies enormously with tissue type, age, sex and other factors. A good summary of the problems of tissue modelling and characterisation can be found in (Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Liu A, Tendick F, Cleary K, Kaufmann C (2003) A survey of surgical simulation: applications, technology, and education. Presence Teleoper Virtual Environ 12(6):599–614" href="/article/10.1007/s10055-008-0101-2#ref-CR19" id="ref-link-section-d19324e385">2003</a>). Therefore, to be able to model how much force is used when carrying out different tasks, experimental data must be acquired. There is relatively little of this type of data reported in the literature, and where it does exist the accuracy is limited as experiments are typically carried out ex vivo, for example (Okamura et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Okamura AM, Simone C, O’Leary MD (2004) Force modeling for needle insertion into soft tissue. IEEE Trans Biomed Eng 51(10):1707–1716" href="/article/10.1007/s10055-008-0101-2#ref-CR23" id="ref-link-section-d19324e388">2004</a>), or in vivo using animal studies, for example (Azar and Hayward <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Azar T, Hayward V (2008) Estimation of the fracture toughness of soft tissue from needle insertion. Springer lecture notes in computer science, vol 5104, pp 166–175. doi: &#xA;                    10.1007/978-3-540-70521-5&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-008-0101-2#ref-CR3" id="ref-link-section-d19324e391">2008</a>). Our collaborators at the Royal Liverpool Hospital have been able to provide data for interventional radiology procedures by designing an in vitro experiment with a tensile tester and pig and ox tissues (Healey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Healey AE, Evans JC, Murphy MG, Powell S et al (2005) In vivo force during arterial interventional radiology needle puncture procedures. In: Medicine meets virtual reality 13. Studies in health technology and informatics, vol 111. IOS Press, pp 178–184" href="/article/10.1007/s10055-008-0101-2#ref-CR10" id="ref-link-section-d19324e394">2005</a>). Force penetration data has been generated of a Chiba needle (which is a flexible, thin wall steel needle) as it is inserted into liver and/or kidney. Without such data then the fidelity of the final simulator will be detrimentally affected. This is discussed further in the case study described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-008-0101-2#Sec10">3</a>.</p><h3 class="c-article__sub-heading" id="Sec4">Processor hardware</h3><p>The computer processor is at the core of the medical simulator and has always been one of the major technological factors responsible for the advancement and assimilation of medical virtual environments. The hardware platform has to be chosen carefully so that real time performance can be maintained whilst executing complex software algorithms and dealing with all user interactions. In a 1998 survey (Soferman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Soferman Z, Blythe D, John NW (1998) Advanced graphics behind medical virtual reality: evolution of algorithms, hardware, and software interfaces. Proc IEEE 86(3):531–554" href="/article/10.1007/s10055-008-0101-2#ref-CR30" id="ref-link-section-d19324e408">1998</a>), we assessed three generations of graphics hardware and the impact that each generation has had on medical applications, and predicted the performance gains to be expected from the next generation. One way in which powerful third generation graphics hardware (such as the SGI Infinite Reality graphics) can be exploited is through remote rendering, and we pioneered a system for guiding hepato-pancreatic surgery using this approach (John et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="John NW, McCloy RF, Herrman S (2004) Interrogation of patient data delivered to the operating theatre during hepato-pancreatic surgery using high performance computing. Comput Aided Surg 9(6):235–242" href="/article/10.1007/s10055-008-0101-2#ref-CR16" id="ref-link-section-d19324e411">2004</a>). Volume rendered images of the patient were delivered across the network to the operating room in real time, where they could be interrogated by the surgeon running a specially designed application on a laptop client. However, it has been evident for some time that the immediate future lies with commodity off-the-shelf computer graphics hardware. In the last few years, the fourth generation of graphics hardware has matured and has far exceeded performance predictions. Stemming from the demands of the gaming industry, companies such as NVIDIA (Santa Clara, California) and AMD/ATI (Sunnyvale, California) have driven down the cost of graphics hardware whilst significantly increasing performance. Combined with simultaneous improvements in CPU technology and the increased bandwidth of the internal communication bus, it is now possible to simulate many complex procedures fast enough for inclusion in a training simulator. The graphics processing unit (GPU) can be programmed directly providing the developer with access to sophisticated parallel processing graphics hardware, which can then be used to implement computationally expensive algorithms (not necessarily graphical algorithms). We have used this approach to simulate X-ray attenuation with a 60 times speed up on an equivalent CPU only implementation (Vidal and John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Vidal, F.P., John, N.W. (2007) Interactive Physically-Based X-Ray Simulation: CPU or GPU? In: Proceedings of medicine meets virtual reality, Stud Health Technol Inform vol 125, pp 479–481" href="/article/10.1007/s10055-008-0101-2#ref-CR32" id="ref-link-section-d19324e414">2007</a>), and there are many other examples of exploiting the GPU for computational tasks (Owens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Owens JD, Luebke D, Govindaraju N, Harris M et al (2007) A survey of general-purpose computation on graphics hardware. Comput Graph Forum 26(1):80–113" href="/article/10.1007/s10055-008-0101-2#ref-CR24" id="ref-link-section-d19324e417">2007</a>). Development environments such as NVIDIA’s CUDA (compute unified device architecture) are now available to the programmer to facilitate access to the GPU, and CUDA applications are already providing significant performance gains in bioinformatics (Schatz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Schatz MC, Trapnell C, Delcher AL, Varshney A (2007) High-throughput sequence alignment using graphics processing units. BMC Bioinformatics 8:474" href="/article/10.1007/s10055-008-0101-2#ref-CR27" id="ref-link-section-d19324e420">2007</a>) and elsewhere. We are starting to use CUDA for the interventional radiology simulator currently being developed in Bangor and described below.</p><h3 class="c-article__sub-heading" id="Sec5">Human computer interaction</h3><p>The face validity of a medical simulator greatly depends on the choices made for providing the human computer interaction (HCI). Issues to consider include whether to use an immersive environment or a mannequin based system; use of haptics devices; tracking of tools/instruments; custom build of devices; and cost. Often the actual instruments from the procedure being simulated can be taken and integrated into the training environment. As with graphics hardware, many of these HCI components have benefited from increases in performance and falling costs, and continue to do so.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Haptics</h4><p>The word “haptics” derives from the Greek hapthai, meaning the sense of touch. Haptics hardware can provide both tactile and force feedback effects. A tactile response provides information on contact surface geometry, roughness, slippage and/or temperature and can therefore provide important cues within a medical simulation. Apart from cell phones that vibrate to provide a tactile cue to the user, the technology is not yet widespread. Pin arrays that fit onto a finger tip, and pneumatic based systems have been investigated for use in medical simulators but much work is still needed.</p><p>Hardware for force feedback is more mature and many medical simulators use either an off-the-shelf haptics device—such as the PHANToM range of robotic arms from SensAble Technologies (Woburn, USA), or a purpose built haptics device—such as the laparoscopic impulse engine from Immersion Medical (Gaithersburg, USA). Ideally the haptics device will provide six degrees of freedom (DOF), with an active force response to both position and orientation. However, devices with six DOF are expensive and most off-the-shelf haptics devices only provide three DOF force feedback to the stylus position. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig1">1</a> (Interaction), two 3 DOF PHANToM Omni devices can be seen, one of which has been set up as a virtual ultrasound transducer and the default stylus has been replaced with a mock up of a real transducer. In this example, the user interacts with the virtual patient using the haptics devices within the immersive environment. An alternative approach is to place the haptics device inside a mannequin. For example, the Medic Vision (Melbourne, Australia) Mediseus<sup>®</sup> Epidural simulator places a PHANToM Desktop robotic arm inside a mannequin and it is used to provide the appropriate resistance as the epidural needle is inserted. In many simulators, however, it is difficult to achieve the exact fidelity of the real life procedure and applying force feedback directly to the user’s hand is a major challenge.</p><p>The games market is set to provide a catalyst for further improvements in price and performance to haptics technology. For example, the NOVINT Falcon haptics device was released in 2008 costing just $190. The Falcon offers similar functionality to a three DOF PHANToM, but does have more limitations such as a smaller workspace. However, it is adequate for certain tasks in a medical simulator and is ten times cheaper than its nearest competitor. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig3">3</a>, we have customised a Falcon by mounting it at 90° from its production configuration so that it can provide force feedback in an up and down direction. A small platform has been fixed to the three haptics arms onto which additional components can be placed to provide a tactile response, e.g. a piezoelectric surface, or even vibrations from a small audio speaker. A particular example of a common procedure that relies on tactile response is palpation, in which a doctor presses lightly on the surface of the body to feel the organs or tissues underneath. We are using the configuration in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig3">3</a> to investigate whether palpation can be simulated through this novel set up.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Customised Falcon for experimenting with combined tactile and force feedback. Insert picture shows three small piezoelectric surfaces (the discs) onto which fingertips are placed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Display technologies</h4><p>If a medical simulator is based on a VE then providing support for stereoscopy vision is important i.e. 3D vision produced by the fusion of two slightly different views of a scene on each retina. There are several techniques for achieving this and stereoscopy is widely supported in medical simulators (John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="John NW (2002) Using stereoscopy for medical VR, In: Medicine meets virtual reality 02/10. Studies in health technology and informatics, IOS Press, ISSN: 0926-9630, ISBN 1-58603-203-8, pp 214–220" href="/article/10.1007/s10055-008-0101-2#ref-CR13" id="ref-link-section-d19324e479">2002</a>). Active and passive stereo are common approaches, requiring the user to wear shutter or polarised glasses, respectively. However, the requirement to wear glassed can be detrimental to the simulation. Autostereoscopy displays are therefore becoming more important as this latest generation of 3D displays do not require the user to wear special glasses but the user must be in the correct position relative to the display. We used such a display in a previous project to build a planning environment for mastoidechtomy (Jackson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Jackson A, John NW, Thacker NA, Ramsden RT et al (2002) Developing a virtual reality environment in petrous bone surgery. Otol Neurotol 23:111–121" href="/article/10.1007/s10055-008-0101-2#ref-CR12" id="ref-link-section-d19324e482">2002</a>).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig1">1</a>, a virtual workbench system [first proposed in (Poston and Serra <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poston T, Serra L (1996) Dextrous virtual work. Commun ACM 39(5):37–45" href="/article/10.1007/s10055-008-0101-2#ref-CR25" id="ref-link-section-d19324e491">1996</a>)] is being used to co-locate 3D vision using active stereo with the haptics devices. Workbench systems are readily available commercially and provide a flexible platform in which to build a VE. The virtual patient appears to be positioned below a semi-transparent mirror, in the same workspace as the haptic devices, which can act as a needle or other instrument. The co-location of 3D vision and haptics greatly enhances the immersive effect. This type of environment has many advantages over the use of a mannequin based simulator, such as the ability to use different data sets easily, and to provide an animated virtual patient with respiration and other physiology processes modelled, rather than just providing a virtual cadaver.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Other hardware components</h4><p>Other hardware components may be needed for the medical simulator, such as a way of tracking the tools being used. Tracking hardware is available based on magnetic, optical, ultrasonic, mechanical and inertial approaches.</p><p>One of our latest projects in Bangor is to investigate the use of games consoles and their peripherals for medical simulation. There is a high demand in developing countries (and elsewhere) for inexpensive training tools for common procedures such as ultrasound scanning. We have started to develop an application for this purpose that uses the Nintendo Company (Kyoto, Japan) Wii Remote Controller and sensor bar, taken from the games console. The Wii Remote only costs $40, but is a sophisticated bluetooth device containing an infrared camera with built-in hardware tracking. It has proved to be remarkably adept at providing a low cost “dummy” ultrasound transducer—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig4">4</a>, where the training application is running on a laptop using the bluetooth wireless protocol to connect the Wii Remote. A tactile vibration is triggered on the Wii Remote when the virtual transducer is in contact with the skin of the virtual patient. We simulate an ultrasound image to correspond to the transducer position using the technique described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-008-0101-2#Sec13">3.2.1</a>. Performance will improve further with the use of the new Wii MotionPlus accessory that attaches to the end of the Wii Remote and allows for more comprehensive tracking of the user’s arm position and orientation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Using the Nintendo <i>Wii Remote</i> for ultrasonography training</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec9">Algorithms and software</h3><p>The simulation software has to make effective use of the hardware components described above and also implement sophisticated techniques including 3D segmentation of medical scan data, soft tissue modelling, collision detection and other dynamic and physiological processes. Low level APIs such as OpenGL and DirectX are commonly used for the visualization component and these continue to support the new functionality of graphics hardware. For surgical simulation and haptics, higher level APIs are becoming available. Chai3D (from Stanford University) and H3D (from SenseGraphics, Sweden) are two examples of open source haptics development environments. Also widely used is the Visualization ToolKit (VTK), which supports scalar, vector, tensor, texture and volumetric methods; and advanced modelling techniques such as implicit modelling, polygon reduction, mesh smoothing and cutting (Schroeder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schroeder W, Martin K, Lorensen B (2006) The visualization toolkit an object-oriented approach to 3D graphics, 4th edn. ISBN: 1-930934-19-X" href="/article/10.1007/s10055-008-0101-2#ref-CR28" id="ref-link-section-d19324e542">2006</a>).</p><p>Software specifically designed for medical simulation is available as open source. The Simulation Open Framework Architecture (SOFA) (Allard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Allard J, Cotin S, Faure F, Bensoussan PJ, Poyer F, Duriez C, Delingette H, Grisoni L (2007) SOFA—an open source framework for medical simulation. In: Medicine meets virtual reality 15. Studies in health technology and informatics, vol 125. IOS Press, pp 13–18" href="/article/10.1007/s10055-008-0101-2#ref-CR2" id="ref-link-section-d19324e548">2007</a>), SPRING (Montgomery et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Montgomery K, Bruyns C, Brown J, Sorkin S, Mazzella F, Thonier G, Tellier A, Lerman B, Menon A (2002) Spring: a general framework for collaborative, real-time surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 85. IOS Press, pp 296–303" href="/article/10.1007/s10055-008-0101-2#ref-CR20" id="ref-link-section-d19324e551">2002</a>), and the general physical simulation interface (GiPSi) (Cavusoglu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Cavusoglu MC, Goktekin TG, Tendick F, Sastry SS (2004) GiPSi: an open source/open architecture software development framework for surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 98. IOS Press, pp 46–48" href="/article/10.1007/s10055-008-0101-2#ref-CR5" id="ref-link-section-d19324e554">2004</a>), are the most well known examples. They aim to provide the developer with ready access to the well established algorithms without having to re-invent the wheel, plus provide an appropriate framework for the introduction of new techniques. For example, these APIs support the two physics-based techniques widely implemented for soft tissue deformations: mass-spring models and finite element modeling (FEM), plus algorithms for fluid flow, articulated bodies and force modelling (for overviews of these techniques refer to Liu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Liu A, Tendick F, Cleary K, Kaufmann C (2003) A survey of surgical simulation: applications, technology, and education. Presence Teleoper Virtual Environ 12(6):599–614" href="/article/10.1007/s10055-008-0101-2#ref-CR19" id="ref-link-section-d19324e557">2003</a>); Vidal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Vidal FP, Bello F, Brodlie K, John NW, Gould DA, Phillips R, Avis N (2006) Principles and applications of computer graphics in medicine. Comput Graph Forum 25(1):113–137" href="/article/10.1007/s10055-008-0101-2#ref-CR33" id="ref-link-section-d19324e560">2006</a>).</p><p>Another challenge in building a medical simulator is to calculate collision detection between instruments and tissues in real time and to provide an appropriate response. For high fidelity, the exact instant of each collision needs to be calculated before updating the configuration of the physical bodies. A closed form solution only exists in simple cases, however, and so a numerical root finder is usually involved. Self intersection of tissues causes additional problems. There is much ongoing research to optimise collision detection algorithms exploiting temporal coherence, low resolution proxies, spatial partitioning, etc. GPU based implementations are also being developed (Govindaraju et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Govindaraju NK, Lin MC, Manocha D (2006) Efficient collision culling among deformable objects using graphics processors. Presence Teleoper Virtual Environ 15(1):62–76" href="/article/10.1007/s10055-008-0101-2#ref-CR8" id="ref-link-section-d19324e566">2006</a>) (and SOFA is using CUDA to speed up collision detection). The collision response must then determine the deformation effect on soft tissues, possible cutting or tearing of the tissue, and an appropriate force and tactile feedback that can be fed into the haptics hardware. At Bangor we are currently investigating fast methods to combine deformation effects with haptics based on the concept of charged particle control points (Buckley and John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Buckley B, John NW (2008) Efficient soft tissue modelling using charged particle control points, Eurographics 2008 short paper, Crete, pp 191–194. ISSN 1017-4656" href="/article/10.1007/s10055-008-0101-2#ref-CR4" id="ref-link-section-d19324e569">2008</a>). The tissue surface is modeled using a particle system. Each particle is treated as if it has an electromagnetic charge and it is this that determines how the particles will interact with each other, and also with the haptic interaction point (HIP), which will ultimately produce the deformation of the object. The particles are required to obey the basic laws of electromagnetic interaction. Particles of a similar charge will repel and those with opposing charges will attract. We have achieved interactive haptic response using over 6,000 charged particles on a standard configuration PC. Each particle is also used as a control point for a much higher resolution Bezier surface to be rendered.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Case study from interventional radiology</h2><div class="c-article-section__content" id="Sec10-content"><p>We are currently working with our partners in the CRaIVE (collaborators in Radiological Interventional Virtual Environments) network to build a physics-based simulator to enable training of the Seldinger Technique (Seldinger <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1953" title="Seldinger SI (1953) Catheter replacement of the needle in percutaneous arteriography; a new technique. Acta Radiologica 39(5):368–376" href="/article/10.1007/s10055-008-0101-2#ref-CR29" id="ref-link-section-d19324e581">1953</a>). In this commonly performed procedure in interventional radiology, the desired vessel is punctured with a sharp hollow needle, with ultrasound guidance if necessary. A round-tipped guidewire is then advanced through the needle, and the needle is withdrawn. A catheter can then be passed over the guidewire into the cavity or vessel and the guidewire withdrawn. This section will provide a case study of building a medical VE for training the Seldinger Technique.</p><h3 class="c-article__sub-heading" id="Sec11">Task analysis</h3><p>A detailed task analysis of the procedure is essential to identify the individual steps and to document the decision process involved at each point. This information can be obtained by analysing video footage of the procedure being carried out, and through interviews with subject experts. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0101-2#Tab1">1</a> shows an excerpt from the task analysis that the CRaIVE network has produced, here describing the beginning of one of the critical steps in the procedure—puncturing an artery. See Johnson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Johnson SJ, Healey AE, Evans JC, Murphy MG, Crawshaw M, Gould DA (2005) Physical and cognitive task analysis in interventional radiology. J Clin Radiol 61(1):97–103" href="/article/10.1007/s10055-008-0101-2#ref-CR18" id="ref-link-section-d19324e594">2005</a>) for full details of the methodology used to obtain this task analysis. This information then informs the actual design and realisation of the simulator. Pre-procedure steps and general preparation such as cleaning the incision site and injection of local anaesthetic have been documented in this way. All of this information must be provided within the training curriculum. At this time, however, our VE begins with the ultrasound image guided needle puncture.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Excerpt from Task Analysis of Arterial Puncture</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-008-0101-2/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The task analysis also identifies the key metrics that will be measured in the VE and used for validation. The metrics are performance indicators and include: any errors made, puncture site location, time taken, path taken by the needle and the final catheter position.</p><h3 class="c-article__sub-heading" id="Sec12">Image guided needle puncture</h3><p>In addition to the task analysis, the design of the VE for training the Seldinger Technique has been influenced by a desire to minimize hardware costs without compromising the fidelity of the simulator. Wherever possible, commercial off the shelf components have been used. A desktop PC with Intel Xeon 3 GHz CPU, 2 GB of RAM, and a NVIDIA Quadro FX graphics card has been used. For the needle puncture step, two PHANToM Omni haptic devices have been connected to the PC using the high speed IEEE 1394 (FireWire) interface—one to act as an ultrasound (US) transducer, and the other to act as the needle. These are housed within the virtual workbench system that can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig1">1</a>. We are using patient specific CT scan data, supplied in DICOM format. The two main technical problems to solve for this stage of the simulation are: to generate ultrasound-like images from CT source data; and to provide an appropriate force model for the ultrasound transducer and needle. An overview of our solutions is presented below, with full details available in (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference Vidal et al. 2008" title="Vidal FP, John NW, Healey AE, Gould DA (2008a) Simulation of ultrasound guided needle puncture using patient specific data with 3D textures and volume haptics. Comput Animat Virtual Worlds 19(2):111–127" href="/article/10.1007/s10055-008-0101-2#ref-CR34" id="ref-link-section-d19324e788">Vidal et al. 2008</a>a).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Virtual ultrasound imaging</h4><p>As the virtual US transducer is moved over the skin of the virtual patient, a two-dimensional ultrasound-like image is generated (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig5">5</a>). When the needle is inserted, it must also be visible on this image. The algorithm used to generate an ultrasound-like image from the CT source data takes advantage of the GPU and proceeds as follows:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Needle puncture virtual environment. The ultrasound-like image has been generated from the CT source data. The needle can be observed in the upper right of the image, penetrating into the liver</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>A 2D multi-planar reconstruction (MPR) image is extracted from the CT voxel data set based on the current position and orientation of the transducer. This can be efficiently achieved using the frame buffer object architecture (FBO)—an extension to OpenGL for flexible off-screen rendering, including rendering to a texture. FBOs are often used to implement image filters, and post processing effects.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>All voxels that have been penetrated by the needle are assigned a high value corresponding to the metallic material of the needle shaft, which reflects US. This is implemented using texture hardware on the graphics card accessed via the OpenGL Shading Language (GLSL).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Acoustic shadowing occurs in ultrasound images due to the presence of dense structures such as bones and reflection at any tissue/gas interface. This effect can be simulated by post-processing the MPR image to compute a shadow mask.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>High frequency noise is added to make the image resemble true ultrasound images.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Bright reflections may also occur in US images at interfaces such as with bone, gas and fat/tissue. This effect can be produced in the final image by detecting and enhancing horizontal edges in the MPR image using a Sobel filter.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">6.</span>
                        
                          <p>The MPR image, the shadow mask and the noise data are then blended using multi-texturing.</p>
                        
                      </li>
                    </ol>
                           <p>Images from both linear and curvilinear US probes can be generated.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Force model</h4><p>The virtual representation of the US transducer and needle can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig5">5</a>. As stated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-008-0101-2#Sec3">2.1</a>, we have experimental data for a needle penetrating pig and ox tissues—see example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Force penetration of a Chiba needle into pig kidney from a tensile tester (needle velocity is 500 mm per min)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>This in vitro data has been calibrated on real patients through the use of capacitance pads and other miniaturised sensors that can be attached unobtrusively to the radiologist (Vidal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Vidal FP, Healy AE, John NW, Gould DA (2008) Force penetration of chiba needles for haptic rendering in ultrasound guided needle puncture training simulator. In: Workshop on needle steering: recent results and future opportunities, MICCAI 2008, New York" href="/article/10.1007/s10055-008-0101-2#ref-CR35" id="ref-link-section-d19324e923">2008b</a>). Note that there is an initial displacement, approximately 10 mm, before the needle tip penetrates the kidney. The needle will at first push the kidney away and then at penetration a rebound force can be observed. This is followed by a plateau phase where the force slightly increases with the depth of penetration. This behaviour can be modelled analytically using a radial-basis function (RBF) network, where each RBF corresponds to a Gaussian function. Thus we obtain essential information on tissue properties and we use this to help to create a haptics look up table (LUT). The position of the tip of the virtual needle is tracked as it penetrates the virtual patient. The voxel in which the needle tip currently resides can therefore be identified in the original CT data set. The CT Hounsfield value of that voxel is used as the index to the LUT, and it returns an appropriate force co-efficient depending on the tissue type. This co-efficient is determined from a combination of the experimental data results and other known properties e.g. the needle cannot penetrate bone. A force calculation (using Hooke’s Law) is then applied to the haptics device.</p><p>A force model for the virtual ultrasound transducer is straightforward. We adopt a proxy-based algorithm that is commonly used for the haptic rendering of surfaces (Ruspini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ruspini DC, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of SIGGRAPH 1997, pp 345–352" href="/article/10.1007/s10055-008-0101-2#ref-CR26" id="ref-link-section-d19324e929">1997</a>). The Omni device interacts with the surface of a 3D object, usually represented as a triangular polygon mesh. In the proxy-based algorithm, a small sphere, the virtual proxy, represents the desired position of the device. Although the device can penetrate into the object with which it is interacting, the proxy always remains outside of the surface. At each iteration of the algorithm, the proxy is moved in order to minimise its distance to the actual device, i.e. the proxy is the point on the surface that is the closest to the actual device position. The surface will then constrain the haptic device to reach the proxy using a force proportional to the distance between the proxy and the actual device, the force feedback is then similar to a spring effect. Face validity is also improved by replacing the Omni stylus with a physical model of a real transducer.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Physiology processes</h4><p>Improvements to the VE are ongoing, in particular, we need a virtual patient that is “alive” and therefore incorporates movement of the diaphragm and rib cage due to respiration, and other physiological processes. The relevant organs are segmented from the patient data and animated accordingly using a mass-spring system with rigid connections (work being led by our CRaIVE collaborators at Imperial College).</p><p>Another important cue to the trainee is whether or not blood spurts out of the needle and the colour of the blood—see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-008-0101-2#Tab1">1</a>. Human blood can be regarded as a homogenous fluid from a macroscopic viewpoint, hence established numerical techniques based on continuum mechanics, such as the finite difference method (FDM), finite volume method (FVM) and finite element method (FEM), have been used to analyse blood flow as homogeneous fluid. At the microscopic level, however, blood is regarded as a suspension in which solid blood cells, such as red and white blood cells and platelets are suspended in fluid plasma. Consequently, a particle systems method is a natural choice for simulating blood flow on a blood cellular scale, in which each component of blood is modelled by an assembly of discrete particles. Our initial implementation uses a simple particle system to model the particle attributes and to produce a blood spurt from the needle. However, we are currently researching a more sophisticated model that will take into account the interface between solid blood cells and liquid blood flow. By directly modelling blood cells and plasma we will be able to investigate blood flow behaviour in different flow conditions (e.g. microcapillaries, bifurcation regimes).</p><h3 class="c-article__sub-heading" id="Sec16">Guidewire and catheter</h3><p>After needle insertion, a round-tipped guidewire is advanced through the needle, and the needle is withdrawn. A catheter is then passed over the guidewire into the cavity or vessel and the guidewire withdrawn. Our task analysis has confirmed that haptics provide important cues to the interventional radiologist during manipulation of the guidewire and catheter. Within the VE, a robotic arm such as the PHANToM is not suitable for the manipulation of these long, flexible instruments. However, the Mentice (Gothenburg, Sweden) Vascular Simulation Platform (VSP) has been designed for use with real catheters and guidewires and can be programmed to give a haptic response—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig7">7</a>a. Two catheters and/or guidewires can be inserted into the VSP. Optical motion sensors are used for tracking the current depth of insertion and solenoids produce the desired force feedback effect. The VSP is currently being used as a second station in our Seldinger Technique trainer. We are modelling the catheter and guidewire using a structured particle system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig7">7</a>b) and performing collision detection with the segmented vascular structure (the aorta, the iliac arteries and the femoral arteries). Realistic motion of theses instruments can then be simulated within the VE. We are using the hardware assisted algorithms implemented within SOFA to provide collision detection at interactive rates.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>
                                       <b>a</b> Vascular simulation platform. <b>b</b> Simulation model of <i>catheter</i> and <i>guidewire</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The current hardware used is not ideal for the Seldinger Technique, however. For example, the VSP can only provide a force effect on a wire after it has been deployed by 50 mm. This means that feedback of the wire being inserted into a needle cannot be realised immediately. We are exploring the use of bespoke hardware customization to solve this problem.</p><p>Fluoroscopy uses X-rays to produce a visible image on a fluorescent screen. An interventional radiologist uses fluoroscopy to guide the positioning of the instruments inside the patient. The attenuation of X-rays can be simulated by ray tracing, and this is a fast process if just using polygonal meshes within the VE. Our solution also supports X-ray image simulation from volume data, which requires more computation. It makes use of an efficient GLSL implementation of ray-tracing through voxel data to compute the attenuation of X-rays with regard to the length of the ray path across each voxel crossed by the incident beam (Vidal and John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Vidal, F.P., John, N.W. (2007) Interactive Physically-Based X-Ray Simulation: CPU or GPU? In: Proceedings of medicine meets virtual reality, Stud Health Technol Inform vol 125, pp 479–481" href="/article/10.1007/s10055-008-0101-2#ref-CR32" id="ref-link-section-d19324e996">2007</a>).</p><h3 class="c-article__sub-heading" id="Sec17">Validation</h3><p>The most important question in any project to build a medical virtual environment is does it actually improve the trainees skill at carrying out the procedure without negative training artifacts? Extensive validation is therefore an ongoing process run in parallel to the development phase. Following the attainment of ethical approval, experienced and trainee subjects have been recruited within the partner hospitals. Construct and content validation studies are currently being carried out. Where the skills necessary for the VE are also those of the real procedure, experts will be expected to perform better than novices (construct validity). The use of different measures may reveal that different aspects of the procedure are more or less well simulated as determined by the experts (content validity). These studies will be followed by a transfer of training test to compare the effectiveness of the VE to traditional training methods in a predictive validation. Two groups of novices will be trained, one by conventional methods, the other with the additional aid of the simulator. An experienced supervisor, blinded to the training groups, will perform objective assessments of competence using a global rating scale with metrics derived from the task analysis. Assessment will include cognitive and psychomotor elements, e.g. rating specific steps, time taken, errors, number of attempts required and movements made. Thus transfer of training can be measured by success in the real world procedure and so provide an overall comparison between the effectiveness of conventional and VE-enhanced training. The use of a range of metrics may show that different aspects of the procedure benefit differentially from simulator training. The results of these validation steps for the Seldinger Technique trainer will be published in due course together with recommendations on integration of the simulator into curricula for radiology training and quantitative measures for certification.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Conclusions and forward look</h2><div class="c-article-section__content" id="Sec18-content"><p>The last decade has been significant for the development of virtual environments for training medical procedures. There are research laboratories across the world endeavouring to increase the fidelity of medical training simulators, and hospitals can already purchase bespoke systems from companies such as Symbionix (Tel Aviv, Israel), Immersion Medical, and Mentice. The first generation of medial simulators targeted laparoscopy procedures but training solutions for other fields of medicine are also starting to appear. Custom hardware and software are needed to deliver these solutions but technology from other fields is having an increasing positive impact on what is achievable. We predict that the next generation of virtual environments will closely replicate the real world operating room. The trainee surgeon, anaesthetist, nurses and other operating room staff will participate in a collaborative environment that will support both team and individual tasks. Mock-ups of all of the equipment used in real life will be provided with a high fidelity virtual patient replacing the real patient. The beginnings of this type of training environment can already be seen, e.g. at the US National Capital Area Medical Simulator Center (National Capital Area Medical Simulator Center Surgical Simulation Laboratory (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="National Capital Area Medical Simulator Center Surgical Simulation Laboratory (2008) Web Site, &#xA;                    http://simcen.org/surgery/index.html&#xA;                    &#xA;                   Last visited August 2008" href="/article/10.1007/s10055-008-0101-2#ref-CR22" id="ref-link-section-d19324e1016">2008</a>)). Their operating room makes use of sophisticated mannequin-based patient simulators and can be configured as a hospital emergency room or an intensive care unit (ICU). The next technology leap will be the ability to replace the mannequin patient with a fully configurable virtual patient. One enabling technology in achieving this will be new display technology such as the interactive autostereoscopic display being developed by KTH (Stockholm, Sweden) (Gustafsson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gustafsson J, Lindfors C, Mattsson L, Kjellberg T. (2005) Large-format 3D interaction table In: Woods AJ, Bolas MT, Merritt JO, McDowall IE (eds) Stereoscopic display and virtual reality systems XII: SPIE 5664, pp 589–595" href="/article/10.1007/s10055-008-0101-2#ref-CR9" id="ref-link-section-d19324e1019">2005</a>). It uses a holographic optical element as a projection screen for number of digital projectors. Multiple users have simultaneous views of the 3D image and virtual objects can be moved or added interactively. It is feasible to integrate haptics devices with the hologram and a proposal between Bangor, KTH and others is underway to develop an open surgery simulator for nissen fundoplication as a test procedure. As noted above, however, there are many limitations with haptic devices. To be able to simulate open surgery, a device that can be attached directly to the hands and arms will be needed, and it has to be light and not restrict the movement of the wearer. Such a device does not exist yet but the CyberGrasp exoskeleton from Immersion (Shahabi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Shahabi C, Ortega A, Kolahdouzan MR (2002) A comparison of different haptic compression techniques. IEEE international conference on multimedia and expo, vol 1, pp 657–660" href="/article/10.1007/s10055-008-0101-2#ref-CR6" id="ref-link-section-d19324e1022">2002</a>) is an indication of what will be possible in the future.</p><p>As well as training applications, diagnosis (e.g. virtual colonoscopy and other screening tests), and surgical navigation are also benefiting from many of the technology advancements highlighted in this paper. An interesting development in the latter domain is the use augmented reality (AR) so that 3D graphics techniques can be used to augment the reality as we see it with digital content—see (Freudenthal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Freudenthal A, Samset E, Gersak B, Declerck J, Schmalsieg D, Casciaro S, Rident O, Vander Sloten J (2005) Augmented reality in surgery ARIS*ER, research training network for minimally invasive therapy technologies. Endosc Rev 10(23):5–10" href="/article/10.1007/s10055-008-0101-2#ref-CR7" id="ref-link-section-d19324e1028">2005</a>) for an introduction to this topic. For example, the view through a surgical microscope can be augmented with information about the location of delicate structures that are normally out of sight below the visible surface. Eventually, ultrasound guided needle puncture could be replaced by an AR system where a 3D rendering of the target anatomy is overlaid onto the actual patient. AR is one of the active research strands at Bangor and we have produced an AR interface for a procedure called transcranial magnetic stimulation (Hughes and John <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hughes CJ, John NW (2006) A flexible infrastructure for delivering augmented reality enabled transcranial magnetic stimulation. In: Proceedings of medicine meets virtual reality 14, 24–27 January 2006, Long Beach, CA, pp 219–114" href="/article/10.1007/s10055-008-0101-2#ref-CR11" id="ref-link-section-d19324e1031">2006</a>). This is a process in which electrical activity in the brain is influenced by a pulsed magnetic field. Common practice is to align an electromagnetic coil onto the subject’s head with points of interest identified on the surface of the brain. An AR interface allows us to superimpose a registered 3D brain surface created from the MRI scan of the subject onto a video feed of the patient, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig8">8</a>a. To be able to use feature points identified in the video frames for real time tracking and alignment we made use of a high performance computer connected to the computational Grid, but this was completely transparent to the user. This requirement will disappear when the CPU power of a desktop PC increases as it undoubtedly will. Another AR project at Bangor is in developing a tool for anatomy education (Thomas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Thomas RG, John NW, Lim IS (2006) A mixed reality anatomy teaching tool. In: Proceedings of theory and practice of computer graphics 2006, Teesside, 165–170" href="/article/10.1007/s10055-008-0101-2#ref-CR31" id="ref-link-section-d19324e1037">2006</a>), see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0101-2#Fig8">8</a>b. We are investigating the hypothesis that an AR anatomy education environment can match the learning achieved from cadaver dissection.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0101-2/MediaObjects/10055_2008_101_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Augmented reality applications at Bangor. <b>a</b> Bangor augmented reality interface for TMS (BART). Feature points are used to align the real and computer generated scenes. <b>b</b> Snapshot from the AR anatomy education tool currently in development</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0101-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>We are in a critical period for the future development of medical training simulators. There are certainly procedures where a VE can already provide added value to the training process. The price performance of the processing hardware and other components needed has improved dramatically and software algorithms are becoming more sophisticated and efficient. We are witnessing a significant ramp up in the number of solutions available from research laboratories and companies. However, there is still a long way to go before the fidelity of training on a real patient can be matched. It is important that comprehensive validation studies accompany any technological development and that full advantage is taken of multi-disciplinary research to achieve the next generation of medical training simulation.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Ackerman, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Ackerman MJ (1998) The visible human project. Proc IEEE 86(3):504–511" /><p class="c-article-references__text" id="ref-CR1">Ackerman MJ (1998) The visible human project. Proc IEEE 86(3):504–511</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5.662875" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visible%20human%20project&amp;journal=Proc%20IEEE&amp;volume=86&amp;issue=3&amp;pages=504-511&amp;publication_year=1998&amp;author=Ackerman%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Allard J, Cotin S, Faure F, Bensoussan PJ, Poyer F, Duriez C, Delingette H, Grisoni L (2007) SOFA—an open sour" /><p class="c-article-references__text" id="ref-CR2">Allard J, Cotin S, Faure F, Bensoussan PJ, Poyer F, Duriez C, Delingette H, Grisoni L (2007) SOFA—an open source framework for medical simulation. In: Medicine meets virtual reality 15. Studies in health technology and informatics, vol 125. IOS Press, pp 13–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Azar T, Hayward V (2008) Estimation of the fracture toughness of soft tissue from needle insertion. Springer l" /><p class="c-article-references__text" id="ref-CR3">Azar T, Hayward V (2008) Estimation of the fracture toughness of soft tissue from needle insertion. Springer lecture notes in computer science, vol 5104, pp 166–175. doi: <a href="https://doi.org/10.1007/978-3-540-70521-5">10.1007/978-3-540-70521-5</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buckley B, John NW (2008) Efficient soft tissue modelling using charged particle control points, Eurographics " /><p class="c-article-references__text" id="ref-CR4">Buckley B, John NW (2008) Efficient soft tissue modelling using charged particle control points, Eurographics 2008 short paper, Crete, pp 191–194. ISSN 1017-4656</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cavusoglu MC, Goktekin TG, Tendick F, Sastry SS (2004) GiPSi: an open source/open architecture software develo" /><p class="c-article-references__text" id="ref-CR5">Cavusoglu MC, Goktekin TG, Tendick F, Sastry SS (2004) GiPSi: an open source/open architecture software development framework for surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 98. IOS Press, pp 46–48</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Freudenthal, E. Samset, B. Gersak, J. Declerck, D. Schmalsieg, S. Casciaro, O. Rident, J. Vander Sloten, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Freudenthal A, Samset E, Gersak B, Declerck J, Schmalsieg D, Casciaro S, Rident O, Vander Sloten J (2005) Augm" /><p class="c-article-references__text" id="ref-CR7">Freudenthal A, Samset E, Gersak B, Declerck J, Schmalsieg D, Casciaro S, Rident O, Vander Sloten J (2005) Augmented reality in surgery ARIS*ER, research training network for minimally invasive therapy technologies. Endosc Rev 10(23):5–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20in%20surgery%20ARIS%2AER%2C%20research%20training%20network%20for%20minimally%20invasive%20therapy%20technologies&amp;journal=Endosc%20Rev&amp;volume=10&amp;issue=23&amp;pages=5-10&amp;publication_year=2005&amp;author=Freudenthal%2CA&amp;author=Samset%2CE&amp;author=Gersak%2CB&amp;author=Declerck%2CJ&amp;author=Schmalsieg%2CD&amp;author=Casciaro%2CS&amp;author=Rident%2CO&amp;author=Vander%20Sloten%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NK. Govindaraju, MC. Lin, D. Manocha, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Govindaraju NK, Lin MC, Manocha D (2006) Efficient collision culling among deformable objects using graphics p" /><p class="c-article-references__text" id="ref-CR8">Govindaraju NK, Lin MC, Manocha D (2006) Efficient collision culling among deformable objects using graphics processors. Presence Teleoper Virtual Environ 15(1):62–76</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.2006.15.1.62" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20collision%20culling%20among%20deformable%20objects%20using%20graphics%20processors&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=15&amp;issue=1&amp;pages=62-76&amp;publication_year=2006&amp;author=Govindaraju%2CNK&amp;author=Lin%2CMC&amp;author=Manocha%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gustafsson J, Lindfors C, Mattsson L, Kjellberg T. (2005) Large-format 3D interaction table In: Woods AJ, Bola" /><p class="c-article-references__text" id="ref-CR9">Gustafsson J, Lindfors C, Mattsson L, Kjellberg T. (2005) Large-format 3D interaction table In: Woods AJ, Bolas MT, Merritt JO, McDowall IE (eds) Stereoscopic display and virtual reality systems XII: SPIE 5664, pp 589–595</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Healey AE, Evans JC, Murphy MG, Powell S et al (2005) In vivo force during arterial interventional radiology n" /><p class="c-article-references__text" id="ref-CR10">Healey AE, Evans JC, Murphy MG, Powell S et al (2005) In vivo force during arterial interventional radiology needle puncture procedures. In: Medicine meets virtual reality 13. Studies in health technology and informatics, vol 111. IOS Press, pp 178–184</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hughes CJ, John NW (2006) A flexible infrastructure for delivering augmented reality enabled transcranial magn" /><p class="c-article-references__text" id="ref-CR11">Hughes CJ, John NW (2006) A flexible infrastructure for delivering augmented reality enabled transcranial magnetic stimulation. In: Proceedings of medicine meets virtual reality 14, 24–27 January 2006, Long Beach, CA, pp 219–114</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Jackson, NW. John, NA. Thacker, RT. Ramsden, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Jackson A, John NW, Thacker NA, Ramsden RT et al (2002) Developing a virtual reality environment in petrous bo" /><p class="c-article-references__text" id="ref-CR12">Jackson A, John NW, Thacker NA, Ramsden RT et al (2002) Developing a virtual reality environment in petrous bone surgery. Otol Neurotol 23:111–121</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F00129492-200203000-00001" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20a%20virtual%20reality%20environment%20in%20petrous%20bone%20surgery&amp;journal=Otol%20Neurotol&amp;volume=23&amp;pages=111-121&amp;publication_year=2002&amp;author=Jackson%2CA&amp;author=John%2CNW&amp;author=Thacker%2CNA&amp;author=Ramsden%2CRT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="John NW (2002) Using stereoscopy for medical VR, In: Medicine meets virtual reality 02/10. Studies in health t" /><p class="c-article-references__text" id="ref-CR13">John NW (2002) Using stereoscopy for medical VR, In: Medicine meets virtual reality 02/10. Studies in health technology and informatics, IOS Press, ISSN: 0926-9630, ISBN 1-58603-203-8, pp 214–220</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NW. John, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="John NW (2007) The impact of Web3D technologies on medical education and training. Comput Educ 49(1):19–31" /><p class="c-article-references__text" id="ref-CR14">John NW (2007) The impact of Web3D technologies on medical education and training. Comput Educ 49(1):19–31</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.compedu.2005.06.003" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20impact%20of%20Web3D%20technologies%20on%20medical%20education%20and%20training&amp;journal=Comput%20Educ&amp;volume=49&amp;issue=1&amp;pages=19-31&amp;publication_year=2007&amp;author=John%2CNW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="John NW, Riding M, Phillips NI, Mackay S, Steineke L, Fontaine B et al (2001) Web-based surgical educational t" /><p class="c-article-references__text" id="ref-CR15">John NW, Riding M, Phillips NI, Mackay S, Steineke L, Fontaine B et al (2001) Web-based surgical educational tools. Medicine meets virtual reality 9. Studies in health technology and informatics. IOS Press, pp 212–217. ISBN 1-58603-143-0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NW. John, RF. McCloy, S. Herrman, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="John NW, McCloy RF, Herrman S (2004) Interrogation of patient data delivered to the operating theatre during h" /><p class="c-article-references__text" id="ref-CR16">John NW, McCloy RF, Herrman S (2004) Interrogation of patient data delivered to the operating theatre during hepato-pancreatic surgery using high performance computing. Comput Aided Surg 9(6):235–242</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10929080500163430" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interrogation%20of%20patient%20data%20delivered%20to%20the%20operating%20theatre%20during%20hepato-pancreatic%20surgery%20using%20high%20performance%20computing&amp;journal=Comput%20Aided%20Surg&amp;volume=9&amp;issue=6&amp;pages=235-242&amp;publication_year=2004&amp;author=John%2CNW&amp;author=McCloy%2CRF&amp;author=Herrman%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="John NW, Aratow M, Couch J, Evestedt D, Hudson AD, Polys N et al (2008) MedX3D: standards enabled desktop medi" /><p class="c-article-references__text" id="ref-CR17">John NW, Aratow M, Couch J, Evestedt D, Hudson AD, Polys N et al (2008) MedX3D: standards enabled desktop medical 3D, Stud Health Technol Inform. In: Proceedings of medicine meets virtual reality 2008, vol 132. pp 189–194. ISBN 978-1-58603-822-9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Johnson, AE. Healey, JC. Evans, MG. Murphy, M. Crawshaw, DA. Gould, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Johnson SJ, Healey AE, Evans JC, Murphy MG, Crawshaw M, Gould DA (2005) Physical and cognitive task analysis i" /><p class="c-article-references__text" id="ref-CR18">Johnson SJ, Healey AE, Evans JC, Murphy MG, Crawshaw M, Gould DA (2005) Physical and cognitive task analysis in interventional radiology. J Clin Radiol 61(1):97–103</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.crad.2005.09.003" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physical%20and%20cognitive%20task%20analysis%20in%20interventional%20radiology&amp;journal=J%20Clin%20Radiol&amp;volume=61&amp;issue=1&amp;pages=97-103&amp;publication_year=2005&amp;author=Johnson%2CSJ&amp;author=Healey%2CAE&amp;author=Evans%2CJC&amp;author=Murphy%2CMG&amp;author=Crawshaw%2CM&amp;author=Gould%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Liu, F. Tendick, K. Cleary, C. Kaufmann, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Liu A, Tendick F, Cleary K, Kaufmann C (2003) A survey of surgical simulation: applications, technology, and e" /><p class="c-article-references__text" id="ref-CR19">Liu A, Tendick F, Cleary K, Kaufmann C (2003) A survey of surgical simulation: applications, technology, and education. Presence Teleoper Virtual Environ 12(6):599–614</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474603322955905" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20surgical%20simulation%3A%20applications%2C%20technology%2C%20and%20education&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=12&amp;issue=6&amp;pages=599-614&amp;publication_year=2003&amp;author=Liu%2CA&amp;author=Tendick%2CF&amp;author=Cleary%2CK&amp;author=Kaufmann%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Montgomery K, Bruyns C, Brown J, Sorkin S, Mazzella F, Thonier G, Tellier A, Lerman B, Menon A (2002) Spring: " /><p class="c-article-references__text" id="ref-CR20">Montgomery K, Bruyns C, Brown J, Sorkin S, Mazzella F, Thonier G, Tellier A, Lerman B, Menon A (2002) Spring: a general framework for collaborative, real-time surgical simulation. In: Medicine meets virtual reality 12. Studies in health technology and informatics, vol 85. IOS Press, pp 296–303</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moorthy K, Mansoori M, Bello F, Hance J, Undre S, Munz Y et al (2004) Evaluation of the benefit of VR simulati" /><p class="c-article-references__text" id="ref-CR21">Moorthy K, Mansoori M, Bello F, Hance J, Undre S, Munz Y et al (2004) Evaluation of the benefit of VR simulation in a multi-media web-based educational tool. In: Medicine meets virtual reality 12. Studies in health technology and informatics. IOS Press, pp 247–252</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="National Capital Area Medical Simulator Center Surgical Simulation Laboratory (2008) Web Site, http://simcen.o" /><p class="c-article-references__text" id="ref-CR22">National Capital Area Medical Simulator Center Surgical Simulation Laboratory (2008) Web Site, <a href="http://simcen.org/surgery/index.html">http://simcen.org/surgery/index.html</a> Last visited August 2008</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AM. Okamura, C. Simone, MD. O’Leary, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Okamura AM, Simone C, O’Leary MD (2004) Force modeling for needle insertion into soft tissue. IEEE Trans Biome" /><p class="c-article-references__text" id="ref-CR23">Okamura AM, Simone C, O’Leary MD (2004) Force modeling for needle insertion into soft tissue. IEEE Trans Biomed Eng 51(10):1707–1716</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTBME.2004.831542" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Force%20modeling%20for%20needle%20insertion%20into%20soft%20tissue&amp;journal=IEEE%20Trans%20Biomed%20Eng&amp;volume=51&amp;issue=10&amp;pages=1707-1716&amp;publication_year=2004&amp;author=Okamura%2CAM&amp;author=Simone%2CC&amp;author=O%E2%80%99Leary%2CMD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JD. Owens, D. Luebke, N. Govindaraju, M. Harris, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Owens JD, Luebke D, Govindaraju N, Harris M et al (2007) A survey of general-purpose computation on graphics h" /><p class="c-article-references__text" id="ref-CR24">Owens JD, Luebke D, Govindaraju N, Harris M et al (2007) A survey of general-purpose computation on graphics hardware. Comput Graph Forum 26(1):80–113</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2007.01012.x" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20general-purpose%20computation%20on%20graphics%20hardware&amp;journal=Comput%20Graph%20Forum&amp;volume=26&amp;issue=1&amp;pages=80-113&amp;publication_year=2007&amp;author=Owens%2CJD&amp;author=Luebke%2CD&amp;author=Govindaraju%2CN&amp;author=Harris%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Poston, L. Serra, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Poston T, Serra L (1996) Dextrous virtual work. Commun ACM 39(5):37–45" /><p class="c-article-references__text" id="ref-CR25">Poston T, Serra L (1996) Dextrous virtual work. Commun ACM 39(5):37–45</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F229459.229464" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dextrous%20virtual%20work&amp;journal=Commun%20ACM&amp;volume=39&amp;issue=5&amp;pages=37-45&amp;publication_year=1996&amp;author=Poston%2CT&amp;author=Serra%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruspini DC, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR26">Ruspini DC, Kolarov K, Khatib O (1997) The haptic display of complex graphical environments. In: Proceedings of SIGGRAPH 1997, pp 345–352</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MC. Schatz, C. Trapnell, AL. Delcher, A. Varshney, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Schatz MC, Trapnell C, Delcher AL, Varshney A (2007) High-throughput sequence alignment using graphics process" /><p class="c-article-references__text" id="ref-CR27">Schatz MC, Trapnell C, Delcher AL, Varshney A (2007) High-throughput sequence alignment using graphics processing units. BMC Bioinformatics 8:474</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1186%2F1471-2105-8-474" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=High-throughput%20sequence%20alignment%20using%20graphics%20processing%20units&amp;journal=BMC%20Bioinformatics&amp;volume=8&amp;publication_year=2007&amp;author=Schatz%2CMC&amp;author=Trapnell%2CC&amp;author=Delcher%2CAL&amp;author=Varshney%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schroeder W, Martin K, Lorensen B (2006) The visualization toolkit an object-oriented approach to 3D graphics," /><p class="c-article-references__text" id="ref-CR28">Schroeder W, Martin K, Lorensen B (2006) The visualization toolkit an object-oriented approach to 3D graphics, 4th edn. ISBN: 1-930934-19-X</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SI. Seldinger, " /><meta itemprop="datePublished" content="1953" /><meta itemprop="headline" content="Seldinger SI (1953) Catheter replacement of the needle in percutaneous arteriography; a new technique. Acta Ra" /><p class="c-article-references__text" id="ref-CR29">Seldinger SI (1953) Catheter replacement of the needle in percutaneous arteriography; a new technique. Acta Radiologica 39(5):368–376</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3109%2F00016925309136722" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Catheter%20replacement%20of%20the%20needle%20in%20percutaneous%20arteriography%3B%20a%20new%20technique&amp;journal=Acta%20Radiologica&amp;volume=39&amp;issue=5&amp;pages=368-376&amp;publication_year=1953&amp;author=Seldinger%2CSI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shahabi C, Ortega A, Kolahdouzan MR (2002) A comparison of different haptic compression techniques. IEEE inter" /><p class="c-article-references__text" id="ref-CR6">Shahabi C, Ortega A, Kolahdouzan MR (2002) A comparison of different haptic compression techniques. IEEE international conference on multimedia and expo, vol 1, pp 657–660</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Soferman, D. Blythe, NW. John, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Soferman Z, Blythe D, John NW (1998) Advanced graphics behind medical virtual reality: evolution of algorithms" /><p class="c-article-references__text" id="ref-CR30">Soferman Z, Blythe D, John NW (1998) Advanced graphics behind medical virtual reality: evolution of algorithms, hardware, and software interfaces. Proc IEEE 86(3):531–554</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5.662878" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advanced%20graphics%20behind%20medical%20virtual%20reality%3A%20evolution%20of%20algorithms%2C%20hardware%2C%20and%20software%20interfaces&amp;journal=Proc%20IEEE&amp;volume=86&amp;issue=3&amp;pages=531-554&amp;publication_year=1998&amp;author=Soferman%2CZ&amp;author=Blythe%2CD&amp;author=John%2CNW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thomas RG, John NW, Lim IS (2006) A mixed reality anatomy teaching tool. In: Proceedings of theory and practic" /><p class="c-article-references__text" id="ref-CR31">Thomas RG, John NW, Lim IS (2006) A mixed reality anatomy teaching tool. In: Proceedings of theory and practice of computer graphics 2006, Teesside, 165–170</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vidal, F.P., John, N.W. (2007) Interactive Physically-Based X-Ray Simulation: CPU or GPU? In: Proceedings of m" /><p class="c-article-references__text" id="ref-CR32">Vidal, F.P., John, N.W. (2007) Interactive Physically-Based X-Ray Simulation: CPU or GPU? In: Proceedings of medicine meets virtual reality, Stud Health Technol Inform vol 125, pp 479–481</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FP. Vidal, F. Bello, K. Brodlie, NW. John, DA. Gould, R. Phillips, N. Avis, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Vidal FP, Bello F, Brodlie K, John NW, Gould DA, Phillips R, Avis N (2006) Principles and applications of comp" /><p class="c-article-references__text" id="ref-CR33">Vidal FP, Bello F, Brodlie K, John NW, Gould DA, Phillips R, Avis N (2006) Principles and applications of computer graphics in medicine. Comput Graph Forum 25(1):113–137</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2006.00822.x" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Principles%20and%20applications%20of%20computer%20graphics%20in%20medicine&amp;journal=Comput%20Graph%20Forum&amp;volume=25&amp;issue=1&amp;pages=113-137&amp;publication_year=2006&amp;author=Vidal%2CFP&amp;author=Bello%2CF&amp;author=Brodlie%2CK&amp;author=John%2CNW&amp;author=Gould%2CDA&amp;author=Phillips%2CR&amp;author=Avis%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FP. Vidal, NW. John, AE. Healey, DA. Gould, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Vidal FP, John NW, Healey AE, Gould DA (2008a) Simulation of ultrasound guided needle puncture using patient s" /><p class="c-article-references__text" id="ref-CR34">Vidal FP, John NW, Healey AE, Gould DA (2008a) Simulation of ultrasound guided needle puncture using patient specific data with 3D textures and volume haptics. Comput Animat Virtual Worlds 19(2):111–127</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fcav.217" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulation%20of%20ultrasound%20guided%20needle%20puncture%20using%20patient%20specific%20data%20with%203D%20textures%20and%20volume%20haptics&amp;journal=Comput%20Animat%20Virtual%20Worlds&amp;volume=19&amp;issue=2&amp;pages=111-127&amp;publication_year=2008&amp;author=Vidal%2CFP&amp;author=John%2CNW&amp;author=Healey%2CAE&amp;author=Gould%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vidal FP, Healy AE, John NW, Gould DA (2008) Force penetration of chiba needles for haptic rendering in ultras" /><p class="c-article-references__text" id="ref-CR35">Vidal FP, Healy AE, John NW, Gould DA (2008) Force penetration of chiba needles for haptic rendering in ultrasound guided needle puncture training simulator. In: Workshop on needle steering: recent results and future opportunities, MICCAI 2008, New York</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-008-0101-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The work presented in this paper is the result of contributions from many talented colleagues and research students, and multi-disciplinary collaborators. It has been a great pleasure to work with them all. Visualization and Medical Graphics Group at Bangor: Franck P Vidal, Chris J Hughes, Ik Soo Lim, Serban Pop, Rhys Thomas, Oli Buckley, Llyr ap Cynydd and Tim Coles. CRaIVE network: Derek A Gould, Sheena J Johnson, Helen Woolnough, Carrie Hunt, Fernando Bello, Vincent Luboz, Pierre-Frederic Villard, Andrew E Healey, Jonathan Evans, Amrita Sinha, Roger Phillips, James W Ward, Yan Zhang, Sandhya Pisharody. Martin Crawshaw, Ken W Brodlie, Andrew Bulpit, Richard Holbrey, Yi Song, Nicholas Chalmers, Thien How and David Kessel. Darwin Caldwell, Italian Institute of Technology. Web3D Consortium Medical Working Group: M. Aratow, J. Couch, D. Evestedt, A.D. Hudson, N. Polys, R.F. Puk, A. Ray, K. Victor, Q. Wang work described in this paper has been funded by the UK Engineering and Physical Sciences Research Council, the UK National Institute for Health Research HTD programme, the European Union and the US Telemedicine and Advanced Technology Research Center.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Bangor University, Bangor, UK</p><p class="c-article-author-affiliation__authors-list">Nigel W. John</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Nigel_W_-John"><span class="c-article-authors-search__title u-h3 js-search-name">Nigel W. John</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Nigel W.+John&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nigel W.+John" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nigel W.+John%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-008-0101-2/email/correspondent/c1/new">Nigel W. John</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>N. W. John is the 12th recipient of the MMVR Satava Award, 2006.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Design%20and%20implementation%20of%20medical%20training%20simulators&amp;author=Nigel%20W.%20John&amp;contentID=10.1007%2Fs10055-008-0101-2&amp;publication=1359-4338&amp;publicationDate=2008-10-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">John, N.W. Design and implementation of medical training simulators.
                    <i>Virtual Reality</i> <b>12, </b>269–279 (2008). https://doi.org/10.1007/s10055-008-0101-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-008-0101-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-08-14">14 August 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-08-25">25 August 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-10-22">22 October 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-12">December 2008</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-008-0101-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-008-0101-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Medical virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D displays</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Graphics hardware</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Seldinger technique</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0101-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=101;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

