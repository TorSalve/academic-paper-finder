<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Real-time adjustment of contrast saliency for improved information vis"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Augmented reality (AR) &#8220;augments&#8221; virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-07-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London Ltd."/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Augmented reality (AR) &#8220;augments&#8221; virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility and readability of the augmented information must be as high as possible amidst the dynamically changing real-world surrounding and background. In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real-world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object&#8217;s representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making real-time computation possible. While the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the &#8220;optimal&#8221; ground truth in terms of correctly perceiving and recognizing the augmented information. In addition, we also present another experiment that explores in what fashion the proposed algorithm can be applied in actual AR applications. The results suggested that the users clearly preferred the automatic contrast modulation upon large movements in the scenery."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-07-05"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="245"/>

    <meta name="prism.endingPage" content="262"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London Ltd."/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0319-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0319-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0319-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0319-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2018/09"/>

    <meta name="citation_online_date" content="2017/07/05"/>

    <meta name="citation_firstpage" content="245"/>

    <meta name="citation_lastpage" content="262"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0319-y"/>

    <meta name="DOI" content="10.1007/s10055-017-0319-y"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0319-y"/>

    <meta name="description" content="Augmented reality (AR) &#8220;augments&#8221; virtual information over the real-world medium and is emerging as an important type of an information visuali"/>

    <meta name="dc.creator" content="Euijai Ahn"/>

    <meta name="dc.creator" content="Sungkil Lee"/>

    <meta name="dc.creator" content="Gerard Jounghyun Kim"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Achanta R, Hemami S, Estrada F, Susstrunk S (2009) Frequency-tuned salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2009, IEEE, pp 1597&#8211;1604"/>

    <meta name="citation_reference" content="Ackerman E (2013) Could Google Glass hurt your eyes? A Harvard vision scientist and project glass advisor responds. 
                    http://www.forbes.com/sites/eliseackerman/2013/03/04/could-google-glass-hurt-your-eyes-a-harvard-vision-scientist-and-project-glass-advisor-responds/
                    
                  
                "/>

    <meta name="citation_reference" content="Avery B, Sandor C, Thomas BH (2009) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality 2009, IEEE, pp 79&#8211;82"/>

    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=What is the primary cause of individual differences in contrast sensitivity?; citation_author=DH Baker; citation_volume=8; citation_issue=7; citation_publication_date=2013; citation_pages=e69536; citation_doi=10.1371/journal.pone.0069536; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Organizational Behavior, v.1.1; citation_publication_date=2010; citation_id=CR5; citation_author=T Bauer; citation_author=B Erdogan; citation_publisher=Flat World Knowledge"/>

    <meta name="citation_reference" content="Birchfield S (2007) KLT: an implementation of the Kanade&#8211;Lucas&#8211;Tomasi feature tracker. 
                    http://www.ces.clemson.edu/~stb/klt/
                    
                  
                "/>

    <meta name="citation_reference" content="Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2011, IEEE, pp 409&#8211;416"/>

    <meta name="citation_reference" content="Eadicicco L (2015) Sony just solved the biggest problem with Google Glass. 
                    http://www.businessinsider.com/sony-smartglasses-attach-solves-the-google-glass-style-problem-2015-1/
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Arq Bras Oftalmol; citation_title=Basics of seeing motion; citation_author=WH Ehrenstein; citation_volume=66; citation_issue=5; citation_publication_date=2003; citation_pages=44-52; citation_doi=10.1590/S0004-27492003000600006; citation_id=CR9"/>

    <meta name="citation_reference" content="Gabbard JL, Swan JE, Hix D, Kim SJ, Fitch G (2007) Active text drawing styles for outdoor augmented reality: a user-based study and design implications. In: Proceedings of the IEEE conference on virtual reality 2007, IEEE, pp 35&#8211;42"/>

    <meta name="citation_reference" content="GOOGLE (2015) Google Glass. 
                    http://www.google.com/glass/start/
                    
                  
                "/>

    <meta name="citation_reference" content="citation_title=ANSI/HFES 100-2007 Human Factors Engineering of Computer Workstations; citation_publication_date=2007; citation_id=CR12; citation_publisher=Human Factors and Ergonomic Society"/>

    <meta name="citation_reference" content="Hincapi&#233;-Ramos JD, Ivanchuk L, Sridharan SK, Irani P (2014) SmartColor: real-time color correction and contrast for optical see-through head-mounted displays. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2014, IEEE, pp 187&#8211;194"/>

    <meta name="citation_reference" content="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2007, IEEE, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Human Benchmark (2017) Reaction time statistics. 
                    http://www.humanbenchmark.com/tests/reactiontime/statistics
                    
                  
                "/>

    <meta name="citation_reference" content="citation_title=ISO 9241-3:1992: ergonomic requirements for office work with visual display terminals (VDTs)&#8212;part 3: visual display requirements; citation_publication_date=1992; citation_id=CR16; citation_publisher=International Organization for Standardization"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A model of saliency-based visual attention for rapid scene analysis; citation_author=L Itti, C Koch, E Niebur; citation_volume=20; citation_issue=11; citation_publication_date=1998; citation_pages=1254-1259; citation_doi=10.1109/34.730558; citation_id=CR17"/>

    <meta name="citation_reference" content="Kalkofen D, Veas E, Zollmann S, Steinberger M, Schmalstieg D (2013) Adaptive ghosted views for augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2013, IEEE, pp 1&#8211;9"/>

    <meta name="citation_reference" content="citation_title=A literature review on reaction time; citation_publication_date=2008; citation_id=CR19; citation_author=RJ Kosinski; citation_publisher=Clemson University"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Real-time tracking of visually attended objects in virtual environments and its application to LOD; citation_author=S Lee, GJ Kim, S Choi; citation_volume=15; citation_issue=1; citation_publication_date=2009; citation_pages=6-19; citation_doi=10.1109/TVCG.2008.82; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=A velocity analogue of brightness contrast; citation_author=JM Loomis, K Nakayama; citation_volume=2; citation_issue=4; citation_publication_date=1973; citation_pages=425-428; citation_doi=10.1068/p020425; citation_id=CR21"/>

    <meta name="citation_reference" content="Ma YF, Zhang HJ (2003) Contrast-based image attention analysis by using fuzzy growing. In: Proceedings of the 11th ACM international conference on multimedia, ACM, pp 374&#8211;381"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis; citation_title=Size matters, but not for everyone: individual differences for contrast discrimination; citation_author=TS Meese, RF Hess, CB Williams; citation_volume=5; citation_issue=11; citation_publication_date=2005; citation_pages=928-947; citation_doi=10.1167/5.11.2; citation_id=CR23"/>

    <meta name="citation_reference" content="National Geographic (2015) Africa&#8217;s Wild West. 
                    http://natgeotv.com/uk/africas-wild-west
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Action-and workflow-driven augmented reality for computer-aided medical procedures; citation_author=N Navab, J Traub, T Sielhorst, M Feuerstein, C Bichlmeier; citation_volume=27; citation_issue=5; citation_publication_date=2007; citation_pages=10-14; citation_doi=10.1109/MCG.2007.117; citation_id=CR25"/>

    <meta name="citation_reference" content="Perazzi F, Kr&#228;henb&#252;hl P, Pritch Y, Hornung A (2012) Saliency filters: contrast based filtering for salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2012, IEEE, pp 733&#8211;740"/>

    <meta name="citation_reference" content="Reid B (2014) Google Glass causing eye pain and muscle fatigue for some users. 
                    http://www.redmondpie.com/google-glass-causing-eye-pain-and-muscle-fatigue-for-some-users/
                    
                  
                "/>

    <meta name="citation_reference" content="Sandor C, Cunningham A, Dey A, Mattila VV (2010) An augmented reality x-ray system based on visual saliency. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 27&#8211;36"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=Image based shadowing in real-time augmented reality; citation_author=P Supan, I Stuppacher, M Haller; citation_volume=5; citation_issue=3; citation_publication_date=2006; citation_pages=1-7; citation_id=CR29"/>

    <meta name="citation_reference" content="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013, IEEE, pp 3&#8211;6"/>

    <meta name="citation_reference" content="Veas EE, Mendez E, Feiner SK, Schmalstieg D (2011) Directing attention and influencing memory with visual saliency modulation. In: Proceedings of the SIGCHI conference on human factors in computing systems 2011, ACM, pp 1471&#8211;1480"/>

    <meta name="citation_reference" content="citation_title=Information visualization, perception for design; citation_publication_date=2012; citation_id=CR32; citation_author=C Ware; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="Zhai Y, Shah M (2006) Visual attention detection in video sequences using spatiotemporal cues. In: Proceedings of the 14th ACM international conference on multimedia, ACM, pp 815&#8211;824"/>

    <meta name="citation_reference" content="Zollmann S, Kalkofen D, Mendez E, Reitmayr G (2010) Image-based ghostings for single layer occlusions in augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 19&#8211;26"/>

    <meta name="citation_author" content="Euijai Ahn"/>

    <meta name="citation_author_institution" content="Department of Computer and Radio Communications Engineering, College of Information and Communications, Korea University, Seoul, Republic of Korea"/>

    <meta name="citation_author" content="Sungkil Lee"/>

    <meta name="citation_author_institution" content="Department of Software, College of Software, Sungkyunkwan University, Suwon, Republic of Korea"/>

    <meta name="citation_author" content="Gerard Jounghyun Kim"/>

    <meta name="citation_author_email" content="gjkim@korea.ac.kr"/>

    <meta name="citation_author_institution" content="Department of Computer Science and Engineering, College of Informatics, Korea University, Seoul, Republic of Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0319-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0319-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality"/>
        <meta property="og:description" content="Augmented reality (AR) “augments” virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility and readability of the augmented information must be as high as possible amidst the dynamically changing real-world surrounding and background. In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real-world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object’s representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making real-time computation possible. While the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the “optimal” ground truth in terms of correctly perceiving and recognizing the augmented information. In addition, we also present another experiment that explores in what fashion the proposed algorithm can be applied in actual AR applications. The results suggested that the users clearly preferred the automatic contrast modulation upon large movements in the scenery."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0319-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Human perception and performance, Augmented reality, See-through display, Saliency, Contrast","kwrd":["Human_perception_and_performance","Augmented_reality","See-through_display","Saliency","Contrast"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0319-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0319-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=319;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0319-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0319-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0319-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-07-05" itemprop="datePublished">05 July 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Euijai-Ahn" data-author-popup="auth-Euijai-Ahn">Euijai Ahn</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="0000 0001 0840 2678, grid.222754.4, Department of Computer and Radio Communications Engineering, College of Information and Communications, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Republic of Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sungkil-Lee" data-author-popup="auth-Sungkil-Lee">Sungkil Lee</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sungkyunkwan University" /><meta itemprop="address" content="0000 0001 2181 989X, grid.264381.a, Department of Software, College of Software, Sungkyunkwan University, 2066 Seobu-ro, Jangan-gu, Suwon, Republic of Korea" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gerard_Jounghyun-Kim" data-author-popup="auth-Gerard_Jounghyun-Kim" data-corresp-id="c1">Gerard Jounghyun Kim<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="0000 0001 0840 2678, grid.222754.4, Department of Computer Science and Engineering, College of Informatics, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Republic of Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">245</span>–<span itemprop="pageEnd">262</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">525 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0319-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Augmented reality (AR) “augments” virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility and readability of the augmented information must be as high as possible amidst the dynamically changing real-world surrounding and background. In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real-world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object’s representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making real-time computation possible. While the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the “optimal” ground truth in terms of correctly perceiving and recognizing the augmented information. In addition, we also present another experiment that explores in what fashion the proposed algorithm can be applied in actual AR applications. The results suggested that the users clearly preferred the automatic contrast modulation upon large movements in the scenery.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The continued innovations and advances in computer vision, mobile/cloud computing and portable display devices have brought about a renewed interest in augmented reality (AR) as a prominent information visualization and interaction medium. In particular, since the recent introduction of the Google Glass (Google <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="GOOGLE (2015) Google Glass. &#xA;                    http://www.google.com/glass/start/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR11" id="ref-link-section-d27769e358">2015</a>), see-through glasses (optical or video-based) are becoming more compact and fashionably designed, thereby getting more accepted by the mass users (Eadicicco <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Eadicicco L (2015) Sony just solved the biggest problem with Google Glass. &#xA;                    http://www.businessinsider.com/sony-smartglasses-attach-solves-the-google-glass-style-problem-2015-1/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR8" id="ref-link-section-d27769e361">2015</a>). Yet, there are some concerns as well (Ackerman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ackerman E (2013) Could Google Glass hurt your eyes? A Harvard vision scientist and project glass advisor responds. &#xA;                    http://www.forbes.com/sites/eliseackerman/2013/03/04/could-google-glass-hurt-your-eyes-a-harvard-vision-scientist-and-project-glass-advisor-responds/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR2" id="ref-link-section-d27769e364">2013</a>; Reid <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Reid B (2014) Google Glass causing eye pain and muscle fatigue for some users. &#xA;                    http://www.redmondpie.com/google-glass-causing-eye-pain-and-muscle-fatigue-for-some-users/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR27" id="ref-link-section-d27769e367">2014</a>), e.g., the eye fatigue and low readability problem, emanating from the conscious effort needed to focus on the small glass display (in the case of optical see-through) and to discern for the even smaller augmented information rendered and overlaid over the real object (often semitransparently, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig1">1</a>). One possible way to lessen the ensuing negative effect is to make the information as visible and conspicuous as possible using contrast.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Small augmentation information (digits and texts) on an optical see-through glass barely visible due to the environment lighting condition despite the semitransparent white rectangular backdrop (<i>left</i>) and visibility improved after contrast modulation (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In fact, one of the well-known principles in human–computer interaction and information visualization is to maintain high contrast (the difference in luminance and color) between the foreground objects and the background for high visibility, readability and comfort (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0319-y#ref-CR32" id="ref-link-section-d27769e402">2012</a>). For example, a contrast ratio of 3:1 is the minimum level recommended by ISO (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="ISO (1992) ISO 9241-3:1992: ergonomic requirements for office work with visual display terminals (VDTs)—part 3: visual display requirements. International Organization for Standardization, Geneva" href="/article/10.1007/s10055-017-0319-y#ref-CR16" id="ref-link-section-d27769e405">1992</a>) and HFES (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="HFES (2007) ANSI/HFES 100-2007 Human Factors Engineering of Computer Workstations. Human Factors and Ergonomic Society, Santa Monica" href="/article/10.1007/s10055-017-0319-y#ref-CR12" id="ref-link-section-d27769e408">2007</a>) for standard text and vision tasks. Mobile AR applications seen through the glasses are likely to have more unpredictably dynamic backgrounds than desktop applications. The unpredictability necessitates a real-time decision of or adjustment for the proper foreground object brightness/color over the changing background for maximum visibility.</p><p>In this paper, we present a technique based on image saliency analysis to improve the conspicuity (and thereby visibility/readability) of the foreground augmentation to the background real-world medium by adjusting its local brightness contrast. In addition, the proposed technique is implemented on a computationally less powerful smart phone considering the mobility of AR applications. We carried out experiments to validate that the proposed simplified technique still improved information visibility and readability without much perceptual difference to the “optimal” ground truth cases. We also look into the problem of how such an algorithm can be applied effectively and in the most usable way by comparing four different representative deployment schemes.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>There are several threads of work in the field of AR visualization. The first is for those that consider how to render augmentation objects that look natural with the real world onto which they are registered and overlaid. Examples include works that attempt to infer the environment lighting conditions and make photorealistic renderings including shadows and reflections (Supan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Supan P, Stuppacher I, Haller M (2006) Image based shadowing in real-time augmented reality. Int J Virtual Real 5(3):1–7" href="/article/10.1007/s10055-017-0319-y#ref-CR29" id="ref-link-section-d27769e422">2006</a>) or provide 3D depth cues consistent with the environment structure (Kalkofen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kalkofen D, Veas E, Zollmann S, Steinberger M, Schmalstieg D (2013) Adaptive ghosted views for augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2013, IEEE, pp 1–9" href="/article/10.1007/s10055-017-0319-y#ref-CR18" id="ref-link-section-d27769e425">2013</a>). Our work only considers simple 2D overlays that are not necessarily registered to an object in the scene (such as in the case of typical Google Glass applications). In addition, pursuing visibility is somewhat an opposing concept to “naturalness,” sometimes having to make the augmentation artificially conspicuous.</p><p>Another thread of related work considers this problem of how to make the target object, augmentation and even the neighboring context as visible and understandable as possible by inserting guiding objects and highlighting certain features [e.g., wireframe boundaries or emphasized edges (Avery et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Avery B, Sandor C, Thomas BH (2009) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality 2009, IEEE, pp 79–82" href="/article/10.1007/s10055-017-0319-y#ref-CR3" id="ref-link-section-d27769e431">2009</a>), arrows and balloons (Tatzgern et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013, IEEE, pp 3–6" href="/article/10.1007/s10055-017-0319-y#ref-CR30" id="ref-link-section-d27769e434">2013</a>), motion guidance and profiles (Navab et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality for computer-aided medical procedures. IEEE Comput Graph Appl 27(5):10–14" href="/article/10.1007/s10055-017-0319-y#ref-CR25" id="ref-link-section-d27769e437">2007</a>)]. Furthermore, Sandor et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Sandor C, Cunningham A, Dey A, Mattila VV (2010) An augmented reality x-ray system based on visual saliency. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 27–36" href="/article/10.1007/s10055-017-0319-y#ref-CR28" id="ref-link-section-d27769e440">2010</a>) presented a method to preserve “salient” features of both the augmented real object and semitransparently overlaid foreground augmentation. Salient regions can be understood as the areas in an image, which are most likely to attract the viewer’s gaze (Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e443">1998</a>). Saliency maps can be generated by the “center-surround” difference that computes how much a pixel stands out with respect to various properties such as the color, luminosity, orientation and motion (Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e447">1998</a>). It has been applied to pseudogaze tracking (Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e450">1998</a>; Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee S, Kim GJ, Choi S (2009) Real-time tracking of visually attended objects in virtual environments and its application to LOD. IEEE Trans Vis Comput Graph 15(1):6–19" href="/article/10.1007/s10055-017-0319-y#ref-CR20" id="ref-link-section-d27769e453">2009</a>), and human attention studies (Veas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Veas EE, Mendez E, Feiner SK, Schmalstieg D (2011) Directing attention and influencing memory with visual saliency modulation. In: Proceedings of the SIGCHI conference on human factors in computing systems 2011, ACM, pp 1471–1480" href="/article/10.1007/s10055-017-0319-y#ref-CR31" id="ref-link-section-d27769e456">2011</a>). Zollmann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Zollmann S, Kalkofen D, Mendez E, Reitmayr G (2010) Image-based ghostings for single layer occlusions in augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 19–26" href="/article/10.1007/s10055-017-0319-y#ref-CR34" id="ref-link-section-d27769e459">2010</a>) applied the saliency computation to “areas” (or super pixels) rather than individual pixels to provide a more uniform and consistently looking augmentation. Kalkofen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kalkofen D, Veas E, Zollmann S, Steinberger M, Schmalstieg D (2013) Adaptive ghosted views for augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2013, IEEE, pp 1–9" href="/article/10.1007/s10055-017-0319-y#ref-CR18" id="ref-link-section-d27769e462">2013</a>) also applied the saliency analysis for adjusting for the proper transparency level of the occluding augmentation object.</p><p>While our work also relies on saliency analysis, it aims at improving the contrast saliency of only the foreground augmentation object. This is an important distinction as we believe that it is difficult to make both the augmentation and its occluded background highly visible and distinctive to each other at the same time. In addition, the saliency computation is simplified and fast, considering only the local luminance property around the target foreground object (object centric). The brightness adjustment is made only over a few discrete levels for a real-time execution on a mobile platform, and its output tested for perceptual sufficiency against the ground truth. Cheng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2011, IEEE, pp 409–416" href="/article/10.1007/s10055-017-0319-y#ref-CR7" id="ref-link-section-d27769e468">2011</a>) developed a fast simplified global contrast-based saliency detection algorithm and demonstrating its high performance and perceptual quality. However, their objective was just to produce a saliency map for the whole image (global) and identify possible regions of interest.</p><p>Gabbard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Gabbard JL, Swan JE, Hix D, Kim SJ, Fitch G (2007) Active text drawing styles for outdoor augmented reality: a user-based study and design implications. In: Proceedings of the IEEE conference on virtual reality 2007, IEEE, pp 35–42" href="/article/10.1007/s10055-017-0319-y#ref-CR10" id="ref-link-section-d27769e474">2007</a>) tried to improve the visibility of the augmentation by changing its presentation “style.” The background environment used in the study was mostly static (e.g., bricked walls, building, sky). Hincapié-Ramos et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Hincapié-Ramos JD, Ivanchuk L, Sridharan SK, Irani P (2014) SmartColor: real-time color correction and contrast for optical see-through head-mounted displays. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2014, IEEE, pp 187–194" href="/article/10.1007/s10055-017-0319-y#ref-CR13" id="ref-link-section-d27769e477">2014</a>) attempted to reduce the difference in color between that of the augmentation target and the blended augmentation for optical see-through display. A real-time operation was made possible even on the low-end desktop PC by using the vertex shader; however, information visibility was not investigated.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Brightness modulation for visibility</h2><div class="c-article-section__content" id="Sec3-content"><p>Contrast can be modulated for both luminance/brightness and color (e.g., hue and saturation). Changing color to achieve proper contrast may not be desirable since the color may be carrying important semantic information. In this work, we only consider the modulation of the brightness channel of the augmented information. Moreover, real-time global or even local optimization of the brightness (and contrast) is nearly impossible if all the dependencies among different objects are taken into account. We vary the brightness values of the (possibly more than one) foreground augmentation objects at seven discrete levels, i.e., 40, 60, 80, 100, 120 and 140% of the original, and select the ones that produce the highest local contrast saliency for each augmentation object. The algorithm proposed in this work is much simplified in the sense that (1) the contrast is changed only for the target foreground objects and (2) it uses only one representative pixel location and averaged CIELAB color of the given augmentation object to compute the local contrast saliency, and (3) it uses the result to adjust the brightness of the whole object one at a time individually without considering the combinatorial effect of other neighborhood objects whose brightness might have changed by the same process. This can drastically reduce the amount of computation especially because there are usually only few foreground objects to process at a given moment in a typical glass-based augmented reality application. Even if the number of foreground objects increases, the required additional computation is minimal since we only consider the single representative color value, and the combinatorial effects during the optimization process can also be partly reflected by the object importance (left as future work). In the case that the augmentation object is relatively large, using a single representative color pixel can be ineffective. While this is one limitation of our approach, in most typical cases, the augmentation objects are kept small so that the neighboring spatial context is not occluded too much. The overall process is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The overall process of computing the contrast saliency for a set of augmentation objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The computation of the saliency is also simplified by considering color only, ignoring other properties like motion and orientation for now. Thus, we refer to such a saliency as “contrast saliency.” We use the saliency detection algorithm based on the center-surround difference operations over the image pyramid as originally proposed by Itti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e512">1998</a>). While there are contrast detection methods (Achanta et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Achanta R, Hemami S, Estrada F, Susstrunk S (2009) Frequency-tuned salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2009, IEEE, pp 1597–1604" href="/article/10.1007/s10055-017-0319-y#ref-CR1" id="ref-link-section-d27769e515">2009</a>; Hou and Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2007, IEEE, pp 1–8" href="/article/10.1007/s10055-017-0319-y#ref-CR14" id="ref-link-section-d27769e518">2007</a>; Ma and Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ma YF, Zhang HJ (2003) Contrast-based image attention analysis by using fuzzy growing. In: Proceedings of the 11th ACM international conference on multimedia, ACM, pp 374–381" href="/article/10.1007/s10055-017-0319-y#ref-CR22" id="ref-link-section-d27769e521">2003</a>; Zhai and Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zhai Y, Shah M (2006) Visual attention detection in video sequences using spatiotemporal cues. In: Proceedings of the 14th ACM international conference on multimedia, ACM, pp 815–824" href="/article/10.1007/s10055-017-0319-y#ref-CR33" id="ref-link-section-d27769e524">2006</a>; Perazzi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Perazzi F, Krähenbühl P, Pritch Y, Hornung A (2012) Saliency filters: contrast based filtering for salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2012, IEEE, pp 733–740" href="/article/10.1007/s10055-017-0319-y#ref-CR26" id="ref-link-section-d27769e528">2012</a>), we consider to use this method for it is generally more accurate [e.g., reflecting the effects of simultaneous contrast (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0319-y#ref-CR32" id="ref-link-section-d27769e531">2012</a>)] and faster than the pixel-based global method (Cheng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2011, IEEE, pp 409–416" href="/article/10.1007/s10055-017-0319-y#ref-CR7" id="ref-link-section-d27769e534">2011</a>), especially given the trend that the image pyramid is available through the fast GPU/hardware computation.</p><p>The color space of the image is converted to the CIELAB; then, a four-level image pyramid is formed upon which the “center-surround” difference is carried out. The center-surround difference refers to computation of measurement of relative brightness of a pixel, whose model was inspired by how the human visual system responds to light as a mixture of two excitatory and inhibitory Gaussian-like detectors (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0319-y#ref-CR32" id="ref-link-section-d27769e540">2012</a>). As mentioned, to reduce the amount of computation, for a given augmentation object, the differencing is carried out using a representative pixel. For example, the representative pixel of a single-colored text augmentation object will be its centroid. For an image object, the centroid pixel is similarly used as the representative with the average color value. Thus, we only generate a four-level (compared to seven-level in the original Itti’s work) image pyramid for the “surround” values (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig2">2</a>①).</p><p>The center-surround subtraction over the scale space is carried out between a center “fine” scale level <i>c</i> and a surround “coarser” scale level <i>s</i> to compute the CIELAB color distance ∆ϵ for a given center pixel (<i>P</i>
                <sub>c</sub>) against its surroundings as follows (<i>P</i>
                <sub>s</sub>).
</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$P_{\text{c}} { \ominus }P_{\text{s}} = \sqrt {\left( {\Delta L^{*} } \right)^{2} + \left( {\Delta a^{*} } \right)^{2} + \left( {\Delta b^{*} } \right)^{2} } = \Delta \epsilon \left( {c, s} \right)$$</span></div></div><p>where <span class="mathjax-tex">\({ \ominus }\)</span> across-scale subtraction,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> <span class="mathjax-tex">\(c \in \left\{ {2, 3, 4} \right\}\)</span> and <span class="mathjax-tex">\(s \in \left\{ {c + 3, c + 4} \right\}\)</span>.</p><p>The final contrast saliency of the image pixel is determined by adding up the ∆ϵ values across the scale as follows (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig2">2</a>③):</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$C = \oplus_{c = 2}^{4} \oplus_{s = c + 3}^{c + 4} \Delta \epsilon \left( {c, s} \right)$$</span></div></div><p>where <i>C</i> conspicuity and <span class="mathjax-tex">\(\oplus_{a}^{b}\)</span> across-scale addition from <i>a</i> to <i>b</i>.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
</p><p>We adjust the brightness value of the foreground object over seven discrete levels (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig2">2</a>②) and choose the one that maximizes the conspicuity as computed above (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig2">2</a>④). The detailed algorithm (in pseudocode style) is presented in “Appendix”.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Performance</h2><div class="c-article-section__content" id="Sec4-content"><p>The proposed saliency computation (denoted as “S” in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab1">1</a>) and brightness adjustment are surely expected to make the resulting image less optimal than the “ground truth.” For validation with respect to its perceptual sufficiency, we compare its output to those generated by two “full-blown” non-real-time algorithms that use a seven-level image pyramid and compute the contrast saliency for all the different pixels of the augmentation objects. One algorithm (denoted as “G”), considered in this paper, produces the “ground truth” output. It searches for the proper set of brightness values over all the combinations of augmentation objects (thus the search space is exponential in the number of augmentation objects), while the other less stringent one (denoted as “L”) only considers each augmentation object in isolation individually. The details of the ground truth algorithms (in pseudocode style) are also presented in “Appendix”. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Performance results of the three brightness adjustment algorithms for two types of sample images (800 each) in milliseconds</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0319-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div> <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab1">1</a> shows the average performance results for two types of augmented image samples used in the validation experiment (shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig3">3</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig4">4</a>). The algorithms were run on a smart phone, Samsung Galaxy S4 LTE-A, running the Android OS v.4.4.2 (KitKat) with a Quad-core 2.3 GHz Krait 400 processor, a Adreno 330 GPU and 2 GB RAM. The image resolution was 1920 × 1080. Note that the proposed algorithm reached a real-time performance (for the given augmented images).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Samples of test images (Type 1: two directional cues) according to the four tested conditions (from the <i>top</i>: N, S, L, G). <b>a</b> No brightness adjustment. <b>b</b> The proposed simplified algorithm. <b>c</b> Ground truth—local. <b>d</b> Ground truth—global</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Samples of test images (Type 2: text and images) according to the four tested conditions (from the <i>top</i>: N, S, L, G). <b>a</b> No brightness adjustment. <b>b</b> The proposed simplified algorithm. <b>c</b> Ground truth—local. <b>d</b> Ground truth—global</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We also make a note that, for instance, the histogram-based algorithm (not using any cross-scale center-surround operation) by Cheng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2011, IEEE, pp 409–416" href="/article/10.1007/s10055-017-0319-y#ref-CR7" id="ref-link-section-d27769e1226">2011</a>) could be modified for a fast local contrast saliency computation. However, the image pyramid-based method is more accurate, equally fast and easily extendable to other saliency features that we plan to further investigate in the future. Plus, for a fair comparison of the perceptual quality of the outputs, the three compared algorithms only differ in the input size rather than in the way the contrast was computed.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Experiment I</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">Experimental design and tasks</h3><p>We have run a series of comparative experiments to validate whether the proposed simplified algorithm for contrast saliency and visibility improvement was effective. The four tested conditions were: (1) no brightness adjustment (N), (2) the proposed simplified algorithm (S), (3) ground truth—local (L) and (4) ground truth—global (G). Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig3">3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig4">4</a> show sample output images for the four conditions. For stable and convenient testing, a static video see-through images generated off-line were used. That is, test images for conditions L and G could not be generated in real time. We generated and used over 800 different test images from ten different background images (e.g., nature scene, daytime street views, nighttime city views) cross matched with different augmentations (e.g., text, directional icons, small images).</p><p>Two perceptual performance tasks with subjective assessments were administered. Task 1 involved displaying two directional cues/patterns and asking the user to choose the corresponding directions from a menu (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig5">5</a>). The directional cues/patterns appear at a random location on the image, and the participant can answer in any order. In Task 2, the user was shown two overlaid information (text/image) at the fixed position (center) and asked whether the look of the image and the text matched semantically (match—O or different—X). For example, if a small image of a “candy” is shown with the accompanying text “milk,” the right answer would be X. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig5">5</a> shows the interface in carrying out the tasks. Depending on the task, the directional arrow or O/X interface was enabled/disabled appropriately. Both the response times and error rates were measured. The subjective assessment was made by running a short survey (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab2">2</a>) asking perceptual qualities of the four tested conditions and ranking the preferred condition. The subjects make the ranking after experiencing all the treatments with the four associated imageries presented again (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig6">6</a>). In summary, the experiment was a one-factor (four levels) within-subject.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Subjective survey assessing the perceptual quality of the augmented information for the four tested conditions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0319-y/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>A scene from the experiment (<i>left</i>) and the menu-based interface used to acquire user response</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The four imageries presented to the subject for ranking one’s preference after the main tasks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As already stated, the foreground objects considered were short text and image as the representative types of augmentation in various colors. As for the background, it is quite difficult to precisely characterize the types of background we chose, but in general, they were moderately dynamic and included scenes from everyday living such as the city and nature in both day and night times.</p><h3 class="c-article__sub-heading" id="Sec7">Detailed procedure</h3><p>Eighteen paid participants (16 men and 2 women) with corrected vision participated in the experiment with the mean age of 26 (between 20 and 35, mostly college or graduate students). After collecting one’s basic background information, the participant was briefed about the purpose of the experiment and instructions for the experimental tasks. Augmented images for the four tested conditions were shown to the user in a balanced order, and the response times and answers were recorded. The test conducted on a table using a laptop computer with a nominal viewing distance of about 30 ~ 40 cm (not rigidly fixed for comfort, users were only advised not to change the viewing position too drastically). Each such session was repeated for eight times using different image contents, resulting in the participant reviewing of total of 32 images. Survey questions 1 and 2 were answered after each conditional treatment trial, while 3, after experiencing all the treatments. For the preference ranking, the user was shown four sample images together at once, eight times for each task image (Types 1 and 2).</p><h3 class="c-article__sub-heading" id="Sec8">Results</h3><p>ANOVA was used for analyzing the experiment results and Tukey’s test for the post hoc test. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig7">7</a> shows the results regarding the perception performance. We posited that the response time (task completion time) was an indirect indicator of how well the participant would perceive the augmented information. For Task 1, while the graph shows a slight trend for differences between N and the rest, no statistical significance was found. For Task 2, a statistical significant difference was found between N and the rest (<i>F</i>(3,68) = 5.984, <i>p</i> = 0.001, <i>R</i>
                    <sup>2</sup> = 0.209). We believe that the observed difference of 480 ms between N and S is not ignorable for time-critical tasks considering that humans are known to, given the best operating conditions, to solve the similar task under 300 ms (Human Benchmark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Human Benchmark (2017) Reaction time statistics. &#xA;                    http://www.humanbenchmark.com/tests/reactiontime/statistics&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR15" id="ref-link-section-d27769e1439">2017</a>; Kosinski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kosinski RJ (2008) A literature review on reaction time. Clemson University, Clemson" href="/article/10.1007/s10055-017-0319-y#ref-CR19" id="ref-link-section-d27769e1442">2008</a>). As for the error rate, because the user did carry out the tasks correctly, the absolute number of errors was not high enough to exhibit any statistical validity. We just note, however, that the N (no brightness adjustment) showed the tendency to produce the most amount of erroneous responses (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Participant’s performance results over two perceptual tasks over four tested conditions (N, S, L, G): task completion (response) time. <b>a</b> Task 1 completion time. <b>b</b> Task 2 completion time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div> <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Participant’s performance results over two perceptual tasks over four tested conditions (N, S, L, G): error count</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0319-y/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div> <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig8">8</a> shows the results of the subjective survey. The graphs show that the N was most problematic in terms of visibility (Q1) (<i>F</i>(3,68) = 10.362, <i>p</i> &lt; 0.001, <i>R</i>
                    <sup>2</sup> = 0.314) and understandability (Q2) (<i>F</i>(3,68) = 16.141, <i>p</i> &lt; 0.001, <i>R</i>
                    <sup>2</sup> = 0.416) with all other conditions (S, L and G) exhibiting higher perceptual qualities, but with no statistical differences among them. For preference, we applied the nonparametric Kruskal–Wallis test and Mann–Whitney <i>U</i> test (for post hoc analysis). The participants also indicated their preference for those with adjusted contrast (<i>χ</i>
                    <sup>2</sup>(3) = 39.554, <i>p</i> &lt; 0.001), but, again, with no marked differences among the S, L and G (N vs. S: <i>Z</i>(3) = −5.030, <i>p</i> &lt; 0.001; N vs. L: <i>Z</i>(3) = −5.098, <i>p</i> &lt; 0.001; N vs. G: <i>Z</i>(3) = −5.122, <i>p</i> &lt; 0.001).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Participant’s subjective perceptual qualities (visibility, understandability and simple preference) over four tested conditions (N, S, L, G). <b>a</b> Visibility. <b>b</b> Understandability. <b>c</b> Simple preference</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec9">Discussion</h3><p>Our experimental results have shown that despite simplifying the contrast saliency computation with respect to the extent of the center-surround operation and using only one representative CIE color value (or brightness) value of the augmentation object, no significant perceptual differences were found compared to the output generated using a more optimized saliency analysis. The reason why no clear performance differences were found for Task 1 seems to be due to the relative task simplicity, only having to determine the direction orientation, whereas in Task 2, a more involved cognitive recognition process is needed. Our algorithm was only tested for two simple tasks and a limited number of augmented image samples, and therefore, no general claims can be made. However, given the resource, device and usage constraints of the mobile augmented reality systems, our algorithm can be put to an effective use to improve their usability. The actual scenario of applying the technique in an application setting remains as a future work. One possible concern is since the environment/background condition will continually change, so will the perceived brightness of the augmented information by the use of the proposed technique, possibly introducing a bothersome flickering effect. A possible solution is to only apply the contrast adjustment intermittently or on user demand. We further explore this issue in Experiment II.</p><p>As for the underlying reason to why the simplified algorithm performs perceptually up to par (also from the perspective of the required amount of computation) with the more optimized algorithm (at least to some degree), we attribute it to the way human perceives contrast. There are two major phenomena that affect the perception of visual contrast. One is the simultaneous contrast which refers to the manner in which the colors of two different objects affect each other (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0319-y#ref-CR32" id="ref-link-section-d27769e1702">2012</a>). Our algorithm only uses one representative pixel to adjust the brightness of the overall object against multi-colored background, and this makes it difficult to predict the effect of the simultaneous contrast, for the worse or the better. On the other hand, contrast constancy, the ability to perceive objects as maintaining a constant contrast independent of size, distance or other varying viewing conditions (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0319-y#ref-CR32" id="ref-link-section-d27769e1705">2012</a>), may be working in our favor such that one perceives the improved conspicuity of the object as a whole (rather than at the individual pixel level). Moreover, visual object perception is often strengthened by one’s focus of attention (Bauer and Erdogan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bauer T, Erdogan B (2010) Organizational Behavior, v.1.1. Flat World Knowledge, Irvington" href="/article/10.1007/s10055-017-0319-y#ref-CR5" id="ref-link-section-d27769e1708">2010</a>) for which augmentation information is drawn in association with the augmentation target (i.e., real object).</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Experiment II</h2><div class="c-article-section__content" id="Sec10-content"><h3 class="c-article__sub-heading" id="Sec11">Applying the proposed algorithm for AR</h3><p>Experiment I has shown the relative effectiveness of the simplified saliency adjustment of the augmented information for improved readability, tested with static images. Experiment II looks at the more practical problem of in what fashion the technique can be deployed in actual AR applications. The problem boils down to determining “when” to apply the algorithm and attempt to improve the readability in the most convenient, usable, preferred and comfortable way. We have considered three automatic and one manual methods, i.e., making the adjustment, (1) naively at every image frame, (2) upon a change in the imagery as detected by the overall color difference, (3) upon a change in the imagery as detected by a significant optical flow and (4) manually requested by the user on demand. The first naïve method simply applies the proposed algorithm at every frame and adjusts the relative brightness of the foreground augmentation. The next two methods only apply the algorithm when there deems to be a major change in the background environment with respect to its relative saliency. We considered two major factors, i.e., (1) change in the overall color (and ensuing brightness) distribution and (2) change in the imagery caused by a large movement (which is likely to entail a scenery change with a new saliency distribution).</p><p>As for detecting sufficient change in color, the image frames were divided into 32 × 18 blocks and the average CIE color value (as a measure that closely reflects the human perception) over the blocks was computed and successive image frames compared using this value. When there was a sufficient difference determined by a threshold value, the brightness adjustment was applied. The optical flow-based algorithm application was implemented similarly for detecting the scenery change by movement. The image frame was down-sampled to a resolution of 320 × 180 to compute the optical flow for the major feature points [e.g., KLT (Birchfield <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Birchfield S (2007) KLT: an implementation of the Kanade–Lucas–Tomasi feature tracker. &#xA;                    http://www.ces.clemson.edu/~stb/klt/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR6" id="ref-link-section-d27769e1728">2007</a>)] in real time. They were summed and compared to that of the previous frame for detecting a major change. In both cases, the threshold values were empirically set. Finally, the on-demand adjustment was simply applied by user interaction (e.g., pressing a button).</p><h3 class="c-article__sub-heading" id="Sec12">Experimental design and tasks</h3><p>The experiment asked the subjects to carry out visual tasks using five different ways (the four methods outlined above + the base line case of no adjustment) of augmentation brightness adjustment on an experimental video see-through setup (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0319-y#Sec13">6.3</a> for details). The task was to count the number of a particular letter from a string made of random alphabets, from A to F (capital or small), read a following statement (about the letter count) overlaid on the background video and judge its correctness. The task involved both a low-level pre-attentive perception and a high-level cognition (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig9">9</a>). Both quantitative performance data and subjective usability survey responses were collected. Thus, the experiment was designed as a one-factor (five levels) repeated measure within-subject.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The task of counting letters and comprehending the following sentence as overlaid on the background video (translated in English)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec13">Experimental setup</h3><p>The experiment was carried out using a video see-through AR setup, although the algorithm can be equally applied to an optical see-through-based AR with the mounted camera doing the visual analysis. The video see-through AR setup was implemented on a mobile phone. For a controlled experiment, instead of the real background, a pre-captured video was played in the background. Samsung Galaxy S6 (Exynos 7420, Cortex-A57 2.1Ghz + Cortex-A53 1.5Ghz Octa-core with Mali-T760 MP8 GPU) was used. For an isolated viewing (e.g., from external distraction), an enclosing box was attached to a goggle through which the AR imagery was shown. Note that the goggle was not an HMD but only served for view isolation with the box as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig10">10</a>. Since the smart phone was inaccessible, to receive and record data entry by the subject, a remote joystick/button device was implemented with an Arduino board and Wii Nunchuck controller, which communicated with the smart phone by Bluetooth (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig10">10</a>). The subject could select the O or X with the joystick and make a final selection of O or X by pressing the button.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>A scene from the actual experiment (<i>left</i>) and the button device used for collecting user input (<i>right</i>). The user wears an elongated goggle for isolating the view into the video see-through imagery shown on the mobile phone. The button device was used for collecting and recording user input</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec14">Detailed experimental process</h3><p>Seventeen paid participants (12 men and 5 women) with corrected vision participated in the experiment with the mean age of 24.24 (between 20 and 32, mostly college or graduate students). After collecting one’s basic background information, the participant was briefed about the purpose of the experiment and instructions for the experimental tasks. The participants also were given a short amount of time to get familiarized to the use of the remote control and indicating their responses to the visual task.</p><p>In the main experiment, the user sat on a chair and wore the headset (goggle + enclosure) with the help of the experiment operator and held the remote button device in one hand. The headset was light enough and did not cause any noticeable discomfort during the experiment. There were five conditions, presented in a balanced order: (1) no adjustment as the base line (NOP), (2) naïve method of adjustment at every frame (Naïve), (3) at major image change by color (CD), (4) at major image change by optical flow (OF) and (5) upon user input on demand (OD). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig10">10</a> shows a snapshot from the actual experiment.</p><p>The video used for the background simulation was excerpted and stitched together from the “Africa’s Wild West” by the National Geographic (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="National Geographic (2015) Africa’s Wild West. &#xA;                    http://natgeotv.com/uk/africas-wild-west&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0319-y#ref-CR24" id="ref-link-section-d27769e1819">2015</a>) which had a mixture of several fast- and slow (or almost still)-changing sceneries. Each treatment (among the five) used the same background video.</p><p>For each treatment, the subject carried out 12 tasks with a sufficient time out value of 20 s (determined by a pilot test which indicated that most users were able to finish the task in about 16 s). After each treatment session, a usability survey was filled out (answered in the 7-point Likert scale), and at the end of all the five treatments, a preference was asked as well (see the Results section for details). The usability questions are provided in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab4">4</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Subjective survey questions used in experiment II</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0319-y/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec15">Results</h3><p>The quantitative measurements, both task completion time and error counts, were analyzed with ANOVA, but no statistically significant differences were detected (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig11">11</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig12">12</a>) among the conditions [<i>F</i>(4,80) = 0.389, <i>p</i> = 0.816 for the task completion time and <i>F</i>(4,80) = 0.377, <i>p</i> = 0.824 for the error count].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Task completion times among the five tested conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Error counts among the five tested conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>As for the responses to the usability survey, all seven questions resulted in statistically significant different effects by the algorithm deployment type. All responses were analyzed by the Mann–Whitney <i>U</i> test.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig13">13</a> shows the response to Q1 asking of the relative saliency of the augmentations against the background video. The graph clearly shows that the application of the algorithm in one way or another was indeed effective compared to the case of NOP (NOP vs. Naïve: <i>Z</i>(4) = −3.686, <i>p</i> &lt; 0.001; NOP vs. OF: <i>Z</i>(4) = −3.419, <i>p</i> = 0.001; NOP vs. CD: <i>Z</i>(4) = −2.462, <i>p</i> = 0.015). Among the four adjustment methods, the Naïve and the OF were the highest rated and the OD the least with a statistically significant difference (<i>Z</i>(4) = −2.162, <i>p</i> = 0.034).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Response to Q1: “The foreground augmentations were perceived to be conspicuous against the background video” (1: not at all ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The brightness adjustment by the optical flow was deemed the most natural. The OD also exhibited very high naturalness because the change was made on demand by the user (NOP vs. OF: <i>Z</i>(4) = −4.155, <i>p</i> &lt; 0.001; NOP vs. CD: <i>Z</i>(4) = −2.571, <i>p</i> = 0.011; NOP vs. OD: <i>Z</i>(4) = −2.973, <i>p</i> = 0.003). While optical flow, caused by a large object or camera motion, may not necessarily entail a change in the saliency distribution (although very likely), the subject seems to be highly sensitive to such a change in the scenery, e.g., that caused by the color difference (vs. OF: <i>Z</i>(4) = −2.266, <i>p</i> = 0.029). However, this could be an artifact of the relative level of the threshold value. On the other hand, the Naïve method was rated very low due to the frequent flickering caused by the frame-by-frame brightness adjustment (Naïve vs. OF: <i>Z</i>(4) = −3.077, <i>p</i> = 0.002; Naïve vs. OD: <i>Z</i>(4) = −2.068, <i>p</i> = 0.041) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig14">14</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Response to Q2: “The change in the brightness of the foreground augmentations was perceived to be natural” (1: not at all ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>   <p>Q3 asked whether how quickly the subject was able to recognize/count the letters/words and correctly understand the given sentences. Similarly to the responses to Q1, subjects showed higher scores when the brightness adjustment was applied with the OF being the highest rated, although with no statistically significant difference with the CD or OD (NOP vs. OF: <i>Z</i>(4) = −3.696, <i>p</i> &lt; 0.001; NOP vs. CD: <i>Z</i>(4) = −2.455, <i>p</i> = 0.015). The Naïve method, while somewhat effective, got a relatively low score due to the aforementioned flicker which seems to have lowered the readability (vs. OF: <i>Z</i>(4) = −2.345, <i>p</i> = 0.020) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig15">15</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Response to Q3: “I was able to quickly recognize the foreground augmentations.” (1: not at all ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Q4 assessed whether the subject could read and understand the augmentations comfortably in the midst of their brightness adjustment. Again the OF showed the highest level of visual comfort with the Naïve method performing even below the NOP (NOP vs. OF: <i>Z</i>(4) = −2.940, <i>p</i> = 0.004; Naïve vs. OF: <i>Z</i>(4) = −3.046, <i>p</i> = 0.002; OF vs. CD: <i>Z</i>(4) = −2.119, <i>p</i> = 0.041). This again was due to the flicker problem of the Naïve method (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig16">16</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Response to Q4: “I could comprehend the foreground augmentations comfortably” (1: not at all ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Q5 directly asked whether the brightness adjustment (if it was felt to be existent) helped the comprehension of the augmentations. Note that the subject was not told whether the brightness was adjusted for the given test condition. Again clearly, the adjustment was thought to be helpful for the comprehension compared to the NOP and mostly so with the OF (NOP vs. OF: <i>Z</i>(4) = −4.686, <i>p</i> &lt; 0.001; NOP vs. CD: <i>Z</i>(4) = −3.873, <i>p</i> &lt; 0.001; NOP vs. OD: <i>Z</i>(4) = −2.984, <i>p</i> = 0.003) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig17">17</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Response to Q5: “Did the brightness of the augmentation seem to change? If so, did it help?” (1: not at all/no adjustment detected ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Q6 had to do with the properness of the timing of the brightness adjustment (if felt to be existent). Consistent with its superior perceived performance, OF was answered to be having the most proper timing (NOP vs. OF: <i>Z</i>(4) = −4.888, <i>p</i> &lt; 0.001; Naïve vs. OF: <i>Z</i>(4) = −2.146, <i>p</i> = 0.038; OF vs. CD: <i>Z</i>(4) = −2.440, <i>p</i> = 0.019; OF vs. OD: <i>Z</i>(4) = −4.123, <i>p</i> &lt; 0.001) and the NOP being the least so (NOP vs. Naïve: <i>Z</i>(4) = −2.498, <i>p</i> = 0.014; NOP vs. CD: <i>Z</i>(4) = −3.555, <i>p</i> &lt; 0.001) (Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig18">18</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Response to Q6: “Did the brightness of the augmentation seem to change in the right time (if felt to be existent)?” (1: not at all/no adjustment detected ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The level of fatigue, due to perceptual/cognitive load or by the dynamics of the brightness modulation (if thought to be existent), was explored through Q7. The Naïve case was felt to be the most tiring case mainly due to the flickering (NOP vs. Naïve: <i>Z</i>(4) = −3.989, <i>p</i> &lt; 0.001; Naïve vs. OF: <i>Z</i>(4) = −4.293, <i>p</i> &lt; 0.001; Naïve vs. CD: <i>Z</i>(4) = −2.678, <i>p</i> = 0.009; Naïve vs. OD: <i>Z</i>(4) = −3.923, <i>p</i> &lt; 0.001). Subjects also mentioned that the CD seems to have caused unnecessary changes in the brightness (of the augmentations) many times and caused more fatigue (NOP vs. CD: <i>Z</i>(4) = −2.055, <i>p</i> = 0.045; OF vs. CD: <i>Z</i>(4) = −2.513, <i>p</i> = 0.014; CD vs. OD: <i>Z</i>(4) = −2.514, <i>p</i> = 0.014) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig19">19</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig19_HTML.gif?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig19_HTML.gif" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Response to Q7: “I was fatigued in reading the augmentations” (1: not at all ~7: very much so)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Finally, regardless of the type of the algorithm used, subjects answered that the brightness modulation was useful compared to NOP with a high average score of 5.83 out of 7.</p><p>The following three graphs in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig20">20</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig21">21</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig22">22</a> show subject responses to the questions of preference, ease of comprehension and fatigue, respectively (the latter two being asked again but in a batch comparison), after the subject experienced all the treatments.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig20_HTML.gif?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig20_HTML.gif" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>Preference among the five brightness adjustment schemes. OF is the most preferred, followed by CD and OD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig21_HTML.gif?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig21_HTML.gif" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Ease of information comprehension among the five brightness adjustment schemes. OF is ranked the highest, followed by CD and OD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-22"><figure><figcaption><b id="Fig22" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 22</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/22" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig22_HTML.gif?as=webp"></source><img aria-describedby="figure-22-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig22_HTML.gif" alt="figure22" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-22-desc"><p>Perceived level of fatigue among the five brightness adjustment schemes. OF is the least tiring and the Naïve and CD the most</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/22" data-track-dest="link:Figure22 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Subjects preferred the OF the most with the CD and OD following in the next. NOP and Naïve were the least preferred; however, there were a significant portion of subjects who ranked the Naïve method to be the best as well. We also converted the preference rank results to numerical values, e.g., by assigning 5 ~ 1 points to rank 1 ~ 5 datum, which brought about similar results: OF—70 pts., OD—56 pts., CD—54 pts., Naïve—42 pts. and NOP—33 pts (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig20">20</a>).</p><p>Collective answers to the ease of comprehension showed a similar trend to the preference results. OF was ranked the highest (easiest) with OD and CD being the distance second. The numerical scores bear the same results (OF—71 pts., OD—56 pts., CD—52 pts., Naïve—46 pts. and NOP—30 pts.) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig21">21</a>). Results with regard to fatigue were not much different from those obtained in Q7. OF was the least tiring and Naïve and CD the most (OF—40 pts., OD—43 pts., CD—57 pts., Naïve—74 pts. and NOP—41 pts.) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig22">22</a>).</p><h3 class="c-article__sub-heading" id="Sec16">Discussion</h3><p>Experiment II explored how to actually apply the brightness adjustment algorithm developed in the early part of this paper in for actual AR applications. Despite the clear differences in the subjective and perceived performance and usability, quantitative measures did not produce statistically significant differences among the five tested methods. However, subjective and qualitative results (even if no strong effects were observed with the quantitative analysis) can still give a good indication of what kind of deployment method might be more effective.</p><p>Any time differences in the initial recognition and discerning of the augmentation were relatively minute with respect to the time for understanding its content (and making the response). This was more apparent when the background was highly dynamic, making the augmentation ultimately or momentarily relatively more visible before the modulation algorithm kicked in. While users perceived subjective differences in the way the information was presented, by concentrating and paying high attention, the task was mostly doable. This makes the results of different effects in the usability much more valuable.</p><p>Consistent with the Experiment I, application of the brightness adjustment did improve the readability over the base line case (NOP). Among the four such methods, the OF was rated the best in most survey categories. There is an interesting report that, similarly to the changed perception of brightness contrast, object velocities are affected by the background movement (Loomis and Nakayama <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1973" title="Loomis JM, Nakayama K (1973) A velocity analogue of brightness contrast. Perception 2(4):425–428" href="/article/10.1007/s10055-017-0319-y#ref-CR21" id="ref-link-section-d27769e2577">1973</a>). For example, even if almost perfectly camouflaged, animals often detected when they start to move (Ehrenstein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ehrenstein WH (2003) Basics of seeing motion. Arq Bras Oftalmol 66(5):44–52" href="/article/10.1007/s10055-017-0319-y#ref-CR9" id="ref-link-section-d27769e2580">2003</a>). Such a phenomenon might explain how even in NOP (where there is no artificial brightness modulation), subjects still reported that they detected brightness change in the static augmentation. CD which activated the adjustment upon a direct detection of sufficient color/brightness contrast change did not show a performance as high as expected. However, this could be due to threshold value improperly set from the limited pilot test data and process. Although none of the subjects were color-deficient, personal differences in color sensitivity could be another factor. Thus, continued research will be needed to find a more scientific way set or even personalize such a threshold value. Another avenue is to combine the two methods of OF and CD.</p><p>The Naïve method, which makes the adjustment at every frame, aggravated and tired the user due to the visual artifact of the noticeable and frequent flickering. This is owing to the relatively small number of brightness levels (only 7) with which the adjustment was made. A smoother and more gradual transition between the adjustment levels is possible but would require more computation, possibly violating the real-time performance requirement (on a general mobile platform). On the other hand, there were a significant portion of the subjects who rated the Naïve method to be the most preferred, which is somewhat at odds with the general findings of this paper. A deeper investigation is needed in the future. Users were generally content with the performance of OD due to its “activate-only-when-needed” nature, but still felt cumbersome to manually indicate such demand, sometimes very often.</p><p>In general, subjects felt the augmented imagery to be more natural when the perceived brightness (or contrast) changed not too abruptly. In this sense, it is easy to understand that the on-demand method is considered natural because the user initiates the change and already has an expectation. Otherwise, the subjects also generally felt the augmented imagery to be more natural when the perceived brightness (or contrast) changed in tune with the changing background or situated environment. OF was rated more natural, reflecting the situational change better than CD, which seems to be overly sensitive to the highly colorful background (at least with our implementation). Oddly, NOP was considered less natural than Naïve (which produced the most flickering), which hints that naturalness and visibility are weakly related as well (i.e., too much low visibility will be felt unnatural).</p><p>While our test background video was composed of several static/slow-moving and dynamic/fast-moving sceneries, it is still far short of representing all types of the real environments in which AR applications would be used. While it would be nearly impossible to devise a universal brightness adjustment method that can adapt to any given environment, it has been shown that suggested methods, based on important visual parameters, can be of good help in many situations for enhanced information readability in AR.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusion</h2><div class="c-article-section__content" id="Sec17-content"><p>In this work, we have presented a real-time simplified algorithm for modulating the contrast saliency and improving readability for augmented reality imageries. Despite the simplifications, the algorithm showed competent perceptual qualities for the experimental tasks and test images, showing promising applicability to improve usability of AR systems. Since the proposed algorithm runs in real time, it will have much more flexibility in real-life application, and as such, the algorithm was put to a test under different AR application schemes, and we have found that adjusting the brightness upon large optical flow showed the best perceived performance, readability and usability.</p><p>We have only compared our algorithm in terms of its perceptual quality (vs. off-line generated ground truth), and thus in the future, we will make further comparison with other real-time algorithms (e.g., those that are not necessarily based on the saliency analysis) in terms of the varying degrees of their perceptual qualities. In addition, we plan to further validate our current approach with a richer data set and extend it by incorporating other saliency properties, trying out other ways of simplifying the original “ground truth—global” algorithm and optimizing for the contrast visibility considering other properties such as background image complexity and object importance. There are variety of issues to consider with regard to contrast sensitivity that were not addressed in this paper, such as the individual difference (Baker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Baker DH (2013) What is the primary cause of individual differences in contrast sensitivity? PLoS ONE 8(7):e69536" href="/article/10.1007/s10055-017-0319-y#ref-CR4" id="ref-link-section-d27769e2605">2013</a>), object size (Meese et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Meese TS, Hess RF, Williams CB (2005) Size matters, but not for everyone: individual differences for contrast discrimination. J Vis 5(11):928–947" href="/article/10.1007/s10055-017-0319-y#ref-CR23" id="ref-link-section-d27769e2608">2005</a>) and content factors. Continued experimentation with a larger subject pool will shed more insight into how to adjust brightness contrast for augmentation for more optimized perceptual performance. Finally, in terms of its real-world application, we plan to embed our algorithm in actual real AR application, e.g., using OF or OF/CD, and further demonstrate the effectiveness of our approach.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>The symbol ‘<span class="mathjax-tex">\({ \ominus }\)</span>’ is called the across-scale difference between two maps and is defined as interpolating the coarser scaled image to the finer one and making point-by-point subtraction (Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e755">1998</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>The symbol ‘<span class="mathjax-tex">\(\oplus\)</span>’ is called the across-scale addition between two maps and is defined as interpolating the coarser scaled image to the finer one and making point-by-point addition (Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0319-y#ref-CR17" id="ref-link-section-d27769e1000">1998</a>).</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Achanta R, Hemami S, Estrada F, Susstrunk S (2009) Frequency-tuned salient region detection. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR1">Achanta R, Hemami S, Estrada F, Susstrunk S (2009) Frequency-tuned salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2009, IEEE, pp 1597–1604</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ackerman E (2013) Could Google Glass hurt your eyes? A Harvard vision scientist and project glass advisor resp" /><p class="c-article-references__text" id="ref-CR2">Ackerman E (2013) Could Google Glass hurt your eyes? A Harvard vision scientist and project glass advisor responds. <a href="http://www.forbes.com/sites/eliseackerman/2013/03/04/could-google-glass-hurt-your-eyes-a-harvard-vision-scientist-and-project-glass-advisor-responds/">http://www.forbes.com/sites/eliseackerman/2013/03/04/could-google-glass-hurt-your-eyes-a-harvard-vision-scientist-and-project-glass-advisor-responds/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avery B, Sandor C, Thomas BH (2009) Improving spatial perception for augmented reality x-ray vision. In: Proce" /><p class="c-article-references__text" id="ref-CR3">Avery B, Sandor C, Thomas BH (2009) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality 2009, IEEE, pp 79–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DH. Baker, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Baker DH (2013) What is the primary cause of individual differences in contrast sensitivity? PLoS ONE 8(7):e69" /><p class="c-article-references__text" id="ref-CR4">Baker DH (2013) What is the primary cause of individual differences in contrast sensitivity? PLoS ONE 8(7):e69536</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1371%2Fjournal.pone.0069536" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20is%20the%20primary%20cause%20of%20individual%20differences%20in%20contrast%20sensitivity%3F&amp;journal=PLoS%20ONE&amp;volume=8&amp;issue=7&amp;publication_year=2013&amp;author=Baker%2CDH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Bauer, B. Erdogan, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Bauer T, Erdogan B (2010) Organizational Behavior, v.1.1. Flat World Knowledge, Irvington" /><p class="c-article-references__text" id="ref-CR5">Bauer T, Erdogan B (2010) Organizational Behavior, v.1.1. Flat World Knowledge, Irvington</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Organizational%20Behavior%2C%20v.1.1&amp;publication_year=2010&amp;author=Bauer%2CT&amp;author=Erdogan%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Birchfield S (2007) KLT: an implementation of the Kanade–Lucas–Tomasi feature tracker. http://www.ces.clemson." /><p class="c-article-references__text" id="ref-CR6">Birchfield S (2007) KLT: an implementation of the Kanade–Lucas–Tomasi feature tracker. <a href="http://www.ces.clemson.edu/%7estb/klt/">http://www.ces.clemson.edu/~stb/klt/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Procee" /><p class="c-article-references__text" id="ref-CR7">Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM (2011) Global contrast based salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2011, IEEE, pp 409–416</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Eadicicco L (2015) Sony just solved the biggest problem with Google Glass. http://www.businessinsider.com/sony" /><p class="c-article-references__text" id="ref-CR8">Eadicicco L (2015) Sony just solved the biggest problem with Google Glass. <a href="http://www.businessinsider.com/sony-smartglasses-attach-solves-the-google-glass-style-problem-2015-1/">http://www.businessinsider.com/sony-smartglasses-attach-solves-the-google-glass-style-problem-2015-1/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WH. Ehrenstein, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ehrenstein WH (2003) Basics of seeing motion. Arq Bras Oftalmol 66(5):44–52" /><p class="c-article-references__text" id="ref-CR9">Ehrenstein WH (2003) Basics of seeing motion. Arq Bras Oftalmol 66(5):44–52</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1590%2FS0004-27492003000600006" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Basics%20of%20seeing%20motion&amp;journal=Arq%20Bras%20Oftalmol&amp;volume=66&amp;issue=5&amp;pages=44-52&amp;publication_year=2003&amp;author=Ehrenstein%2CWH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gabbard JL, Swan JE, Hix D, Kim SJ, Fitch G (2007) Active text drawing styles for outdoor augmented reality: a" /><p class="c-article-references__text" id="ref-CR10">Gabbard JL, Swan JE, Hix D, Kim SJ, Fitch G (2007) Active text drawing styles for outdoor augmented reality: a user-based study and design implications. In: Proceedings of the IEEE conference on virtual reality 2007, IEEE, pp 35–42</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="GOOGLE (2015) Google Glass. http://www.google.com/glass/start/&#xA;                " /><p class="c-article-references__text" id="ref-CR11">GOOGLE (2015) Google Glass. <a href="http://www.google.com/glass/start/">http://www.google.com/glass/start/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="HFES (2007) ANSI/HFES 100-2007 Human Factors Engineering of Computer Workstations. Human Factors and Ergonomic" /><p class="c-article-references__text" id="ref-CR12">HFES (2007) ANSI/HFES 100-2007 Human Factors Engineering of Computer Workstations. Human Factors and Ergonomic Society, Santa Monica</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ANSI%2FHFES%20100-2007%20Human%20Factors%20Engineering%20of%20Computer%20Workstations&amp;publication_year=2007">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hincapié-Ramos JD, Ivanchuk L, Sridharan SK, Irani P (2014) SmartColor: real-time color correction and contras" /><p class="c-article-references__text" id="ref-CR13">Hincapié-Ramos JD, Ivanchuk L, Sridharan SK, Irani P (2014) SmartColor: real-time color correction and contrast for optical see-through head-mounted displays. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2014, IEEE, pp 187–194</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Proceedings of the IEEE conference" /><p class="c-article-references__text" id="ref-CR14">Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2007, IEEE, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Human Benchmark (2017) Reaction time statistics. http://www.humanbenchmark.com/tests/reactiontime/statistics&#xA; " /><p class="c-article-references__text" id="ref-CR15">Human Benchmark (2017) Reaction time statistics. <a href="http://www.humanbenchmark.com/tests/reactiontime/statistics">http://www.humanbenchmark.com/tests/reactiontime/statistics</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="ISO (1992) ISO 9241-3:1992: ergonomic requirements for office work with visual display terminals (VDTs)—part 3" /><p class="c-article-references__text" id="ref-CR16">ISO (1992) ISO 9241-3:1992: ergonomic requirements for office work with visual display terminals (VDTs)—part 3: visual display requirements. International Organization for Standardization, Geneva</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ISO%209241-3%3A1992%3A%20ergonomic%20requirements%20for%20office%20work%20with%20visual%20display%20terminals%20%28VDTs%29%E2%80%94part%203%3A%20visual%20display%20requirements&amp;publication_year=1992">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Itti, C. Koch, E. Niebur, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Tran" /><p class="c-article-references__text" id="ref-CR17">Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.730558" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20model%20of%20saliency-based%20visual%20attention%20for%20rapid%20scene%20analysis&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=20&amp;issue=11&amp;pages=1254-1259&amp;publication_year=1998&amp;author=Itti%2CL&amp;author=Koch%2CC&amp;author=Niebur%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kalkofen D, Veas E, Zollmann S, Steinberger M, Schmalstieg D (2013) Adaptive ghosted views for augmented reali" /><p class="c-article-references__text" id="ref-CR18">Kalkofen D, Veas E, Zollmann S, Steinberger M, Schmalstieg D (2013) Adaptive ghosted views for augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2013, IEEE, pp 1–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="RJ. Kosinski, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Kosinski RJ (2008) A literature review on reaction time. Clemson University, Clemson" /><p class="c-article-references__text" id="ref-CR19">Kosinski RJ (2008) A literature review on reaction time. Clemson University, Clemson</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20literature%20review%20on%20reaction%20time&amp;publication_year=2008&amp;author=Kosinski%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Lee, GJ. Kim, S. Choi, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Lee S, Kim GJ, Choi S (2009) Real-time tracking of visually attended objects in virtual environments and its a" /><p class="c-article-references__text" id="ref-CR20">Lee S, Kim GJ, Choi S (2009) Real-time tracking of visually attended objects in virtual environments and its application to LOD. IEEE Trans Vis Comput Graph 15(1):6–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2008.82" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20tracking%20of%20visually%20attended%20objects%20in%20virtual%20environments%20and%20its%20application%20to%20LOD&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=15&amp;issue=1&amp;pages=6-19&amp;publication_year=2009&amp;author=Lee%2CS&amp;author=Kim%2CGJ&amp;author=Choi%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Loomis, K. Nakayama, " /><meta itemprop="datePublished" content="1973" /><meta itemprop="headline" content="Loomis JM, Nakayama K (1973) A velocity analogue of brightness contrast. Perception 2(4):425–428" /><p class="c-article-references__text" id="ref-CR21">Loomis JM, Nakayama K (1973) A velocity analogue of brightness contrast. Perception 2(4):425–428</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp020425" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20velocity%20analogue%20of%20brightness%20contrast&amp;journal=Perception&amp;volume=2&amp;issue=4&amp;pages=425-428&amp;publication_year=1973&amp;author=Loomis%2CJM&amp;author=Nakayama%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ma YF, Zhang HJ (2003) Contrast-based image attention analysis by using fuzzy growing. In: Proceedings of the " /><p class="c-article-references__text" id="ref-CR22">Ma YF, Zhang HJ (2003) Contrast-based image attention analysis by using fuzzy growing. In: Proceedings of the 11th ACM international conference on multimedia, ACM, pp 374–381</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TS. Meese, RF. Hess, CB. Williams, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Meese TS, Hess RF, Williams CB (2005) Size matters, but not for everyone: individual differences for contrast " /><p class="c-article-references__text" id="ref-CR23">Meese TS, Hess RF, Williams CB (2005) Size matters, but not for everyone: individual differences for contrast discrimination. J Vis 5(11):928–947</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1167%2F5.11.2" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Size%20matters%2C%20but%20not%20for%20everyone%3A%20individual%20differences%20for%20contrast%20discrimination&amp;journal=J%20Vis&amp;volume=5&amp;issue=11&amp;pages=928-947&amp;publication_year=2005&amp;author=Meese%2CTS&amp;author=Hess%2CRF&amp;author=Williams%2CCB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="National Geographic (2015) Africa’s Wild West. http://natgeotv.com/uk/africas-wild-west&#xA;                " /><p class="c-article-references__text" id="ref-CR24">National Geographic (2015) Africa’s Wild West. <a href="http://natgeotv.com/uk/africas-wild-west">http://natgeotv.com/uk/africas-wild-west</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Navab, J. Traub, T. Sielhorst, M. Feuerstein, C. Bichlmeier, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality " /><p class="c-article-references__text" id="ref-CR25">Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality for computer-aided medical procedures. IEEE Comput Graph Appl 27(5):10–14</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2007.117" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Action-and%20workflow-driven%20augmented%20reality%20for%20computer-aided%20medical%20procedures&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=27&amp;issue=5&amp;pages=10-14&amp;publication_year=2007&amp;author=Navab%2CN&amp;author=Traub%2CJ&amp;author=Sielhorst%2CT&amp;author=Feuerstein%2CM&amp;author=Bichlmeier%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Perazzi F, Krähenbühl P, Pritch Y, Hornung A (2012) Saliency filters: contrast based filtering for salient reg" /><p class="c-article-references__text" id="ref-CR26">Perazzi F, Krähenbühl P, Pritch Y, Hornung A (2012) Saliency filters: contrast based filtering for salient region detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition 2012, IEEE, pp 733–740</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reid B (2014) Google Glass causing eye pain and muscle fatigue for some users. http://www.redmondpie.com/googl" /><p class="c-article-references__text" id="ref-CR27">Reid B (2014) Google Glass causing eye pain and muscle fatigue for some users. <a href="http://www.redmondpie.com/google-glass-causing-eye-pain-and-muscle-fatigue-for-some-users/">http://www.redmondpie.com/google-glass-causing-eye-pain-and-muscle-fatigue-for-some-users/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sandor C, Cunningham A, Dey A, Mattila VV (2010) An augmented reality x-ray system based on visual saliency. I" /><p class="c-article-references__text" id="ref-CR28">Sandor C, Cunningham A, Dey A, Mattila VV (2010) An augmented reality x-ray system based on visual saliency. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 27–36</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Supan, I. Stuppacher, M. Haller, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Supan P, Stuppacher I, Haller M (2006) Image based shadowing in real-time augmented reality. Int J Virtual Rea" /><p class="c-article-references__text" id="ref-CR29">Supan P, Stuppacher I, Haller M (2006) Image based shadowing in real-time augmented reality. Int J Virtual Real 5(3):1–7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20based%20shadowing%20in%20real-time%20augmented%20reality&amp;journal=Int%20J%20Virtual%20Real&amp;volume=5&amp;issue=3&amp;pages=1-7&amp;publication_year=2006&amp;author=Supan%2CP&amp;author=Stuppacher%2CI&amp;author=Haller%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceed" /><p class="c-article-references__text" id="ref-CR30">Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013, IEEE, pp 3–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Veas EE, Mendez E, Feiner SK, Schmalstieg D (2011) Directing attention and influencing memory with visual sali" /><p class="c-article-references__text" id="ref-CR31">Veas EE, Mendez E, Feiner SK, Schmalstieg D (2011) Directing attention and influencing memory with visual saliency modulation. In: Proceedings of the SIGCHI conference on human factors in computing systems 2011, ACM, pp 1471–1480</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Ware, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington" /><p class="c-article-references__text" id="ref-CR32">Ware C (2012) Information visualization, perception for design, 3rd edn. Morgan Kaufmann, Burlington</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20visualization%2C%20perception%20for%20design&amp;publication_year=2012&amp;author=Ware%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhai Y, Shah M (2006) Visual attention detection in video sequences using spatiotemporal cues. In: Proceedings" /><p class="c-article-references__text" id="ref-CR33">Zhai Y, Shah M (2006) Visual attention detection in video sequences using spatiotemporal cues. In: Proceedings of the 14th ACM international conference on multimedia, ACM, pp 815–824</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zollmann S, Kalkofen D, Mendez E, Reitmayr G (2010) Image-based ghostings for single layer occlusions in augme" /><p class="c-article-references__text" id="ref-CR34">Zollmann S, Kalkofen D, Mendez E, Reitmayr G (2010) Image-based ghostings for single layer occlusions in augmented reality. In: Proceedings of the IEEE international symposium on mixed and augmented reality 2010, IEEE, pp 19–26</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0319-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported in part by the Basic Science Research Program funded by the National Research Foundation (NRF) and the Ministry of Science, ICT &amp; Future Planning (MSIP) - No. 2011-0030079, and by the Institute for Information &amp; communications Technology Promotion (IITP) grant also funded by MSIP - No. 2017-0-00179, “HD Haptic Technology for Hyper Reality Contents”.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer and Radio Communications Engineering, College of Information and Communications, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Euijai Ahn</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Software, College of Software, Sungkyunkwan University, 2066 Seobu-ro, Jangan-gu, Suwon, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Sungkil Lee</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Computer Science and Engineering, College of Informatics, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Gerard Jounghyun Kim</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Euijai-Ahn"><span class="c-article-authors-search__title u-h3 js-search-name">Euijai Ahn</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Euijai+Ahn&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Euijai+Ahn" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Euijai+Ahn%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Sungkil-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Sungkil Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sungkil+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sungkil+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sungkil+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gerard_Jounghyun-Kim"><span class="c-article-authors-search__title u-h3 js-search-name">Gerard Jounghyun Kim</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gerard Jounghyun+Kim&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gerard Jounghyun+Kim" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gerard Jounghyun+Kim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0319-y/email/correspondent/c1/new">Gerard Jounghyun Kim</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix</h3><p>Appendix provides the details of the three brightness modulation algorithms compared in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0319-y#Sec3">3</a>. The first algorithm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig23">23</a>, labeled “global” or “G” in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab1">1</a>), computes the ground truth brightness adjustment value by considering all possible combinations of the foreground object’s variations and maximizing for the total saliency value. The second (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig24">24</a>, labeled “local” or “L” in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab1">1</a>), derives the near ground truth brightness only considering each augmentation object in isolation and therefore maximizing only the local saliency value for each of them. The third is the proposed simplified algorithm (labeled “<i>S</i>” in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0319-y#Tab1">1</a>) shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0319-y#Fig25">25</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-23"><figure><figcaption><b id="Fig23" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 23</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/23" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig23_HTML.gif?as=webp"></source><img aria-describedby="figure-23-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig23_HTML.gif" alt="figure23" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-23-desc"><p>Pseudocode for ground truth—global (“G”)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/23" data-track-dest="link:Figure23 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-24"><figure><figcaption><b id="Fig24" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 24</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/24" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig24_HTML.gif?as=webp"></source><img aria-describedby="figure-24-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig24_HTML.gif" alt="figure24" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-24-desc"><p>Pseudocode for ground truth—local (“L”)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/24" data-track-dest="link:Figure24 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-25"><figure><figcaption><b id="Fig25" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 25</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/25" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig25_HTML.gif?as=webp"></source><img aria-describedby="figure-25-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0319-y/MediaObjects/10055_2017_319_Fig25_HTML.gif" alt="figure25" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-25-desc"><p>Pseudocode for the proposed simplified real-time algorithm (“S”)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0319-y/figures/25" data-track-dest="link:Figure25 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Real-time%20adjustment%20of%20contrast%20saliency%20for%20improved%20information%20visibility%20in%20mobile%20augmented%20reality&amp;author=Euijai%20Ahn%20et%20al&amp;contentID=10.1007%2Fs10055-017-0319-y&amp;publication=1359-4338&amp;publicationDate=2017-07-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0319-y" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0319-y" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ahn, E., Lee, S. &amp; Kim, G.J. Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality.
                    <i>Virtual Reality</i> <b>22, </b>245–262 (2018). https://doi.org/10.1007/s10055-017-0319-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0319-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-06-23">23 June 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-06-14">14 June 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-07-05">05 July 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-09">September 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0319-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0319-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Human perception and performance</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">See-through display</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Saliency</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Contrast</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0319-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=319;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

