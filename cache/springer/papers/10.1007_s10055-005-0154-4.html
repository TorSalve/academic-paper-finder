<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Model&#8211;based video tracking for gestural interaction"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Among many techniques to interact with 3D environments, gesture-based input appears promising. However, due to insufficient computing hardware capabilities, such interfaces have to be built either..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Model&#8211;based video tracking for gestural interaction"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-07-19"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Among many techniques to interact with 3D environments, gesture-based input appears promising. However, due to insufficient computing hardware capabilities, such interfaces have to be built either upon standard tracking devices or using limited image-based video tracking algorithms. As today computing power tends to be more and more powerful, more complex video analysis such as real-time model-based tracking is at hand. Considering the use of a model-based approach to allow unencumbered input gives us the advantage of extracting a low-level hand description useful to build natural interfaces. The algorithm we developed relies on a 3D polygonal hand model. Its pose parametrization is iteratively refined so that its 2D projection matches more closely the input 2D image. Relying on the graphics hardware to handle fast 2D projection is critical, while adding more cameras is useful to cope with the occlusion issue."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-07-19"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="213"/>

    <meta name="prism.endingPage" content="221"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0154-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0154-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0154-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0154-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Model&#8211;based video tracking for gestural interaction"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2005/09"/>

    <meta name="citation_online_date" content="2005/07/19"/>

    <meta name="citation_firstpage" content="213"/>

    <meta name="citation_lastpage" content="221"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0154-4"/>

    <meta name="DOI" content="10.1007/s10055-005-0154-4"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0154-4"/>

    <meta name="description" content="Among many techniques to interact with 3D environments, gesture-based input appears promising. However, due to insufficient computing hardware capabilities"/>

    <meta name="dc.creator" content="J. -B. de la. Rivi&#232;re"/>

    <meta name="dc.creator" content="P. Guitton"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Segen J, Kumar S (1999) Shadow gestures: 3d hand pose estimation using a single camera. In: IEEE conference on computer vision and pattern recognition, pp 479&#8211;485"/>

    <meta name="citation_reference" content="Freeman W, Tanaka K, Ohta J, Kyuma K (1996) Computer vision for computer games. FG&#8217;96 (1996), pp 100&#8211;105"/>

    <meta name="citation_reference" content="Davis J (1998) Recognizing movement using motion histograms. Technical Report 487, MIT Media Lab"/>

    <meta name="citation_reference" content="Kwatra V, Bobick A, Johnson A (2001) Temporal integration of multiple silhouette&#8211;based body&#8211;part hypotheses. In: Computer Vision and Pattern Recognition"/>

    <meta name="citation_reference" content="Rehg J, Kanade T (1994) Digiteyes: vision&#8211;based hand tracking for human computer interaction. In: Workshop on motion of non&#8211;rigid and articulated objects. pp 16&#8211;22"/>

    <meta name="citation_reference" content="Wu Y, Huang T (1999) Capturing articulated human hand motion: A divide&#8211;and&#8211;conquer approach. In: International Conference on Computer Vision. pp 606&#8211;611"/>

    <meta name="citation_reference" content="Heap T, Hogg D (1996) Towards 3d hand tracking using a deformable model. In: Conference on automatic face and gesture recognition. pp 140&#8211;145"/>

    <meta name="citation_reference" content="Shimada N, Shirai Y, Kuno Y, Miura J (1998) Hand gesture estimation and model refinment using monocular camera &#8211; ambiguity limitation by inequality constraints. In: 3rd conference on Face and Gesture Recognition. pp 268&#8211;273"/>

    <meta name="citation_reference" content="Stenger B, Mendon&#231;a P, Cipolla R (2001) Model&#8211;based hand tracking using an unscented kalman filter. In: British machine vision conference. vol 1, pp 63&#8211;72"/>

    <meta name="citation_reference" content="Deawele G, Devernay F, Horaud R (2003) Hand motion from 3d point trajectories and a smooth surface model. In: 8th European conference on computer vision"/>

    <meta name="citation_reference" content="Leubner C, Brockman C, M&#252;ller H (2001) Computer&#8211;vision&#8211;based human-computer interaction with a back projection wall using arm gestures. Euromicro conference"/>

    <meta name="citation_reference" content="Moeslund T, St&#246;rring M, Granum E (2000) Vision&#8211;based user interface for interacting with a virtual environment. In: conference DANKOMB"/>

    <meta name="citation_reference" content="N&#246;lker C, Ritter H (1998) Illumination independant recognition of deictic arm postures. In: 24th annual conference of the IEEE industrial electronic society. pp 2006&#8211;2011"/>

    <meta name="citation_reference" content="Sato Y, Saito M, Koike H (2001) Real&#8211;time input of 3d hand pose and gestures of a user&#8217;s hand and its applications for hci. In: Virtual reality conference. p 79"/>

    <meta name="citation_reference" content="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: Toward spontaneous and natural interaction in semi&#8211;immersive virtual environments. In: IEEE conference on virtual reality. pp 13&#8211;20"/>

    <meta name="citation_reference" content="Davis J, Bobick A (1998) Sideshow: a silhouette&#8211;based interactive dual&#8211;screen environment. Technical Report 457, MIT Media Lab Perceptual Computing Group"/>

    <meta name="citation_reference" content="Ouhaddi H, Horain P, Mikolajczyk K (1998) Mod&#233;lisation et suivi de la main. In: Compression et repr&#233;sentation des signaux audiovisuels. pp 109&#8211;114"/>

    <meta name="citation_reference" content="citation_journal_title=The Visual Computer; citation_title=An improved articulated model of the human hand; citation_author=J McDonald, J Toro, K Alkoby, A Berthiaume, R Carter, P Chomwong, J Christopher, M Davidson, J Furst, B Konie, G Lancaster, L Roychoudhuri, E Sedgwick, N Tomuro, R Wolfe; citation_volume=17; citation_publication_date=2001; citation_pages=158-166; citation_id=CR18"/>

    <meta name="citation_reference" content="Delamarre Q, Faugeras O (1998) Finding pose of hand in video images: a stereo&#8211;based approach. In: FG&#8217;98"/>

    <meta name="citation_reference" content="de la Rivi&#232;re JB (2004) 
                    http://www.labri.fr/perso/&#8764;leproux/modelbased.htm
                    
                  "/>

    <meta name="citation_reference" content="Athitsos V, Sclaroff S (2003) Database indexing methods for 3d hand pose estimation. In: International Gesture Workshop, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_author" content="J. -B. de la. Rivi&#232;re"/>

    <meta name="citation_author_email" content="leproux@labri.fr"/>

    <meta name="citation_author_institution" content="LaBRI, INRIA Futurs, Universite&#8217; Bordeaux 1, Talence Cedex, France"/>

    <meta name="citation_author" content="P. Guitton"/>

    <meta name="citation_author_email" content="guitton@labri.fr"/>

    <meta name="citation_author_institution" content="LaBRI, INRIA Futurs, Universite&#8217; Bordeaux 1, Talence Cedex, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0154-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0154-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Model–based video tracking for gestural interaction"/>
        <meta property="og:description" content="Among many techniques to interact with 3D environments, gesture-based input appears promising. However, due to insufficient computing hardware capabilities, such interfaces have to be built either upon standard tracking devices or using limited image-based video tracking algorithms. As today computing power tends to be more and more powerful, more complex video analysis such as real-time model-based tracking is at hand. Considering the use of a model-based approach to allow unencumbered input gives us the advantage of extracting a low-level hand description useful to build natural interfaces. The algorithm we developed relies on a 3D polygonal hand model. Its pose parametrization is iteratively refined so that its 2D projection matches more closely the input 2D image. Relying on the graphics hardware to handle fast 2D projection is critical, while adding more cameras is useful to cope with the occlusion issue."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Model–based video tracking for gestural interaction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0154-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Full hand pose estimation, Real-time video analysis, Articulated tracking, VR interaction techniques","kwrd":["Full_hand_pose_estimation","Real-time_video_analysis","Articulated_tracking","VR_interaction_techniques"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0154-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0154-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=154;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0154-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Model–based video tracking for gestural interaction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0154-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0154-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-07-19" itemprop="datePublished">19 July 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Model–based video tracking for gestural interaction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J___B__de_la_-Rivi_re" data-author-popup="auth-J___B__de_la_-Rivi_re" data-corresp-id="c1">J. -B. de la. Rivière<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universite’ Bordeaux 1" /><meta itemprop="address" content="grid.412041.2, 000000012106639X, LaBRI, INRIA Futurs, Universite’ Bordeaux 1, 351 cours de la libé ratia, 33405, Talence Cedex, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-P_-Guitton" data-author-popup="auth-P_-Guitton" data-corresp-id="c2">P. Guitton<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universite’ Bordeaux 1" /><meta itemprop="address" content="grid.412041.2, 000000012106639X, LaBRI, INRIA Futurs, Universite’ Bordeaux 1, 351 cours de la libé ratia, 33405, Talence Cedex, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">213</span>–<span itemprop="pageEnd">221</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">88 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0154-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Among many techniques to interact with 3D environments, gesture-based input appears promising. However, due to insufficient computing hardware capabilities, such interfaces have to be built either upon standard tracking devices or using limited image-based video tracking algorithms. As today computing power tends to be more and more powerful, more complex video analysis such as real-time model-based tracking is at hand. Considering the use of a model-based approach to allow unencumbered input gives us the advantage of extracting a low-level hand description useful to build natural interfaces. The algorithm we developed relies on a 3D polygonal hand model. Its pose parametrization is iteratively refined so that its 2D projection matches more closely the input 2D image. Relying on the graphics hardware to handle fast 2D projection is critical, while adding more cameras is useful to cope with the occlusion issue.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>On one hand, Virtual Reality (VR) display hardware evolved in the last decades from small-size screens to larger ones. Most recent graphics hardware features excellent image quality thanks to high resolution, accurate color restitution, and stereo display. Likewise, human-size image projection and high refresh rates ensure lifelike visualization. On the other hand, interaction with 3D environments is mainly handled by classic devices such as the DataGlove or the wand, which are provided in many everyday-used VR systems. Even if they are fully functional, they still exhibit some well-known drawbacks such that the user is encumbered, for instance.</p><p>Natural interfaces are often built upon speech and gesture recognition. We are focusing on the gesture evaluation part, which is often based on gloves or tracker-made devices. By achieving gesture recognition through video analysis, a step toward transparent input devices will be made. Whether they focus on the user’s hand or body, many systems have already been proposed. But most address a particular application and do not provide the lower level information that would be needed in other areas. Now a days, computing power has proved to be enough to run more complex video analysis algorithms in real–time.</p><p>Thus, to analyze which capabilities current systems could offer toward more natural gesture interfaces, we started by studying the analysis of the unequipped hand motion. Our goal is to handle tracking results as a low-level hand configuration description. Thus, building natural interfaces will be easier as hand poses will not be restricted to a predefined set of gestures. We are focusing on life-like 3D manipulation in VR applications, while keeping in mind the naturalness of other interaction tasks such as selection and navigation. Therefore, we are investigating what kind of real-time video tracking results should be expected using ordinary recent PC hardware. Subsequently, we will analyze which interaction tasks could efficiently be built upon such algorithms.</p><p>In the first section, we will briefly describe some of the previous systems we know and introduce the video constraints linked to the VR domain and large displays consideration. Then, we will discuss the specificities of our model-based algorithm. Finally, before commenting on the results we obtained, we will give an in-depth description of the various parameters needed to be set up in our video tracking configuration.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous works</h2><div class="c-article-section__content" id="Sec2-content"><p>We do not restrict our bibliography to real-time hand tracking systems. Contrariwise, we have tried to explore the articulated object tracking area in general.</p><h3 class="c-article__sub-heading" id="Sec3">Video analysis</h3><p>Image-based approach to video tracking consists in applying image-analysis operations to retrieve particular information about the object. It consists extracting various geometrical measures, as well as statistically classifying the input image relative to a predefined gesture set. For example, Segen and Kumar [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Segen J, Kumar S (1999) Shadow gestures: 3d hand pose estimation using a single camera. In: IEEE conference on computer vision and pattern recognition, pp 479–485" href="/article/10.1007/s10055-005-0154-4#ref-CR1" id="ref-link-section-d54578e289">1</a>] compute contour minima and maxima angles in images containing the hand and its shadow. Thus, they can classify the pose of the hand and evaluate different parameters such as finger spacing or index height thanks to its shadow. To control the 2D position and orientation of some object, Freeman et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Freeman W, Tanaka K, Ohta J, Kyuma K (1996) Computer vision for computer games. FG’96 (1996), pp 100–105" href="/article/10.1007/s10055-005-0154-4#ref-CR2" id="ref-link-section-d54578e292">2</a>] use image moments computation. Similarly, Davis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Davis J (1998) Recognizing movement using motion histograms. Technical Report 487, MIT Media Lab" href="/article/10.1007/s10055-005-0154-4#ref-CR3" id="ref-link-section-d54578e295">3</a>] relies on motion history images to compute motion histograms and classify the current body movement according to a predefined gesture set. Kwatra et al. extract in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Kwatra V, Bobick A, Johnson A (2001) Temporal integration of multiple silhouette–based body–part hypotheses. In: Computer Vision and Pattern Recognition" href="/article/10.1007/s10055-005-0154-4#ref-CR4" id="ref-link-section-d54578e298">4</a>] another kind of information as they use probabilistic evaluations to extract the location of body parts in the silhouette.</p><p>On the contrary, model-based approaches rely on matching a parametric model of the articulated object onto the input frames. Thus, thanks to a lower level object representation, they seem to be capable of offering many of the information the previous systems search. One of the earliest implementation comes from Rehg and Kanade [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Rehg J, Kanade T (1994) Digiteyes: vision–based hand tracking for human computer interaction. In: Workshop on motion of non–rigid and articulated objects. pp 16–22" href="/article/10.1007/s10055-005-0154-4#ref-CR5" id="ref-link-section-d54578e304">5</a>] where they rely on edges and fingertips features to drive a 21° of freedom hand model. They use a Levenberg–Marquardt minimization scheme applied on stereo input. The main issue is related to finger occlusions. Wu and Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Wu Y, Huang T (1999) Capturing articulated human hand motion: A divide–and–conquer approach. In: International Conference on Computer Vision. pp 606–611" href="/article/10.1007/s10055-005-0154-4#ref-CR6" id="ref-link-section-d54578e307">6</a>] have chosen to combine various articulated tracking techniques such as inverse kinematics, genetic-algorithms and least-median-square minimization in order to track a 21 degrees of freedom hand. But fingertips occlusion still is an issue, and their results do not seem a lot more precise than others. A common solution to reduce the hand model degrees of freedom has been found in the work of Heap and Hogg [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Heap T, Hogg D (1996) Towards 3d hand tracking using a deformable model. In: Conference on automatic face and gesture recognition. pp 140–145" href="/article/10.1007/s10055-005-0154-4#ref-CR7" id="ref-link-section-d54578e310">7</a>]. They use a principal component analysis (PCA) and measure model and image similarity by comparing edge distances. The originality of adapting the hand model to each input hand is seen in the work of Shimada et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Shimada N, Shirai Y, Kuno Y, Miura J (1998) Hand gesture estimation and model refinment using monocular camera – ambiguity limitation by inequality constraints. In: 3rd conference on Face and Gesture Recognition. pp 268–273" href="/article/10.1007/s10055-005-0154-4#ref-CR8" id="ref-link-section-d54578e313">8</a>]. But their results are limited to a 3 degrees of freedom finger moving in a plane. The work of Stenger et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Stenger B, Mendonça P, Cipolla R (2001) Model–based hand tracking using an unscented kalman filter. In: British machine vision conference. vol 1, pp 63–72" href="/article/10.1007/s10055-005-0154-4#ref-CR9" id="ref-link-section-d54578e316">9</a>] describes a software 3D engine that projects their quadric-made hand model into the image plane while keeping information about fingers that are projected on the same pixels. However, it is unsure how their system will work with requirements higher than their 6 degrees of freedom at 3 Hz. Recently, Deawele et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Deawele G, Devernay F, Horaud R (2003) Hand motion from 3d point trajectories and a smooth surface model. In: 8th European conference on computer vision" href="/article/10.1007/s10055-005-0154-4#ref-CR10" id="ref-link-section-d54578e320">10</a>] mix point-to-point stereo correlation and model-based tracking to compute 3D hand motion.</p><h3 class="c-article__sub-heading" id="Sec4">VR gesture recognition</h3><p>Some video tracking systems have already been applied to VR environments. For instance, Leubner et al.’s [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Leubner C, Brockman C, Müller H (2001) Computer–vision–based human-computer interaction with a back projection wall using arm gestures. Euromicro conference" href="/article/10.1007/s10055-005-0154-4#ref-CR11" id="ref-link-section-d54578e331">11</a>] attempt at using two cameras to retrieve pointing gestures confirms video usability for VR interfaces, even if the user is at a small distance from the wall in natural light conditions. Pointing direction is also successfully considered by Moeslund et al. in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Moeslund T, Störring M, Granum E (2000) Vision–based user interface for interacting with a virtual environment. In: conference DANKOMB" href="/article/10.1007/s10055-005-0154-4#ref-CR12" id="ref-link-section-d54578e334">12</a>] but uses head trackers. Deictic postures are once again dealt with by Nölker and Ritter [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Nölker C, Ritter H (1998) Illumination independant recognition of deictic arm postures. In: 24th annual conference of the IEEE industrial electronic society. pp 2006–2011" href="/article/10.1007/s10055-005-0154-4#ref-CR13" id="ref-link-section-d54578e337">13</a>]. Information is retrieved thanks to neural networks.</p><p>Sato et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Sato Y, Saito M, Koike H (2001) Real–time input of 3d hand pose and gestures of a user’s hand and its applications for hci. In: Virtual reality conference. p 79" href="/article/10.1007/s10055-005-0154-4#ref-CR14" id="ref-link-section-d54578e343">14</a>] is one example of extracting hand geometric information to instantiate a complete video based interface, including navigation and manipulation. Their system needs two video cameras and is set in a VR Reality Center under normal lighting conditions. Leibe et al.’s work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: Toward spontaneous and natural interaction in semi–immersive virtual environments. In: IEEE conference on virtual reality. pp 13–20" href="/article/10.1007/s10055-005-0154-4#ref-CR15" id="ref-link-section-d54578e346">15</a>] tries to address the issue of object selection in a workbench configuration. Their system proposes to rely on multiple cameras combined with several infrared light sources. Finally, the work from Davis and Bobick [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Davis J, Bobick A (1998) Sideshow: a silhouette–based interactive dual–screen environment. Technical Report 457, MIT Media Lab Perceptual Computing Group" href="/article/10.1007/s10055-005-0154-4#ref-CR16" id="ref-link-section-d54578e349">16</a>] proposes some ways to extract and interpret body silhouette in front of a rear-projected screen by using the IR shadows it casts.</p><p>Those video tracking algorithms, developed for large-display VR systems, have to be classified as image-based ones. Some model-based approaches, on the other hand, manage to provide most of the information they extract thanks to a lower level representation.</p><h3 class="c-article__sub-heading" id="Sec5">VR specificities</h3><p>Besides the need of high computing and graphical power due to complex 3D environments and simulation requirements, VR environments show many new constraints when compared with standard desktop systems. Those constraints will affect the input interface design. Indeed, video analysis choices will be induced by the large display VR configurations we consider. High input device refresh rate is necessary to ensure fast system response. Video tracking algorithms have to extract results at a near 10 Hz refresh rate to be quick enough.</p><p>Rooms where large display VR configurations stand are usually very dark, and feature a large empty surface in front of the projection screen. To cope with the lack of visible light, most video tracking algorithms add infrared light sources. While the background can be easily extracted, contrast in those images is often very low. Thus, we are dealing with binary images of an unequipped hand or body. To be able to place video cameras far away from the target, we limit ourselves to low-resolution video frames.</p><p>In unrestricted hand motion retrieval, internal occlusions will often arise. Moreover, as large displays are collective visualization equipments, multiple users can move in front of each other leading to external occlusions. Thus, even if the large size of the room ensures the abillity on several video cameras, frequent occlusions will render many cameras ineffective. That is why we focus on a system which can make full use of multiple cameras, but which will not fail if a single camera only is available.</p><p>Mixing those constraints with the solutions adopted by earlier systems, we extracted the following set of properties the video tracking algorithm must cope with: real-time results, low resolution binary images, maximum occlusion independance, and one or more video cameras.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Our model-based algorithm proposition</h2><div class="c-article-section__content" id="Sec6-content"><h3 class="c-article__sub-heading" id="Sec7">General description</h3><p>All model-based tracking algorithms follow the same steps, which are depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb1.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb1.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Stages of a standard model–based algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>First, the articulated object must be modeled according to its degrees of freedom as well as the way it will be compared with the input video. Models may feature various degrees of freedom, may be 3D or 2D, polygonal or wireframe... Then, starting from an initial configuration such as the pose of the hand model in the last image, the vector encoding the model degrees of freedom is modified to make the model pose gradually converge to the input one. Practically, for each frame of the video input, the following loop is iterated until a new analysis has to be performed (that is, each time a new frame is grabbed because we are working in real-time): </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>extract features, such as edges, fingertips or silhouettes from the input images;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>compare the features with the corresponding representation of the model in its current pose to compute an error measure;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>update the model pose in order to optimize the error.</p>
                      
                    </li>
                  </ol><p>We have opted for comparing the rendering of an articulated hand with the input image. Thus, our optimization method will attempt to maximize the similarity between the frame which is rendered using a given model pose and the input image. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig2">2</a> describes the model-based choices we did, which are discussed in the following subsections.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb2.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb2.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Stages of our model–based algorithm</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">Image features</h3><p>In large part of the previous work we are aware of relying on the usual fingertips feature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Rehg J, Kanade T (1994) Digiteyes: vision–based hand tracking for human computer interaction. In: Workshop on motion of non–rigid and articulated objects. pp 16–22" href="/article/10.1007/s10055-005-0154-4#ref-CR5" id="ref-link-section-d54578e466">5</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Wu Y, Huang T (1999) Capturing articulated human hand motion: A divide–and–conquer approach. In: International Conference on Computer Vision. pp 606–611" href="/article/10.1007/s10055-005-0154-4#ref-CR6" id="ref-link-section-d54578e469">6</a>]. However, such a feature cannot provide robust tracking in occlusion situations where a fingertip does not appear in the input image. This can happen when a finger is moved in front of another one, as well as when the palm is rotated. Edges are another feature often met, and help to build interesting systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Heap T, Hogg D (1996) Towards 3d hand tracking using a deformable model. In: Conference on automatic face and gesture recognition. pp 140–145" href="/article/10.1007/s10055-005-0154-4#ref-CR7" id="ref-link-section-d54578e472">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Ouhaddi H, Horain P, Mikolajczyk K (1998) Modélisation et suivi de la main. In: Compression et représentation des signaux audiovisuels. pp 109–114" href="/article/10.1007/s10055-005-0154-4#ref-CR17" id="ref-link-section-d54578e475">17</a>]. But our binarized images constraint takes away most of its advantages as inter-finger borders are hidden when, for instance, stretched fingers are touching one another. Moreover, edges will greatly vary between low resolution frames.</p><p>Silhouette information seems to be promising. Such a basic feature is easily extracted from any binary image. It proves to be very useful as it encodes the position of the objects pixels in the image. The error measure <i>r</i><sub>
                    <i>f</i>
                  </sub> (<i>q</i>) related to this feature at the input frame <i>f</i> can be defined as the Hamming distance: </p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$r_{f} (q) = \sum\limits_{x=0}^{w}{\sum\limits_{y=0}^{h}{|I_{f} (x, y) - S_{q} (x, y)| }}$$</span></div></div><p> where <i>q</i> is the vector of parameters encoding the articulated model’s current pose, <i>w</i> and <i>h</i> define the image resolution. Finally, <i>I</i><sub>
                    <i>f</i>
                  </sub> stands for the input image corresponding to frame <i>f</i> and <i>S</i><sub>
                    <i>q</i>
                  </sub> for the synthetic image associated to the pose <i>q</i>. It counts the number of pixels which are different between the input image and the synthetic one. This can be interpreted as a covering measure. The silhouette feature also introduces an answer to video noise problems, as every pixel is as important as its neighbors, only the edge pixels are affected by noise. In addition, since occluding body points appear in the input image instead of occluded ones, the occlusion information is implicitly encoded in the silhouette image.</p><h3 class="c-article__sub-heading" id="Sec9">Input</h3><p>The input video flow is made of binary images. They are taken from a single or several video cameras. While trying to keep the required camera number as low as possible, it is well-known that many hand poses cannot be extracted when using too few cameras. Similarly, we do not impose any specific position to the video cameras but many gestures will not be correctly retrieved from badly placed cameras. Indeed, only new views onto the tracked object must be added in order for an additional camera to be useful. Thus it will help to reduce occlusion ambiguities.</p><p>Binarization stage takes away the background from the input frames, and keeps the object silhouette according to a threshold. While being currently static, this threshold value can be adapted to suit specific binarization needs. We are working with images featuring a resolution of 160×120 pixels. Although corresponding to our webcam resolution, we believe it is representative of the subpart of some 640×480 input images showing the video of a moving hand. Even if the working resolution can easily be upgraded, the real-time constraint does not allow to work with a too high pixel count as it is directly related to the algorithm complexity through the covering-measure computing.</p><h3 class="c-article__sub-heading" id="Sec10">Model</h3><p>To compare real silhouette with synthetic articulated object projection, we need a detailed enough model. It must allow to quickly compute the synthetic silhouette image from any point of view using any configuration of the articulated virtual object. That is why we choose to use a polygonal 3D representation. Each rigid part is composed of a single triangle set. To this single geometry the set of parameters that models the part behavior is associated.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Hand model under various points of view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Our hand model is built with 1,500 triangles. It allows 21 degrees of freedom. There are 6 global dof to encode the model pose: 3 dof are related to the translation component, while the 3 others encode the rotation one. Then, each finger offers 3 local dof, where 1 dof corresponds to the rotation of the finger in the palm plane and the 2 others parameterize the flexion of the first and second phalanxes. Classically, the angle related to the third phalanx flexion is constrained to be two thirds of the second flexion angle (see, for example, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="McDonald J, Toro J, Alkoby K, Berthiaume A, Carter R, Chomwong P, Christopher J, Davidson M, Furst J, Konie B, Lancaster G, Roychoudhuri L, Sedgwick E, Tomuro N, Wolfe R (2001) An improved articulated model of the human hand. The Visual Computer 17:158–166" href="/article/10.1007/s10055-005-0154-4#ref-CR18" id="ref-link-section-d54578e578">18</a>] for a brief description of hand modeling). In our model, the thumb is not currently different from the other fingers, except for the various rotation axes orientations. The palm is modeled as a single rigid part. We also built a human body model featuring 28 dof. There are six global dof, three encoding torso rotation as well as three others related to head rotation. Arms and legs are built according to the same scheme: three dof are associated to orientation of the upper part and one dof is devoted to the flexion of the lower part.</p><p>Those parameters are affected by two distinct constraints. First, the values they take can be restricted in a predefined set, as will be for example the flexion of a finger. Second, it will be useful to know the parameters which will modify each rigid part position in the the resulting image. For example, considering the human hand, the projection of the index first phalanx depends solely on the six global dof and the two local ones. Those links can be modeled in a hierarchical tree structure as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb4.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4flb4.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Dof hierarchy of our hand model. Modifying the parameters associated to a node will affect each of its children’s position</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>All those information help us to be aware of how the articulated object we consider will vary over time. As synthetic views of the object must be generated for each video camera, virtual cameras have to be modelled according to real cameras’ properties. In a calibration stage we will describe later, we set the parameters of the virtual cameras by determining their focal length and center of projection, and their position and orientation related to a central point which is located near the articulated body initial place. An OpenGL application synthetizes the model 2D projection by interpreting those parameters. This library is able to take advantage of the latest graphics hardware improvements, thus generating synthetic images at a high frame rate. However, in order to bring the rendering results from graphics video memory back to central memory, we must read the framebuffer corresponding to each camera render result. This operation is far less hardware optimized than the projection one.</p><h3 class="c-article__sub-heading" id="Sec11">Optimization</h3><p>The real-time constraint influences the choice of the optimization method. Indeed, the chosen method must be able to take advantage of every single error computation it can perform. Even with a reduced iteration number, the model has to converge to the correct pose. That is why we opted for an optimization method relying on the iterative modification of the articulated model to make it converge to the configuration corresponding to a local minimum.</p><p>All dof are estimated in a breadth-first-search order. The idea which justifies this order is that parameters located higher, near the root of the hierarchical tree (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig4">4</a>), affect larger areas in the resulting image than the lower ones. Moreover, a higher parameter also modifies the projection of the body parts related to its children dof. Thus, evaluating lower dof before higher ones will often be useless. For example, trying to find out the effect of the index flexion on the silhouette covering rate is unnecessary if the hand palm is not yet in its correct posture.</p><p>An iterative search along one direction is made to determine the pose of each dof. Its value is decreased or increased by a fixed step at each iteraction, depending on the direction which maximizes the silhouette matching. All dof of a given height are evaluated simultaneously. The covering rate modified by a single dof alteration is compared to the current Hamming distance. This reference measure has been affected by the modification of the parameters considered before. The set of values a dof can take is updated at each iteration, thus allowing to take into account the hand constraints we talked about.</p><p>To extend the error measure computation from a single to several cameras is quite a hard task. Nevertheless, many observations show us that defining the error associated to <i>n</i> cameras as the sum of the <i>n</i> Hamming distances offers good results. They correspond quite well to what can be expected from the additional camera.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">System configuration</h2><div class="c-article-section__content" id="Sec12-content"><p>To set up a video tracking system requires focusing on some aspects that go beyond the choice of a kind of image analysis algorithm. In our situation, those details are related to camera position, camera calibration, and initialization stage. How to set up good lighting conditions will not take part in our concerns as we chose to deal with binary images, which are easily obtained in front of a dark background.</p><h3 class="c-article__sub-heading" id="Sec13">Additional steps</h3><p>First, one must choose the position and the number of video cameras used. One camera may be sufficient under certain circumstances, but a few others may be required to keep the freedom of movement that will ensure the naturalness of the future interface. Two cameras side by side will not be useful, unless stereo correlation is taken into account, as in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Delamarre Q, Faugeras O (1998) Finding pose of hand in video images: a stereo–based approach. In: FG’98" href="/article/10.1007/s10055-005-0154-4#ref-CR19" id="ref-link-section-d54578e646">19</a>]. In a 3D environment, a set of three cameras linked to three orthogonal axis seems a good choice. However, in the case of hand tracking, one camera in front of the hand and another one on the other side still offer an interesting viewing volume.</p><p>Relying on multiple views of the same object obliges to know the original position of each of the camera. That is why, prior to running the tracking system, we insert a calibration step. Its goal is to compute the transformation matrices that encode position and orientation of each camera in the reference frame located at each other one. We wrote this application using OpenCV, thus it is based on waving a chessboard in front of each camera to extract its intrinsic parameters. To determine extrinsic parameters, and furthermore the transformation matrices, when the cameras are perpendicular, we grab frames of two orthogonal chessboards stuck on a rigid box. Knowing the transformation matrix from one grid to the other and the transformation matrices from each camera to its grid, it is easy to compute the resulting transformation matrix. Finally, importing those matrices in the OpenGL rendering process allows us to accurately model the position of the real–world cameras. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig5">5</a> gives an idea of the efficiency of this process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Calibration results. The two images correspond to each of the two cameras. We surimposed OpenGL 3D renderings of the grid in front of the camera, the second grid, and the reference frame associated to the other camera</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Before launching the first iteration of the optimization loop, every algorithm must start with a good guess of the initial object position and configuration. That is why many systems include an initialization stage. To make the two silhouettes more similar, the geometrical properties of the hand model are adapted to the input hand. Like in some others applications, the user is asked to show is hand stretched in a camera projection plane. To compute the initial position and orientation, which are the hand global parameters, we rely on the images moments as previously did [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Freeman W, Tanaka K, Ohta J, Kyuma K (1996) Computer vision for computer games. FG’96 (1996), pp 100–105" href="/article/10.1007/s10055-005-0154-4#ref-CR2" id="ref-link-section-d54578e675">2</a>]: </p><ul class="u-list-style-dash">
                    <li>
                      <p>the moment angle shows the amount of rotation to apply to the hand model to match the orientation of the input hand;</p>
                    </li>
                    <li>
                      <p>the moment center gives the position in the image plane of a particular point on the palm, which we translate into the 3D reference frame linked to the palm to obtain the necessary translation vector to apply to the hand.</p>
                    </li>
                  </ul><p>To extract the hand local parameters, that are restricted to the abduction angles, we analyze the relation between upper and lower bound angles in the hand contour (see Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Segen J, Kumar S (1999) Shadow gestures: 3d hand pose estimation using a single camera. In: IEEE conference on computer vision and pattern recognition, pp 479–485" href="/article/10.1007/s10055-005-0154-4#ref-CR1" id="ref-link-section-d54578e693">1</a>] for another application of those angles). Finally, hand model geometry is matched to the real hand by applying the same kind of optimization algorithm as in the tracking process. Of course, optimized parameters are related here to the morphology of the hand (length and radius of the palm and fingers, for example). This stage is automatic, and does not require any manual setting.</p><h3 class="c-article__sub-heading" id="Sec14">Current development state</h3><p>We are currently working on modifying the initialization stage so that several cameras are taken into account, to optimize the model pose initialization. As every other point is functional in our present application, we have run many different tests. We first observed how our algorithm behaves with a single video camera and various resolution and refresh rates. As the step of adding several video cameras is limited by the need of a better initialization stage, we investigated this benefit using synthetic sequences. They present the advantage that we know exactly the camera position and the initial model pose, thus concentrating our observation on the consequences of adding more cameras. In order to experiment in different ways, we validated our system using synthetic sequences of a human body model on which the same model is matched using our articulated tracking algorithm.</p><h3 class="c-article__sub-heading" id="Sec15">Performances</h3><p>Minimization results directly depend on the number of optimization loop iterations, which include the 3D model projection, the error computation and the optimization process. Some performacnce results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0154-4#Tab1">1</a>. Mean values are computed over five different input sequences, each of which has been analyzed for about 10 s.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Mean number of optimization loop iterations depending on image resolution and video frame rate</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0154-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Notice that the number of iterations not only depends on the number of input pixels, but also on the time which is spent in reading the framebuffer to bring its image into central memory. Indeed, because this way of accessing the framebuffer is quite rare in computer graphics, even today hardware is not optimized for this operation.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Results</h2><div class="c-article-section__content" id="Sec16-content"><p>Before discussing the general algorithm’s strengths and weaknesses, we will describe various typical examples. In all of the following sequences, the entire dof set is evaluated. Video input has been analyzed at 10 Hz, thus allowing the tracking algorithm to make enough iterations. Input and rendered frames feature a resolution of 160x120 pixels. Complete video sequences of those results are available on the internet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="de la Rivière JB (2004) &#xA;                    http://www.labri.fr/perso/∼leproux/modelbased.htm&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0154-4#ref-CR20" id="ref-link-section-d54578e814">20</a>]. In the following illustrations, results are shown in pairs. The input image is drawn on the left, whereas the corresponding model pose is rendered on the right part of the picture. The different colors of the projected model correspond to different body or hand parts. In the case of the hand model, different color intensities show the various phalanxes of the same finger.</p><h3 class="c-article__sub-heading" id="Sec17">Examples</h3><p>Some frames of a first sequence are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig6">6</a>. It shows the rotation of the fingers in the palm plane. The major difficulty here is related to a high finger movement. The results we obtained are visually good, since the model fingers accurately follow the real hand ones.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Fingers abduction, 1/10 s between each frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The next example, illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig7">7</a>, follows the previous rotations by the flexions of four fingers. Results show again a correct visual registration, while the movement speed is still high. However, in this configuration, it could happen that some flexion angle does not follow a finger unbending. Indeed, the finger occlusion due to the palm could lead to bad registration that is not corrected when the finger is stretched out. Reducing the possible finger abduction when the flexion is high could solve that, but proves not to be robust to slight rotations of the palm. Nevertheless, adding a camera on the side could oblige the flexion angle to decrease, leading the abduction angle to be corrected later.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb7.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb7.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Fingers flexions, 1/10 s between each frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig8">8</a> points out the behavior of our tracking algorithm in case of critical occlusion. After having correctly extracted the real hand rotation, all the fingers are projected onto a single small area of the image. That is why we observe a flexion of the wrong finger, while the algorithm tends to recover every pixel of the input silhouette.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Occlusion due to global rotation, with 1 s between each of those example frames. Flexion is applied to a wrong finger, and some abduction angles are badly evaluated</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>To validate our assertions about the benefit of additional cameras on the occlusion issue, we have run various tests. They consisted in recording a synthetic sequence from two points of view, and trying to run the tracking algorithm using one, then two cameras. While the issues of </p><ul class="u-list-style-dash">
                    <li>
                      <p>correctly recovering the position of one camera relative to the other</p>
                    </li>
                    <li>
                      <p>and properly extracting the initial position and orientation of the hand</p>
                    </li>
                  </ul><p> are avoided, the gain of using several cameras is obvious. Two different such results obtained from the same record are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p><i>Upper left image</i>: flexions extracted from one camera, fingers are a little too streched. Right image: flexions retrieved from two cameras, better angles extraction. <i>Lower left image</i>: comparison of the three poses with, from left to right, the original one, the one retrieved with one camera then with two cameras</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig10">10</a> illustrates the same improvement, but in the case of human body tracking. In the two tests (one and three cameras), silhouettes are correctly matching. Nonetheless, the arm pose using a unique camera is not correct, while adding two cameras solve this issue.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p><i>Upper left image</i>: arm tracking with one camera, the result has been brought closer to the image plane. <i>Right image</i>: extraction using three cameras. <i>Lower left image</i>: comparison of the original arm pose, the pose extracted with one camera then with three cameras</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Finally, it is interesting to illustrate the effect of a temporary occlusion on the whole tracking process. In the case of the hand, a fingertip occlusion does not cause any issue, as is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig11">11</a>. Adding a camera will not help, as the only ambiguity lies in which finger is on top of the other. Even a side camera may not be sufficient to solve this problem.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Fingertip occlusion, time between frames is, respectively, 0.3, 0.6 and 0.3 s</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>More cameras also help to increase the accuracy of the tracking process, as observable in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0154-4#Fig12">12</a>. In this example, arms often go in front of the others between those two frames. Results seem visually satisfactory, and the model is correctly configured even in cameras where arms are occluded.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0154-4/MediaObjects/s10055-005-0154-4fhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Multiple occlusions, 5 s between the two frames</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0154-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec18">Survey</h3><p>The results we obtained, both on real and synthetic video sequences, seem to us very promising. Movement speed does not need to be limited too much, and real-time analysis is reached. Our algorithm manages occlusion seamlessly as one camera could be sufficient and a second camera will certainly help to gain some accuracy. When adding several cameras, our algorithm behaves as expected at the cost of more computing power. Finally, many of the typical hand gestures needed by natural interaction techniques are correctly retrieved, which is a great step toward building efficient interfaces. Furthermore, our numerous experiments lead us to extract some observations that must be kept in mind when building a similar system.</p><p>First, because of the hierarchical tree, the correct extraction of one dof value entirely depends on the correct evaluation of its parent dof. For example, a misevaluation of the palm pose may result in an incorrect pose of all other joint angles. This is a consequence of our optimization method choice, and can be avoided by using a different one, such as those based on the steepest descent. Second, we observed that silhouette information is not detailed enough to benefit from an increased resolution. Stepping from a 80×60 to a 160×120 resolution allow for better accuracy, but going for larger images will not help much. This fact diminishes the interest of a multiresolution analysis, which we have experimented. Besides, increasing the resolution leads to consider more pixels and definitely reduces performance.</p><p>Except as consequences of occlusion, most of the failed attempts of our tracking algorithm are due to too fast movements. In that case, the model goes into a configuration which corresponds to a local minimum but is incorrect. Thus, the following parameters evaluation may be perturbed. Those false registrations, that cannot be totally avoided, highlight the need of a special stage that will allow to pause the tracking until the computing necessary to place the model in the correct pose is performed. A simple solution may be to send the system back to the initialization stage when a misregistration is detected. The issue of determining if the current result is correct or not is not so problematic, as it can be resolved by carefully choosing thresholds the error measure must not exceed.</p><p>Finally, we must keep in mind that all those conclusions are based on binarized images. Their quality depends on the thresholding algorithm. But in complex scenes, it may be difficult to guarantee a very accurate silhouette extraction. Even if our observations show that the use of the silhouette feature leads to more robustness relative to slight perturbations of the hand borders, larger variations are more a trouble. They should be reduced by using appropriate light settings.</p><h3 class="c-article__sub-heading" id="Sec19">Future work</h3><p>As we already mentioned, the initialization stage must be improved. Beyond the management of several cameras as well as its speedup, it may be interesting to see how it can help to retrieve the correct configuration after a failed analysis. Without returning to the computation of contour angles, it may be possible to rely on some key renders of the 3D model in order to find the nearest one as done in Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Athitsos V, Sclaroff S (2003) Database indexing methods for 3d hand pose estimation. In: International Gesture Workshop, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-005-0154-4#ref-CR21" id="ref-link-section-d54578e1032">21</a>].</p><p>Thumb representation must be modified in our hand model. A solution more complex than the three local angles with different rotation axes should be used. Furthermore, we believe the best improvement to our system may come from the insertion of an image-based analysis prior to the model-based algorithm. Simple ideas can be thought of, such as using the image difference between previous and current frame to evaluate which parameter has moved.</p><p>Finally, we must experiment in real-world VR conditions. Even if we took into account many constraints, the large empty surface the video cameras will look at will be an issue. We will be obliged to reduce the interaction surface to a smaller area, and other algorithms will have to look for the articulated objects to track. Besides, using computing hardware specific to the VR domain such as PC clusters could prove to allow faster analysis since our tree structure enables it to run on different hardware at a time.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusion</h2><div class="c-article-section__content" id="Sec20-content"><p>We proposed an original real-time model-based tracking system. Given the quality of the results we obtained, we believe that such algorithms can replace, in many situations, standard glove devices as well as specific image-based solutions. Furthermore, the low-level results extracted from live video can help to build more complex natural interfaces. While there is still room for improvement, the results we observe encourage us to go on in this way.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Segen J, Kumar S (1999) Shadow gestures: 3d hand pose estimation using a single camera. In: IEEE conference on" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Segen J, Kumar S (1999) Shadow gestures: 3d hand pose estimation using a single camera. In: IEEE conference on computer vision and pattern recognition, pp 479–485</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Freeman W, Tanaka K, Ohta J, Kyuma K (1996) Computer vision for computer games. FG’96 (1996), pp 100–105" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Freeman W, Tanaka K, Ohta J, Kyuma K (1996) Computer vision for computer games. FG’96 (1996), pp 100–105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Davis J (1998) Recognizing movement using motion histograms. Technical Report 487, MIT Media Lab" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Davis J (1998) Recognizing movement using motion histograms. Technical Report 487, MIT Media Lab</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kwatra V, Bobick A, Johnson A (2001) Temporal integration of multiple silhouette–based body–part hypotheses. I" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Kwatra V, Bobick A, Johnson A (2001) Temporal integration of multiple silhouette–based body–part hypotheses. In: Computer Vision and Pattern Recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rehg J, Kanade T (1994) Digiteyes: vision–based hand tracking for human computer interaction. In: Workshop on " /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Rehg J, Kanade T (1994) Digiteyes: vision–based hand tracking for human computer interaction. In: Workshop on motion of non–rigid and articulated objects. pp 16–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu Y, Huang T (1999) Capturing articulated human hand motion: A divide–and–conquer approach. In: International" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Wu Y, Huang T (1999) Capturing articulated human hand motion: A divide–and–conquer approach. In: International Conference on Computer Vision. pp 606–611</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heap T, Hogg D (1996) Towards 3d hand tracking using a deformable model. In: Conference on automatic face and " /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Heap T, Hogg D (1996) Towards 3d hand tracking using a deformable model. In: Conference on automatic face and gesture recognition. pp 140–145</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shimada N, Shirai Y, Kuno Y, Miura J (1998) Hand gesture estimation and model refinment using monocular camera" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Shimada N, Shirai Y, Kuno Y, Miura J (1998) Hand gesture estimation and model refinment using monocular camera – ambiguity limitation by inequality constraints. In: 3rd conference on Face and Gesture Recognition. pp 268–273</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stenger B, Mendonça P, Cipolla R (2001) Model–based hand tracking using an unscented kalman filter. In: Britis" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Stenger B, Mendonça P, Cipolla R (2001) Model–based hand tracking using an unscented kalman filter. In: British machine vision conference. vol 1, pp 63–72</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Deawele G, Devernay F, Horaud R (2003) Hand motion from 3d point trajectories and a smooth surface model. In: " /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Deawele G, Devernay F, Horaud R (2003) Hand motion from 3d point trajectories and a smooth surface model. In: 8th European conference on computer vision</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leubner C, Brockman C, Müller H (2001) Computer–vision–based human-computer interaction with a back projection" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Leubner C, Brockman C, Müller H (2001) Computer–vision–based human-computer interaction with a back projection wall using arm gestures. Euromicro conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moeslund T, Störring M, Granum E (2000) Vision–based user interface for interacting with a virtual environment" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Moeslund T, Störring M, Granum E (2000) Vision–based user interface for interacting with a virtual environment. In: conference DANKOMB</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nölker C, Ritter H (1998) Illumination independant recognition of deictic arm postures. In: 24th annual confer" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Nölker C, Ritter H (1998) Illumination independant recognition of deictic arm postures. In: 24th annual conference of the IEEE industrial electronic society. pp 2006–2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato Y, Saito M, Koike H (2001) Real–time input of 3d hand pose and gestures of a user’s hand and its applicat" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Sato Y, Saito M, Koike H (2001) Real–time input of 3d hand pose and gestures of a user’s hand and its applications for hci. In: Virtual reality conference. p 79</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: Tow" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: Toward spontaneous and natural interaction in semi–immersive virtual environments. In: IEEE conference on virtual reality. pp 13–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Davis J, Bobick A (1998) Sideshow: a silhouette–based interactive dual–screen environment. Technical Report 45" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Davis J, Bobick A (1998) Sideshow: a silhouette–based interactive dual–screen environment. Technical Report 457, MIT Media Lab Perceptual Computing Group</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouhaddi H, Horain P, Mikolajczyk K (1998) Modélisation et suivi de la main. In: Compression et représentation " /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Ouhaddi H, Horain P, Mikolajczyk K (1998) Modélisation et suivi de la main. In: Compression et représentation des signaux audiovisuels. pp 109–114</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. McDonald, J. Toro, K. Alkoby, A. Berthiaume, R. Carter, P. Chomwong, J. Christopher, M. Davidson, J. Furst, B. Konie, G. Lancaster, L. Roychoudhuri, E. Sedgwick, N. Tomuro, R. Wolfe, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="McDonald J, Toro J, Alkoby K, Berthiaume A, Carter R, Chomwong P, Christopher J, Davidson M, Furst J, Konie B," /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">McDonald J, Toro J, Alkoby K, Berthiaume A, Carter R, Chomwong P, Christopher J, Davidson M, Furst J, Konie B, Lancaster G, Roychoudhuri L, Sedgwick E, Tomuro N, Wolfe R (2001) An improved articulated model of the human hand. The Visual Computer 17:158–166</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20improved%20articulated%20model%20of%20the%20human%20hand&amp;journal=The%20Visual%20Computer&amp;volume=17&amp;pages=158-166&amp;publication_year=2001&amp;author=McDonald%2CJ&amp;author=Toro%2CJ&amp;author=Alkoby%2CK&amp;author=Berthiaume%2CA&amp;author=Carter%2CR&amp;author=Chomwong%2CP&amp;author=Christopher%2CJ&amp;author=Davidson%2CM&amp;author=Furst%2CJ&amp;author=Konie%2CB&amp;author=Lancaster%2CG&amp;author=Roychoudhuri%2CL&amp;author=Sedgwick%2CE&amp;author=Tomuro%2CN&amp;author=Wolfe%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Delamarre Q, Faugeras O (1998) Finding pose of hand in video images: a stereo–based approach. In: FG’98" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Delamarre Q, Faugeras O (1998) Finding pose of hand in video images: a stereo–based approach. In: FG’98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="de la Rivière JB (2004) http://www.labri.fr/perso/∼leproux/modelbased.htm" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">de la Rivière JB (2004) <a href="http://www.labri.fr/perso/$\sim$leproux/modelbased.htm">http://www.labri.fr/perso/∼leproux/modelbased.htm</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Athitsos V, Sclaroff S (2003) Database indexing methods for 3d hand pose estimation. In: International Gesture" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Athitsos V, Sclaroff S (2003) Database indexing methods for 3d hand pose estimation. In: International Gesture Workshop, Springer, Berlin Heidelberg New York</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0154-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LaBRI, INRIA Futurs, Universite’ Bordeaux 1, 351 cours de la libé ratia, 33405, Talence Cedex, France</p><p class="c-article-author-affiliation__authors-list">J. -B. de la. Rivière &amp; P. Guitton</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-J___B__de_la_-Rivi_re"><span class="c-article-authors-search__title u-h3 js-search-name">J. -B. de la. Rivière</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J. -B. de la.+Rivi%C3%A8re&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J. -B. de la.+Rivi%C3%A8re" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J. -B. de la.+Rivi%C3%A8re%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-P_-Guitton"><span class="c-article-authors-search__title u-h3 js-search-name">P. Guitton</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;P.+Guitton&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=P.+Guitton" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22P.+Guitton%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0154-4/email/correspondent/c1/new">J. -B. de la. Rivière</a> or <a id="corresp-c2" rel="nofollow" href="/article/10.1007/s10055-005-0154-4/email/correspondent/c2/new">P. Guitton</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Model%E2%80%93based%20video%20tracking%20for%20gestural%20interaction&amp;author=J.%20-B.%20de%20la.%20Rivi%C3%A8re%20et%20al&amp;contentID=10.1007%2Fs10055-005-0154-4&amp;publication=1359-4338&amp;publicationDate=2005-07-19&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Rivière, J.-.d.l., Guitton, P. Model–based video tracking for gestural interaction.
                    <i>Virtual Reality</i> <b>8, </b>213–221 (2005). https://doi.org/10.1007/s10055-005-0154-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0154-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-19">19 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-09">September 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0154-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0154-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Full hand pose estimation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Real-time video analysis</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Articulated tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">VR interaction techniques</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0154-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=154;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

