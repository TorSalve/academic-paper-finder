<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Usability issues in the design of an intuitive interface for planning "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants). The general aim was to integrate..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/7/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment"/>

    <meta name="dc.source" content="Virtual Reality 2004 7:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-05-25"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants). The general aim was to integrate environmental scanning technologies with human modelling and radiological dose estimation tools, and to deliver an intuitive and cost-effective system for use by operators involved with interventions in radiologically controlled areas. The usability of the resulting products was one of the main success criteria. This paper describes the general approach and design mechanisms used in the HeSPI (HeSPI stands for the Spanish for Herramienta para la Simulaci&#243;n y Planificaci&#243;n de Intervenciones, or tool for the simulation and planning of interventions) tool that has been developed by one of the teams. The tool provides the designer of an intervention with a humanoid 3D model, or mannequin, that can be loaded into the desired environment and will be used by the designer as if he was manipulating a puppet, making it move around the environment and perform different kinds of actions, adopting varied postures, interacting with the objects in the environment and manipulating tools and equipment. A combination of a graphical user interface (GUI) and a voice recognition system, together with the selected design mechanisms, has proven to offer good enough interaction possibilities for this kind of desktop virtual environment."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-05-25"/>

    <meta name="prism.volume" content="7"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="212"/>

    <meta name="prism.endingPage" content="221"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0129-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0129-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0129-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0129-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment"/>

    <meta name="citation_volume" content="7"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2004/05/25"/>

    <meta name="citation_firstpage" content="212"/>

    <meta name="citation_lastpage" content="221"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0129-x"/>

    <meta name="DOI" content="10.1007/s10055-004-0129-x"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0129-x"/>

    <meta name="description" content="This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power "/>

    <meta name="dc.creator" content="Ang&#233;lica de Antonio"/>

    <meta name="dc.creator" content="Ricardo Imbert"/>

    <meta name="dc.creator" content="Jaime Ram&#237;rez"/>

    <meta name="dc.creator" content="Xavier Ferr&#233;"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="de Antonio A, Ferr&#233; X, Ram&#237;rez J (2003) Combining virtual reality with an easy to use and learn interface in a tool for planning and simulating interventions in radiologically controlled areas. In: Proceedings of the international conference on human computer interaction (HCI 2003), Crete, Greece, June 2003"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleop Virt; citation_author=null Bowman; citation_volume=10; citation_publication_date=2001; citation_pages=75; citation_doi=10.1162/105474601750182333; citation_id=CR2"/>

    <meta name="citation_reference" content="Granieri JP, Badler NI (1994) Simulating humans in VR. In: Jones H, Earnshaw R, Vince J (eds) Proceedings of the international conference on applications of virtual reality (The British Computer Society)"/>

    <meta name="citation_reference" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction in collaborative virtual environments. ACM T Comput Hum Int (TOCHI) (Special issue on human-computer interaction and collaborative virtual environments) 7(4):477&#8211;509"/>

    <meta name="citation_reference" content="Kamm C, Helander M (1997) Design issues for interfaces using voice Input. In: Helander M et al (eds) Handbook of human-computer interaction, 2nd edn, Elsevier Science, Amsterdam, The Netherlands"/>

    <meta name="citation_reference" content="Lee D, Salve R, de Antonio A, Herrero P, P&#233;rez JM, Vermeersch F, Dalton G (2001) Virtual reality for inspection, maintenance, operation and repair of nuclear power plant. In: Proceedings of the EU research in reactor safety mid-term symposium on shared-cost and concerted actions FISA 2001, Luxembourg, November 2001"/>

    <meta name="citation_reference" content="Mine MR (1995) Virtual environment interaction techniques. Technical report TR95-018. Department of Computer Science, University of North Carolina"/>

    <meta name="citation_reference" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th ACM symposium on user interface software and technology (UIST&#8217;96), Seattle, Washington, November 1996"/>

    <meta name="citation_reference" content="Rickel J, Johnson WL (1999) Virtual humans for team training in virtual reality. In: Proceedings of the 9th international conference on AI in education (AI-ED&#8217;99), LeMans, France, July 1999. IOS Press, pp 578&#8211;585"/>

    <meta name="citation_reference" content="citation_journal_title=Designing the user; citation_author=null Schneiderman; citation_volume=interface; citation_publication_date=1998; citation_pages=strategies; citation_id=CR10"/>

    <meta name="citation_reference" content="Vrimor Consortium (2003) Technological and user perspective review report. VRIMOR Project Deliverable D5"/>

    <meta name="citation_author" content="Ang&#233;lica de Antonio"/>

    <meta name="citation_author_email" content="angelica@fi.upm.es"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Campus de Montegancedo s/n, Madrid, Spain"/>

    <meta name="citation_author" content="Ricardo Imbert"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Campus de Montegancedo s/n, Madrid, Spain"/>

    <meta name="citation_author" content="Jaime Ram&#237;rez"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Campus de Montegancedo s/n, Madrid, Spain"/>

    <meta name="citation_author" content="Xavier Ferr&#233;"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Campus de Montegancedo s/n, Madrid, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0129-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0129-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment"/>
        <meta property="og:description" content="This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants). The general aim was to integrate environmental scanning technologies with human modelling and radiological dose estimation tools, and to deliver an intuitive and cost-effective system for use by operators involved with interventions in radiologically controlled areas. The usability of the resulting products was one of the main success criteria. This paper describes the general approach and design mechanisms used in the HeSPI (HeSPI stands for the Spanish for Herramienta para la Simulación y Planificación de Intervenciones, or tool for the simulation and planning of interventions) tool that has been developed by one of the teams. The tool provides the designer of an intervention with a humanoid 3D model, or mannequin, that can be loaded into the desired environment and will be used by the designer as if he was manipulating a puppet, making it move around the environment and perform different kinds of actions, adopting varied postures, interacting with the objects in the environment and manipulating tools and equipment. A combination of a graphical user interface (GUI) and a voice recognition system, together with the selected design mechanisms, has proven to offer good enough interaction possibilities for this kind of desktop virtual environment."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0129-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Planning and design of interventions, Object manipulation, Voice recognition, Generic actions, Usability of desktop virtual environments","kwrd":["Planning_and_design_of_interventions","Object_manipulation","Voice_recognition","Generic_actions","Usability_of_desktop_virtual_environments"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0129-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0129-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=129;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0129-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0129-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0129-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-05-25" itemprop="datePublished">25 May 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ang_lica-de_Antonio" data-author-popup="auth-Ang_lica-de_Antonio" data-corresp-id="c1">Angélica de Antonio<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid, Campus de Montegancedo s/n" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo s/n, 28660 Boadilla del Monte, Madrid, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ricardo-Imbert" data-author-popup="auth-Ricardo-Imbert">Ricardo Imbert</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid, Campus de Montegancedo s/n" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo s/n, 28660 Boadilla del Monte, Madrid, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jaime-Ram_rez" data-author-popup="auth-Jaime-Ram_rez">Jaime Ramírez</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid, Campus de Montegancedo s/n" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo s/n, 28660 Boadilla del Monte, Madrid, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xavier-Ferr_" data-author-popup="auth-Xavier-Ferr_">Xavier Ferré</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid, Campus de Montegancedo s/n" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo s/n, 28660 Boadilla del Monte, Madrid, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 7</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">212</span>–<span itemprop="pageEnd">221</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">136 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0129-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants). The general aim was to integrate environmental scanning technologies with human modelling and radiological dose estimation tools, and to deliver an intuitive and cost-effective system for use by operators involved with interventions in radiologically controlled areas. The usability of the resulting products was one of the main success criteria. This paper describes the general approach and design mechanisms used in the HeSPI (HeSPI stands for the Spanish for Herramienta para la Simulación y Planificación de Intervenciones, or tool for the simulation and planning of interventions) tool that has been developed by one of the teams. The tool provides the designer of an intervention with a humanoid 3D model, or mannequin, that can be loaded into the desired environment and will be used by the designer as if he was manipulating a puppet, making it move around the environment and perform different kinds of actions, adopting varied postures, interacting with the objects in the environment and manipulating tools and equipment. A combination of a graphical user interface (GUI) and a voice recognition system, together with the selected design mechanisms, has proven to offer good enough interaction possibilities for this kind of desktop virtual environment.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Introduction</h2><div class="c-article-section__content" id="Sec2-content"><p>This paper describes some of the results obtained in the VRIMOR project (virtual reality for inspection, maintenance, operation and repair of nuclear power plants), funded by the European Commission under the 5th Framework Programme, which ran from February 2001 to February 2003. The consortium was led by NNC Ltd. (UK) and included Tecnatom (Spain), Z+F (UK), CIEMAT (Spain), SCK-CEN (Belgium) and Universidad Politécnica de Madrid—UPM (Spain). A Spanish nuclear power plant, Almaraz, played the role of the final user. The general aim of the project was to integrate environmental scanning technologies (a laser-based geometrical scanner and a radiological scanner) with 3D human modelling and radiological dose estimation tools, and to deliver an intuitive and cost-effective system for use by operators involved with interventions in radiologically controlled areas [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Lee D, Salve R, de Antonio A, Herrero P, Pérez JM, Vermeersch F, Dalton G (2001) Virtual reality for inspection, maintenance, operation and repair of nuclear power plant. In: Proceedings of the EU research in reactor safety mid-term symposium on shared-cost and concerted actions FISA 2001, Luxembourg, November 2001" href="/article/10.1007/s10055-004-0129-x#ref-CR6" id="ref-link-section-d33200e302">6</a>].</p><p>The first step in using the VRIMOR methodology would be the generation of integrated geometrical and radiological 3D models of the radiologically controlled areas in a nuclear power plant (NPP). The environmental model would be generated primarily from sensory data obtained from a laser scanner, provided by Z+F, and a gamma scanner, provided by CIEMAT. Both sensors operate in a similar scan mode by physically panning and tilting around to build up a complete scan picture of the surroundings. This data would be supplemented by engineering data from drawings from the plant. Materials properties, such as materials density and pipe contents, are important when calculating dose uptake. An integrated geometrical–radiological 3D model of the environment is produced by a tool called LFM (light form modeller), developed by Z+F, which combines the output of the laser and gamma scanner, in the form of a VRML (virtual reality modelling language) file.</p><p>Once the 3D model has been produced, if a maintenance operation or any other kind of intervention has to be carried out, a team of experts (in maintenance and in radiological protection) would cooperate in the planning and design of this intervention in a way that minimises the amount of radiation received by the operators. This process is currently done without the help of tools, being just based on 2D maps of the areas, some radiation measures and the expert knowledge of the people involved.</p><p>The planning and design of an intervention implies determining the number of operators to employ, the tasks to be performed by each operator, the paths to be followed within the controlled area and the duration and sequence of these tasks. Minimisation of radiation take-up is achieved by trying to minimise the time invested and to avoid high radiation dose points in the space.</p><p>This plan is then communicated to the operators that will perform the operation. Some of them may be familiar with the area in which the work will be carried out, but others will have to imagine the future intervention from just the verbal description of the task and the 2D maps of the area.</p><p>The use of 3D virtual environments (VEs) in this context was very promising in several ways. Firstly, to facilitate the planning and design process; secondly, to communicate the plan to the operators; and thirdly, to train the operators and to familiarise them with the work area. The planning and design process was the main focus of the VRIMOR project. To date, some VEs have been developed for training [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Rickel J, Johnson WL (1999) Virtual humans for team training in virtual reality. In: Proceedings of the 9th international conference on AI in education (AI-ED’99), LeMans, France, July 1999. IOS Press, pp 578–585" href="/article/10.1007/s10055-004-0129-x#ref-CR9" id="ref-link-section-d33200e317">9</a>], but the kind of usability and interaction problems that arise in such applications are different to the ones related to the planning phase.</p><p>Many NPPs are not willing to invest in very expensive immersive virtual reality equipment. Moreover, our previous experiences in the development of VE systems for this kind of users (we had already developed some training applications for NPP operators) indicated that NPP personnel are not used to interacting with computer programs in general, and most of them have never used a 3D interface before. Therefore, if VE systems were to be used in NPPs, they had to be made cheap and easy to use. This led us to the selection of desktop instead of immersive VEs, and to the search for usability-oriented design mechanisms.</p><p>In this respect, the main research issue addressed by the UPM and NNC teams was the construction of an easy to use and learn planning and design tool, based on a desktop VE, to be used by the NPP personnel. In addition to the 3D models produced by LFM, we also needed to provide the designers of an intervention with some humanoid 3D models or mannequins that could be loaded into the desired environment to play the role of the operators. The resulting simulation would then be used as a way to communicate the plan to the operators. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0129-x#Fig1">1</a> shows a snapshot of a simulation in which two operators are changing a filter.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p> Snapshot of a simulation of two operators changing a filter</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Two commercial 3D human mannequins were selected: Jack [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Granieri JP, Badler NI (1994) Simulating humans in VR. In: Jones H, Earnshaw R, Vince J (eds) Proceedings of the international conference on applications of virtual reality (The British Computer Society)" href="/article/10.1007/s10055-004-0129-x#ref-CR3" id="ref-link-section-d33200e349">3</a>], distributed by EDS, and Envision/Ergo, by Delmia Corp. Both of them are very complex and powerful 3D mannequins, with many degrees of freedom, inverse kinematics and ergonomic constraints. However, it requires a long training period to be able to design simulations using Jack or Ergo. Our main challenge was to encapsulate Jack and Ergo into easy to use and learn interfaces, taking into account that the intended users are not willing to spend a long time learning to use the original tools.</p><p>Two approaches to the design of the interface were explored. A tool called HeSPI was built by UPM on top of Jack, and a tool called Ergodose was built by NNC on top of Ergo. HeSPI stands for the Spanish for Herramienta para la Simulación y Planificación de Intervenciones, or tool for the simulation and planning of interventions. The hypothesis behind HeSPI was that a combination of a GUI and a voice recognition system would offer good enough interaction possibilities for success in the objectives of the project. Ergodose’s approach was based on the use of a mouse with six degrees of freedom and shutter glasses.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0129-x#Fig2">2</a> shows a general overview of the various components in the VRIMOR methodology, the function that each component plays in the whole process and the partners responsible for them. The rest of this paper describes the usability issues that were considered in the design of HeSPI.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p> Overview of the VRIMOR methodology</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Usability-oriented design mechanisms in HeSPI</h2><div class="c-article-section__content" id="Sec3-content"><p>Our goal was not to provide NPP personnel with a complete human simulation tool. Usability was the main success criterion, and many design decisions have been made driven by it [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="de Antonio A, Ferré X, Ramírez J (2003) Combining virtual reality with an easy to use and learn interface in a tool for planning and simulating interventions in radiologically controlled areas. In: Proceedings of the international conference on human computer interaction (HCI 2003), Crete, Greece, June 2003" href="/article/10.1007/s10055-004-0129-x#ref-CR1" id="ref-link-section-d33200e387">1</a>]. Many of the functionalities provided by Jack are, in fact, hidden to the user of HeSPI. We have completely substituted Jack’s user interface with a new user interface that only gives the user access to the functionalities that are required for their needs.</p><p>Simplification of the user interaction is mainly achieved in HeSPI through five mechanisms:</p><ul class="u-list-style-dash">
                  <li>
                    <p>A pre-defined set of generic and configurable mannequin actions from which the user can select the desired ones</p>
                  </li>
                  <li>
                    <p>A voice recognition package</p>
                  </li>
                  <li>
                    <p>A mechanism to facilitate the interaction of the virtual operator with the objects in the scenario</p>
                  </li>
                  <li>
                    <p>Camera management</p>
                  </li>
                  <li>
                    <p>Plan visualisation</p>
                  </li>
                </ul>
<p>The next sections will explain these mechanisms.</p><h3 class="c-article__sub-heading" id="Sec4">Pre-defined generic and configurable actions</h3><p>We decided to reduce the large complexity inherent to the interaction with 3D humanoid models by “reducing the degrees of freedom.” The idea is to provide the user with a set of pre-defined animations and actions, such as<i> walk</i>,<i> climb a ladder</i>,<i> pick up an object</i>, etc. When the designer selects an action, he will only have to set values for some of the parameters. For instance, to make the mannequin walk, the user will have to select the destination point; to make the mannequin rotate, the user will have to specify the rotation angle, etc.</p><p>This decision was based on the hypothesis that most of the actions that maintenance operators have to perform in different scenarios are similar. Therefore, by incorporating into the tool an extensive set of these pre-defined actions, the design of a new intervention will consist mainly of the selection, configuration and sequencing of the appropriate pre-defined actions. The particular tasks that need to be modelled in an NPP have been studied through different interventions and we have abstracted from them the set of general actions that are currently pre-defined in HeSPI. The evaluation has shown that considerable reductions in modelling time can be achieved through this mechanism.</p><p>Building actions from scratch is also possible in HeSPI by using the facilities provided by Jack for the definition of new postures. The capability of Jack to automatically animate the mannequin from one posture to another one in a realistic way is then used for the construction of a new action as the transition between postures.</p><p>In this way, new pre-defined actions can be added to the HeSPI tool. Therefore, if the tool is applied on a regular basis for the planning of interventions and the newly designed actions are added to the tool, the percentage of actions to be designed from scratch will be decreasing over time and the user efficiency will be increasing.</p><p>Three kinds of pre-defined actions have been included:</p><ul class="u-list-style-dash">
                    <li>
                      <p>Movement: walk, rotate, kneel, stand up, squat, walk sideways, climb ladder, go down ladder, etc.</p>
                    </li>
                    <li>
                      <p>Clothing: put on/take off mask, put on/take off gloves, put on/take off overalls, etc.</p>
                    </li>
                    <li>
                      <p>Interaction with objects: pick up an object, leave an object, rotate an object, use a tool on an object, tie a rope on an object, put an object inside another object, give an object to another operator, etc.</p>
                    </li>
                  </ul>
<p>A future extension to this mechanism is the construction of higher-order tasks by grouping a sequence of subtasks. For instance, the sequence “walk to a table + pick up an object + walk to the exit” could be grouped into a higher-order task “take object out of the environment”. This would be done if the user feels that the higher-order action is frequent and can reutilised in other interventions. The efficiency in using the system would improve significantly as new higher-order actions are created and reused.</p><p>The problem with these higher-order tasks is their smooth concatenation with previous actions. For instance, if the subtask previous to “take object out of the environment” left the mannequin sat down, an intermediate subtask would be necessary to make the mannequin stand up. Undesired jumps could happen if this is not taken into account either by the user or by the system. Assigning this responsibility to the user would require them to remember the pre-conditions that must hold for a higher-order action to be applicable, so a more usable solution would be letting the system check the pre- and post-conditions of actions. If violations are detected, the system would advise the user about possible solutions. This is one of our current research lines.</p><h3 class="c-article__sub-heading" id="Sec5">Voice recognition</h3><p>In the design of HeSPI, we considered that issuing voice commands was a natural way of composing a new intervention [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kamm C, Helander M (1997) Design issues for interfaces using voice Input. In: Helander M et al (eds) Handbook of human-computer interaction, 2nd edn, Elsevier Science, Amsterdam, The Netherlands" href="/article/10.1007/s10055-004-0129-x#ref-CR5" id="ref-link-section-d33200e480">5</a>]. We thought of the scenario as if it was a kind of theatre stage and the mannequins were virtual actors. In this setting, the maintenance specialist would act as the director, indicating each actor what to do at every moment. Voice arises as very intuitive way of interacting with those virtual actors. To enforce this idea, we decided to restrict the use of voice to the composition of the operation. The rest of the user interface is a conventional GUI based on windows, buttons, etc. IBM’s Via Voice was selected as the voice recogniser.</p><p>We also decided that the voice recognition could be activated or de-activated at wish by the operators. We assumed that the user who activates voice recognition seeks simplicity, and is willing to sacrifice some of the flexibility provided by the tool. Therefore, when using voice recognition, some of the parameters available for the configuration of the pre-defined actions are set automatically to the default value. The result is that the use of the tool is simpler and the design of the operation is quicker, but the precision in the execution of some subtasks by the human mannequin can be lower. Our hypothesis was that, for most of the applications of this tool, the required precision would not be too high. The results of the evaluation seem to confirm that our intuition was correct. The Almaraz operators clearly preferred simplicity over precision.</p><p>When using voice recognition, three additional windows open up in the interface (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0129-x#Fig3">3</a>):</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p> Voice recognition help windows</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<ul class="u-list-style-dash">
                    <li>
                      <p>The Recognised Command window (“Palabra reconocida”) that shows the last command as it was recognised</p>
                    </li>
                    <li>
                      <p>The Available Commands window (“Comandos”) that shows the context-dependent list of commands that are available in a given situation</p>
                    </li>
                    <li>
                      <p>The Recognised Sequence window (“Secuencia reconocida”) that shows the history of recognised commands</p>
                    </li>
                  </ul>
<p>These windows are meant to help the user of the voice recognition system. Unfortunately, voice recognition systems are not perfect, even after previous training, and sometimes they fail in recognising utterances. It is very frustrating to be talking to a machine, seeing that it does not respond properly and not knowing why. The<i> Recognised Command</i> window helps the user to corroborate that the machine has understood their last utterance. Otherwise, they can repeat it, trying to speak clearer.</p><p>The<i> Available Commands</i> window, on the other hand, helps to avoid situations in which the user does not know which expressions they can use in a given situation. It is also very frustrating having to talk to a machine and not knowing what to say, or saying something just to find out that the machine is not programmed to understand it. Understanding general natural language is a very complex task for a machine, even in the present day. Recognising the words is simple, but understanding the meaning of the sentences is another question. Therefore, practical systems restrict the range of sentences or commands that are available for the user to communicate with the system. In the case of HeSPI, we decided to restrict the grammar of our language analyser to a set of commands linked to pre-defined actions, and some additional commands to interact with the system. Thus, the pre-defined action of using a tool on an object has to be commanded with the sentence “use<i> tool</i> on<i> object</i>” where the words<i> tool</i> and<i> object</i> will be substituted by the names of the specific tool and object, respectively (for instance “use hammer on bolt”). If the user says “hit with the hammer on the bolt”, the system will not understand it.</p><p>This solution imposes on the user the overload of having to remember the exact structure of the commands. The<i> Available Commands</i> window helps the user when they do not remember. However, the interaction is faster if the user remembers, so it would be desirable to have a small set of possible commands for the user to learn. In a way, we are faced with a paradox: if the system is not able to recognise a very wide subset of natural language, it is better, from a usability point of view, that the system is only able to recognise a quite small subset. In other words, if the user does not have the freedom to talk naturally but they have to learn an “artificial” language, it is better if that language is simple and small. In the HeSPI prototype, approximately 90 commands are recognised, but many of them are very similar to each other and easy to remember, such as “put on gloves”, “put on overall”, “put on mask”, “take off gloves”, etc. The vocabulary can be easily modified for a different intervention.</p><h3 class="c-article__sub-heading" id="Sec6">Interaction with objects</h3><p>Interaction with the objects in the environment is probably the most complex kind of action in HeSPI, and we wanted to provide the user with mechanisms that were, at the same time, generic to be applicable in any possible environment, but simple to use.</p><p>Several authors have already dealt with the problem of object manipulation in VEs. Bowman et al [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Bowman DA, Johnson DB, Hodges LF (2001) Testbed evaluation of virtual environment interaction techniques. Presence Teleop Virt 10(1):75–95" href="/article/10.1007/s10055-004-0129-x#ref-CR2" id="ref-link-section-d33200e568">2</a>] present a testbed evaluation of several interaction techniques for the tasks of travel and selection/manipulation of virtual objects. They also define an interesting taxonomy for the tasks of object selection and manipulation. Mine [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Mine MR (1995) Virtual environment interaction techniques. Technical report TR95-018. Department of Computer Science, University of North Carolina" href="/article/10.1007/s10055-004-0129-x#ref-CR7" id="ref-link-section-d33200e571">7</a>] is also a good overview of interaction techniques for immersive VEs. However, most of the proposed interaction techniques are meant to be used in immersive environments, while HeSPI is a desktop system.</p><p>An example of a desktop system that explores and evaluates the support for object-focused interaction is described in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction in collaborative virtual environments. ACM T Comput Hum Int (TOCHI) (Special issue on human-computer interaction and collaborative virtual environments) 7(4):477–509" href="/article/10.1007/s10055-004-0129-x#ref-CR4" id="ref-link-section-d33200e577">4</a>]. There, they provide the users with a desktop collaborative virtual environment (CVE). The user can reference an object by selecting it with the right mouse button, and their embodiment then raises its nearest arm to point at the target. Participants are also allowed to grasp and move objects in a restricted way. The embodiment automatically raises an arm to point at the object and a connecting line is drawn between the embodiment and the object being moved. Then, the movement is controlled with buttons. This is an adaptation of the Ray-casting technique [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Mine MR (1995) Virtual environment interaction techniques. Technical report TR95-018. Department of Computer Science, University of North Carolina" href="/article/10.1007/s10055-004-0129-x#ref-CR7" id="ref-link-section-d33200e580">7</a>]. Having encountered difficulties with this approach, several extensions are suggested: to distinguish grasping from pointing by raising two arms instead of one; to show the grasped object as a wireframe; and to show grasping by extending the embodiment’s arms to reach and touch the object, in a way analogous to the “go-go” arm-extension technique previously proposed by other authors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th ACM symposium on user interface software and technology (UIST’96), Seattle, Washington, November 1996" href="/article/10.1007/s10055-004-0129-x#ref-CR8" id="ref-link-section-d33200e583">8</a>]. The main problem with this solution, and others, is the strong over-simplification and distortion of mannequin–object interactions with respect to reality. This can be valid for a CVE, but not for a tool such as HeSPI. Therefore, we had to look for different solutions.</p><p>Regarding object manipulation, the first difficulty that we found in HeSPI was that, if the designer had to order the mannequin to interact with an object, we had to find a way to designate this object. A possible solution was using generic orders (something like “pick up object”) and then pointing to the object with the mouse, but this solution was slower and less intuitive than just naming the object (“pick up bucket”). However, objects do not have names in the VRML file that is received from the scanners, so this solution implied adding a pre-processing step in which the designers would add semantic information, such as a name, to “interesting” objects in the environment. We decided to use this solution because the work overload is not very high. The pre-processing has to be done only once for each environment, and then the names can be used in any number of interventions.</p><p>In addition to naming the objects, we had to face the problem of adapting a generic action to the features of a certain object. For instance, we wanted to have only one “pick up” action that could be used to pick up<i> any</i> object, be it a box, a hammer or a chair. Picking up an object in Jack implies attaching the object to one or both of the mannequin’s hands. The problem is how to decide where to place the mannequin’s hands.</p><p>Our solution is also based on some pre-processing on the objects that can be manipulated in a certain environment. When a given object is classified during the pre-processing step as manipulable, our system automatically attaches to this object a set of “handles” and a set of “position markers”. These are selected points in the space. An example can be visualised in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0129-x#Fig4">4</a> as small cubes around a box object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p> Handles and position markers for a box</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>For objects that can be grasped, six of these points will act as handles, and the mannequin will have the built-in capability to hold the object by any of these points. The remaining four points will act as position markers, and the mannequin will have the built-in capability to walk to the object and stop in front of it on the position marked by one of the cubes. This allows the user to just say “grasp the box”, without having to go into a low level and detailed design, but nevertheless obtaining quite a precise mannequin animation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0129-x#Fig5">5</a> illustrates how the mannequin will grasp a box upon receiving the command, “grasp the box”.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0129-x/MediaObjects/s10055-004-0129-xfhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p> Mannequin grasping a box</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0129-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Using the default options, the operator does not have to deal with cubes at all. But if the resulting animation is not totally satisfactory for them, they can make the cubes visible and select the cube to be used in the animation. It is also possible to change the position of any cube for better accuracy. In this way, the detailed and complex alternative is always available, but, most of the time, the more approximate and simpler alternative will be acceptable. The evaluation of HeSPI showed that the users did not feel the need to adjust the position of the cubes in any case, and that the default animations were good enough for their purposes of planning and design.</p><p>However, if HeSPI’s results were to be used for demonstration of the intervention or training of the operators, the prevalence of usability in design over precision in the result could change. The degree of expertise of the human operators would be a decisive factor. If the operator already knows how to unscrew a bolt, for instance, it would be useless to spend a long time designing a very precise animation for unscrewing. For skilled operators, the principal aim of the simulation would be learning the sequence of tasks to be performed, the spatial configuration of the work environment and the timing and dependencies among subtasks. This is the case in most of the interventions in an NPP. Only for unskilled operators or complex manipulations would the benefits obtained compensate for the effort required in the design of a detailed animation.</p><h3 class="c-article__sub-heading" id="Sec7">Camera management</h3><p>One of the most complex design aspects in HeSPI was how to facilitate the manipulation of the point of view over the virtual scene, that is, moving and positioning the camera along the “virtual representation.” Moving the camera required that the user navigated through the virtual space. In Jack, this was achieved with a combination of keyboard and mouse inputs. Pressing a key, the navigation mode was activated, and then the three buttons of the mouse controlled navigation along the space. We were afraid that this manoeuvre would be difficult to master for the users, and it certainly took a while to get the users dexterous in navigation, but was a shorter time than we expected. We found that it was easier if we transformed the problem of navigating into the problem of moving the environment with respect to the static position of the observer. With one button, the user could move the world up-down-left-right; with another button, the world could be brought closer-farther; and the third button controlled the rotation of the world.</p><p>Once the designer has positioned the world in a suitable way, a special “camera repositioning action” is introduced into the plan. It is as if the camera was a special actor that receives repositioning commands at certain points in time.</p><p>We considered, as an alternative, the design of an automatic camera management system that was able to follow the movements of operators. However, there can be several operators acting at the same time. In that case, a general view would be preferable, but, sometimes, it might be impossible to view all of the operators at the same time. On the other hand, if some operators are doing routine tasks while another is performing the critical subtask, the camera should concentrate on them. Moreover, it could happen that the important thing to show in a certain object was hidden if the position of the camera was selected automatically. For these reasons, we decided to leave the responsibility of camera management to the user, even if it added complexity to the interaction.</p><p>In order to minimise the interference that this responsibility might provoke with the goal of planning and designing an intervention, camera management is something that can be done and refined once the plan has been completed. With a trial and error procedure, the user can reposition the camera and view the results as many times as necessary.</p><h3 class="c-article__sub-heading" id="Sec8">Plan visualisation</h3><p>In order to facilitate the temporal planning and adjustment of an intervention being designed, a<i> Task Planning</i> window is offered. There is a section for each operator in which the tasks performed by that operator are shown as rectangles. Time flows from left to right so that the position of a task rectangle shows its start time, and the length of the rectangle shows its duration. The representation is similar to a Gannt chart. This similarity with traditional scheduling tools makes it easier for the user to get familiar with it. Using this window, the start time and duration of any task, as well as the camera repositioning instants, can be adjusted.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Evaluation of HeSPI</h2><div class="c-article-section__content" id="Sec9-content"><p>The evaluation of the HeSPI tool has adopted three forms:</p><ul class="u-list-style-dash">
                  <li>
                    <p>A technical evaluation of the correctness, reliability, efficiency and robustness of the tool was performed by the UPM’s staff prior to the user evaluation</p>
                  </li>
                  <li>
                    <p>A user evaluation at Almaraz, mainly centred on the usability factor and on the effectiveness of the planning and design process</p>
                  </li>
                  <li>
                    <p>An experiment with experienced human modellers, mainly centred on the efficiency gain in the use of the tool</p>
                  </li>
                </ul>
<p>We ran usability tests with four participants (two maintenance specialists, one radioprotection specialist and one computer technician) and we passed questionnaires [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schneiderman B (1998) Designing the user interface: strategies for effective human-computer interaction. Addison-Wesley, Reading, Massachusetts" href="/article/10.1007/s10055-004-0129-x#ref-CR10" id="ref-link-section-d33200e701">10</a>] to them and to other potential users that observed the session. During two days, members of the Almaraz staff were trained in HeSPI and Ergodose. The training on each application was broken up into five stages:</p><ul class="u-list-style-dash">
                  <li>
                    <p>A presentation describing the application (around 15 min)</p>
                  </li>
                  <li>
                    <p>A demonstration of the different functions (around 45 min)</p>
                  </li>
                  <li>
                    <p>A tutorial to be followed by the trainees to get them used to the application, with the trainer answering questions (around 1 h)</p>
                  </li>
                  <li>
                    <p>An exercise (planning and design of a simple intervention) to be performed by the trainees without any help from the trainer (around 30 min)</p>
                  </li>
                  <li>
                    <p>A questionnaire to be completed by the trainees</p>
                  </li>
                </ul>
<p>The intervention to be designed during the exercise involved two virtual operators and 18 subtasks (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0129-x#Tab1">1</a>). It is quite a complex operation to be designed if we consider that it has to be done in just half an hour and after a training of just two hours.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 
Intervention to be designed in the exercise</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0129-x/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>During the exercise, an observational study was conducted to complete the information gathered through the questionnaires. Two people, one from Tecnatom and one from UPM, observed the free exercises. Very often, the trainees had difficulties writing ideas and comments in a questionnaire but these ideas are naturally expressed during the exercised performance. These comments, as well as problems and incidences with the application, were manually written down, without asking direct questions to the trainees.</p><p>Most of the comments from the operators were quite positive, showing their interest on the methodology and recognising its usefulness and effectiveness. The NPP personnel stated that, if they had the set of technologies provided by VRIMOR, they would use them, without any doubt.</p><p>The most remarkable drawback noted by the operators was the perceived complexity of the applications. This, in a way, confirms the validity of our motivation to build simplified interfaces for the use of 3D human simulation tools by NPP operators. Although the complexity perceived by the operators is still high, they were, in fact, able to build quite complex simulations after only two hours of training.</p><p>The evaluation at Almaraz demonstrated that HeSPI’s main hypotheses were correct. The evaluators felt comfortable with the completeness and precision of the set of pre-defined actions provided by HeSPI. The answer to the question “the possibility to use voice commands makes the system easier to use”, was “quite agree”, and several answers to the open questions mentioned that the voice interface made the use of the tool simpler and quicker. Evaluators showed a clear preference for HeSPI’s solution over Ergodose’s. For a more detailed description of the user evaluation and its results see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Vrimor Consortium (2003) Technological and user perspective review report. VRIMOR Project Deliverable D5" href="/article/10.1007/s10055-004-0129-x#ref-CR11" id="ref-link-section-d33200e918">11</a>].</p><p>We are conscious that the usability evaluation should have been conducted in a more rigorous way, with a larger number of users and also a control group. However, Almaraz severely limited both the number of personnel and the length of time available for the evaluation of the project.</p><p>In order to compensate for the limitations in the user evaluation, we decided to conduct an additional and more controlled experiment with experienced human modellers. The goal was to determine if the use of HeSPI would be beneficial when compared to the use of conventional human modelling and simulation tools, such as Jack. The variable that we measured was the speed in the construction of a set of simulations for three different operations (similar to the one sketched in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0129-x#Tab1">1</a>). One group of designers used HeSPI and the second used Jack. Both groups were experienced in the use of Jack. The results obtained showed significant reductions in modelling time for each of the operations. We can conclude that, in addition to saving the long learning time required by Jack, HeSPI also allows for a faster work in day-to-day construction of new simulations.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Conclusion</h2><div class="c-article-section__content" id="Sec10-content"><p>A set of design mechanisms have been described to improve usability in desktop VEs for planning and designing interventions when the users are not familiar with 3D applications.</p><p>The use of a set of pre-defined and configurable actions prevents the user from having to manipulate the mannequin in detail, and is applicable when most of the actions that maintenance operators have to perform in different scenarios are similar. The generalisation of this mechanism to build higher-order actions is currently under research.</p><p>Interaction with objects is the most complex action to be designed because of its variability. A mechanism based on the automatic attachment of handles and position markers to manipulable objects has been devised, and the virtual human has been endowed with the capabilities to use these artefacts.</p><p>The manipulation of the point of view or camera is another delicate aspect that has to be analysed from a usability perspective. Treating the camera as an actor and considering the world movable and the observer static are strategies that have shown to be useful.</p><p>Voice recognition has proven to be an intuitive way to interact with virtual humans. Naming objects, the use of help windows and the use of restricted languages are advisable to make this mechanism more usable.</p><p>The evaluation of HeSPI with real users has allowed us to test the usability of our system. The use of the previously mentioned mechanisms has been valued as positive by the users.</p><p>The availability of a human modelling tool enables the introduction of the worker in the planning and design process, leading to a more realistic dose evaluation for the activities. It also permits the previous familiarisation of the worker with the site, avoiding unnecessary dose uptake due to the learning curve. Moreover, the fact that the activities can be planned in a group session enables a better collaborative work to find the best approach. The use of a VR representation facilitates the communication between the experts.</p><p>In addition to these benefits, the combination of interaction mechanisms described in this paper resulted in a significant reduction in learning time and an increase in work efficiency when compared to existing human modelling and simulation tools.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="de Antonio A, Ferré X, Ramírez J (2003) Combining virtual reality with an easy to use and learn interface in a" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">de Antonio A, Ferré X, Ramírez J (2003) Combining virtual reality with an easy to use and learn interface in a tool for planning and simulating interventions in radiologically controlled areas. In: Proceedings of the international conference on human computer interaction (HCI 2003), Crete, Greece, June 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Bowman, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Bowman DA, Johnson DB, Hodges LF (2001) Testbed evaluation of virtual environment interaction techniques. Pres" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Bowman DA, Johnson DB, Hodges LF (2001) Testbed evaluation of virtual environment interaction techniques. Presence Teleop Virt 10(1):75–95</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474601750182333" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Presence%20Teleop%20Virt&amp;volume=10&amp;publication_year=2001&amp;author=Bowman%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Granieri JP, Badler NI (1994) Simulating humans in VR. In: Jones H, Earnshaw R, Vince J (eds) Proceedings of t" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Granieri JP, Badler NI (1994) Simulating humans in VR. In: Jones H, Earnshaw R, Vince J (eds) Proceedings of the international conference on applications of virtual reality (The British Computer Society)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction in collaborative vir" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction in collaborative virtual environments. ACM T Comput Hum Int (TOCHI) (Special issue on human-computer interaction and collaborative virtual environments) 7(4):477–509</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kamm C, Helander M (1997) Design issues for interfaces using voice Input. In: Helander M et al (eds) Handbook " /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Kamm C, Helander M (1997) Design issues for interfaces using voice Input. In: Helander M et al (eds) Handbook of human-computer interaction, 2nd edn, Elsevier Science, Amsterdam, The Netherlands</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee D, Salve R, de Antonio A, Herrero P, Pérez JM, Vermeersch F, Dalton G (2001) Virtual reality for inspectio" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Lee D, Salve R, de Antonio A, Herrero P, Pérez JM, Vermeersch F, Dalton G (2001) Virtual reality for inspection, maintenance, operation and repair of nuclear power plant. In: Proceedings of the EU research in reactor safety mid-term symposium on shared-cost and concerted actions FISA 2001, Luxembourg, November 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine MR (1995) Virtual environment interaction techniques. Technical report TR95-018. Department of Computer S" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Mine MR (1995) Virtual environment interaction techniques. Technical report TR95-018. Department of Computer Science, University of North Carolina</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping " /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th ACM symposium on user interface software and technology (UIST’96), Seattle, Washington, November 1996</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rickel J, Johnson WL (1999) Virtual humans for team training in virtual reality. In: Proceedings of the 9th in" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Rickel J, Johnson WL (1999) Virtual humans for team training in virtual reality. In: Proceedings of the 9th international conference on AI in education (AI-ED’99), LeMans, France, July 1999. IOS Press, pp 578–585</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Schneiderman, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Schneiderman B (1998) Designing the user interface: strategies for effective human-computer interaction. Addis" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Schneiderman B (1998) Designing the user interface: strategies for effective human-computer interaction. Addison-Wesley, Reading, Massachusetts</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Designing%20the%20user&amp;volume=interface&amp;publication_year=1998&amp;author=Schneiderman%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vrimor Consortium (2003) Technological and user perspective review report. VRIMOR Project Deliverable D5" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Vrimor Consortium (2003) Technological and user perspective review report. VRIMOR Project Deliverable D5</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0129-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo s/n, 28660 Boadilla del Monte, Madrid, Spain</p><p class="c-article-author-affiliation__authors-list">Angélica de Antonio, Ricardo Imbert, Jaime Ramírez &amp; Xavier Ferré</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Ang_lica-de_Antonio"><span class="c-article-authors-search__title u-h3 js-search-name">Angélica de Antonio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ang%C3%A9lica+de Antonio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ang%C3%A9lica+de Antonio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ang%C3%A9lica+de Antonio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ricardo-Imbert"><span class="c-article-authors-search__title u-h3 js-search-name">Ricardo Imbert</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ricardo+Imbert&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ricardo+Imbert" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ricardo+Imbert%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jaime-Ram_rez"><span class="c-article-authors-search__title u-h3 js-search-name">Jaime Ramírez</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jaime+Ram%C3%ADrez&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jaime+Ram%C3%ADrez" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jaime+Ram%C3%ADrez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Xavier-Ferr_"><span class="c-article-authors-search__title u-h3 js-search-name">Xavier Ferré</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Xavier+Ferr%C3%A9&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xavier+Ferr%C3%A9" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xavier+Ferr%C3%A9%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0129-x/email/correspondent/c1/new">Angélica de Antonio</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Usability%20issues%20in%20the%20design%20of%20an%20intuitive%20interface%20for%20planning%20and%20simulating%20maintenance%20interventions%20using%20a%20virtual%20environment&amp;author=Ang%C3%A9lica%20de%20Antonio%20et%20al&amp;contentID=10.1007%2Fs10055-004-0129-x&amp;publication=1359-4338&amp;publicationDate=2004-05-25&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">de Antonio, A., Imbert, R., Ramírez, J. <i>et al.</i> Usability issues in the design of an intuitive interface for planning and simulating maintenance interventions using a virtual environment.
                    <i>Virtual Reality</i> <b>7, </b>212–221 (2004). https://doi.org/10.1007/s10055-004-0129-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0129-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-12">12 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-03-30">30 March 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-05-25">25 May 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0129-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0129-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Planning and design of interventions</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Object manipulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Voice recognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Generic actions</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Usability of desktop virtual environments</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0129-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=129;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

