<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="DSCVR: designing a commodity hybrid virtual reality system"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer technology has enabled a reduced cost for..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="DSCVR: designing a commodity hybrid virtual reality system"/>

    <meta name="dc.source" content="Virtual Reality 2014 19:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-11-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer technology has enabled a reduced cost for both 3D tracking and screens, enabling a new means for the creation of immersive display environments. However, this technology also presents many challenges, which need to be designed for and around. We compare the DSCVR System to other existing VR environments to analyze the trade-offs being made."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-11-13"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="57"/>

    <meta name="prism.endingPage" content="70"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0254-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0254-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0254-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0254-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="DSCVR: designing a commodity hybrid virtual reality system"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2015/03"/>

    <meta name="citation_online_date" content="2014/11/13"/>

    <meta name="citation_firstpage" content="57"/>

    <meta name="citation_lastpage" content="70"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0254-0"/>

    <meta name="DOI" content="10.1007/s10055-014-0254-0"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0254-0"/>

    <meta name="description" content="This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer t"/>

    <meta name="dc.creator" content="Kevin Ponto"/>

    <meta name="dc.creator" content="Joe Kohlmann"/>

    <meta name="dc.creator" content="Ross Tredinnick"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ainsworth RA, Sandin DJ, Schulze JP, Prudhomme A, DeFanti TA, Srinivasan M (2011) Acquisition of stereo panoramas for display in vr environments"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE MultiMedia; citation_title=The allosphere: immersive multimedia for scientific discovery and artistic exploration; citation_author=X Amatriain, J Kuchera-Morin, T Hollerer, ST Pope; citation_volume=16; citation_issue=2; citation_publication_date=2009; citation_pages=0064-75; citation_doi=10.1109/MMUL.2009.35; citation_id=CR16"/>

    <meta name="citation_reference" content="Arthur K (1996) Effects of field of view on task performance with head-mounted displays. In: Conference companion on human factors in computing systems. ACM, New York, pp 29&#8211;30"/>

    <meta name="citation_reference" content="Avery B, Thomas BH, Velikovsky J, Piekarski W (2005) Outdoor augmented reality gaming on five dollars a day. In: Proceedings of the 6th Australasian conference on User interface&#8212;volume 40, AUIC &#8217;05, pp 79&#8211;88. Australian Computer Society Inc, Darlinghurst, Australia"/>

    <meta name="citation_reference" content="Bacim F, Ragan E, Scerbo S, Polys NF, Setareh M, Jones BD (2013) The effects of display fidelity, visual complexity, and task scope on spatial understanding of 3d graphs. In: Proceedings of graphics interface 2013, GI &#8217;13, pp 25&#8211;32. Canadian Information Processing Society, Toronto, Ont., Canada"/>

    <meta name="citation_reference" content="Basu A, Saupe C, Refour E, Raij A, Johnsen K (2012) Immersive 3dui on one dollar a day. In: 2012 IEEE symposium on 3D user interfaces (3DUI), pp 97&#8211;100"/>

    <meta name="citation_reference" content="Bayer, BE (1976) Color imaging array. US Patent 3,971,065"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Virtual reality: how much immersion is enough?; citation_author=DA Bowman, RP McMahan; citation_volume=40; citation_issue=7; citation_publication_date=2007; citation_pages=36-43; citation_doi=10.1109/MC.2007.257; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Gait Posture; citation_title=Validity of the microsoft kinect for assessment of postural control; citation_author=RA Clark, YH Pua, K Fortin, C Ritchie, KE Webster, L Denehy, AL Bryant; citation_volume=36; citation_issue=3; citation_publication_date=2012; citation_pages=372-377; citation_doi=10.1016/j.gaitpost.2012.03.033; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The cave: audio visual experience automatic virtual environment; citation_author=C CruzNeira, DJ Sandin, TA DeFanti, RV Kenyon, JC Hart; citation_volume=35; citation_issue=6; citation_publication_date=1992; citation_pages=64-72; citation_doi=10.1145/129888.129892; citation_id=CR15"/>

    <meta name="citation_reference" content="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the cave. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, pp 135&#8211;142"/>

    <meta name="citation_reference" content="citation_journal_title=Cent Eur J Eng; citation_title=The future of the cave; citation_author=TA DeFanti, D Acevedo, RA Ainsworth, MD Brown, S Cutchin, G Dawe, KU Doerr, A Johnson, C Knox, R Kooima; citation_volume=1; citation_issue=1; citation_publication_date=2011; citation_pages=16-37; citation_doi=10.2478/s13531-010-0002-5; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Cent Eur J Eng; citation_title=The future of the cave; citation_author=T DeFanti, D Acevedo, R Ainsworth, M Brown, S Cutchin, G Dawe, KU Doerr, A Johnson, C Knox, R Kooima, F Kuester, J Leigh, L Long, P Otto, V Petrovic, K Ponto, A Prudhomme, R Rao, L Renambot, D Sandin, J Schulze, L Smarr, M Srinivasan, P Weber, G Wickham; citation_volume=1; citation_issue=1; citation_publication_date=2011; citation_pages=16-37; citation_doi=10.2478/s13531-010-0002-5; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Cglx: a scalable, high-performance visualization framework for networked display environments; citation_author=K Doerr, F Kuester; citation_volume=17; citation_issue=3; citation_publication_date=2011; citation_pages=320-332; citation_doi=10.1109/TVCG.2010.59; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Equalizer: a scalable parallel rendering framework; citation_author=S Eilemann, M Makhinya, R Pajarola; citation_volume=15; citation_issue=3; citation_publication_date=2009; citation_pages=436-452; citation_doi=10.1109/TVCG.2008.104; citation_id=CR34"/>

    <meta name="citation_reference" content="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903&#8211;864903&#8211;12. SPIE"/>

    <meta name="citation_reference" content="Heddle B. The New Generation Kinect for Windows Sensor is Coming Next Year - Kinect for Windows Product Blog&#8212;Site Home&#8212;MSDN Blogs. 
                    http://blogs.msdn.com/b/kinectforwindows/archive/2013/05/23/the-new-generation-kinect-for-windows-sensor-is-coming-next-year.aspx
                    
                  
                "/>

    <meta name="citation_reference" content="Higgins T (2010) Unity-3d game engine"/>

    <meta name="citation_reference" content="citation_journal_title=J Soc Inf Disp; citation_title=Analysis of angular dependence of 3-d technology using polarized eyeglasses; citation_author=H Hong, J Jang, D Lee, M Lim, H Shin; citation_volume=18; citation_issue=1; citation_publication_date=2010; citation_pages=8-12; citation_doi=10.1889/JSID18.1.8; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=J Mol Graph; citation_title=Vmd: visual molecular dynamics; citation_author=W Humphrey, A Dalke, K Schulten; citation_volume=14; citation_issue=1; citation_publication_date=1996; citation_pages=33-38; citation_doi=10.1016/0263-7855(96)00018-5; citation_id=CR40"/>

    <meta name="citation_reference" content="Johnson GP, Abram GD, Westing B, Navr&#8217;til P, Gaither K (2012) Displaycluster: an interactive visualization environment for tiled displays. In: 2012 IEEE international conference on cluster computing (CLUSTER), pp 239&#8211;247. IEEE"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Consum Electr; citation_title=3d Crosstalk compensation to enhance 3d image quality of plasma display panel; citation_author=T Kim, JM Ra, JH Lee, SH Moon, KY Choi; citation_volume=57; citation_issue=4; citation_publication_date=2011; citation_pages=1471-1477; citation_doi=10.1109/TCE.2011.6131113; citation_id=CR47"/>

    <meta name="citation_reference" content="Knox C, Brown M, Doerr K, Jenks S, Zender C, Kuester F (2005) Simultaneous visualization of the ipcc ar4 model ensemble on an extremely high resolution display wall (hiperwall). In: AGU fall meeting abstracts, 1:1140"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Gr; citation_title=Effects of immersion on visual analysis of volume data; citation_author=B Laha, K Sensharma, J Schiffbauer, D Bowman; citation_volume=18; citation_issue=4; citation_publication_date=2012; citation_pages=597-606; citation_doi=10.1109/TVCG.2012.42; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Disabil Rehabil; citation_title=Designing informed game-based rehabilitation tasks leveraging advances in virtual reality; citation_author=B Lange, S Koenig, CY Chang, E McConnell, E Suma, M Bolas, A Rizzo; citation_volume=34; citation_issue=22; citation_publication_date=2012; citation_pages=1863-1870; citation_doi=10.3109/09638288.2012.670029; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Scalable resolution display walls; citation_author=J Leigh, A Johnson, L Renambot, T Peterka, B Jeong, DJ Sandin, J Talandis, R Jagodic, S Nam, H Hur; citation_volume=101; citation_issue=1; citation_publication_date=2013; citation_pages=115-129; citation_doi=10.1109/JPROC.2012.2191609; citation_id=CR35"/>

    <meta name="citation_reference" content="Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. In: Virtual reality short papers and posters (VRW), 2012 IEEE, pp 119&#8211;120"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=Gpu rendering for tiled multi-projector autostereoscopic display based on chromium; citation_author=J Luo, K Qin, Y Zhou, M Mao, R Li; citation_volume=26; citation_issue=6&#8211;8; citation_publication_date=2010; citation_pages=457-465; citation_doi=10.1007/s00371-010-0479-1; citation_id=CR36"/>

    <meta name="citation_reference" content="Margolis T, DeFanti TA, &#160;Dawe G, Prudhomme A, Schulze JP, Cutchin S (2011) Low cost heads-up virtual reality (HUVR) with optical tracking and haptic feedback. IS&amp;T/SPIE Electronic Imaging, p 786417"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Evaluating display fidelity and interaction fidelity in a virtual reality game; citation_author=RP McMahan, DA Bowman, DJ Zielinski, RB Brady; citation_volume=18; citation_issue=4; citation_publication_date=2012; citation_pages=626-633; citation_doi=10.1109/TVCG.2012.43; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Voreen: a rapid-prototyping environment for ray-casting-based volume visualizations; citation_author=J Meyer-Spradow, T Ropinski, J Mensmann, K Hinrichs; citation_volume=29; citation_issue=6; citation_publication_date=2009; citation_pages=6-13; citation_doi=10.1109/MCG.2009.130; citation_id=CR39"/>

    <meta name="citation_reference" content="MYO&#8212;Gesture control armband by Thalmic Labs. 
                    https://www.thalmic.com/en/myo/
                    
                  
                "/>

    <meta name="citation_reference" content="Pausch R (1991) Virtual reality on five dollars a day. Proceedings of the SIGCHI conference on human factors in computing systems, CHI &#8217;91. ACM, New York, NY, USA, pp 265&#8211;270"/>

    <meta name="citation_reference" content="Polys NF, Kim S, Bowman DA (2007) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. Comput Animat Virtual Worlds 18(1):19&#8211;38. doi:
                    10.1002/cav.159
                    
                  ."/>

    <meta name="citation_reference" content="Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network bandwidth method for multimedia playback on tiled display systems. In: 11th IEEE international symposium on multimedia, 2009. ISM&#8217;09, pp 201&#8211;206. IEEE"/>

    <meta name="citation_reference" content="citation_journal_title=Future Gener Comput Syst; citation_title=Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays; citation_author=K Ponto, K Doerr, F Kuester; citation_volume=26; citation_issue=5; citation_publication_date=2010; citation_pages=693-700; citation_doi=10.1016/j.future.2009.12.007; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Gr; citation_title=A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets; citation_author=null Prabhat, A Forsberg, M Katzourin, K Wharton, M Slater; citation_volume=14; citation_issue=3; citation_publication_date=2008; citation_pages=551-563; citation_doi=10.1109/TVCG.2007.70433; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Gr; citation_title=Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task; citation_author=E Ragan, R Kopper, P Schuchardt, D Bowman; citation_volume=19; citation_issue=5; citation_publication_date=2013; citation_pages=886-896; citation_doi=10.1109/TVCG.2012.163; citation_id=CR9"/>

    <meta name="citation_reference" content="Rash C, McLean W, Mozo B, Licina J, McEntire B (1999) Human factors and performance concerns for the design of helmet-mounted displays. In: RTO HFM symposium on current aeromedical issues in rotary wing operation"/>

    <meta name="citation_reference" content="citation_journal_title=Future Gener Comput Syst; citation_title=Enabling high resolution collaborative visualization in display rich virtual organizations; citation_author=L Renambot, B Jeong, H Hur, A Johnson, J Leigh; citation_volume=25; citation_issue=2; citation_publication_date=2009; citation_pages=161-168; citation_doi=10.1016/j.future.2008.07.004; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_title=Usability engineering: scenario-based development of human-computer interaction; citation_publication_date=2001; citation_id=CR24; citation_author=MB Rosson; citation_author=JM Carroll; citation_publisher=Elsevier"/>

    <meta name="citation_reference" content="Sampaio PN, de Freitas RIC, Cardoso GNP (2008) Ogre-multimedia: an api for the design of multimedia and virtual reality applications. In: Knowledge-based intelligent information and engineering systems. Springer, New York, pp 465&#8211;472"/>

    <meta name="citation_reference" content="Schou T, Gardner HJ (2007) A Wii remote, a game engine, five sensor bars and a virtual reality theatre. In: OZCHI &#8217;07. ACM Press, New York, pp 231&#8211;234"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comput Interact; citation_title=Shaping the display of the future: the effects of display size and curvature on user performance and insights; citation_author=L Shupp, C Andrews, M Dickey-Kurdziolek, B Yost, C North; citation_volume=24; citation_issue=1&#8211;2; citation_publication_date=2009; citation_pages=230-272; citation_doi=10.1080/07370020902739429; citation_id=CR26"/>

    <meta name="citation_reference" content="Simon A, Gobel M (2002) The i-cone trade;&#8212;a panoramic display system for virtual environments. In: Proceedings of the 10th Pacific conference on computer graphics and applications, 2002, pp 3&#8211;7. doi:
                    10.1109/PCCGA.2002.1167834
                    
                  
                "/>

    <meta name="citation_reference" content="STEM System: The Best Way to Interact with Virtual Worlds by Sixense&#8212;Kickstarter. 
                    http://www.kickstarter.com/projects/89577853/stem-system-the-best-way-to-interact-with-virtual
                    
                  
                "/>

    <meta name="citation_reference" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology. ACM, New Year, pp 55&#8211;61"/>

    <meta name="citation_reference" content="Teather RJ, Pavlovych A, Stuerzlinger W, MacKenzie IS (2009) Effects of tracking technology, latency, and spatial jitter on object movement. In: IEEE symposium on 3D user interfaces, 2009. 3DUI 2009, pp 43&#8211;50. IEEE"/>

    <meta name="citation_reference" content="Tracked THE Device Driver Software for Immersive Displays. 
                    http://www.mechdyne.com/trackd.aspx
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Opt Eng; citation_title=Performance and head movements using a helmet-mounted display with different sized fields-of-view; citation_author=MJ Wells, M Venturino; citation_volume=29; citation_issue=8; citation_publication_date=1990; citation_pages=870-877; citation_doi=10.1117/12.55672; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Natl Acad Sci; citation_title=Immersive visualization; citation_author=SC Williams; citation_volume=110; citation_issue=12; citation_publication_date=2013; citation_pages=4438-4438; citation_doi=10.1073/pnas.1302989110; citation_id=CR21"/>

    <meta name="citation_reference" content="Woods A (2010) Understanding crosstalk in stereoscopic displays. In: Keynote presentation at the three-dimensional systems and applications conference. Tokyo, Japan, pp 19&#8211;21"/>

    <meta name="citation_author" content="Kevin Ponto"/>

    <meta name="citation_author_email" content="kponto@discovery.wisc.edu"/>

    <meta name="citation_author_institution" content="Wisconsin Institute for Discovery, Madison, USA"/>

    <meta name="citation_author" content="Joe Kohlmann"/>

    <meta name="citation_author_email" content="jkohlmann@discovery.wisc.edu"/>

    <meta name="citation_author_institution" content="Wisconsin Institute for Discovery, Madison, USA"/>

    <meta name="citation_author" content="Ross Tredinnick"/>

    <meta name="citation_author_email" content="rtredinnick@discovery.wisc.edu"/>

    <meta name="citation_author_institution" content="Wisconsin Institute for Discovery, Madison, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0254-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0254-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="DSCVR: designing a commodity hybrid virtual reality system"/>
        <meta property="og:description" content="This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer technology has enabled a reduced cost for both 3D tracking and screens, enabling a new means for the creation of immersive display environments. However, this technology also presents many challenges, which need to be designed for and around. We compare the DSCVR System to other existing VR environments to analyze the trade-offs being made."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>DSCVR: designing a commodity hybrid virtual reality system | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0254-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Hybrid reality, Virtual reality, Display wall, Immersive systems, Commodity hardware, 3D, High resolution, Passive stereo","kwrd":["Hybrid_reality","Virtual_reality","Display_wall","Immersive_systems","Commodity_hardware","3D","High_resolution","Passive_stereo"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0254-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0254-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=254;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0254-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            DSCVR: designing a commodity hybrid virtual reality system
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0254-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0254-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-11-13" itemprop="datePublished">13 November 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">DSCVR: designing a commodity hybrid virtual reality system</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kevin-Ponto" data-author-popup="auth-Kevin-Ponto" data-corresp-id="c1">Kevin Ponto<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Wisconsin Institute for Discovery" /><meta itemprop="address" content="grid.484731.d, 0000 0004 0405 1091, Wisconsin Institute for Discovery, Room 3176, 330 N. Orchard Street, Madison, WI, 53715, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Joe-Kohlmann" data-author-popup="auth-Joe-Kohlmann">Joe Kohlmann</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Wisconsin Institute for Discovery" /><meta itemprop="address" content="grid.484731.d, 0000 0004 0405 1091, Wisconsin Institute for Discovery, 330 N. Orchard Street, Madison, WI, 53715, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ross-Tredinnick" data-author-popup="auth-Ross-Tredinnick">Ross Tredinnick</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Wisconsin Institute for Discovery" /><meta itemprop="address" content="grid.484731.d, 0000 0004 0405 1091, Wisconsin Institute for Discovery, Room 1144B, 330 N. Orchard Street, Madison, WI, 53715, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">57</span>–<span itemprop="pageEnd">70</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">647 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0254-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer technology has enabled a reduced cost for both 3D tracking and screens, enabling a new means for the creation of immersive display environments. However, this technology also presents many challenges, which need to be designed for and around. We compare the DSCVR System to other existing VR environments to analyze the trade-offs being made.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Recent advancements in consumer-grade 3D display and gesture input technology have enabled new pathways for the creation of immersive virtual reality systems. Previous methods for constructing these systems required customized room configurations, tracking hardware, projections, and screens. This in turn has meant that most VR systems have become an exclusive enterprise, as the systems are thus designed and utilized by a limited number of privileged individuals.</p><p>Given this outlook, our mission is to develop a system entirely from commodity, off-the-shelf hardware that has comparable performance to commercially built environments. However, as we discovered in the process of building this system, the current generation of commodity-grade technologies provides a significant number of challenges to creating effective immersive virtual environments.</p><p>In this paper, we present the design considerations, specifications, and lessons learned for building the “DSCVR System”, a hybrid reality environment (HRE) constructed from commodity-grade hardware. HREs, as defined by Febretti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e381">2013</a>), enable the benefits of both tiled display environments along with the immersive characteristics of virtual reality systems. Specific contributions include:</p><ul class="u-list-style-dash">
                  <li>
                    <p>Design guidelines for the construction of a virtual reality system utilizing commodity hardware, such as micropolarization 3D displays.</p>
                  </li>
                  <li>
                    <p>Quantification of attributes and performance of the system compared to professionally constructed virtual reality systems.</p>
                  </li>
                  <li>
                    <p>Discussion of lessons learned and considerations for others attempting to create these types of systems.</p>
                  </li>
                </ul>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Design schematics for the DSCVR System. <b>a</b> A subset of the designs considered, including horizontally oriented displays, larger displays, a wider or tighter curvature, and positioning techniques to hide bezels. <b>b</b> The final, implemented structure, with the back of the system shown <i>inset</i>. <b>c</b> The 80/20 components of one of the final design’s ten columns</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec2">Related work</h3><p>Many researchers have attempted to balance the trade-offs between cost and fidelity in the creation of virtual reality systems. Pausch (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Pausch R (1991) Virtual reality on five dollars a day. Proceedings of the SIGCHI conference on human factors in computing systems, CHI ’91. ACM, New York, NY, USA, pp 265–270" href="/article/10.1007/s10055-014-0254-0#ref-CR2" id="ref-link-section-d12794e440">1991</a>) proposed building a VR system on the budget of five dollars a day. The system was developed using a HMD with a Nintendo PowerGlove for interaction and a Polhemus Isotrak magnetic tracker for a cost around $5,000. Basu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Basu A, Saupe C, Refour E, Raij A, Johnsen K (2012) Immersive 3dui on one dollar a day. In: 2012 IEEE symposium on 3D user interfaces (3DUI), pp 97–100" href="/article/10.1007/s10055-014-0254-0#ref-CR3" id="ref-link-section-d12794e443">2012</a>) updated this concept, showcasing the ability to build a virtual reality system for a dollar a day. Others such as Avery et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Avery B, Thomas BH, Velikovsky J, Piekarski W (2005) Outdoor augmented reality gaming on five dollars a day. In: Proceedings of the 6th Australasian conference on User interface—volume 40, AUIC ’05, pp 79–88. Australian Computer Society Inc, Darlinghurst, Australia" href="/article/10.1007/s10055-014-0254-0#ref-CR4" id="ref-link-section-d12794e446">2005</a>) have utilized custom HMDs to develop low-cost augmented reality.</p><p> Bowman and McMahan (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" href="/article/10.1007/s10055-014-0254-0#ref-CR5" id="ref-link-section-d12794e452">2007</a>) have posed the question of how much immersion is enough for the field of virtual reality. This question has been studied from a variety of angles. Prabhat et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets. IEEE Trans Vis Comput Gr 14(3):551–563. doi:&#xA;                    10.1109/TVCG.2007.70433&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR6" id="ref-link-section-d12794e455">2008</a>) have tried to study the difference between low-fidelity fishtank VR systems in relation to more immersive CAVE-style systems. Bacim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Bacim F, Ragan E, Scerbo S, Polys NF, Setareh M, Jones BD (2013) The effects of display fidelity, visual complexity, and task scope on spatial understanding of 3d graphs. In: Proceedings of graphics interface 2013, GI ’13, pp 25–32. Canadian Information Processing Society, Toronto, Ont., Canada" href="/article/10.1007/s10055-014-0254-0#ref-CR7" id="ref-link-section-d12794e458">2013</a>) have attempted to study how the level of immersion in CAVE environments affects task performance. Laha et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Laha B, Sensharma K, Schiffbauer J, Bowman D (2012) Effects of immersion on visual analysis of volume data. IEEE Trans Vis Comput Gr 18(4):597–606. doi:&#xA;                    10.1109/TVCG.2012.42&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR8" id="ref-link-section-d12794e461">2012</a>) have studied the effects of immersion on the analysis of volumetric data in virtual environments. Ragan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ragan E, Kopper R, Schuchardt P, Bowman D (2013) Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task. IEEE Trans Vis Comput Gr 19(5):886–896. doi:&#xA;                    10.1109/TVCG.2012.163&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR9" id="ref-link-section-d12794e464">2013</a>) have studied how spatial judgment tasks were affected by stereo, head tracking, and field of regard. Polys et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Polys NF, Kim S, Bowman DA (2007) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. Comput Animat Virtual Worlds 18(1):19–38. doi:&#xA;                    10.1002/cav.159&#xA;                    &#xA;                  ." href="/article/10.1007/s10055-014-0254-0#ref-CR10" id="ref-link-section-d12794e468">2007</a>) studied how screen size and field of view affected performance using a tiled display environment. Finally, McMahan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="McMahan RP, Bowman DA, Zielinski DJ, Brady RB (2012) Evaluating display fidelity and interaction fidelity in a virtual reality game. IEEE Trans Vis Comput Graph 18(4):626–633" href="/article/10.1007/s10055-014-0254-0#ref-CR11" id="ref-link-section-d12794e471">2012</a>) studied how immersion and fidelity affected performance in a first-person shooter video game.</p><p>The video gaming industry has generated a large amount of motion-tracking hardware that has also spurred interest in low-cost virtual reality systems. For example, Schou and Gardner (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Schou T, Gardner HJ (2007) A Wii remote, a game engine, five sensor bars and a virtual reality theatre. In: OZCHI ’07. ACM Press, New York, pp 231–234" href="/article/10.1007/s10055-014-0254-0#ref-CR12" id="ref-link-section-d12794e477">2007</a>) combined the Nintendo Wii Remote, multiple infrared sensor bars, and a two-wall immersive VR theater. Lange et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Lange B, Koenig S, Chang CY, McConnell E, Suma E, Bolas M, Rizzo A (2012) Designing informed game-based rehabilitation tasks leveraging advances in virtual reality. Disabil Rehabil 34(22):1863–1870" href="/article/10.1007/s10055-014-0254-0#ref-CR13" id="ref-link-section-d12794e480">2012</a>) also examined the use of a Microsoft Kinect motion-tracking sensor in a clinical VR rehabilitation task.</p><p>Immersive display environments have also seen several design iterations. Cruz proposed the original design and implementation of the CAVE in the early 1990s (Cruz-Neira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="CruzNeira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72" href="/article/10.1007/s10055-014-0254-0#ref-CR15" id="ref-link-section-d12794e486">1992</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the cave. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, pp 135–142" href="/article/10.1007/s10055-014-0254-0#ref-CR14" id="ref-link-section-d12794e489">1993</a>). This projection-based multi-wall design became the de facto standard for immersive, room-sized virtual reality environments. These types of systems range from a three-wall setup with a floor, to a fully immersive six-sided system. However, other designs which curve around the user have also been created for virtual reality, such as the allosphere (Amatriain et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Amatriain X, Kuchera-Morin J, Hollerer T, Pope ST (2009) The allosphere: immersive multimedia for scientific discovery and artistic exploration. IEEE MultiMedia 16(2):0064–75" href="/article/10.1007/s10055-014-0254-0#ref-CR16" id="ref-link-section-d12794e492">2009</a>) and the i-Cone (Simon and Gobel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Simon A, Gobel M (2002) The i-cone trade;—a panoramic display system for virtual environments. In: Proceedings of the 10th Pacific conference on computer graphics and applications, 2002, pp 3–7. doi:&#xA;                    10.1109/PCCGA.2002.1167834&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR17" id="ref-link-section-d12794e495">2002</a>).</p><p>Tiled display walls rose in popularity in the mid-2000s for their ability to provide a large viewing area while maintaining a high image resolution. Systems such as HIPerWall at the University of California, Irvine (Knox et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Knox C, Brown M, Doerr K, Jenks S, Zender C, Kuester F (2005) Simultaneous visualization of the ipcc ar4 model ensemble on an extremely high resolution display wall (hiperwall). In: AGU fall meeting abstracts, 1:1140" href="/article/10.1007/s10055-014-0254-0#ref-CR18" id="ref-link-section-d12794e502">2005</a>), HIPerSpace at the University of California, San Diego (Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays. Future Gener Comput Syst 26(5):693–700" href="/article/10.1007/s10055-014-0254-0#ref-CR19" id="ref-link-section-d12794e505">2010</a>), Stallion at the University of Texas (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Johnson GP, Abram GD, Westing B, Navr’til P, Gaither K (2012) Displaycluster: an interactive visualization environment for tiled displays. In: 2012 IEEE international conference on cluster computing (CLUSTER), pp 239–247. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR20" id="ref-link-section-d12794e508">2012</a>) and the Reality Deck at New York’s Stony Brook University (Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Williams SC (2013) Immersive visualization. Proc Natl Acad Sci 110(12):4438–4438" href="/article/10.1007/s10055-014-0254-0#ref-CR21" id="ref-link-section-d12794e511">2013</a>) have shown the ability to create high-resolution data visualizations. However, these systems do not provide an efficient method to present stereoscopic imagery.</p><p>DeFanti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011a" title="DeFanti TA, Acevedo D, Ainsworth RA, Brown MD, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R et al (2011a) The future of the cave. Cent Eur J Eng 1(1):16–37" href="/article/10.1007/s10055-014-0254-0#ref-CR22" id="ref-link-section-d12794e517">2011a</a>) proposed new methods for creating CAVE-style systems from the same components used in tiled display walls. Since this time, new types of immersive display environments have been created, from the desk-sized, 3DTV-based HUVR device at University of California, San Diego (Margolis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Margolis T, DeFanti TA,  Dawe G, Prudhomme A, Schulze JP, Cutchin S (2011) Low cost heads-up virtual reality (HUVR) with optical tracking and haptic feedback. IS&amp;T/SPIE Electronic Imaging, p 786417" href="/article/10.1007/s10055-014-0254-0#ref-CR23" id="ref-link-section-d12794e520">2011</a>) to the large-scale NexCAVE at King Abdullah University of Science and Technology (KAUST) and CAVE2 at the University of Illinois at Chicago (Febretti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e523">2013</a>).</p><p>The development of CAVE2, in addition to showcasing many advances in VR hardware and software, underscored the challenges of working with micropolarization displays. The CAVE2 implementation used specialized filters and displays to address these issues. With these lessons in mind, one early goal for the DSCVR System was to recreate this type of system entirely with consumer-grade hardware. As with all computer systems, many factors need to be taken into account in the design process (Rosson and Carroll <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Rosson MB, Carroll JM (2001) Usability engineering: scenario-based development of human-computer interaction. Elsevier, Amsterdam" href="/article/10.1007/s10055-014-0254-0#ref-CR24" id="ref-link-section-d12794e529">2001</a>). We describe the design decisions made in the creation of the DSCVR System below.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Design</h2><div class="c-article-section__content" id="Sec3-content"><p>DSCVR’s design and implementation are ultimately a balance between these financial, technological, and structural goals:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Implement a hybrid reality system in a cost-effective way, using unmodified, consumer-grade hardware, such that its performance rivals that of more expensive systems.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Reduce the appearance of bezels and stereo image cross talk in users’ fields of view.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Build a frame that supports both display position adjustments and display upgrades.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Balance design trade-offs between cost and performance.</p>
                    
                  </li>
                </ol><p>To accomplish these goals, compromises were inevitably made, making DSCVR neither the best, worst, most expensive, or cheapest environment of its kind. However, as shown in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec11">4</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec17">5</a>, the performance of the implemented system can equate or exceed that of much more expensive systems.</p><h3 class="c-article__sub-heading" id="Sec4">Display technology</h3><p>One of the early decisions while designing the DSCVR System was the choice of display technologies. Projectors were known to have substantial drawbacks such as the need to replace bulbs, use specialized projection material, accomodate throw distance with extra space, and perform repeated color calibrations. It was therefore prudent to utilize one of the increasingly capable consumer-grade 3D television models available on the market.</p><p>Consumer 3DTVs currently use either active or passive stereo display technology. Active stereo, accomplished through synchronizing display frame swaps with shutter glasses, is the most common format in use today. Unfortunately, this technique becomes problematic when multiple TV screens are used, as each TV must show the image intended for the viewer's left or right eye at the same time. Synchronizing this type of swapping would have required specialized and costly hardware and thus was rejected in favor of passive stereo technologies.</p><p>While traditional passive stereo displays utilize linear polarizers, newer ones use micropolarization technology. Febretti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e601">2013</a>) provide a detailed description how this technology works. Notably, this type of polarization filter can be easily produced for thin-bezel displays. The major problem with these types of displays is that, while the images are of good quality when viewed in the direction parallel to the micropolarizer lines, the images have substantial “cross talk” when viewed from the direction perpendicular to the micropolarizer lines. As these types of TVs provide the best image quality, contrast, and 3D capability for the cost, we chose to tackle the hurdle of cross talk through our arrangement. As of this writing, LG Electronics Inc., is the prominent manufacturer of consumer-grade, passive stereo 3DTVs, so we focused on the capabilities of the LG LM7600 television. Additional details on model selection are given below.</p><h3 class="c-article__sub-heading" id="Sec5">Structure and arrangement</h3><p>The limitations of these LG TVs meant that the system’s shape and structure would have to accommodate the displays’ unmodifiable polarization filters, as well as their wider 1-in. bottom bezels. Early development on the system required accounting for uncertainties, such as the displays’ panel sizes—either 47 or 55 in. diagonal—and the unknown final location of the system. Therefore, regardless of the final shape, display quantity, or panel size, we decided to build the system as a series of modular, functionally independent columns, using 80/20 aluminum framing for the structure.</p><p>The first designs, modeled in Trimble SketchUp for its ease of use, adapted a tiled display wall layout into a cylindrical shape, making it a variation on systems such as the KAUST NexCAVE (DeFanti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011a" title="DeFanti TA, Acevedo D, Ainsworth RA, Brown MD, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R et al (2011a) The future of the cave. Cent Eur J Eng 1(1):16–37" href="/article/10.1007/s10055-014-0254-0#ref-CR22" id="ref-link-section-d12794e614">2011a</a>), in which users stand near the center of the display array’s radius. Landscape and portrait display orientations were both considered, as well as other techniques to hide the wider bezels behind the previous or next column of displays. These unimplemented designs, some of which used up to 28 displays, are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig1">1</a>a.</p><p>Analyzing the trade-offs posed by these design experiments informed our decisions for the final model. A wider, more gradual curving arrangement, for example, may have accommodated more viewers, but would have exacerbated the appearance of off-axis viewing artifacts, such as stereo image cross talk. This curved layout has also been shown to have several benefits over a linear layout by Shupp et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Shupp L, Andrews C, Dickey-Kurdziolek M, Yost B, North C (2009) Shaping the display of the future: the effects of display size and curvature on user performance and insights. Hum Comput Interact 24(1–2):230–272" href="/article/10.1007/s10055-014-0254-0#ref-CR26" id="ref-link-section-d12794e623">2009</a>). Framing constructions with four displays grouped together may have minimized the number of components, but would have reduced the modularity of the design. 55-in. displays may have made the system fill a larger field of view, but 47-in. displays would increase the pixel density, shorten the height of the system, and enable the mounting of tracking hardware on top of the frame, all while staying under the height of a standard office ceiling.</p><p>Portrait orientation became a particularly important consideration early on, since the wide bezels of the landscape-oriented displays may have cut a tall and wide horizontal gap through the viewer’s entire field of view, regardless of their height. Though tucking away the wider bezels may have further reduced the appearance of gaps, positioning the displays edge-to-edge along the cylinder’s interior radius would instead minimize changes in display depth, enhancing the appearance of the array as a seamless, curved surface.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An overhead view of the DSCVR System’s final design, showing the displays’ estimated viewing ranges and the center region in which all the viewing ranges overlap</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Understanding the limitations of the displays’ micropolarization filters further validated the use of a cylindrical, portrait-oriented layout. Purchasing one display allowed us to determine the range of horizontal and vertical viewing angles for which viewers could not observe any off-axis viewing artifacts. To accomplish this, we used the method described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec16">4.3</a> to measure the level of cross talk (Woods <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Woods A (2010) Understanding crosstalk in stereoscopic displays. In: Keynote presentation at the three-dimensional systems and applications conference. Tokyo, Japan, pp 19–21" href="/article/10.1007/s10055-014-0254-0#ref-CR27" id="ref-link-section-d12794e654">2010</a>).</p><p>Measurements were taken from distances of 3, 5, and 10 feet from the TV. For each measurement, the instrument was positioned in the center of the monitor and was then slid parallel to the monitor until cross talk occured. The process was repeated 3 times at each distance and was tested with the monitor in both landscape and portrait orientations. From these measurements, the angle from the edge of the TV was calculated for which the 3D effect would work correctly. From this initial test, it was determined that when the TV was positioned in a landscape orientation, the horizontal artifact-free field of view was approximately <span class="mathjax-tex">\(170^\circ \hbox {s}\)</span>, while the vertical artifact-free field of view was approximately <span class="mathjax-tex">\(20^\circ \hbox {s}\)</span>.</p><p>Given the display’s very wide horizontal viewing range, mounting the displays in portrait would provide an artifact-free image to viewers of different statures. The narrower vertical viewing range, however, indicated the need for a cylindrical arrangement small enough to have each column’s viewing ranges overlap, but large enough to support multiple viewers for 2D applications and spectator viewing.</p><p>This analysis allowed us to visualize a central “sweet spot” for our cylindrical models, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig2">2</a>. The understood constraints on viewing ranges, system size, and budget thus led to DSCVR’s final design, consisting of 10 columns and 20 displays arranged in a half-cylinder shape, with a “sweet spot” approximately 4 feet in diameter. The final design is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig1">1</a>b.</p><p>One additional finding was that orienting the displays in portrait introduced a minor visual artifact when viewing stereo images through the included 3D glasses. Orienting the linear components of the glasses’ polarization filters <i>perpendicularly</i> to the filters on the displays produced an additional color-fringing artifact, most noticeable when viewing images with high-contrast edges, such as white text on a black background. We found that 3D glasses with circular polarized filters at <span class="mathjax-tex">\(90^\circ \)</span> left and <span class="mathjax-tex">\(90^\circ \)</span> right completely eliminated this artifact. After modifying a pair of included 3D glasses, we ordered two inexpensive pairs of built-to-order glasses with these orientation changes.</p><p>Finally, the system was created to only comprise half of a circle as opposed to being fully encompassing to enable benefits seen in tiled display environments such as high-resolution image and cinema viewing (Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network bandwidth method for multimedia playback on tiled display systems. In: 11th IEEE international symposium on multimedia, 2009. ISM’09, pp 201–206. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR28" id="ref-link-section-d12794e786">2009</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays. Future Gener Comput Syst 26(5):693–700" href="/article/10.1007/s10055-014-0254-0#ref-CR19" id="ref-link-section-d12794e789">2010</a>; Renambot et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Renambot L, Jeong B, Hur H, Johnson A, Leigh J (2009) Enabling high resolution collaborative visualization in display rich virtual organizations. Future Gener Comput Syst 25(2):161–168" href="/article/10.1007/s10055-014-0254-0#ref-CR29" id="ref-link-section-d12794e792">2009</a>). This arrangement also enabled an audience to easily observe a virtual experience from behind the participant in 3D. If one was inclined, extending the system to cover a full circle would simply be an extension of the described methods.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Implementation</h2><div class="c-article-section__content" id="Sec6-content"><p>Based on the design described above, the system was implemented as described below.</p><h3 class="c-article__sub-heading" id="Sec7">Framing</h3><p>After finalizing the DSCVR System’s shape, a single prototype column was constructed using 80/20 aluminum framing. 80/20, which is manufactured according to order, allowed us to develop a custom frame with greater utility and lower cost than any of the expensive, proprietary display stands we considered. Consumer-grade VESA mounts were initially used to attach two displays to the structure. Assessing the prototype allowed us to revise several components for the final construction, such as the frame’s depth and its ease of assembly. We replaced the VESA mounts with custom horizontal pieces of 80/20 with precisely machined screw holes, enabling the displays to be mounted on the frame without sloping downwards.</p><p>The final single-column frame is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig1">1</a>c. It stands 90 in. tall, 24 in. wide, and 22.5 in. deep and uses inside-to-inside corner connectors to join the 80/20 pieces along their interior tracks. The 21-in. VESA mount pieces, with two precision holes drilled 200 mm apart, attach the displays to the frame. These pieces can be loosened, moved vertically along the inside of the frame, and reattached, thereby supporting precision height adjustments for both current and future displays. A separate 7-in. piece with <span class="mathjax-tex">\(10^\circ \)</span> cuts attaches multiple columns to each other, simplifying inter-column alignment and increasing structural stability. An extra metal brace is attached between the tops of adjacent columns for even more stability.</p><h3 class="c-article__sub-heading" id="Sec8">Hardware</h3><p>The DSCVR System utilizes several Alienware X51 mini gaming desktops, which were chosen for their high CPU and GPU performance, comparatively low power requirements, and comparatively low price. Each of 12 machines is equipped with an Intel Core i7-3770, 8 GB of 1600 MHz DDR3 SDRAM, a 1 TB SATA hard disk, gigabit Ethernet, an NVIDIA GTX 660 GPU with 1.5 GB of GDDR5 VRAM, and a 330-W power supply. Ten “cluster nodes” drive the 20 displays, one “head node” hosts one or more VRPN tracking servers, and one “workstation” supports development and cluster control. A 13th “hot spare” machine is available to replace a malfunctioning one. Driving just two displays per cluster machine balances the capabilities of these single-GPU gaming PCs with the need to render a high-resolution, distributed, 3D viewport. The CentOS operating system provides a stable, UNIX-like software environment, which also enables us to administer cluster commands via SSH and tentakel.</p><p>A USB to RS-232 serial interface connects each column’s two displays to its cluster machine, enabling programmatic control over display visibility and 3D modes. The serial cable is split once to send the same command to both displays simultaneously.</p><h3 class="c-article__sub-heading" id="Sec9">Tracking system</h3><p>For a number of reasons, the Microsoft Kinect system was selected to track the user. Clark et al. found that Kinect, in combination with the Microsoft Kinect for Windows SDK, was able to provide data comparable to that of a commercial 3D motion analysis system (Clark et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Clark RA, Pua YH, Fortin K, Ritchie C, Webster KE, Denehy L, Bryant AL (2012) Validity of the microsoft kinect for assessment of postural control. Gait Posture 36(3):372–377" href="/article/10.1007/s10055-014-0254-0#ref-CR30" id="ref-link-section-d12794e856">2012</a>). The price of Kinect was substantially lower than specialized ultrasonic or optical tracking hardware. Furthermore, Kinect does not require the user to wear any specialized tracking equipment, such as a tracking bar, enabling users to easily move in and out of the tracking space.</p><p>As the “sweet spot” for the system is not overly large, the entire area can be easily covered by a single Kinect system mounted on top of the framing. On initialization, we use the Microsoft Kinect SDK to tilt the sensor to its lowest possible level of declination. The accelerometer value is read to determine the actual orientation of the sensor. The Kinect SDK’s skeleton-tracking API<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> is used to determine the position and orientation of the user. All system-level transformations are “undone” before packing and sending the tracking data to client applications using VRPN (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology. ACM, New Year, pp 55–61" href="/article/10.1007/s10055-014-0254-0#ref-CR32" id="ref-link-section-d12794e871">2001</a>). In practice, simple temporal averaging was able to alleviate most tracking signal artifacts. However, the steep downward tilting angle mixed with certain hair colors and styles has been shown to confuse hair regions and head regions. Future work will attempt to mitigate these effects.</p><p>To allow for multiple individuals to be in the space, we used the “sticky user” flag in the Kinect skeleton-tracking API. This allows a tracked user to utilize the space while other individuals are in the same area. A tracked user can “switch” with another user by simply walking out of the space, which allows Kinect to then detect and track the next available skeleton. This process occurs without the exchange of any glasses or equipment.</p><h3 class="c-article__sub-heading" id="Sec10">Software</h3><p>While the DSCVR System does not approach the resolution of environments such as Stallion (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Johnson GP, Abram GD, Westing B, Navr’til P, Gaither K (2012) Displaycluster: an interactive visualization environment for tiled displays. In: 2012 IEEE international conference on cluster computing (CLUSTER), pp 239–247. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR20" id="ref-link-section-d12794e885">2012</a>), HIPerSpace (Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays. Future Gener Comput Syst 26(5):693–700" href="/article/10.1007/s10055-014-0254-0#ref-CR19" id="ref-link-section-d12794e888">2010</a>), or Reality Desk (Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Williams SC (2013) Immersive visualization. Proc Natl Acad Sci 110(12):4438–4438" href="/article/10.1007/s10055-014-0254-0#ref-CR21" id="ref-link-section-d12794e891">2013</a>), the resolution is still quite high compared to many still-image capture technologies. In this regard, the system is well suited for tiled display software, such as CGLX (Doerr and Kuester <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Doerr K, Kuester F (2011) Cglx: a scalable, high-performance visualization framework for networked display environments. IEEE Trans Vis Comput Graph 17(3):320–332" href="/article/10.1007/s10055-014-0254-0#ref-CR33" id="ref-link-section-d12794e894">2011</a>), Equalizer (Eilemann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Eilemann S, Makhinya M, Pajarola R (2009) Equalizer: a scalable parallel rendering framework. IEEE Trans Vis Comput Graph 15(3):436–452" href="/article/10.1007/s10055-014-0254-0#ref-CR34" id="ref-link-section-d12794e897">2009</a>), Chromium (Leigh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Leigh J, Johnson A, Renambot L, Peterka T, Jeong B, Sandin DJ, Talandis J, Jagodic R, Nam S, Hur H et al (2013) Scalable resolution display walls. Proc IEEE 101(1):115–129" href="/article/10.1007/s10055-014-0254-0#ref-CR35" id="ref-link-section-d12794e901">2013</a>), and others (Luo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Luo J, Qin K, Zhou Y, Mao M, Li R (2010) Gpu rendering for tiled multi-projector autostereoscopic display based on chromium. Vis Comput 26(6–8):457–465" href="/article/10.1007/s10055-014-0254-0#ref-CR36" id="ref-link-section-d12794e904">2010</a>). We demonstrate the ability to view this type of high-resolution content in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The DSCVR System, shown displaying (<b>a</b>) high-resolution panoramic imagery, (<b>b</b>) playing 4K stereo video at 30 FPS using custom software, and (<b>c</b>) rendering an interior environment in real-time stereo 3D using the OGRE 3D engine</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>One feature of the DSCVR System compared to other high-resolution displays (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Johnson GP, Abram GD, Westing B, Navr’til P, Gaither K (2012) Displaycluster: an interactive visualization environment for tiled displays. In: 2012 IEEE international conference on cluster computing (CLUSTER), pp 239–247. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR20" id="ref-link-section-d12794e941">2012</a>, Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays. Future Gener Comput Syst 26(5):693–700" href="/article/10.1007/s10055-014-0254-0#ref-CR19" id="ref-link-section-d12794e944">2010</a>, Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Williams SC (2013) Immersive visualization. Proc Natl Acad Sci 110(12):4438–4438" href="/article/10.1007/s10055-014-0254-0#ref-CR21" id="ref-link-section-d12794e947">2013</a>) is its ability to display 3D media. Stereo panoramas, such as those outlined by Ainsworth et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ainsworth RA, Sandin DJ, Schulze JP, Prudhomme A, DeFanti TA, Srinivasan M (2011) Acquisition of stereo panoramas for display in vr environments" href="/article/10.1007/s10055-014-0254-0#ref-CR37" id="ref-link-section-d12794e950">2011</a>), are particularly well-suited to the display capabilities of the DSCVR System.</p><p>Another source of interesting data comes from 3D video, which has become readily available on the Internet. Unfortunately, tiled video players such as VideoBlaster (Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network bandwidth method for multimedia playback on tiled display systems. In: 11th IEEE international symposium on multimedia, 2009. ISM’09, pp 201–206. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR28" id="ref-link-section-d12794e956">2009</a>) and SAGE (Renambot et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Renambot L, Jeong B, Hur H, Johnson A, Leigh J (2009) Enabling high resolution collaborative visualization in display rich virtual organizations. Future Gener Comput Syst 25(2):161–168" href="/article/10.1007/s10055-014-0254-0#ref-CR29" id="ref-link-section-d12794e959">2009</a>) do not have native support for this 3D content. Having a desire to view this type of content, we developed a distributed video player application capable of playing back such 3D content.</p><p>The software is based on the VideoBlaster framework (Ponto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network bandwidth method for multimedia playback on tiled display systems. In: 11th IEEE international symposium on multimedia, 2009. ISM’09, pp 201–206. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR28" id="ref-link-section-d12794e965">2009</a>), which utilizes a message-based protocol as opposed to a streaming-based technique. 3D media content can be provided in two ways: multiple video streams can be encapsulated in a single video file for each eye, or video frames can contain the left and right frames in a single video stream. The software takes the content for each eye and uploads the YUV frames to the graphics card. A single video viewport can thus be moved around the display environment, with the appropriate content for each eye being shown in the appropriate location. This technique enables the playback of stereo 4K content at 30 FPS as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig3">3</a>b.</p><p>The DSCVR System successfully enables several open-source and free-to-use visualization and software applications such as the Unity 3D game engine (Higgins <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Higgins T (2010) Unity-3d game engine" href="/article/10.1007/s10055-014-0254-0#ref-CR38" id="ref-link-section-d12794e975">2010</a>) with stereoscopic rendering via side-by-side stereo as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig4">4</a>. In addition, DSCVR makes use of a custom-built software framework that runs a point cloud renderer, a volume renderer via the open-source software Voreen (Meyer-Spradow et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Meyer-Spradow J, Ropinski T, Mensmann J, Hinrichs K (2009) Voreen: a rapid-prototyping environment for ray-casting-based volume visualizations. IEEE Comput Graph Appl 29(6):6–13" href="/article/10.1007/s10055-014-0254-0#ref-CR39" id="ref-link-section-d12794e981">2009</a>), molecular visualization with the application VMD (Humphrey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Humphrey W, Dalke A, Schulten K (1996) Vmd: visual molecular dynamics. J Mol Graph 14(1):33–38" href="/article/10.1007/s10055-014-0254-0#ref-CR40" id="ref-link-section-d12794e984">1996</a>), and rendering of 3D models via the OGRE 3D engine (Sampaio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Sampaio PN, de Freitas RIC, Cardoso GNP (2008) Ogre-multimedia: an api for the design of multimedia and virtual reality applications. In: Knowledge-based intelligent information and engineering systems. Springer, New York, pp 465–472" href="/article/10.1007/s10055-014-0254-0#ref-CR41" id="ref-link-section-d12794e987">2008</a>), as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig3">3</a>c. The software framework uses VRPN (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology. ACM, New Year, pp 55–61" href="/article/10.1007/s10055-014-0254-0#ref-CR32" id="ref-link-section-d12794e994">2001</a>) for head tracking and generates asymmetric viewing frustums to create a seemingly seamless 3D viewport (Cruz-Neira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the cave. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, pp 135–142" href="/article/10.1007/s10055-014-0254-0#ref-CR14" id="ref-link-section-d12794e997">1993</a>). The virtual binocular disparity (i.e., the distance between the virtual eyes) was set via a configuration file at startup. All input for these applications is handled with a wireless PS3 Dual Analog controller. Future work will seek to add additional user input controls, such as the Leap Motion, and continue to add new VR-enabled applications.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The DSCVR System running a scene built using the Unity 3D game engine</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Evaluation</h2><div class="c-article-section__content" id="Sec11-content"><p>The DSCVR System was designed with commodity-grade hardware in an effort to reduce costs. This effort introduced trade-offs for a variety of factors such as resolution, field of regard, and latency.</p><p>As a reference, we compare our system against a professionally built CAVE environment 2.93 m <span class="mathjax-tex">\(\times \)</span> 2.93 m <span class="mathjax-tex">\(\times \)</span> 2.93 m in size. The CAVE system utilizes four workstations, each with 2 <span class="mathjax-tex">\(\times \)</span>Quad-Core Intel Xeon processors and 2 NVIDIA Quadro 5000 GPUs. Two 3D projectors (Titan model 1080p 3D, Digital Projection), with a maximum brightness of 4500 lumens per projector, are used to generate projections with a resolution of 1,920 <span class="mathjax-tex">\(\times \)</span> 1,920 per display wall. The system utilizes an InterSense ultrasonic tracking system, VETracker Processor model IS-900 with MicroTrax model 100-91000-EWWD and MicroTrax model 100-91300-AWHT used for wand and head tracking, respectively.</p><p>We also compare the DSCVR System to the specifications to the CAVE2 system (Febretti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1108">2013</a>). The CAVE2 system was selected as it implements a similar screen-based, cylindrical approach to immersive virtual reality. While direct comparison of these systems could not be achieved, as the authors did not have access to CAVE2, a comparison of DSCVR against published specifications was performed.</p><h3 class="c-article__sub-heading" id="Sec12">Human vision factors</h3><p>When comparing the immersiveness of different systems, several factors need to be accounted for simultaneously. For example, as someone moves their head closer to a screen, the amount of screen filling their field of vision increases. However, as the user’s eyes move closer to a screen, the size of the pixels as projected onto their retinas increases, thus reducing the effective resolution. To this end, we analyze both field of view and resolution metrics simultaneously.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The determination of coverage for a stationary position by determining the percent of view that the display covers (<i>purple</i>) compared to the human field of view (<i>blue</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Stationary viewpoint</h4><p>For certain simulation tasks, users generally stay in a fixed location viewing the virtual display environment. In these scenarios, providing views behind the user is not considered important and performance can be estimated given a single view. Using previous literature, we can estimate the average human’s field of view to be <span class="mathjax-tex">\(175^\circ \)</span> horizontally and <span class="mathjax-tex">\(135^\circ \)</span> vertically (Arthur <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Arthur K (1996) Effects of field of view on task performance with head-mounted displays. In: Conference companion on human factors in computing systems. ACM, New York, pp 29–30" href="/article/10.1007/s10055-014-0254-0#ref-CR42" id="ref-link-section-d12794e1198">1996</a>, Rash et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rash C, McLean W, Mozo B, Licina J, McEntire B (1999) Human factors and performance concerns for the design of helmet-mounted displays. In: RTO HFM symposium on current aeromedical issues in rotary wing operation" href="/article/10.1007/s10055-014-0254-0#ref-CR43" id="ref-link-section-d12794e1201">1999</a>, Wells and Venturino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Wells MJ, Venturino M (1990) Performance and head movements using a helmet-mounted display with different sized fields-of-view. Opt Eng 29(8):870–877" href="/article/10.1007/s10055-014-0254-0#ref-CR44" id="ref-link-section-d12794e1204">1990</a>). Using this knowledge, we analyzed the following factors for each system:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>
                            <b>3D system resolution</b>: the number of coordinated 3D megapixels which the system can display.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>
                            <b>System viewable area</b>: the percentage of the system which can be seen when the user is viewing a stereo image while standing in the center of the system.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>
                            <b>Viewable 3D resolution</b>: the number of 3D megapixels that can be seen by the eye when the user is viewing a stereo image while standing in the center of the system.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>
                            <b>FOV horizontal coverage</b>: the percentage of the view which the display surface covers, using the average human’s estimated <i>horizontal</i> field of view. See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig5">5</a>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>
                            <b>FOV vertical coverage</b>: the percentage of the view which the display surface covers, using the average human’s estimated <i>vertical</i> field of view.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">6.</span>
                        
                          <p>
                            <b>Immersive resolution</b>: the product of the viewable 3D resolution and vertical and horizontal coverage values. This attempts to balance how much the display surrounds the user, while also accounting for display resolution.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">7.</span>
                        
                          <p>
                            <b>Refresh per eye</b>: a system specification describing the refresh rate per image seen by a single eye. As the CAVE uses frame interleaving to transmit left and right-eye images, its value was nearly half the value of its counterparts.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">8.</span>
                        
                          <p>
                            <b>Immersive bandwidth</b>: the product of the immersive resolution and the refresh per eye values. This number accounts for frame interleaving by attempting to provide a fixed-viewpoint measure of immersion.</p>
                        
                      </li>
                    </ol><p>We show the comparison between the different systems in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Eight human vision and perception-based factors calculated for three virtual reality systems (color figure online)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0254-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Moving viewpoint</h4>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The determination of field of regard or coverage for a moving position by determining the percent of view that the display covers (<i>purple</i>) compared to the field of view surrounding the user (<i>blue</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p> As shown, the DSCVR System performs admirably compared to the other three systems while the user is stationary. However, in other applications, it may be important to for the user to look in different directions. This is often referred to as field of regard, being the range of the virtual environment that can be viewed with physical rotation (Ragan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ragan E, Kopper R, Schuchardt P, Bowman D (2013) Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task. IEEE Trans Vis Comput Gr 19(5):886–896. doi:&#xA;                    10.1109/TVCG.2012.163&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR9" id="ref-link-section-d12794e1392">2013</a>).</p><p>Using this motivation, we analyzed the following factors for each system:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>
                            <b>Horizontal field of regard</b>: the percentage of the horizontal view which the display surface covers for any viewing direction. See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig6">6</a>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>
                            <b>Vertical field of regard</b>: the percentage of the vertical view which the display surface covers for any viewing direction.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>
                            <b>Motion immersive resolution</b>: the product of the viewable 3D resolution and the vertical and horizontal field of regard values. This attempts to balance how much the display surrounds the user, while also accounting for display resolution.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>
                            <b>Motion immersive bandwidth</b>: the product of the motion immersive resolution and the refresh per eye values. This number accounts for frame interleaving by attempting to provide a moving viewpoint measure of immersion.</p>
                        
                      </li>
                    </ol><p>We show the comparison between the systems in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab2">2</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Four human vision and perception-based factors calculated for three virtual reality systems (color figure online)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0254-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec15">Latency</h3><p>Latency is a common measurement of virtual reality systems. One common way to accomplish this is to use a pendulum model (Teather et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Teather RJ, Pavlovych A, Stuerzlinger W, MacKenzie IS (2009) Effects of tracking technology, latency, and spatial jitter on object movement. In: IEEE symposium on 3D user interfaces, 2009. 3DUI 2009, pp 43–50. IEEE" href="/article/10.1007/s10055-014-0254-0#ref-CR45" id="ref-link-section-d12794e1496">2009</a>). As our system utilizes Microsoft Kinect, we chose to use a variation on the method proposed by (Livingston et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. In: Virtual reality short papers and posters (VRW), 2012 IEEE, pp 119–120" href="/article/10.1007/s10055-014-0254-0#ref-CR31" id="ref-link-section-d12794e1499">2012</a>).</p><p>The first step in the process was for the participant to orient their arm parallel to the ground, setting the “zero point” in the virtual system and in the video. The user then waved their arm up and down, mimicking the motion of a pendulum. On the screen in front of the user, the virtual height of the marker was displayed using colored rectangles. To make tracking easier, heights above the zero point were shown with a red rectangle, while heights below the zero point were shown with a blue rectangle. Images were captured with a GoPro Hero3 Black Edition camera, which was selected for its ability to capture images at a rate of 240 Hz at WGA resolution. Each video frame was extracted, and both marker height and virtual height were tagged in OpenCV, as demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig7">7</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The variation of the pendulum model used to evaluate DSCVR’s latency. The user first sets the zero point with their arm straight out. The user then waves their arm up and down, like a pendulum</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>These tagged heights were then imported into statistical analysis software. While the height of physical and projector marker are not identical, the important component is the phase shift between the two signals. From this, the latency amount can be found, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig8">8</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The vertical position of the physical and projected markers for several iterations of the subject’s movement. The latency is determined by the phase shift of the two waves</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Using 18 samples, we found an average latency of approximately 150 ms with a standard deviation of 23 ms. This result is similar to the latency of Kinect found by Livingston et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. In: Virtual reality short papers and posters (VRW), 2012 IEEE, pp 119–120" href="/article/10.1007/s10055-014-0254-0#ref-CR31" id="ref-link-section-d12794e1553">2012</a>) of 146 ms. As a comparison, we repeated this same procedure for the CAVE system. For the CAVE system, the position of the marker was tracked using the InterSense ultrasonic tracking system described previously. The CAVE system produced very similar results, with a latency of 150 ms with a standard deviation of 24 ms. This result is discussed further in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec18">5.1</a>.</p><p>Latency is not reported in Febretti et al.’s paper on the CAVE2 (Febretti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1563">2013</a>), so no direct comparisons can be made.</p><h3 class="c-article__sub-heading" id="Sec16">Stereo cross talk</h3><p>As stated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec4">2.1</a>, micropolarization technology has a limited effective viewing range. When the viewer is not inside of the viewing range, cross talk between the stereo images occurs. DSCVR’s arrangement, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig2">2</a>, attempts to minimize cross talk by creating a region in the center of the system for optimal viewing. We felt it was important to quantify the degradation of visual quantity outside of this zone. Early photographic analysis showed evidence of this phenomenon, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Panoramic images shot through a left-eye circular polarization filter (<i>inset</i>), showing stereo cross talk artifacts. <b>a</b> A panorama captured at the center of the system, with all displays showing a red hue (apart from reflections). <b>b</b> A panorama taken to the left of center. <i>Pink</i> and <i>purple hues</i> indicate cross talk due to off-axis viewing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Previously, Febretti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1620">2013</a>) attempted to measure cross talk utilizing Weissman cross talk patterns. This measurement approach requires a human’s subjective assessment, meaning that precise measurements may require a large sample size. The process was also quite laborious as each monitor needed to be checked from each location, meaning each participant would need to make 500 evaluations. For these reasons, we choose to use an optical approach (Woods <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Woods A (2010) Understanding crosstalk in stereoscopic displays. In: Keynote presentation at the three-dimensional systems and applications conference. Tokyo, Japan, pp 19–21" href="/article/10.1007/s10055-014-0254-0#ref-CR27" id="ref-link-section-d12794e1623">2010</a>).</p><p>To accomplish this, we used a digital camera with an 8 megapixel sensor and 35 mm fixed focal length. As opposed to using patterns to assess stereo cross talk, we used a luminance-based approach similar to Hong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hong H, Jang J, Lee D, Lim M, Shin H (2010) Analysis of angular dependence of 3-d technology using polarized eyeglasses. J Soc Inf Disp 18(1):8–12" href="/article/10.1007/s10055-014-0254-0#ref-CR46" id="ref-link-section-d12794e1629">2010</a>); however, as opposed to separating the signals based on spatial locations, we instead separate the signals based on color (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kim T, Ra JM, Lee JH, Moon SH, Choi KY (2011) 3d Crosstalk compensation to enhance 3d image quality of plasma display panel. IEEE Trans Consum Electr 57(4):1471–1477. doi:&#xA;                    10.1109/TCE.2011.6131113&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0254-0#ref-CR47" id="ref-link-section-d12794e1632">2011</a>). We used red and blue, as these colors have an equal number of sensors which pass through a Bayer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1976" title="Bayer, BE (1976) Color imaging array. US Patent 3,971,065" href="/article/10.1007/s10055-014-0254-0#ref-CR48" id="ref-link-section-d12794e1635">1976</a>) filter. We chose 25 locations at which to sample the cross talk amount for each of the ten columns. Three photographs were taken at each sample location for each column, with different configurations of left-eye/right-eye images: one with both images red (labeled R), one with both images blue (labeled B), and one in which one image was blue and the other was red (labeled T). To reduce indirect illumination from other displays, columns not being photographed were visually muted.</p><p>The three images were used to compute the amount of cross talk for each column at each position. The first step was to compute the amount of cross talk, done for the “red-eye image” by measuring the normalized signal loss of the red component and the normalized signal gain of the blue component across the display (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0254-0#Equ1">1</a>). The second step was to determine the amount of the opposite eye’s image seen—which should not be seen under optimal conditions—by computing the gain in blue signal normalized to the difference between the blue component of the blue and red images (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0254-0#Equ2">2</a>). Finally, we computed the cross talk amount as the sum of the red loss and blue gain (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0254-0#Equ3">3</a>).</p><p>Red Loss:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L = \frac{R_{\mathrm{r}} - T_\mathrm{r}}{R_{\mathrm{r}} - B_{\mathrm{r}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>Blue Gain:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} G = \frac{T_{\mathrm{b}} - R_{\mathrm{b}}}{B_{\mathrm{b}} - R_{\mathrm{b}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Cross talk:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} C = L + G \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig10">10</a> shows the average amount of stereo cross talk for all columns in the system. As shown, the “sweet spot”, described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec5">2.2</a>, is clearly visible at the center of the system. We found the average H value from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0254-0#Equ3">3</a> to be 0.04. For positions extremely close to the system, however, values were close to 1.0, where the majority of columns were viewed off-axis. This result is further discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec19">5.2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0254-0/MediaObjects/10055_2014_254_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>A heat map showing the amount of cross talk measured at 25 locations within the viewing ranges of the DSCVR System’s displays. Bright spots indicate low cross talk, while darker spots indicate high cross talk. Test locations and display viewing ranges are overlaid</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0254-0/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Discussion</h2><div class="c-article-section__content" id="Sec17-content"><p>We first provide a discussion of the results from the evaluation and then present a discussion of the challenges and future work.</p><h3 class="c-article__sub-heading" id="Sec18">Latency</h3><p>Tracking using the Microsoft Kinect for Windows sensor was generally acceptable. While small jitters were sometimes evident, the flexibility of Kinect made it an excellent low-cost alternative to the InterSense system. As stated above, the small, centered optimal viewing area made the use of a single Kinect a viable option. However, multiple Kinects could provide a greater coverage area and a way to further improve the quality of the tracking data. Eventually, replacing the single Kinect with the announced next-generation Kinect Heddle, featuring a higher-resolution sensor and lower latency, will likely have a substantial positive impact on the quality of the tracked user’s experience.</p><p>While the calculated latency for the DSCVR System met expectations, the determined latency for the CAVE was somewhat surprising. To verify the result, the test was performed using both TrackD and VRPN software (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology. ACM, New Year, pp 55–61" href="/article/10.1007/s10055-014-0254-0#ref-CR32" id="ref-link-section-d12794e1912">2001</a>), and was performed on multiple software infrastructures. While the InterSense tracking system specifications reported very low latencies, our estimation is the smoothing parameters enabled by default on these trackers increased their latencies substantially.</p><h3 class="c-article__sub-heading" id="Sec19">Cross talk</h3><p>The results of the cross talk test described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec16">4.3</a> adequately quantify and validate the convergence of viewing ranges we predicted during development. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0254-0#Fig10">10</a>, the measured amounts of cross talk were significantly less inside the area where all ten columns’ viewing ranges overlapped. While the measured average cross talk amount inside of this “sweet spot” was never measured at 0, the results conform to previous studies, which have shown that a range of cross talk in which 2–5 % is considered to be very good and a range of 5–8 % is considered to be acceptable (Febretti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1929">2013</a>).</p><p>Several improvements to this test could improve its accuracy, however. The photos captured for the test may have been affected by the uneven lighting in DSCVR’s installation location, leading to variations in luminance between the left half and the right half. Furthermore, light from other sources, such as the windows on the right side of the room, produced reflections on the left half’s displays, contributing to luminance and hue variations. Though we tried to minimize these effects, improvements could be made to future versions of this test.</p><p>While the sweet spot in which all displays were without cross talk was relatively small, the direct view facing the system was able to provide stereo imagery for spectators and audiences. Traditionally, CAVE systems have enabled non-tracked viewers to share experiences through incorrect viewpoints; therefore, we chose to focus on a single-user virtual experience. However, synchronizable active stereo televisions would mitigate these cross talk issues.</p><h3 class="c-article__sub-heading" id="Sec20">Comparison with existing systems</h3><p>As shown throughout this paper, virtual reality systems present a plethora of trade-offs. In this regard, comparing and contrasting different virtual reality systems is extremely difficult. In Febretti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1944">2013</a>), one metric used is cost per megapixel. We show this metric, along with overall cost and cost per immersive bandwidth (as described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab1">1</a>) CAVE, CAVE2, and DSCVR in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab3">3</a>
                  <sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The cost of various virtual reality systems for different factors (color figure online)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0254-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>However, even this simple comparison is somewhat problematic. For example, only sections of the CAVE and CAVE2 can ever be seen from a given viewpoint. On the other hand, the CAVE is the only system which provides total field of regard allowing the user to look in any direction. For the DSCVR System, we attempted to maximize the viewing characteristics from a single immersive viewpoint.</p><p>Our calculations of per-eye human vision characteristics (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab1">1</a>) show that DSCVR is competitive with similar systems, with a 3D system resolution only slightly below that of the CAVE, and a higher viewable 3D resolution than any other system evaluated. Furthermore, the immersive resolution and immersive bandwidth values show that DSCVR is, in fact, a superior high-bandwidth virtual reality environment to any of the other three systems. These performance characteristics are direct results of DSCVR’s use of higher-resolution 1080p stereo displays—perhaps an expected year-over-year improvement—and its smaller size—a deliberate choice, given the system’s design constraints. Finally, of the three systems surveyed, the DSCVR System has both the lowest cost per 3D megapixel and lowest cost per immersive bandwidth, demonstrating that a smaller, less expensive virtual reality environment can be a viable alternative to costly commercial-grade counterparts. As the quality of consumer-grade technology continues to increase—and prices continue to decrease—we expect these systems to someday become commonplace.</p><h3 class="c-article__sub-heading" id="Sec21">Challenges and future work</h3><p>Utilizing consumer-grade televisions provided many challenges in the design of the system. One of the unexpected challenges was that the consumer-grade LG displays shipped with many automatic image “optimization” features enabled by default. One particular setting, auto-stereo adjustment, uses a “depth” value to shift 3D images in the horizontal direction. When the televisions were positioned in portrait orientation, this shift instead resulted in undesirable <i>vertical</i> image shifts. This problem was solved by setting the depth value to 10, apparently the “zero depth” point on a scale from 0 to 20. Additionally, the displays had a pattern detection feature enabled by default, which would shift the images in an attempt to find an ideal disparity. As the displays had been reoriented, this option needed to be disabled. Finally, like most modern televisions, latency-inducing image processing techniques had to be disabled by switching to the “Game” picture mode.</p><p>After evaluating both infrared and HDMI-CEC control methods, RS-232 communication was chosen for display communication because it offered the simplest, best-documented control scheme for these particular LG displays. Unfortunately, the displays, such as the cluster machines, required relatively expensive USB serial adapters to access their embedded RS-232 hardware. HDMI-CEC appears to be a reluctant successor to decades-old serial control, but it is mostly a vendor proprietary protocol as of this writing. Future developments in the field of consumer electronics may lead to better documentation and standardization of this protocol.</p><p>While the LG TVs’ bezels are significantly smaller than those shown in the NexCAVE (DeFanti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011b" title="Margolis T, DeFanti TA,  Dawe G, Prudhomme A, Schulze JP, Cutchin S (2011) Low cost heads-up virtual reality (HUVR) with optical tracking and haptic feedback. IS&amp;T/SPIE Electronic Imaging, p 786417" href="/article/10.1007/s10055-014-0254-0#ref-CR23" id="ref-link-section-d12794e2010">2011b</a>), they are still noticeable. Three of the four bezels were approximately 5 mm across, but the forth bezel was five times larger, with a width of 25 mm. While professional-grade televisions can be bought without this larger bezel, the cost of these displays is over eight times that of their consumer-grade counterparts at the time of this writing.</p><p>Furthermore, higher-resolution 4K televisions have recently shown up in consumer markets. As the prices for these displays continue to fall, higher-resolution HREs will be able to be built without substantial jumps in price. In the process of designing the DSCVR system, thought was put into how to make the system accessible for future upgrades. The DSCVR’s framing design, described in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec5">2.2</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0254-0#Sec7">3.1</a>, enables columns to be easily repositioned and adapted to new display hardware, offering the possibility to swap different-sized monitors for system hardware upgrades.</p><p>Beyond TV, there is also a recent and earnest push toward consumer-grade virtual reality technology. The next-generation Kinect promises better resolution, a higher frame rate, and a more accurate sensor (Heddle). This technology will likely mitigate many of the issues raised by the utilization of a first-generation Kinect. Commodity input device technologies, such as the MYO wireless EMG armband developed by Thalmic Labs (MYO) or the STEM wireless, modular motion-tracking system (STEM System), offer new means of virtual interaction at consumer-level pricing.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Conclusion</h2><div class="c-article-section__content" id="Sec22-content"><p>In this paper, we have demonstrated the DSCVR System, a hybrid reality environment (HRE) built with commodity hardware. As part of the DSCVR System’s goals was to implement the system for under $100,000, we present a breakdown of component costs and average energy consumption in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0254-0#Tab4">4</a>. The final implementation of DSCVR cost just over $40,000 and consumes slightly more than 3 kW on average when in active use. The overall expenditures of the project, combined with the quality of the implementation, emphatically demonstrate that reasonably high-quality, large-scale HRE can be economically constructed from commodity off-the-shelf hardware.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Costs and energy consumption specifications for the DSCVR System and its individual components</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0254-0/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>While the price point for DSCVR is much too high for most consumers, it is a very reasonable price for many small business and research labs. The ability to give clients a virtual walkthrough of an environment would be extremely useful for architects, real estate agents and interior designers, to name just a few beneficiaries. While virtual reality has been used in all of these fields, the cost has generally proved too high for smaller firms, limiting their interest in and utilization of VR and 3D user interfaces. We believe that by significantly reducing the price point of larger-scale immersive display environments, lower-cost systems like DSCVR will become commonplace in the future.</p><p>As with all virtual reality systems, many trade-offs were considered during its development. While this approach has several shortcomings, such as cross talk and display bezels, the DSCVR System has comparable and sometimes better performance characteristics than commercially built systems, at a fraction of their cost. As the quality of consumer-grade technology continues to increase while prices continue to decrease, it is likely that future consumer-grade HREs, using higher-resolution displays and higher-fidelity commodity-tracking hardware, will have even better performance and lower costs than DSCVR. We see this as a democratizing trend that could enable new research and use cases in fields, industries, and businesses that have previously been priced out of using VR technology.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Livingston et al. provide evaluations for the noise, accuracy, resolution, and latency of the skeleton-tracking software provided by Kinect (Livingston et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. In: Virtual reality short papers and posters (VRW), 2012 IEEE, pp 119–120" href="/article/10.1007/s10055-014-0254-0#ref-CR31" id="ref-link-section-d12794e866">2012</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>
Febretti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE" href="/article/10.1007/s10055-014-0254-0#ref-CR1" id="ref-link-section-d12794e1957">2013</a>) list the cost per megapixel as 14,000. However, dividing the listed cost by the listed megapixels gives a number of approximately $25,000. We have chosen to use the self-reported number in the paper for our analysis.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ainsworth RA, Sandin DJ, Schulze JP, Prudhomme A, DeFanti TA, Srinivasan M (2011) Acquisition of stereo panora" /><p class="c-article-references__text" id="ref-CR37">Ainsworth RA, Sandin DJ, Schulze JP, Prudhomme A, DeFanti TA, Srinivasan M (2011) Acquisition of stereo panoramas for display in vr environments</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Amatriain, J. Kuchera-Morin, T. Hollerer, ST. Pope, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Amatriain X, Kuchera-Morin J, Hollerer T, Pope ST (2009) The allosphere: immersive multimedia for scientific d" /><p class="c-article-references__text" id="ref-CR16">Amatriain X, Kuchera-Morin J, Hollerer T, Pope ST (2009) The allosphere: immersive multimedia for scientific discovery and artistic exploration. IEEE MultiMedia 16(2):0064–75</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMMUL.2009.35" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20allosphere%3A%20immersive%20multimedia%20for%20scientific%20discovery%20and%20artistic%20exploration&amp;journal=IEEE%20MultiMedia&amp;volume=16&amp;issue=2&amp;pages=0064-75&amp;publication_year=2009&amp;author=Amatriain%2CX&amp;author=Kuchera-Morin%2CJ&amp;author=Hollerer%2CT&amp;author=Pope%2CST">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Arthur K (1996) Effects of field of view on task performance with head-mounted displays. In: Conference compan" /><p class="c-article-references__text" id="ref-CR42">Arthur K (1996) Effects of field of view on task performance with head-mounted displays. In: Conference companion on human factors in computing systems. ACM, New York, pp 29–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avery B, Thomas BH, Velikovsky J, Piekarski W (2005) Outdoor augmented reality gaming on five dollars a day. I" /><p class="c-article-references__text" id="ref-CR4">Avery B, Thomas BH, Velikovsky J, Piekarski W (2005) Outdoor augmented reality gaming on five dollars a day. In: Proceedings of the 6th Australasian conference on User interface—volume 40, AUIC ’05, pp 79–88. Australian Computer Society Inc, Darlinghurst, Australia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bacim F, Ragan E, Scerbo S, Polys NF, Setareh M, Jones BD (2013) The effects of display fidelity, visual compl" /><p class="c-article-references__text" id="ref-CR7">Bacim F, Ragan E, Scerbo S, Polys NF, Setareh M, Jones BD (2013) The effects of display fidelity, visual complexity, and task scope on spatial understanding of 3d graphs. In: Proceedings of graphics interface 2013, GI ’13, pp 25–32. Canadian Information Processing Society, Toronto, Ont., Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Basu A, Saupe C, Refour E, Raij A, Johnsen K (2012) Immersive 3dui on one dollar a day. In: 2012 IEEE symposiu" /><p class="c-article-references__text" id="ref-CR3">Basu A, Saupe C, Refour E, Raij A, Johnsen K (2012) Immersive 3dui on one dollar a day. In: 2012 IEEE symposium on 3D user interfaces (3DUI), pp 97–100</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bayer, BE (1976) Color imaging array. US Patent 3,971,065" /><p class="c-article-references__text" id="ref-CR48">Bayer, BE (1976) Color imaging array. US Patent 3,971,065</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, RP. McMahan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" /><p class="c-article-references__text" id="ref-CR5">Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2007.257" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20how%20much%20immersion%20is%20enough%3F&amp;journal=Computer&amp;volume=40&amp;issue=7&amp;pages=36-43&amp;publication_year=2007&amp;author=Bowman%2CDA&amp;author=McMahan%2CRP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Clark, YH. Pua, K. Fortin, C. Ritchie, KE. Webster, L. Denehy, AL. Bryant, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Clark RA, Pua YH, Fortin K, Ritchie C, Webster KE, Denehy L, Bryant AL (2012) Validity of the microsoft kinect" /><p class="c-article-references__text" id="ref-CR30">Clark RA, Pua YH, Fortin K, Ritchie C, Webster KE, Denehy L, Bryant AL (2012) Validity of the microsoft kinect for assessment of postural control. Gait Posture 36(3):372–377</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.gaitpost.2012.03.033" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Validity%20of%20the%20microsoft%20kinect%20for%20assessment%20of%20postural%20control&amp;journal=Gait%20Posture&amp;volume=36&amp;issue=3&amp;pages=372-377&amp;publication_year=2012&amp;author=Clark%2CRA&amp;author=Pua%2CYH&amp;author=Fortin%2CK&amp;author=Ritchie%2CC&amp;author=Webster%2CKE&amp;author=Denehy%2CL&amp;author=Bryant%2CAL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. CruzNeira, DJ. Sandin, TA. DeFanti, RV. Kenyon, JC. Hart, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="CruzNeira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave: audio visual experience automatic virt" /><p class="c-article-references__text" id="ref-CR15">CruzNeira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F129888.129892" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cave%3A%20audio%20visual%20experience%20automatic%20virtual%20environment&amp;journal=Commun%20ACM&amp;volume=35&amp;issue=6&amp;pages=64-72&amp;publication_year=1992&amp;author=CruzNeira%2CC&amp;author=Sandin%2CDJ&amp;author=DeFanti%2CTA&amp;author=Kenyon%2CRV&amp;author=Hart%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and im" /><p class="c-article-references__text" id="ref-CR14">Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the cave. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, pp 135–142</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TA. DeFanti, D. Acevedo, RA. Ainsworth, MD. Brown, S. Cutchin, G. Dawe, KU. Doerr, A. Johnson, C. Knox, R. Kooima, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="DeFanti TA, Acevedo D, Ainsworth RA, Brown MD, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R et al " /><p class="c-article-references__text" id="ref-CR22">DeFanti TA, Acevedo D, Ainsworth RA, Brown MD, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R et al (2011a) The future of the cave. Cent Eur J Eng 1(1):16–37</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2478%2Fs13531-010-0002-5" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20future%20of%20the%20cave&amp;journal=Cent%20Eur%20J%20Eng&amp;volume=1&amp;issue=1&amp;pages=16-37&amp;publication_year=2011&amp;author=DeFanti%2CTA&amp;author=Acevedo%2CD&amp;author=Ainsworth%2CRA&amp;author=Brown%2CMD&amp;author=Cutchin%2CS&amp;author=Dawe%2CG&amp;author=Doerr%2CKU&amp;author=Johnson%2CA&amp;author=Knox%2CC&amp;author=Kooima%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. DeFanti, D. Acevedo, R. Ainsworth, M. Brown, S. Cutchin, G. Dawe, KU. Doerr, A. Johnson, C. Knox, R. Kooima, F. Kuester, J. Leigh, L. Long, P. Otto, V. Petrovic, K. Ponto, A. Prudhomme, R. Rao, L. Renambot, D. Sandin, J. Schulze, L. Smarr, M. Srinivasan, P. Weber, G. Wickham, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="DeFanti T, Acevedo D, Ainsworth R, Brown M, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R, Kuester " /><p class="c-article-references__text" id="ref-CR25">DeFanti T, Acevedo D, Ainsworth R, Brown M, Cutchin S, Dawe G, Doerr KU, Johnson A, Knox C, Kooima R, Kuester F, Leigh J, Long L, Otto P, Petrovic V, Ponto K, Prudhomme A, Rao R, Renambot L, Sandin D, Schulze J, Smarr L, Srinivasan M, Weber P, Wickham G (2011b) The future of the cave. Cent Eur J Eng 1(1):16–37</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2478%2Fs13531-010-0002-5" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20future%20of%20the%20cave&amp;journal=Cent%20Eur%20J%20Eng&amp;volume=1&amp;issue=1&amp;pages=16-37&amp;publication_year=2011&amp;author=DeFanti%2CT&amp;author=Acevedo%2CD&amp;author=Ainsworth%2CR&amp;author=Brown%2CM&amp;author=Cutchin%2CS&amp;author=Dawe%2CG&amp;author=Doerr%2CKU&amp;author=Johnson%2CA&amp;author=Knox%2CC&amp;author=Kooima%2CR&amp;author=Kuester%2CF&amp;author=Leigh%2CJ&amp;author=Long%2CL&amp;author=Otto%2CP&amp;author=Petrovic%2CV&amp;author=Ponto%2CK&amp;author=Prudhomme%2CA&amp;author=Rao%2CR&amp;author=Renambot%2CL&amp;author=Sandin%2CD&amp;author=Schulze%2CJ&amp;author=Smarr%2CL&amp;author=Srinivasan%2CM&amp;author=Weber%2CP&amp;author=Wickham%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Doerr, F. Kuester, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Doerr K, Kuester F (2011) Cglx: a scalable, high-performance visualization framework for networked display env" /><p class="c-article-references__text" id="ref-CR33">Doerr K, Kuester F (2011) Cglx: a scalable, high-performance visualization framework for networked display environments. IEEE Trans Vis Comput Graph 17(3):320–332</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2010.59" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cglx%3A%20a%20scalable%2C%20high-performance%20visualization%20framework%20for%20networked%20display%20environments&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=17&amp;issue=3&amp;pages=320-332&amp;publication_year=2011&amp;author=Doerr%2CK&amp;author=Kuester%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Eilemann, M. Makhinya, R. Pajarola, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Eilemann S, Makhinya M, Pajarola R (2009) Equalizer: a scalable parallel rendering framework. IEEE Trans Vis C" /><p class="c-article-references__text" id="ref-CR34">Eilemann S, Makhinya M, Pajarola R (2009) Equalizer: a scalable parallel rendering framework. IEEE Trans Vis Comput Graph 15(3):436–452</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2008.104" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Equalizer%3A%20a%20scalable%20parallel%20rendering%20framework&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=15&amp;issue=3&amp;pages=436-452&amp;publication_year=2009&amp;author=Eilemann%2CS&amp;author=Makhinya%2CM&amp;author=Pajarola%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sand" /><p class="c-article-references__text" id="ref-CR1">Febretti A, Nishimoto A, Thigpen T, Talandis J, Long L, Pirtle JD, Peterka T, Verlo A, Brown M, Plepys D, Sandin D, Renambot L, Johnson A, Leigh J (2013) CAVE2: a hybrid reality environment for immersive simulation and information analysis. In: M Dolinsky, IE McDowall (eds) IS&amp;T/SPIE Electronic Imaging, pp 864903–864903–12. SPIE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heddle B. The New Generation Kinect for Windows Sensor is Coming Next Year - Kinect for Windows Product Blog—S" /><p class="c-article-references__text" id="ref-CR49">Heddle B. The New Generation Kinect for Windows Sensor is Coming Next Year - Kinect for Windows Product Blog—Site Home—MSDN Blogs. <a href="http://blogs.msdn.com/b/kinectforwindows/archive/2013/05/23/the-new-generation-kinect-for-windows-sensor-is-coming-next-year.aspx">http://blogs.msdn.com/b/kinectforwindows/archive/2013/05/23/the-new-generation-kinect-for-windows-sensor-is-coming-next-year.aspx</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Higgins T (2010) Unity-3d game engine" /><p class="c-article-references__text" id="ref-CR38">Higgins T (2010) Unity-3d game engine</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Hong, J. Jang, D. Lee, M. Lim, H. Shin, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Hong H, Jang J, Lee D, Lim M, Shin H (2010) Analysis of angular dependence of 3-d technology using polarized e" /><p class="c-article-references__text" id="ref-CR46">Hong H, Jang J, Lee D, Lim M, Shin H (2010) Analysis of angular dependence of 3-d technology using polarized eyeglasses. J Soc Inf Disp 18(1):8–12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1889%2FJSID18.1.8" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20angular%20dependence%20of%203-d%20technology%20using%20polarized%20eyeglasses&amp;journal=J%20Soc%20Inf%20Disp&amp;volume=18&amp;issue=1&amp;pages=8-12&amp;publication_year=2010&amp;author=Hong%2CH&amp;author=Jang%2CJ&amp;author=Lee%2CD&amp;author=Lim%2CM&amp;author=Shin%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Humphrey, A. Dalke, K. Schulten, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Humphrey W, Dalke A, Schulten K (1996) Vmd: visual molecular dynamics. J Mol Graph 14(1):33–38" /><p class="c-article-references__text" id="ref-CR40">Humphrey W, Dalke A, Schulten K (1996) Vmd: visual molecular dynamics. J Mol Graph 14(1):33–38</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0263-7855%2896%2900018-5" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vmd%3A%20visual%20molecular%20dynamics&amp;journal=J%20Mol%20Graph&amp;volume=14&amp;issue=1&amp;pages=33-38&amp;publication_year=1996&amp;author=Humphrey%2CW&amp;author=Dalke%2CA&amp;author=Schulten%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson GP, Abram GD, Westing B, Navr’til P, Gaither K (2012) Displaycluster: an interactive visualization env" /><p class="c-article-references__text" id="ref-CR20">Johnson GP, Abram GD, Westing B, Navr’til P, Gaither K (2012) Displaycluster: an interactive visualization environment for tiled displays. In: 2012 IEEE international conference on cluster computing (CLUSTER), pp 239–247. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Kim, JM. Ra, JH. Lee, SH. Moon, KY. Choi, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Kim T, Ra JM, Lee JH, Moon SH, Choi KY (2011) 3d Crosstalk compensation to enhance 3d image quality of plasma " /><p class="c-article-references__text" id="ref-CR47">Kim T, Ra JM, Lee JH, Moon SH, Choi KY (2011) 3d Crosstalk compensation to enhance 3d image quality of plasma display panel. IEEE Trans Consum Electr 57(4):1471–1477. doi:<a href="https://doi.org/10.1109/TCE.2011.6131113">10.1109/TCE.2011.6131113</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTCE.2011.6131113" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3d%20Crosstalk%20compensation%20to%20enhance%203d%20image%20quality%20of%20plasma%20display%20panel&amp;journal=IEEE%20Trans%20Consum%20Electr&amp;doi=10.1109%2FTCE.2011.6131113&amp;volume=57&amp;issue=4&amp;pages=1471-1477&amp;publication_year=2011&amp;author=Kim%2CT&amp;author=Ra%2CJM&amp;author=Lee%2CJH&amp;author=Moon%2CSH&amp;author=Choi%2CKY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knox C, Brown M, Doerr K, Jenks S, Zender C, Kuester F (2005) Simultaneous visualization of the ipcc ar4 model" /><p class="c-article-references__text" id="ref-CR18">Knox C, Brown M, Doerr K, Jenks S, Zender C, Kuester F (2005) Simultaneous visualization of the ipcc ar4 model ensemble on an extremely high resolution display wall (hiperwall). In: AGU fall meeting abstracts, 1:1140</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Laha, K. Sensharma, J. Schiffbauer, D. Bowman, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Laha B, Sensharma K, Schiffbauer J, Bowman D (2012) Effects of immersion on visual analysis of volume data. IE" /><p class="c-article-references__text" id="ref-CR8">Laha B, Sensharma K, Schiffbauer J, Bowman D (2012) Effects of immersion on visual analysis of volume data. IEEE Trans Vis Comput Gr 18(4):597–606. doi:<a href="https://doi.org/10.1109/TVCG.2012.42">10.1109/TVCG.2012.42</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2012.42" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20immersion%20on%20visual%20analysis%20of%20volume%20data&amp;journal=IEEE%20Trans%20Vis%20Comput%20Gr&amp;doi=10.1109%2FTVCG.2012.42&amp;volume=18&amp;issue=4&amp;pages=597-606&amp;publication_year=2012&amp;author=Laha%2CB&amp;author=Sensharma%2CK&amp;author=Schiffbauer%2CJ&amp;author=Bowman%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Lange, S. Koenig, CY. Chang, E. McConnell, E. Suma, M. Bolas, A. Rizzo, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Lange B, Koenig S, Chang CY, McConnell E, Suma E, Bolas M, Rizzo A (2012) Designing informed game-based rehabi" /><p class="c-article-references__text" id="ref-CR13">Lange B, Koenig S, Chang CY, McConnell E, Suma E, Bolas M, Rizzo A (2012) Designing informed game-based rehabilitation tasks leveraging advances in virtual reality. Disabil Rehabil 34(22):1863–1870</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3109%2F09638288.2012.670029" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Designing%20informed%20game-based%20rehabilitation%20tasks%20leveraging%20advances%20in%20virtual%20reality&amp;journal=Disabil%20Rehabil&amp;volume=34&amp;issue=22&amp;pages=1863-1870&amp;publication_year=2012&amp;author=Lange%2CB&amp;author=Koenig%2CS&amp;author=Chang%2CCY&amp;author=McConnell%2CE&amp;author=Suma%2CE&amp;author=Bolas%2CM&amp;author=Rizzo%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Leigh, A. Johnson, L. Renambot, T. Peterka, B. Jeong, DJ. Sandin, J. Talandis, R. Jagodic, S. Nam, H. Hur, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Leigh J, Johnson A, Renambot L, Peterka T, Jeong B, Sandin DJ, Talandis J, Jagodic R, Nam S, Hur H et al (2013" /><p class="c-article-references__text" id="ref-CR35">Leigh J, Johnson A, Renambot L, Peterka T, Jeong B, Sandin DJ, Talandis J, Jagodic R, Nam S, Hur H et al (2013) Scalable resolution display walls. Proc IEEE 101(1):115–129</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJPROC.2012.2191609" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%20resolution%20display%20walls&amp;journal=Proc%20IEEE&amp;volume=101&amp;issue=1&amp;pages=115-129&amp;publication_year=2013&amp;author=Leigh%2CJ&amp;author=Johnson%2CA&amp;author=Renambot%2CL&amp;author=Peterka%2CT&amp;author=Jeong%2CB&amp;author=Sandin%2CDJ&amp;author=Talandis%2CJ&amp;author=Jagodic%2CR&amp;author=Nam%2CS&amp;author=Hur%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. I" /><p class="c-article-references__text" id="ref-CR31">Livingston M, Sebastian J, Ai Z, Decker J (2012) Performance measurements for the microsoft kinect skeleton. In: Virtual reality short papers and posters (VRW), 2012 IEEE, pp 119–120</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Luo, K. Qin, Y. Zhou, M. Mao, R. Li, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Luo J, Qin K, Zhou Y, Mao M, Li R (2010) Gpu rendering for tiled multi-projector autostereoscopic display base" /><p class="c-article-references__text" id="ref-CR36">Luo J, Qin K, Zhou Y, Mao M, Li R (2010) Gpu rendering for tiled multi-projector autostereoscopic display based on chromium. Vis Comput 26(6–8):457–465</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-010-0479-1" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gpu%20rendering%20for%20tiled%20multi-projector%20autostereoscopic%20display%20based%20on%20chromium&amp;journal=Vis%20Comput&amp;volume=26&amp;issue=6%E2%80%938&amp;pages=457-465&amp;publication_year=2010&amp;author=Luo%2CJ&amp;author=Qin%2CK&amp;author=Zhou%2CY&amp;author=Mao%2CM&amp;author=Li%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Margolis T, DeFanti TA,  Dawe G, Prudhomme A, Schulze JP, Cutchin S (2011) Low cost heads-up virtual reality (" /><p class="c-article-references__text" id="ref-CR23">Margolis T, DeFanti TA,  Dawe G, Prudhomme A, Schulze JP, Cutchin S (2011) Low cost heads-up virtual reality (HUVR) with optical tracking and haptic feedback. IS&amp;T/SPIE Electronic Imaging, p 786417</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RP. McMahan, DA. Bowman, DJ. Zielinski, RB. Brady, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="McMahan RP, Bowman DA, Zielinski DJ, Brady RB (2012) Evaluating display fidelity and interaction fidelity in a" /><p class="c-article-references__text" id="ref-CR11">McMahan RP, Bowman DA, Zielinski DJ, Brady RB (2012) Evaluating display fidelity and interaction fidelity in a virtual reality game. IEEE Trans Vis Comput Graph 18(4):626–633</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2012.43" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20display%20fidelity%20and%20interaction%20fidelity%20in%20a%20virtual%20reality%20game&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=18&amp;issue=4&amp;pages=626-633&amp;publication_year=2012&amp;author=McMahan%2CRP&amp;author=Bowman%2CDA&amp;author=Zielinski%2CDJ&amp;author=Brady%2CRB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Meyer-Spradow, T. Ropinski, J. Mensmann, K. Hinrichs, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Meyer-Spradow J, Ropinski T, Mensmann J, Hinrichs K (2009) Voreen: a rapid-prototyping environment for ray-cas" /><p class="c-article-references__text" id="ref-CR39">Meyer-Spradow J, Ropinski T, Mensmann J, Hinrichs K (2009) Voreen: a rapid-prototyping environment for ray-casting-based volume visualizations. IEEE Comput Graph Appl 29(6):6–13</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2009.130" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Voreen%3A%20a%20rapid-prototyping%20environment%20for%20ray-casting-based%20volume%20visualizations&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=29&amp;issue=6&amp;pages=6-13&amp;publication_year=2009&amp;author=Meyer-Spradow%2CJ&amp;author=Ropinski%2CT&amp;author=Mensmann%2CJ&amp;author=Hinrichs%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="MYO—Gesture control armband by Thalmic Labs. https://www.thalmic.com/en/myo/&#xA;                " /><p class="c-article-references__text" id="ref-CR51">MYO—Gesture control armband by Thalmic Labs. <a href="https://www.thalmic.com/en/myo/">https://www.thalmic.com/en/myo/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pausch R (1991) Virtual reality on five dollars a day. Proceedings of the SIGCHI conference on human factors i" /><p class="c-article-references__text" id="ref-CR2">Pausch R (1991) Virtual reality on five dollars a day. Proceedings of the SIGCHI conference on human factors in computing systems, CHI ’91. ACM, New York, NY, USA, pp 265–270</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Polys NF, Kim S, Bowman DA (2007) Effects of information layout, screen size, and field of view on user perfor" /><p class="c-article-references__text" id="ref-CR10">Polys NF, Kim S, Bowman DA (2007) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. Comput Animat Virtual Worlds 18(1):19–38. doi:<a href="https://doi.org/10.1002/cav.159">10.1002/cav.159</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network ba" /><p class="c-article-references__text" id="ref-CR28">Ponto K, Wypych T, Doerr K, Yamaoka S, Kimball J, Kuester F (2009) Videoblaster: a distributed, low-network bandwidth method for multimedia playback on tiled display systems. In: 11th IEEE international symposium on multimedia, 2009. ISM’09, pp 201–206. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Ponto, K. Doerr, F. Kuester, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massivel" /><p class="c-article-references__text" id="ref-CR19">Ponto K, Doerr K, Kuester F (2010) Giga-stack: a method for visualizing giga-pixel layered imagery on massively tiled displays. Future Gener Comput Syst 26(5):693–700</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.future.2009.12.007" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Giga-stack%3A%20a%20method%20for%20visualizing%20giga-pixel%20layered%20imagery%20on%20massively%20tiled%20displays&amp;journal=Future%20Gener%20Comput%20Syst&amp;volume=26&amp;issue=5&amp;pages=693-700&amp;publication_year=2010&amp;author=Ponto%2CK&amp;author=Doerr%2CK&amp;author=Kuester%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Prabhat, A. Forsberg, M. Katzourin, K. Wharton, M. Slater, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cav" /><p class="c-article-references__text" id="ref-CR6">Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets. IEEE Trans Vis Comput Gr 14(3):551–563. doi:<a href="https://doi.org/10.1109/TVCG.2007.70433">10.1109/TVCG.2007.70433</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2007.70433" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20desktop%2C%20fishtank%2C%20and%20cave%20systems%20for%20the%20exploration%20of%20volume%20rendered%20confocal%20data%20sets&amp;journal=IEEE%20Trans%20Vis%20Comput%20Gr&amp;doi=10.1109%2FTVCG.2007.70433&amp;volume=14&amp;issue=3&amp;pages=551-563&amp;publication_year=2008&amp;author=Prabhat%2C&amp;author=Forsberg%2CA&amp;author=Katzourin%2CM&amp;author=Wharton%2CK&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Ragan, R. Kopper, P. Schuchardt, D. Bowman, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Ragan E, Kopper R, Schuchardt P, Bowman D (2013) Studying the effects of stereo, head tracking, and field of r" /><p class="c-article-references__text" id="ref-CR9">Ragan E, Kopper R, Schuchardt P, Bowman D (2013) Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task. IEEE Trans Vis Comput Gr 19(5):886–896. doi:<a href="https://doi.org/10.1109/TVCG.2012.163">10.1109/TVCG.2012.163</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2012.163" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Studying%20the%20effects%20of%20stereo%2C%20head%20tracking%2C%20and%20field%20of%20regard%20on%20a%20small-scale%20spatial%20judgment%20task&amp;journal=IEEE%20Trans%20Vis%20Comput%20Gr&amp;doi=10.1109%2FTVCG.2012.163&amp;volume=19&amp;issue=5&amp;pages=886-896&amp;publication_year=2013&amp;author=Ragan%2CE&amp;author=Kopper%2CR&amp;author=Schuchardt%2CP&amp;author=Bowman%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rash C, McLean W, Mozo B, Licina J, McEntire B (1999) Human factors and performance concerns for the design of" /><p class="c-article-references__text" id="ref-CR43">Rash C, McLean W, Mozo B, Licina J, McEntire B (1999) Human factors and performance concerns for the design of helmet-mounted displays. In: RTO HFM symposium on current aeromedical issues in rotary wing operation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Renambot, B. Jeong, H. Hur, A. Johnson, J. Leigh, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Renambot L, Jeong B, Hur H, Johnson A, Leigh J (2009) Enabling high resolution collaborative visualization in " /><p class="c-article-references__text" id="ref-CR29">Renambot L, Jeong B, Hur H, Johnson A, Leigh J (2009) Enabling high resolution collaborative visualization in display rich virtual organizations. Future Gener Comput Syst 25(2):161–168</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.future.2008.07.004" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Enabling%20high%20resolution%20collaborative%20visualization%20in%20display%20rich%20virtual%20organizations&amp;journal=Future%20Gener%20Comput%20Syst&amp;volume=25&amp;issue=2&amp;pages=161-168&amp;publication_year=2009&amp;author=Renambot%2CL&amp;author=Jeong%2CB&amp;author=Hur%2CH&amp;author=Johnson%2CA&amp;author=Leigh%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="MB. Rosson, JM. Carroll, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Rosson MB, Carroll JM (2001) Usability engineering: scenario-based development of human-computer interaction. " /><p class="c-article-references__text" id="ref-CR24">Rosson MB, Carroll JM (2001) Usability engineering: scenario-based development of human-computer interaction. Elsevier, Amsterdam</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Usability%20engineering%3A%20scenario-based%20development%20of%20human-computer%20interaction&amp;publication_year=2001&amp;author=Rosson%2CMB&amp;author=Carroll%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sampaio PN, de Freitas RIC, Cardoso GNP (2008) Ogre-multimedia: an api for the design of multimedia and virtua" /><p class="c-article-references__text" id="ref-CR41">Sampaio PN, de Freitas RIC, Cardoso GNP (2008) Ogre-multimedia: an api for the design of multimedia and virtual reality applications. In: Knowledge-based intelligent information and engineering systems. Springer, New York, pp 465–472</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schou T, Gardner HJ (2007) A Wii remote, a game engine, five sensor bars and a virtual reality theatre. In: OZ" /><p class="c-article-references__text" id="ref-CR12">Schou T, Gardner HJ (2007) A Wii remote, a game engine, five sensor bars and a virtual reality theatre. In: OZCHI ’07. ACM Press, New York, pp 231–234</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Shupp, C. Andrews, M. Dickey-Kurdziolek, B. Yost, C. North, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Shupp L, Andrews C, Dickey-Kurdziolek M, Yost B, North C (2009) Shaping the display of the future: the effects" /><p class="c-article-references__text" id="ref-CR26">Shupp L, Andrews C, Dickey-Kurdziolek M, Yost B, North C (2009) Shaping the display of the future: the effects of display size and curvature on user performance and insights. Hum Comput Interact 24(1–2):230–272</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F07370020902739429" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Shaping%20the%20display%20of%20the%20future%3A%20the%20effects%20of%20display%20size%20and%20curvature%20on%20user%20performance%20and%20insights&amp;journal=Hum%20Comput%20Interact&amp;volume=24&amp;issue=1%E2%80%932&amp;pages=230-272&amp;publication_year=2009&amp;author=Shupp%2CL&amp;author=Andrews%2CC&amp;author=Dickey-Kurdziolek%2CM&amp;author=Yost%2CB&amp;author=North%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simon A, Gobel M (2002) The i-cone trade;—a panoramic display system for virtual environments. In: Proceedings" /><p class="c-article-references__text" id="ref-CR17">Simon A, Gobel M (2002) The i-cone trade;—a panoramic display system for virtual environments. In: Proceedings of the 10th Pacific conference on computer graphics and applications, 2002, pp 3–7. doi:<a href="https://doi.org/10.1109/PCCGA.2002.1167834">10.1109/PCCGA.2002.1167834</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="STEM System: The Best Way to Interact with Virtual Worlds by Sixense—Kickstarter. http://www.kickstarter.com/p" /><p class="c-article-references__text" id="ref-CR52">STEM System: The Best Way to Interact with Virtual Worlds by Sixense—Kickstarter. <a href="http://www.kickstarter.com/projects/89577853/stem-system-the-best-way-to-interact-with-virtual">http://www.kickstarter.com/projects/89577853/stem-system-the-best-way-to-interact-with-virtual</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-tr" /><p class="c-article-references__text" id="ref-CR32">Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology. ACM, New Year, pp 55–61</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Teather RJ, Pavlovych A, Stuerzlinger W, MacKenzie IS (2009) Effects of tracking technology, latency, and spat" /><p class="c-article-references__text" id="ref-CR45">Teather RJ, Pavlovych A, Stuerzlinger W, MacKenzie IS (2009) Effects of tracking technology, latency, and spatial jitter on object movement. In: IEEE symposium on 3D user interfaces, 2009. 3DUI 2009, pp 43–50. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tracked THE Device Driver Software for Immersive Displays. http://www.mechdyne.com/trackd.aspx&#xA;               " /><p class="c-article-references__text" id="ref-CR50">Tracked THE Device Driver Software for Immersive Displays. <a href="http://www.mechdyne.com/trackd.aspx">http://www.mechdyne.com/trackd.aspx</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Wells, M. Venturino, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Wells MJ, Venturino M (1990) Performance and head movements using a helmet-mounted display with different size" /><p class="c-article-references__text" id="ref-CR44">Wells MJ, Venturino M (1990) Performance and head movements using a helmet-mounted display with different sized fields-of-view. Opt Eng 29(8):870–877</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1117%2F12.55672" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance%20and%20head%20movements%20using%20a%20helmet-mounted%20display%20with%20different%20sized%20fields-of-view&amp;journal=Opt%20Eng&amp;volume=29&amp;issue=8&amp;pages=870-877&amp;publication_year=1990&amp;author=Wells%2CMJ&amp;author=Venturino%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SC. Williams, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Williams SC (2013) Immersive visualization. Proc Natl Acad Sci 110(12):4438–4438" /><p class="c-article-references__text" id="ref-CR21">Williams SC (2013) Immersive visualization. Proc Natl Acad Sci 110(12):4438–4438</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1302989110" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20visualization&amp;journal=Proc%20Natl%20Acad%20Sci&amp;volume=110&amp;issue=12&amp;pages=4438-4438&amp;publication_year=2013&amp;author=Williams%2CSC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Woods A (2010) Understanding crosstalk in stereoscopic displays. In: Keynote presentation at the three-dimensi" /><p class="c-article-references__text" id="ref-CR27">Woods A (2010) Understanding crosstalk in stereoscopic displays. In: Keynote presentation at the three-dimensional systems and applications conference. Tokyo, Japan, pp 19–21</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0254-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We would like to acknowledge the support of the Living Environments Laboratory, the School of Human Ecology, the UW–Madison Graduate School and the Wisconsin Institute for Discovery. We would specifically like to thank Vito Freese for his assistance with installation and Patricia Brennan, Kendra Kreutz, Andrew Wagner, John Hilgers, and Roberto Rengel for their support and assistance in this project.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Wisconsin Institute for Discovery, Room 3176, 330 N. Orchard Street, Madison, WI, 53715, USA</p><p class="c-article-author-affiliation__authors-list">Kevin Ponto</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Wisconsin Institute for Discovery, 330 N. Orchard Street, Madison, WI, 53715, USA</p><p class="c-article-author-affiliation__authors-list">Joe Kohlmann</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Wisconsin Institute for Discovery, Room 1144B, 330 N. Orchard Street, Madison, WI, 53715, USA</p><p class="c-article-author-affiliation__authors-list">Ross Tredinnick</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Kevin-Ponto"><span class="c-article-authors-search__title u-h3 js-search-name">Kevin Ponto</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kevin+Ponto&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kevin+Ponto" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kevin+Ponto%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Joe-Kohlmann"><span class="c-article-authors-search__title u-h3 js-search-name">Joe Kohlmann</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Joe+Kohlmann&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Joe+Kohlmann" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joe+Kohlmann%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ross-Tredinnick"><span class="c-article-authors-search__title u-h3 js-search-name">Ross Tredinnick</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ross+Tredinnick&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ross+Tredinnick" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ross+Tredinnick%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0254-0/email/correspondent/c1/new">Kevin Ponto</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=DSCVR%3A%20designing%20a%20commodity%20hybrid%20virtual%20reality%20system&amp;author=Kevin%20Ponto%20et%20al&amp;contentID=10.1007%2Fs10055-014-0254-0&amp;publication=1359-4338&amp;publicationDate=2014-11-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0254-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0254-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ponto, K., Kohlmann, J. &amp; Tredinnick, R. DSCVR: designing a commodity hybrid virtual reality system.
                    <i>Virtual Reality</i> <b>19, </b>57–70 (2015). https://doi.org/10.1007/s10055-014-0254-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0254-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-01-06">06 January 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-10-02">02 October 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11-13">13 November 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-03">March 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0254-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0254-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Hybrid reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Display wall</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Commodity hardware</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D</span></li><li class="c-article-subject-list__subject"><span itemprop="about">High resolution</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Passive stereo</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0254-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=254;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

