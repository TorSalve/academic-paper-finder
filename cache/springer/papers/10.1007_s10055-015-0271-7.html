<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Sounding fire for immersive virtual reality"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fire sound that is synchronized with..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Sounding fire for immersive virtual reality"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-08-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fire sound that is synchronized with physically based fire animations. We divide fire sound into two parts: low frequency and mid- to high frequency, and use two processes to separately synthesize these two parts. By simplifying calculations using a novel combustion sound model as well as leveraging GPU parallel computing in a marching-cube-like manner, our method speeds up the computation of low-frequency part by an order of magnitude. To run the time-stepping fire simulation at a relative low frequency rather than the audio rate, we add synchronized mid- and high-frequency wavelet details to low-frequency simulation contents with a post-process to generate complete fire sound. We validated our method with various experiments to build a solid physically based basis for real-time acoustic rendering that can be used for immersive virtual reality scenarios."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-08-12"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="291"/>

    <meta name="prism.endingPage" content="302"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0271-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0271-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0271-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0271-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Sounding fire for immersive virtual reality"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2015/11"/>

    <meta name="citation_online_date" content="2015/08/12"/>

    <meta name="citation_firstpage" content="291"/>

    <meta name="citation_lastpage" content="302"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0271-7"/>

    <meta name="DOI" content="10.1007/s10055-015-0271-7"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0271-7"/>

    <meta name="description" content="Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fir"/>

    <meta name="dc.creator" content="Shiguang Liu"/>

    <meta name="dc.creator" content="Zhuojun Yu"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Combust Explos Shock Waves; citation_title=Acoustic noise in turbulent flames; citation_author=DI Abugov, OI Obrezkov; citation_volume=14; citation_issue=5; citation_publication_date=1978; citation_pages=606-612; citation_doi=10.1007/BF00789719; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Animating fire with sound; citation_author=JN Chadwick, DL James; citation_volume=30; citation_issue=4; citation_publication_date=2011; citation_pages=1-8; citation_doi=10.1145/2010324.1964979; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Electron Lett; citation_title=FFT-based interpolation for multipath detection in GPS/GLONASS receivers; citation_author=A Coenen, A Vos; citation_volume=28; citation_issue=19; citation_publication_date=1992; citation_pages=1787-1788; citation_doi=10.1049/el:19921139; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Wavelet noise; citation_author=RL Cook, T DeRose; citation_volume=24; citation_issue=3; citation_publication_date=2005; citation_pages=803-811; citation_doi=10.1145/1073204.1073264; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Modern methods in analytical acoustics; citation_publication_date=1992; citation_id=CR5; citation_author=DG Crighton; citation_author=AP Dowling; citation_author=JE Ffwocs Williams; citation_author=MA Heckl; citation_author=FA Leppington; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics; citation_author=Y Dobashi, T Yamamoto, T Nishita; citation_volume=22; citation_issue=3; citation_publication_date=2003; citation_pages=539-545; citation_doi=10.1145/882262.882339; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Synthesizing sound from turbulent fields using sound textures for interactive fluid simulation; citation_author=Y Dobashi, T Yamamoto, T Nishita; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=736-744; citation_doi=10.1111/j.1467-8659.2004.00785.x; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph; citation_title=Example-based super-resolution; citation_author=WT Freeman, TR Jones, EC Pasztor; citation_volume=22; citation_issue=2; citation_publication_date=2002; citation_pages=56-65; citation_doi=10.1109/38.988747; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Image analogies; citation_author=A Hertzmann, CE Jacobs, N Oliver, B Curless, D Salesin; citation_volume=20; citation_issue=3; citation_publication_date=2001; citation_pages=327-340; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Combust Instit; citation_title=Radiation of noise in turbulent non-premixed flames; citation_author=M Ihme, H Pitsch, D Bodony; citation_volume=32; citation_issue=1; citation_publication_date=2009; citation_pages=1545-1553; citation_doi=10.1016/j.proci.2008.06.137; citation_id=CR10"/>

    <meta name="citation_reference" content="Imura M, Nakano Y, Yasumuro Y, Manabe Y, Chihara K (2007), Real-time generation of CG and sound of liquid with bubble. In Proceedings of SIGGRAPH posters"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Process; citation_title=Cubic convolution interpolation for digital image processing; citation_author=RG Keys; citation_volume=29; citation_issue=6; citation_publication_date=1981; citation_pages=1153-1160; citation_doi=10.1109/TASSP.1981.1163711; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Wavelet turbulence for fluid simulation; citation_author=T Kim, N Thurey, D James, M Gross; citation_volume=27; citation_issue=3; citation_publication_date=2008; citation_pages=50; citation_doi=10.1145/1360612.1360649; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Trans Edutain; citation_title=Physically based simulation of solid objects&#8217; burning; citation_author=S Liu, T An, Z Gong, I Hagiwara; citation_volume=7; citation_publication_date=2012; citation_pages=110-120; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A theory for multiresolution signal decomposition: the wavelet representation; citation_author=SG Mallat; citation_volume=11; citation_issue=7; citation_publication_date=1989; citation_pages=674-693; citation_doi=10.1109/34.192463; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Process; citation_title=Time-frequency synthesis of noisy sounds with narrow spectral components; citation_author=D Marelli, M Aramaki, R Kronland-Martinet, C Verron; citation_volume=18; citation_issue=8; citation_publication_date=2010; citation_pages=399-414; citation_doi=10.1109/TASL.2010.2040532; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Reconstruction filters in computer graphics; citation_author=DP Mitchell, AN Netravali; citation_volume=22; citation_issue=4; citation_publication_date=1988; citation_pages=221-228; citation_doi=10.1145/378456.378514; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Physically based modeling and animation of fire; citation_author=DQ Nguyen, R Fedkiw, HW Jensen; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=721-728; citation_doi=10.1145/566654.566643; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Synthesizing sounds from physically based motion; citation_author=JF O&#8217;Brien, PR Cook, G Essl; citation_volume=20; citation_issue=3; citation_publication_date=2001; citation_pages=529-536; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=An image synthesizer; citation_author=K Perlin; citation_volume=19; citation_issue=3; citation_publication_date=1985; citation_pages=287-296; citation_doi=10.1145/325165.325247; citation_id=CR20"/>

    <meta name="citation_reference" content="Ren Z, Yeh H, Lin M (2010) Synthesizing contact sounds between textured models. In: Proceedings of virtual reality (VR), pp 139&#8211;146"/>

    <meta name="citation_reference" content="citation_title=Microsound; citation_publication_date=2004; citation_id=CR22; citation_author=C Roads; citation_publisher=The MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Music J; citation_title=Spectral modeling synthesis: a sound analysis/synthesis system based on a deterministic plus stochastic decomposition; citation_author=X Serra, J Smith; citation_volume=14; citation_issue=4; citation_publication_date=1990; citation_pages=12-24; citation_doi=10.2307/3680788; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Using immersive game-based virtual reality to teach fire-safety skills to children; citation_author=S Smith, E Ericson; citation_volume=13; citation_issue=2; citation_publication_date=2009; citation_pages=87-99; citation_doi=10.1007/s10055-009-0113-6; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=SubSafe: a games-based training system for submarine safety and spatial awareness (Part 1); citation_author=RJ Stone, A Caird-Daley, K Bessell; citation_volume=13; citation_issue=1; citation_publication_date=2009; citation_pages=3-12; citation_doi=10.1007/s10055-008-0110-1; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Foley automatic: physically-based sound effects for interactive simulation and animation; citation_author=K Doel, P Kry, D Pai; citation_volume=20; citation_issue=3; citation_publication_date=2001; citation_pages=537-544; citation_id=CR26"/>

    <meta name="citation_reference" content="Wang J, Yang X, Lu W (2011) FFT-upsampling for smoke animation. In Proceedings of ACM/eurographics symposium on computer animation poster"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Computer games technology and higher education; citation_author=A Watt, SC Maddock; citation_volume=5; citation_issue=4; citation_publication_date=2000; citation_pages=185-194; citation_doi=10.1007/BF01408517; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Harmonic fluids; citation_author=C Zheng, DL James; citation_volume=28; citation_issue=3; citation_publication_date=2009; citation_pages=37; citation_doi=10.1145/1531326.1531343; citation_id=CR29"/>

    <meta name="citation_author" content="Shiguang Liu"/>

    <meta name="citation_author_email" content="shgliu@126.com"/>

    <meta name="citation_author_institution" content="School of Computer Science and Technology, Tianjin University, Tianjin, People&#8217;s Republic of China"/>

    <meta name="citation_author_institution" content="Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Zhuojun Yu"/>

    <meta name="citation_author_institution" content="School of Computer Science and Technology, Tianjin University, Tianjin, People&#8217;s Republic of China"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0271-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0271-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Sounding fire for immersive virtual reality"/>
        <meta property="og:description" content="Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fire sound that is synchronized with physically based fire animations. We divide fire sound into two parts: low frequency and mid- to high frequency, and use two processes to separately synthesize these two parts. By simplifying calculations using a novel combustion sound model as well as leveraging GPU parallel computing in a marching-cube-like manner, our method speeds up the computation of low-frequency part by an order of magnitude. To run the time-stepping fire simulation at a relative low frequency rather than the audio rate, we add synchronized mid- and high-frequency wavelet details to low-frequency simulation contents with a post-process to generate complete fire sound. We validated our method with various experiments to build a solid physically based basis for real-time acoustic rendering that can be used for immersive virtual reality scenarios."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Sounding fire for immersive virtual reality | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0271-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Fire sound, Physically based simulation, GPU, Immersive virtual reality","kwrd":["Fire_sound","Physically_based_simulation","GPU","Immersive_virtual_reality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0271-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0271-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=271;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0271-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Sounding fire for immersive virtual reality
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0271-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0271-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-08-12" itemprop="datePublished">12 August 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Sounding fire for immersive virtual reality</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shiguang-Liu" data-author-popup="auth-Shiguang-Liu" data-corresp-id="c1">Shiguang Liu<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tianjin University" /><meta itemprop="address" content="grid.33763.32, 0000000417612484, School of Computer Science and Technology, Tianjin University, Tianjin, People’s Republic of China" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tianjin Key Laboratory of Cognitive Computing and Application" /><meta itemprop="address" content="Tianjin Key Laboratory of Cognitive Computing and Application, No. 92, Weijin Road, Nankai District, Tianjin, 300072, People’s Republic of China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Zhuojun-Yu" data-author-popup="auth-Zhuojun-Yu">Zhuojun Yu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tianjin University" /><meta itemprop="address" content="grid.33763.32, 0000000417612484, School of Computer Science and Technology, Tianjin University, Tianjin, People’s Republic of China" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">291</span>–<span itemprop="pageEnd">302</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">508 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0271-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fire sound that is synchronized with physically based fire animations. We divide fire sound into two parts: low frequency and mid- to high frequency, and use two processes to separately synthesize these two parts. By simplifying calculations using a novel combustion sound model as well as leveraging GPU parallel computing in a marching-cube-like manner, our method speeds up the computation of low-frequency part by an order of magnitude. To run the time-stepping fire simulation at a relative low frequency rather than the audio rate, we add synchronized mid- and high-frequency wavelet details to low-frequency simulation contents with a post-process to generate complete fire sound. We validated our method with various experiments to build a solid physically based basis for real-time acoustic rendering that can be used for immersive virtual reality scenarios.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Like smoke and water, fire is also a type of common fluid phenomena in our daily life, which is a crucial element in virtual reality (Smith and Ericson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Smith S, Ericson E (2009) Using immersive game-based virtual reality to teach fire-safety skills to children. Virtual Real 13(2):87–99" href="/article/10.1007/s10055-015-0271-7#ref-CR24" id="ref-link-section-d101265e370">2009</a>), computer games (Watt and Maddock <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Watt A, Maddock SC (2000) Computer games technology and higher education. Virtual Real 5(4):185–194" href="/article/10.1007/s10055-015-0271-7#ref-CR28" id="ref-link-section-d101265e373">2000</a>), virtual training (Stone et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stone RJ, Caird-Daley A, Bessell K (2009) SubSafe: a games-based training system for submarine safety and spatial awareness (Part 1). Virtual Real 13(1):3–12" href="/article/10.1007/s10055-015-0271-7#ref-CR25" id="ref-link-section-d101265e376">2009</a>), etc. Fire simulation involves the techniques from fluid dynamics, combustion thermodynamics, acoustics, and even radiology, making it one of the most difficult research topics. Nevertheless, much effort from both industry and academia has been devoted to such a challenge. Although great success and progress have been achieved in the past decades, it was mostly dedicated to visual simulation of fire, ignoring the generation of the synchronous fire sound.</p><p>Sound refers to vibrations transmitted through an elastic solid, or a liquid, or gas, with frequencies in the approximate range of 20–20,000 Hz, capable of being detected by human organs of hearing. Sound is essentially produced due to the surface vibrations of an object under force. These vibrations travel through the surrounding medium to the human ear, and the pressure changes are perceived as sound. This pressure change is also called sound pressure. Similarly to visual rendering, auditory display is also central to creating compelling virtual worlds. Physically based simulation of fire will enrich visual behavior; however, the lack of synchronous sound leads to poor user experience. Up to now, the sound synthesis of fire still remains immature, and the related progress is limited as well.</p><p>Traditionally, recorded combustion sound was often adopted to resolve the problem of auditory display. However, recording different sound clips needs manual intervention, and most interactive behavior in dynamic scenarios (e.g., animating fire with wind) is difficult to predict in advance, which also makes the recording method less practical. Dobashi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Dobashi Y, Yamamoto T, Nishita T (2003) Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. ACM Trans Graph 22(3):539–545" href="/article/10.1007/s10055-015-0271-7#ref-CR6" id="ref-link-section-d101265e385">2003</a>) proposed a physics-based vortex sound generation method, which was further extended to fire sound synthesis (Dobashi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Dobashi Y, Yamamoto T, Nishita T (2004) Synthesizing sound from turbulent fields using sound textures for interactive fluid simulation. Comput Graph Forum 23(3):736–744" href="/article/10.1007/s10055-015-0271-7#ref-CR7" id="ref-link-section-d101265e388">2004</a>). This method can remedy the above problems to some extent, but it is worthy to note that the vortex sound only contributes a small part to the total fire sound. Later, Chadwick and James (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e391">2011</a>) proposed a fire sound synthesis method, focusing on combustion sound, which is actually the dominant source of sound from dynamic fire. Although their method is able to synthesize plausible fire sound, the huge computational cost restricts it from applications in complex scenes and fast simulation scenarios. Even for a simple fire scene such as a candle’s flame moving back and forth, this method would cost dozens of hours or more of computation. While simply reducing the simulation frame rate can significantly improve the computational efficiency, the realism of the sound will inevitably be greatly harmed. Hence, to achieve fast simulation rates while maintaining the sound details remains a challenge.</p><p>In this paper, we divide fire sound into two parts: low frequency and mid- to high frequency. We use two processes to separately synthesize these two parts. First, a low-frequency flame sound is synthesized using a novel physically based combustion sound model driven by data from a visual flame simulation run at a relatively low temporal sampling rate. Second, we enhance the low-frequency fire sound with synchronized wavelet noise, which represents the mid- to high-frequency details. This bandwidth extension method for synthesizing additional higher-frequency flame sound content gives plausible results and matches from theory to experiment. Compared with the previous methods, at low-frequency part, our new model is free from the computational intensive tasks such as explicit extraction of the flame front and normal vector calculations on each triangle of the fire front. The introduction of a marching-cube-like integral calculation method is also very conducive and suitable for GPU parallel acceleration. However, at high frequency, we use wavelet analysis to further improve the sound signal quality using such post-process to add mid- and high-frequency details. Our approach is not only applicable to fire sound mid- to high-frequency generation, but could also be applied to similar problem (e.g., the recovery of sound signals) which need to recover or add lost detail information using synchronized wavelet noise with desired signal distribution. The distribution can be derived from theoretical analysis and experimental investigation.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Over the past decades, sound synthesis has attracted much attention due to the strong demand in virtual reality, immersive PC games, and more. The earliest sound synthesis method is recording. The user makes a corresponding recording for each specific application. As this form of recording is not easy to reuse, people began to depend on short sound recordings; a sequence of audio signals matching a specific application can be produced by editing and manipulating several short recorded grains. Such method is called granular synthesis. Roads (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Roads C (2004) Microsound. The MIT Press, Cambridge" href="/article/10.1007/s10055-015-0271-7#ref-CR22" id="ref-link-section-d101265e405">2004</a>) gives an excellent review on the theories and implementations of synthesizing sounds with this approach.</p><h3 class="c-article__sub-heading" id="Sec3">Physically based sound synthesis</h3><p>Physically based methods are generally based on specific mathematical models to synthesize sound. The sound generated in this way would be automatically synchronized with visual rendering, since the sound is computed from the visual simulation model. Physics-based methods can be roughly classified into rigid body-based modeling methods and fluid-based modeling methods. For the former, O’Brien et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. ACM Trans Graph 20(3):529–536" href="/article/10.1007/s10055-015-0271-7#ref-CR19" id="ref-link-section-d101265e415">2001</a>) simulated the vibration of the surface of a moving rigid body so as to calculate the resulting pressure change in the sound. This method can generate reliable sound caused by the movement of the rigid body. However, it is difficult to deal with contact sound for rigid body. Van Den Doel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Van Den Doel K, Kry P, Pai D (2001) Foley automatic: physically-based sound effects for interactive simulation and animation. ACM Trans Graph 20(3):537–544" href="/article/10.1007/s10055-015-0271-7#ref-CR26" id="ref-link-section-d101265e418">2001</a>) presented a method for synthesizing the impact, sliding, and rolling sound of rigid bodies. They treated contact forces as excitations on modal models to generate sound. Ren et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ren Z, Yeh H, Lin M (2010) Synthesizing contact sounds between textured models. In: Proceedings of virtual reality (VR), pp 139–146" href="/article/10.1007/s10055-015-0271-7#ref-CR21" id="ref-link-section-d101265e421">2010</a>) described an interaction handling method for physics-based sound synthesis. They employed a three-level surface representation for expressing object shapes, which enabled the synthesis of more accurate contact sound for rigid bodies. For the fluid-based modeling methods, Imura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Imura M, Nakano Y, Yasumuro Y, Manabe Y, Chihara K (2007), Real-time generation of CG and sound of liquid with bubble. In Proceedings of SIGGRAPH posters" href="/article/10.1007/s10055-015-0271-7#ref-CR11" id="ref-link-section-d101265e424">2007</a>) utilized harmonic bubbles to synthesize water sound. They point out that water sound is closely related to the bubble diameter and the movement of bubbles as well. They hence incorporated the displacement of bubbles and other related factors into their computational model of water sound. Zheng and James (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zheng C, James DL (2009) Harmonic fluids. ACM Trans Graph 28(3):37" href="/article/10.1007/s10055-015-0271-7#ref-CR29" id="ref-link-section-d101265e427">2009</a>) augmented existing incompressible fluid solvers by adding particles, which accounted for the bubble creation, vibration, advection, and radiation. This method avoids calculating the fluid motion at the audio rate so that it can achieve a fast simulation. They used bubble oscillators to model sound radiation from harmonic fluid vibrations, which were weighted by its bubble-to-ear acoustic transfer function modeled as a discrete Green’s function of the Helmholtz equation. To solve potentially millions of 3D Helmholtz problems, they proposed a fast dual-domain multipole boundary integral solver. Dobashi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Dobashi Y, Yamamoto T, Nishita T (2003) Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. ACM Trans Graph 22(3):539–545" href="/article/10.1007/s10055-015-0271-7#ref-CR6" id="ref-link-section-d101265e431">2003</a>) devised a method for the synthesis of the aerodynamic sound due to vortex-based noise. They use a computational model approach to create sound textures which were further applied to real-time rendering of aerodynamic sound. In a follow-up work, Dobashi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Dobashi Y, Yamamoto T, Nishita T (2004) Synthesizing sound from turbulent fields using sound textures for interactive fluid simulation. Comput Graph Forum 23(3):736–744" href="/article/10.1007/s10055-015-0271-7#ref-CR7" id="ref-link-section-d101265e434">2004</a>) presented a computational method for general aerodynamic flow noise. It was successfully applied to the simulation of the sound of turbulent flames. Note that the above aerodynamic models can generate faithful sound for some fluid phenomena; however, they may be not suitable for fire sound since the dominant source of combustion sound is not vortex-based noise. Numerical and experimental results by Ihme et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ihme M, Pitsch H, Bodony D (2009) Radiation of noise in turbulent non-premixed flames. Proc Combust Instit 32(1):1545–1553" href="/article/10.1007/s10055-015-0271-7#ref-CR10" id="ref-link-section-d101265e437">2009</a>) suggest that direct combustion noise is actually the dominant source of the sound from combustion phenomena. Just recently, Chadwick and James (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e440">2011</a>) proposed a method for synthesizing combustion sound. They computed the low-frequency fire sound by using a linear wave equation in (Crighton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Crighton DG, Dowling AP, Ffwocs Williams JE, Heckl MA, Leppington FA (1992) Modern methods in analytical acoustics. Springer, Berlin" href="/article/10.1007/s10055-015-0271-7#ref-CR5" id="ref-link-section-d101265e443">1992</a>) for describing combustion sound. The high-frequency detail was generated according to the power law proposed by Abugov and Obrezkov (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Abugov DI, Obrezkov OI (1978) Acoustic noise in turbulent flames. Combust Explos Shock Waves 14(5):606–612" href="/article/10.1007/s10055-015-0271-7#ref-CR1" id="ref-link-section-d101265e446">1978</a>), which indicates that the power spectrum of acoustic emissions from flame exhibits a power law when the frequency is beyond a certain peak. Based on the above observations, they created the high-frequency contents of the sound signals, added it to the low-frequency bands via fast Fourier transformation, and finally, formed the complete sound signals. Although this method can synthesize plausible fire sound, it cannot be used for simulation at interactive rates because of huge amount of calculations.</p><h3 class="c-article__sub-heading" id="Sec4">Non-physically based sound synthesis</h3><p>Non-physically based methods usually synthesize sound using heuristic or procedural approaches, such as spectral synthesis, sound texture synthesis, etc. Serra and Smith (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Serra X, Smith J III (1990) Spectral modeling synthesis: a sound analysis/synthesis system based on a deterministic plus stochastic decomposition. Comput Music J 14(4):12–24" href="/article/10.1007/s10055-015-0271-7#ref-CR23" id="ref-link-section-d101265e457">1990</a>) proposed a technique called spectral modeling synthesis for sound analysis. This method divided sound spectrums into two parts, namely a deterministic part and a stochastic part. The two parts are separately calculated and finally combined together to generate the sound. Marelli et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Marelli D, Aramaki M, Kronland-Martinet R, Verron C (2010) Time-frequency synthesis of noisy sounds with narrow spectral components. IEEE Trans Audio Speech Lang Process 18(8):399–414" href="/article/10.1007/s10055-015-0271-7#ref-CR16" id="ref-link-section-d101265e460">2010</a>) synthesize the fire sound with narrow spectral components. Since the sound synthesis is based on the spectrum rather than the simulation data, the result generated by this type of methods tends to lack synchronization with different fire states.</p><h3 class="c-article__sub-heading" id="Sec5">Signal upsampling</h3><p>Another area related to our method is signal upsampling. The most straightforward methods for signal upsampling are interpolation-based approaches, such as linear interpolation, cubic interpolation, and spline interpolation (Keys <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Keys RG (1981) Cubic convolution interpolation for digital image processing. IEEE Trans Audio Speech Lang Process 29(6):1153–1160" href="/article/10.1007/s10055-015-0271-7#ref-CR12" id="ref-link-section-d101265e471">1981</a>). Although these methods are fast, they tend to generate relatively ambiguous results. Freeman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Freeman WT, Jones TR, Pasztor EC (2002) Example-based super-resolution. IEEE Comput Graph 22(2):56–65" href="/article/10.1007/s10055-015-0271-7#ref-CR8" id="ref-link-section-d101265e474">2002</a>) presented an example-based upsampling method inspired by the idea of the image analogy (Hertzmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hertzmann A, Jacobs CE, Oliver N, Curless B, Salesin D (2001) Image analogies. ACM Trans Graph 20(3):327–340" href="/article/10.1007/s10055-015-0271-7#ref-CR9" id="ref-link-section-d101265e477">2001</a>). The example-based method can create plausible high-frequency signal details, but it is generally time-consuming (e.g., two seconds to enlarge an image of 100 × 100 pixels to 200 × 200 pixels). Wang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang J, Yang X, Lu W (2011) FFT-upsampling for smoke animation. In Proceedings of ACM/eurographics symposium on computer animation poster" href="/article/10.1007/s10055-015-0271-7#ref-CR27" id="ref-link-section-d101265e480">2011</a>) proposed a frequency domain upsampling method to produce small-scale details for realistic smoke animation. They used a zero-padding technique from signal processing field (Coenen and De Vos <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Coenen A, De Vos A (1992) FFT-based interpolation for multipath detection in GPS/GLONASS receivers. Electron Lett 28(19):1787–1788" href="/article/10.1007/s10055-015-0271-7#ref-CR3" id="ref-link-section-d101265e483">1992</a>). Inspired by this technique, we use a similar strategy to refine the simulated data. The experiments by Coenen and De Vos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Coenen A, De Vos A (1992) FFT-based interpolation for multipath detection in GPS/GLONASS receivers. Electron Lett 28(19):1787–1788" href="/article/10.1007/s10055-015-0271-7#ref-CR3" id="ref-link-section-d101265e487">1992</a>) show that, even for the low-frequency data, it would be possible to obtain acceptable sound effects.</p><h3 class="c-article__sub-heading" id="Sec6">Wavelet noise</h3><p>Wavelet noise was first proposed by Cook and DeRose (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Cook RL, DeRose T (2005) Wavelet noise. ACM Trans Graph 24(3):803–811" href="/article/10.1007/s10055-015-0271-7#ref-CR4" id="ref-link-section-d101265e499">2005</a>) and used for replacing Perlin noise (Perlin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Perlin K (1985) An image synthesizer. Comput Graph 19(3):287–296" href="/article/10.1007/s10055-015-0271-7#ref-CR20" id="ref-link-section-d101265e502">1985</a>). The noise is guaranteed to exist only over a narrow spectral band, which makes it possible to construct signals with arbitrary spectral distribution. Kim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kim T, Thurey N, James D, Gross M (2008) Wavelet turbulence for fluid simulation. ACM Trans Graph 27(3):50" href="/article/10.1007/s10055-015-0271-7#ref-CR13" id="ref-link-section-d101265e505">2008</a>) successfully introduced wavelet turbulence for fluid simulation. Our method is similar in that it uses wavelet decomposition to detect where high-frequency details are lost and apply a novel physics-driven power-law noise for enhancement based on space decomposition theory (Mallat <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Mallat SG (1989) A theory for multiresolution signal decomposition: the wavelet representation. IEEE Trans Pattern Anal Mach Intell 11(7):674–693" href="/article/10.1007/s10055-015-0271-7#ref-CR15" id="ref-link-section-d101265e508">1989</a>).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Overview</h2><div class="c-article-section__content" id="Sec7-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a> illustrates the main steps of our approach. Firstly, we employed a physics-based solver for visual simulation of fire, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>a. The velocity and burn field in each step are extracted for the sound synthesis. Since the calculation of the heat release rate is not involved in the fire solver, we instead transform it into the computation of the velocity divergence, which can be calculated more easily. We utilized the exported velocity and burn field from Houdini (Chadwick and James <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e526">2011</a>) to evaluate the velocity divergence integral, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>b. To recover high-frequency details as much as possible, we designed a novel FFT-upsampling method to refine the divergence integral’s signal. The illustration of the zero-padding technique, which is used to refine the FFT upsampling, is shown in the upper part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>c. The lower part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>c demonstrates the upsampled result for the signal in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>b. We then differentiated the velocity divergence of the above signal to obtain the low-frequency sound pressure in the upper part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>d by interpolation using a Mitchell-Netravali cubic filter (Mitchell and Netravali <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Mitchell DP, Netravali AN (1988) Reconstruction filters in computer graphics. Comput Graph 22(4):221–228" href="/article/10.1007/s10055-015-0271-7#ref-CR17" id="ref-link-section-d101265e545">1988</a>), and the lower part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>d shows that the mid- to high-frequency part is missing. We decomposed our low-frequency sound pressure signal into two parts as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>e, which in the upper part is the approximation part and in the lower part is the detail part. We used wavelet noise as our baseband signal and superposed its frequency-enhanced parts on each other in accordance with the power-law distribution as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>f to form our wavelet details. To synchronize wavelet details with simulated low-frequency content, we employed the detail part of the low-frequency signal to control the amplitudes of the wavelet details. Finally, we added wavelet details to low-frequency sound pressure signal to generate the complete fire sound as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>g. The lower part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig1">1</a>g shows that the mid- to high-frequency part has been recovered and follows the power-law distribution.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Our simulation framework. The exported data from the solver (<b>a</b>) were used to evaluate the velocity divergence integral (<b>b</b>). Next, we refined (<b>c</b>) and differentiated (<b>d</b>) the divergence to obtain the low-frequency sound pressure. We decomposed the low-frequency signal (<b>e</b>) and used the power-law distribution (<b>f</b>) to form the wavelet details. The final result is shown in (<b>g</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Visual simulation of fire</h2><div class="c-article-section__content" id="Sec8-content"><p>Unlike smoke and water, fire is a type of multiphase flow consisting of the gaseous fuel and the hot gaseous products. We therefore employ the blue core model (Nguyen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Nguyen DQ, Fedkiw R, Jensen HW (2002) Physically based modeling and animation of fire. ACM Trans Graph 21(3):721–728" href="/article/10.1007/s10055-015-0271-7#ref-CR18" id="ref-link-section-d101265e613">2002</a>) for visual simulation of fire. In this model, we assume that the gaseous fuel will burn and release heat immediately once it crosses the flame front interface, which separates the gaseous products produced by the rapid combustion products from the unburned fuel. We use separate sets of flow equations to model the flow of unburned fuel and gaseous products inside and outside of the flame front. The level set method is utilized to track a moving implicit surface representing the dynamic flame front. We calculate the motion of the gaseous fuel and the hot gaseous products using Navier–Stokes equations, as follows:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\frac{\partial u}{\partial t} = - (\varvec{u} \cdot \nabla )\varvec{u} - \nabla p/\rho + \varvec{f},$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                        <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\nabla \cdot \varvec{u} = \varPhi ,$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                     <p>Equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ1">1</a>) is the momentum equation, where <span class="mathjax-tex">\(\varvec{u}\)</span> denotes velocity, <i>t</i> is time, <i>p</i> is pressure, <i>ρ</i> is density, and <i>f</i> represents external forces, such as buoyancy and gravity. Equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ2">2</a>) is the mass equation. Here, <i>Φ</i> is an optional divergence source term. <i>Φ</i> is zero everywhere except where some process generates additional fluid or causes existing fluid to expand due to heat. In addition to the velocity field, the density field can be used to model the state of smoke produced by the combustion, while the temperature field can be employed to model the generated heat during combustion.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Low-frequency fire sound synthesis</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10">Fire sound synthesis model</h3><p>Fire sound can be considered as the sum of contributions from multiple sound sources. Numerical and experimental results (Ihme et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ihme M, Pitsch H, Bodony D (2009) Radiation of noise in turbulent non-premixed flames. Proc Combust Instit 32(1):1545–1553" href="/article/10.1007/s10055-015-0271-7#ref-CR10" id="ref-link-section-d101265e816">2009</a>) suggest that other sources, such as vortex-based sound, can be ignored under the assumption of the blue core model. That is, only the fluctuation of the heat release rate produces sound. The heat release fluctuation causes the fluctuation of the air density, leading to the fluctuation in volume which behaves as monopole sound sources. A wave equation (Crighton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Crighton DG, Dowling AP, Ffwocs Williams JE, Heckl MA, Leppington FA (1992) Modern methods in analytical acoustics. Springer, Berlin" href="/article/10.1007/s10055-015-0271-7#ref-CR5" id="ref-link-section-d101265e819">1992</a>) can be used to describe the sound source, and its radiation as follows:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\frac{1}{{\rho_{0} c_{0}^{2} }}\frac{{\partial^{2} p}}{{\partial t^{2} }} - \nabla \cdot \left( {\nabla \frac{1}{\rho }p} \right) = \frac{\partial }{\partial t}\left( {\frac{\gamma - 1}{{c_{0}^{2} }}q} \right),$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <i>p</i>, <i>c</i>, <i>ρ</i>, <i>q</i>, and <i>γ</i> refer to the unsteady pressure, the sound speed, the density, the heat release rate, and the ratio of specific heats of air, respectively; <i>c</i>
                           <sub>0</sub> and <i>ρ</i>
                           <sub>0</sub> denote the ambient speed of sound and the average environment density. As can be seen from the left side of Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ3">3</a>), the derivative of heat release with respect to time behaves as an acoustic source. The free-space Green’s function (Crighton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Crighton DG, Dowling AP, Ffwocs Williams JE, Heckl MA, Leppington FA (1992) Modern methods in analytical acoustics. Springer, Berlin" href="/article/10.1007/s10055-015-0271-7#ref-CR5" id="ref-link-section-d101265e1009">1992</a>) can be adopted to solve Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ3">3</a>) for the evaluation of the sound pressure.</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$p(\varvec{x},t) = \frac{1}{4\pi }\frac{\gamma - 1}{{c_{0}^{2} }}\frac{\partial }{\partial t}\int {\frac{{q(\varvec{y},t - {{|\varvec{x} - \varvec{y}|} \mathord{\left/ {\vphantom {{|\varvec{x} - \varvec{y}|} {c_{0} }}} \right. \kern-0pt} {c_{0} }})}}{{|\varvec{x} - \varvec{y}|}}} d^{3} \varvec{y},$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>x</i> and <i>t</i> are the position of the listener and the current time, respectively. Intuitively, this equation shows that the sound pressure is generated due to the fluctuation of the heat release.</p><p>We evaluate the sound pressure based on velocity divergence integral, and the computation of the sound pressure at <i>x</i> can be formulated as</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$p(t) = \frac{d}{dt}\int_{V} {\nabla \cdot \varvec{u}(\varvec{x},t)dv(\varvec{x})} ,$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                        <h3 class="c-article__sub-heading" id="Sec11">Marching-cube-like method for the evaluation of the velocity divergence integral</h3><p>The traditional marching cube method is known for three-dimensional iso-surface extraction from regular data fields. The iso-surface is a set of points holding the same value, which can be expressed as follows:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\left\{ {\left( {x,y,z} \right)|f\left( {x,y,z} \right) = c} \right\},$$</span></div></div><p>where <i>c</i> is the given threshold for three-dimensional reconstruction. The basic idea of the algorithm is to process the cubes (voxels) in the data field sequentially and classify the cubes which intersect with the iso-surface. It uses linear interpolation method to calculate the intersections between the iso-surface and each cube, and connects these intersections into triangles in some way to approximately represent the iso-surface. Since the marching cube method is fast, easy to implement, and the evaluation order of single cubes is irrelevant, it is very suitable for parallel implementation. We borrowed the above idea for the evaluation of the velocity divergence integral.</p><p>Based on the idea of the marching cube method, we discretize the simulation space into M × N × L cubes with uniform spacing <i>h</i>. At the corners of each cube, we define the value <span class="mathjax-tex">\(\phi_{{{\mathbf{x}}_{i} }}\)</span> for all cubes in the data field, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig2">2</a> by using a numerical solver. The data field used here can change with the solver. If using the solver proposed by Nguyen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Nguyen DQ, Fedkiw R, Jensen HW (2002) Physically based modeling and animation of fire. ACM Trans Graph 21(3):721–728" href="/article/10.1007/s10055-015-0271-7#ref-CR18" id="ref-link-section-d101265e1466">2002</a>), the level set method would be employed to trace the flame front. The level set value can thereby be used directly; if using the fuel-based solver, such as the fire solver in Houdini (Chadwick and James <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e1469">2011</a>), then the burn field can be used. Once <span class="mathjax-tex">\(\phi_{{{\text{x}}_{i} }}\)</span> is determined, the marching cube algorithm is started. The difference to the traditional marching cube algorithm is that we do not need to compute the approximate representation of the iso-surface because we have already transformed the surface integral of the velocity flux into the volumetric integral of the velocity divergence. What needs to be decided is if a cube is inside the iso-surface or not. Assuming <i>c</i> as the threshold value to reconstruct the iso-surface, then if the value <span class="mathjax-tex">\(\phi_{{{\mathbf{x}}_{i} }}\)</span> at a vertex is above <i>c</i> that vertex is outside the iso-surface, otherwise it is inside the iso-surface. If a cube has any vertex falling inside the iso-surface then that cube falls inside the iso-surface. Next, the volume <i>δv</i> of the part of a cube inside the iso-surface is computed (see the hatched area of the cube detail in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig2">2</a>) as follows:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\delta v = \frac{1}{8}h^{3} \sum\limits_{i = 0}^{7} {f\left( {\phi_{{{\mathbf{x}}_{i} }} } \right)} ,$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <span class="mathjax-tex">\(f(\phi_{{{\mathbf{x}}_{i} }} ) = \left\{ {\begin{array}{*{20}c} {1,\quad \phi_{{{\mathbf{x}}_{i} }} \le c} \\ {0,\quad \phi_{{{\mathbf{x}}_{i} }} &gt; c} \\ \end{array} } \right.,\)</span> and <i>h</i> refers to the side length of each cube.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Illustration of the discretization of the simulation space, and the calculation of both <i>δv</i> and <span class="mathjax-tex">\(\nabla \cdot \varvec{u}\)</span> on each cube</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Once the volume <i>δv</i> is determined, the velocity divergence <span class="mathjax-tex">\(\nabla \cdot \varvec{u}\)</span> in the center of each cube is computed if <i>δv</i> &gt; 0, otherwise <span class="mathjax-tex">\(\nabla \cdot \varvec{u}\)</span> is set to zero. After marching through all cubes, we multiply <span class="mathjax-tex">\(\nabla \cdot \varvec{u}\)</span> by <i>δv</i> for each cube and add the products together to obtain the integral:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\int_{V} {\nabla \cdot \varvec{u}d{\text{v}} = \sum {\nabla \cdot \varvec{u}} } \delta v,$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                        <h3 class="c-article__sub-heading" id="Sec12">GPU parallel computing</h3><p>With the observation that when computing the integral of velocity divergence, cubes can be independently processed, and we therefore employ GPU parallel computing to accelerate such calculation. We put the iteration through cubes on GPU and compute the volume <i>δv</i> and the divergence <span class="mathjax-tex">\(\nabla \cdot \varvec{u}\)</span> in parallel. Algorithm 1 shows the pseudo-code of our implementation, where the output <i>divIntegral</i> is the integral of the velocity divergence to be calculated. The input of Algorithm 1 includes the value <span class="mathjax-tex">\(\phi_{{{\mathbf{x}}_{i} }}\)</span> in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ6">6</a>) which is employed to compute the volume <i>δv</i> in each cube and the velocity <b><i>u</i></b>. Since the calculation for each cube is the same, we performed these procedures parallelly by exploiting the powerful parallel computing ability of GPU.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Figa_HTML.gif" alt="figurea" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec13">FFT-upsampling refinement</h3><p>We adopt the exported data from the fire simulation to compute the integral of the velocity divergence <i>d</i>(<i>t</i>) in each step. However, the data between these steps are missing. Restricting the time steps of the pre-computed simulation data provides faster execution at the expense of severe under-sampling. Consequently, we need an approach to reconstruct the missing signal information. The zero-padding technique can provide near-perfect re-sampling of signals to the limit of the Nyquist frequency (Coenen and De Vos <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Coenen A, De Vos A (1992) FFT-based interpolation for multipath detection in GPS/GLONASS receivers. Electron Lett 28(19):1787–1788" href="/article/10.1007/s10055-015-0271-7#ref-CR3" id="ref-link-section-d101265e2106">1992</a>).</p><p>We define a FFT-upsampling operator using the zero-padding technique, which extends the spectrum by filling it with zeros. With zero-padding, we can extend a signal spectrum of length L to N (&gt;L), which can be expressed as follows:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ZeroPad_{N,L} (x) = \left\{ \begin{aligned} x(\omega ),\quad |\omega | &lt; L/2 \hfill \\ 0,\quad {\text{otherwise}} \hfill \\ \end{aligned} \right. ,$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where L is the length of the original spectrum, and N refers to the length of the spectrum after zero-padding. Generally, N is equal to kL, where k is a scale factor. The near-perfect upsampling characteristic to the limit of the signal’s Nyquist frequency makes this technique ideal for refining the signal of <i>d</i>(<i>t</i>).</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig3">3</a>, we first convert <i>d</i>(<i>t</i>) from the time domain (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig3">3</a>a) to the frequency domain (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig3">3</a>b) using FFT. We then perform zero-padding for the high-frequency part, while keeping the low frequency unchanged, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig3">3</a>c. Here, we use a scale factor k = 4. Finally, we obtain the upsampled result in time domain with an inverse FFT transform. A visual comparison of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig3">3</a>d and a indicates that the information between the steps in the original signal is well recovered with the zero-padding technique.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Illustration of our FFT-upsampling refinement. We first use FFT to convert the signal from the time domain (<b>a</b>) to the frequency domain (<b>b</b>). Then, zero-padding is performed on the high-frequency part of the spectrum (<b>c</b>), and finally, using the inverse FFT, the upsampling result is obtained (<b>d</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec14">Sound pressure evaluation</h3><p>To evaluate the sound pressure <i>p</i>(<i>t</i>), we need to find the derivative of the velocity divergence <i>d</i>(<i>t</i>). Since <i>d</i>(<i>t</i>) is obtained only at discrete steps 0, Δ<i>t</i>, 2Δ<i>t</i>,…,NΔ<i>t</i>, we would adopt interpolation operations to compute the continuous signal. Here, we select a Mitchell-Netravali cubic filter (Mitchell and Netravali <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Mitchell DP, Netravali AN (1988) Reconstruction filters in computer graphics. Comput Graph 22(4):221–228" href="/article/10.1007/s10055-015-0271-7#ref-CR17" id="ref-link-section-d101265e2355">1988</a>) to interpolate <i>d</i>(<i>t</i>):</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$d(t) = \sum\limits_{n = 0}^{N} {d(n} \varDelta t)k[(t - n\varDelta t)/\varDelta t],$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>where <span class="mathjax-tex">\(k(x) = \frac{1}{6}\left\{ {\begin{array}{*{20}l} {7|x|^{3} - 12|x|^{2} + \frac{16}{3},} \hfill &amp; {\left| {x &lt; 1} \right|} \hfill \\ { - \frac{7}{3}|x|^{3} + 12|x|^{2} - 20|x| + \frac{32}{3},} \hfill &amp; {1 \le |x| &lt; 2} \hfill \\ {0,} \hfill &amp; {\text{otherwise}} \hfill \\ \end{array} } \right.\)</span>.</p><p>Then, the sound pressure <i>p</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) can be evaluated at any point with respect to time as follows:</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$p_{l} (t) = \frac{d}{dt}d(t) = \sum\limits_{n = 0}^{N} {d(n} \varDelta t)\frac{d}{dt}k[(t - n\varDelta t)/\varDelta t],$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div>
                        </div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Mid- to high-frequency fire sound synthesis</h2><div class="c-article-section__content" id="Sec15-content"><p>In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0271-7#Sec9">5</a>, as the sound was calculated from the physical simulation model, the sound frequency is about 200–360 Hz. However, the highest frequency of sound which can be heard by a human is about 20 kHz. In other words, fire sound synthesized using the method in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0271-7#Sec9">5</a> merely contains low-frequency content. We need to reconstruct the high-frequency part of the sound signal. In this section, we present a method which enhances the fire sound using synchronized wavelet noise, which can be regarded as mid- to high-frequency details. Experimental results (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kim T, Thurey N, James D, Gross M (2008) Wavelet turbulence for fluid simulation. ACM Trans Graph 27(3):50" href="/article/10.1007/s10055-015-0271-7#ref-CR13" id="ref-link-section-d101265e2936">2008</a>) suggest that in the mid- to high-frequency range, flame sounds exhibit a power-law spectrum <i>f</i>
                        <sup>-<i>α</i></sup> as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig4">4</a>, with <i>α</i> ∊ [2.2, 3.4]. We can thus use wavelet analysis to generate high-frequency noise with desired distribution such as power law, and synchronize with low-frequency sound.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Spectrum of a recorded fire sound. <i>f</i>
                                    <sub>
                          <i>peak</i>
                        </sub> divides the low-frequency part and the mid- to high-frequency part</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec16">Space decomposition theory</h3><p>Discrete samples can be converted to a continuous signal using interpolation, a process called reconstruction in signal theory. Reconstruction is accomplished by convolving the discrete samples <i>f</i>
                           <sub>
                    <i>s</i>
                  </sub>(<i>x</i>) with a reconstruction filter kernel <i>ϕ</i>(<i>x</i>):</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$f_{r} (x) = f_{s} (x) * \phi (x)\; = \sum\limits_{i} {f_{s} (i)} \phi (x - i) = \sum\limits_{i} {f_{i} } \phi (x - i)$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <i>f</i>
                           <sub>
                    <i>r</i>
                  </sub>(<i>x</i>) refers to the reconstructed signal. We define <i>ϕ</i>(<i>x</i> − <i>i</i>) as the scaling function, and then, a series of scaling spaces can be expressed as follows:</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$V^{n} \oplus \left\{ F(x)|F(x) = \sum\limits_{i} {f_{i} } \phi (2^{n} x - i)\right\} ,$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where <span class="mathjax-tex">\(V^{0} \subset V^{1} \subset V^{2} \subset \ldots \ldots \subset V^{\infty } {\text{ = L}}^{ 2} ( {\text{R)}}\)</span> as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig5">5</a>. Under the definition above, we can naturally find that <span class="mathjax-tex">\(f_{r} (x) \in V^{ 0} .\)</span> We call <span class="mathjax-tex">\(V^{ 0}\)</span> the resolution 0 space. If we scale the sampling frequency by a factor of two, i.e., scale down the width of <i>ϕ</i>(<i>x</i>), the reconstructed signal can be given as:</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$f_{r} (x) = \sum\limits_{i} {f_{i} } \phi ( 2x - i) ,$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>which is to say <span class="mathjax-tex">\(f_{r} (x) \in V^{ 1}\)</span>, where <span class="mathjax-tex">\(V^{ 1}\)</span> is a larger, resolution 1 space. In this case, we subtly establish some relationship among sampling frequency, reconstructed signal, and scaling space. When we increase the sampling frequency, the resolution of the scaling space, in which the reconstructed signal is represented, correspondingly increases. The higher the resolution of the space is, the more detail can be represented, i.e., the reconstructed signal is closer to the original signal.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Space decomposition. Note that a scaling space <i>V</i>
                                       <sup><i>n</i></sup> with resolution <i>n</i> can be represented by summing a scaling space <i>V</i>
                                       <sup><i>n</i>−1</sup> and a wavelet space <i>W</i>
                                       <sup><i>n</i>−1</sup> both with resolution <i>n</i> − 1</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In wavelet analysis, a scaling space with resolution <i>n</i> can be represented by a direct sum of a scaling space and a wavelet space both with resolution <i>n</i> − 1 as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig5">5</a>:</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$V^{n} = V^{n - 1} \oplus W^{n - 1} ,$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>where <i>W</i>
                           <sup><i>n</i>−1</sup> = {<i>G</i>(<i>x</i>)|<i>G</i>(<i>x</i>) = ∑ <sub>
                    <i>i</i>
                  </sub>
                           <i>g</i>
                           <sub>
                    <i>i</i>
                  </sub>
                           <i>φ</i>(2<sup><i>n</i>−1</sup>
                           <i>x</i> − <i>i</i>)}, <i>φ</i>(<i>x</i>) is the wavelet function. We know that <i>V</i>
                           <sup><i>n</i></sup> contains the higher-frequency information compared to <i>V</i>
                           <sup><i>n</i>−1</sup>, as a result of a higher resolution. High-frequency information cannot be represented in <i>V</i>
                           <sup><i>n</i>−1</sup> but can be represented in <i>W</i>
                           <sup><i>n</i>−1</sup>, i.e., each <i>W</i>
                           <sup><i>n</i></sup> contains information of a fixed band. If the highest frequency information of some band-limited signal belongs to <i>W</i>
                           <sup><i>i</i></sup> and we require the reconstructed signal to fully express the original signal, then the corresponding scaling space of sampling frequency is <i>V</i>
                           <sup><i>i</i>+1</sup>, which is twice the highest frequency of the original signal.</p><p>Next, we adapt the scaling space method to our fire sound synthesis. We assume that the complete fire sound signal belongs to <i>V</i>
                           <sup><i>j</i></sup>, and the low-frequency fire sound belongs to <i>V</i>
                           <sup>0</sup>. In the following subsections, we will introduce the recovery method for the mid- and high-frequency information in <i>W</i>
                           <sup>0</sup>, <i>W</i>
                           <sup>1</sup>,…,<i>W</i>
                           <sup><i>j</i>−1</sup>. We need to enhance the low-frequency fire sound <i>p</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) with mid- and high frequency to get the complete fire sound in the post-process:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$p\left( t \right) = p_{l} \left( t \right) + \sum_{i = 0}^{j - 1} w_{i} ,\quad {\text{where}}\;w_{i} \in W_{i} .$$</span></div></div>
                        <h3 class="c-article__sub-heading" id="Sec17">Spectrum analysis</h3><p>Given a power spectrum <i>P</i>(<i>f</i>) ∝ <i>f</i>
                           <sup>−<i>α</i></sup>, the corresponding frequency domain pressure amplitudes are <span class="mathjax-tex">\(|p(f)| = f^{{ - \frac{\alpha }{2}}} .\)</span> Based on the wavelet decomposition, we generate the sub-signal in each wavelet space and control the amplitudes to fit the power-law distribution of the mid- and high-frequency pressure signals. We then add all sub-signals and receive the mid- and high-frequency noise signals. With such a method, we can not only procedurally construct a noise signal that produces this power distribution, but can also synchronize the noise signal with low-frequency fire sound. This can be rewritten as a recurrence relation: <span class="mathjax-tex">\(|p(2f)| = 2^{{ - \frac{\alpha }{2}}} \left| {p(f)} \right|.\)</span>
                        </p><p>We assume <i>N</i>(<i>t</i>) ∊ <i>W</i>
                           <sup>0</sup> and <i>N</i>(2<sup><i>n</i></sup>
                           <i>t</i>) ∊ <i>W</i>
                           <sup><i>n</i></sup>. Since <i>N</i>(<i>t</i>) is band-limited, it can be substituted for <i>p</i>(<i>f</i>). Our wavelet noise signal is then a series of <span class="mathjax-tex">\(W(t) = \sum\nolimits_{i = 0}^{n} {2^{{ - \frac{\alpha }{2}i}} } N_{{2^{i} f}} = \sum\nolimits_{i = 0}^{n} {2^{{ - \frac{\alpha }{2}i}} } N(2^{i} t),\)</span> which follows the power law <i>P</i>(<i>f</i>) ∝ <i>f</i>
                           <sup>−<i>α</i></sup>. We call <i>N</i>(<i>t</i>) the baseband signal.</p><h3 class="c-article__sub-heading" id="Sec18">Baseband signal synthesis</h3><p>We use the method proposed in (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kim T, Thurey N, James D, Gross M (2008) Wavelet turbulence for fluid simulation. ACM Trans Graph 27(3):50" href="/article/10.1007/s10055-015-0271-7#ref-CR13" id="ref-link-section-d101265e4405">2008</a>) to synthesize the baseband signal. First, we employ a random number generator to create a coefficient sequence <i>R</i> = (<i>r</i>
                           <sub>1</sub>, <i>r</i>
                           <sub>2</sub>, …, <i>r</i>
                           <sub>
                    <i>n</i>
                  </sub>). This sequence defines a signal in <i>V</i>
                           <sup>1</sup>: <i>R</i>(<i>t</i>) = ∑ <sub>
                    <i>i</i>
                  </sub>
                           <i>r</i>
                           <sub>
                    <i>i</i>
                  </sub>
                           <i>ϕ</i>(2<i>x</i> − <i>i</i>).</p><p>We decompose <i>R</i>(<i>t</i>) into two parts: <i>R</i>(<i>t</i>) = <i>R</i>
                           <sup>↓</sup>(<i>t</i>) + <i>N</i>(<i>t</i>), where <i>R</i>
                           <sup>↓</sup>(<i>t</i>) ∊ <i>V</i>
                           <sup>0</sup> represents the low-frequency part and <i>N</i>(<i>t</i>) ∊ <i>W</i>
                           <sup>0</sup> represents the high-frequency part. We downsample and upsample the sequence <i>R</i> to get the sequence <i>R</i>
                           <sup>↓↑</sup> = (<i>r</i>
                           <span class="c-stack">
                    <sup>↓↑</sup><sub>1</sub>
                    
                  </span>, <i>r</i>
                           <span class="c-stack">
                    <sup>↓↑</sup><sub>2</sub>
                    
                  </span>, …, <i>r</i>
                           <span class="c-stack">
                    <sup>↓↑</sup><sub>
                      <i>n</i>
                    </sub>
                    
                  </span>), which is the coefficient of <i>R</i>
                           <sup>↓</sup>(<i>t</i>) in <i>V</i>
                           <sup>1</sup>. We subtract <i>R</i>
                           <sup>↓↑</sup>from the original <i>R</i> to find <i>N</i>(<i>t</i>):</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$N(t) = \sum\limits_{i} {r_{i} } \phi (2x - i) - \sum\limits_{i} {r_{i}^{ \downarrow \uparrow } } \phi (2x - i) = \sum\limits_{i} {n_{i} } \phi (2x - i).$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>Generated by these steps, <i>N</i>(<i>t</i>) conforms to our prior assumption <i>N</i>(<i>t</i>) ∊ <i>W</i>
                           <sup>0</sup>.</p><h3 class="c-article__sub-heading" id="Sec19">Signal synchronization</h3><p>We need to appropriately synchronize the power-law noise with the simulated low-frequency fire sound. We decompose <i>p</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) into two parts: <i>p</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) = <i>p</i>
                           <span class="c-stack">
                    <sup>↓</sup><sub>
                      <i>l</i>
                    </sub>
                    
                  </span>(<i>t</i>) + <i>w</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>), where <i>w</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) = ∑ <sub>
                    <i>i</i>
                  </sub>
                           <i>ɛ</i>
                           <sub>
                    <i>i</i>
                  </sub>
                           <i>ϕ</i>(<i>x</i> − <i>i</i>).</p><p>It is easy to see that <i>p</i>
                           <span class="c-stack">
                    <sup>↓</sup><sub>
                      <i>l</i>
                    </sub>
                    
                  </span>(<i>t</i>) ∊ <i>V</i>
                           <sup>−1</sup> and <i>w</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) ∊ <i>W</i>
                           <sup>−1</sup>. That is to say, <i>w</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) contains information that belongs to <i>W</i>
                           <sup>−1</sup>. We can use (<i>ɛ</i>
                           <sub>1</sub>, <i>ɛ</i>
                           <sub>2</sub>, …, <i>ɛ</i>
                           <sub>
                    <i>n</i>
                  </sub>) to control the amplitude of mid- and high-frequency noise. Intuitively, this approach assumes that <i>w</i>
                           <sub>
                    <i>l</i>
                  </sub>(<i>t</i>) falls into the band which follows the power law, and can be used to extrapolate the amplitude of each higher-frequency signal. Since (<i>ɛ</i>
                           <sub>1</sub>, <i>ɛ</i>
                           <sub>2</sub>, …, <i>ɛ</i>
                           <sub>
                    <i>n</i>
                  </sub>) is only known at low-frequency time steps, we use linear interpolation to form its interpolated signal: <i>ɛ</i>(<i>t</i>) = ∑ <sub>
                    <i>i</i>
                  </sub>
                           <i>ɛ</i>
                           <sub>
                    <i>i</i>
                  </sub>
                           <i>l</i>(<i>t</i> − <i>i</i>), where <i>l</i>(<i>x</i>) refers to the linear interpolation function.</p><p>Finally, the complete fire sound can be computed by Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0271-7#Equ16">16</a>):</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$p(t) = p_{l} (t) + \varepsilon (t)2^{{ - \frac{\alpha }{2}}} \sum\limits_{i = 0}^{j - 1} {2^{{ - \frac{\alpha }{2}i}} } N(2^{i} t).$$</span></div><div class="c-article-equation__number">
                    (16)
                </div></div>
                        </div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Results and discussions</h2><div class="c-article-section__content" id="Sec20-content"><p>We created various fire sound effects using our method and performed comparisons with other state-of-the-art methods. All experiments were conducted using the same hardware environment: Intel Core i5 3.00 GHz CPU, NVIDIA GeForce GT 530 GPU, 4 GB RAM.</p><p>All fire simulations were performed using the Pyro FX2 solver of Houdini 12.5 and ran at an average time-stepping rate of 5 and 2.8 ms. We exported the data fields of every simulation step and read them into our software using Houdini Development Kit. The data fields include the velocity field and the burn field. We used the CUDA compute language to accelerate the calculation of the velocity divergence integral on the GPU. All scenes were rendered using the Side Effects Software’s Mantra renderer. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0271-7#Tab1">1</a> shows the values for different parameters for our experiments as well as the timings of our implementations. The size denotes the resolution of the simulation domain, and sound synthesis time in ms is given for execution on CPU and GPU per frame, respectively.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Parameter values and timings for different experiments</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0271-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec21">Burning torch 1</h3><p>We modeled a burning torch in this experiment. The torch was first quickly moved to the left for a distance and then moved to the right. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig6">6</a> shows four screen shots from the simulation and the comparison of the resulting sound synthesis between our method and the method used by Chadwick and James (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e5524">2011</a>). During the movement, the flame front of the torch dramatically fluctuates, leading to the fluctuation of the velocity divergence integral which forms the characteristic ruffling noises, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig6">6</a>a.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Four screen shots of the experiment of burning torch 1 and the comparison of the resulting sound synthesis between our method (<b>a</b>) and the method used by Chadwick and James (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e5543">2011</a>) (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We also compared our result with that generated using Chadwick and James’s method at low frequency (Chadwick and James <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" href="/article/10.1007/s10055-015-0271-7#ref-CR2" id="ref-link-section-d101265e5561">2011</a>). As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig6">6</a>a and b, the entire waveform produced by our method and that generated using Chadwick and James’s method are substantially the same. Owing to the nature of our FFT-upsampling enhancing strategy, our method holds relatively richer details than Chadwick and James’s method. However, our method performed more than 10 times faster than their method. The computational time per frame for synthesizing sound from exporting data in Experiment 1 using Chadwick and James’s method on CPU is about 2256 ms, and the time using our method on CPU is 927 ms, while the time can be further reduced to 95 ms by exploiting GPU parallel acceleration using our method.</p><h3 class="c-article__sub-heading" id="Sec22">Burning torch 2</h3><p>In this experiment, we applied wind to a burning torch so as to affect the motion of fire periodically rather than by moving the torch, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig7">7</a>. The wind blows from right to left which behaves as a force added to the velocity field in the simulation. The wind force affects the fire and leads to the fluctuation of the flame front. The fluctuation of the flame front thereby causes the fluctuation of the velocity divergence integral which generates the sound.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Screen shots and sound synthesis signal diagram for “burning torch 2” experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec23">Flame jet 1</h3><p>We modeled a flame jet which spouted gaseous fuel at a high velocity, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig8">8</a>. The direction of the buoyancy force was horizontal so as to construct the spout effects. Once the outflow fuel is ignited, it will react with the air dramatically leading to irregular changes in velocity and thus the turbulence forms. Due to an unsteady of the velocity, the flame jet behaves as a noise source. On the other hand, since this change is fairly smooth, the sound is relatively stable (see the lower part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig8">8</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Screen shots and sound synthesis signal diagram for “flame jet 1” experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We can see that in the upper part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig9">9</a> that the mid- to high frequencies are missing. After enhancement with synchronized wavelet noise, the mid- and high frequencies have been recovered and follow a power-law distribution as shown in the lower part of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Mid- to high-frequency spectrum of experiment of flame jet 1. The upper row (<b>a</b>) and the lower row (<b>b</b>) show the low-frequency fire sound result and the complete fire sound result, respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec24">Flame jet 2</h3><p>We applied wind periodically to impact the flame jet which spouted gaseous fuel at a high velocity in the above experiment. The wind blows from the bottom up which behaves as a force added to the velocity field in the simulation, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0271-7#Fig10">10</a>. The effect of a wind force breaks the original stability. During the application of a wind force, the change in the velocity divergence integral is relatively sharp, which corresponds to the maximum pressure of the generated sound signals.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0271-7/MediaObjects/10055_2015_271_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Screen shots and sound synthesis signal diagram for “flame jet 2” experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0271-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusions and future work</h2><div class="c-article-section__content" id="Sec25-content"><p>We have presented a novel computational model for the fast synthesis of plausible fire sound synchronized with a physically based fire simulation. Our model avoids the explicit extraction of the flame front surface and allows for leveraging the computational power of GPUs, improving the computational efficiency by an order of magnitude compared to other state-of-the-art methods. We further proposed a new FFT-upsampling operator inspired by the zero-padding technique, so as to improve the interpolation quality, which can add high-frequency details to the synthesized sound as well.</p><h3 class="c-article__sub-heading" id="Sec26">Limitations</h3><p>Since we adopted a physically based fire sound synthesis model in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0271-7#Sec10">5.1</a>, our method is restricted by the limited application of this model. However, there are other, more sophisticated physically based computational models of combustion phenomena (e.g., Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Liu S, An T, Gong Z, Hagiwara I (2012) Physically based simulation of solid objects’ burning. Trans Edutain 7:110–120" href="/article/10.1007/s10055-015-0271-7#ref-CR14" id="ref-link-section-d101265e5711">2012</a>), which might have greater applicability and would provide better results for synthesizing fire sound in general.</p><h3 class="c-article__sub-heading" id="Sec27">Future work</h3><p>Our method significantly improves the computational speed for fire sound synthesis. Potentially, it can be employed for future real-time acoustic rendering. By considering the distance delay of the sound source, we can compute the sound pressure produced a few milliseconds ago but just traveled to our ears. The only problem may lie in the current hardware environment, which may not be able to simultaneously render visual and acoustic simulation results at real-time rates. If this problem can be solved, our sound synthesis can be applied in many interactive and even real-time fire scenes, which seems impossible for traditional solutions in the graphics and audio community.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DI. Abugov, OI. Obrezkov, " /><meta itemprop="datePublished" content="1978" /><meta itemprop="headline" content="Abugov DI, Obrezkov OI (1978) Acoustic noise in turbulent flames. Combust Explos Shock Waves 14(5):606–612" /><p class="c-article-references__text" id="ref-CR1">Abugov DI, Obrezkov OI (1978) Acoustic noise in turbulent flames. Combust Explos Shock Waves 14(5):606–612</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00789719" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20noise%20in%20turbulent%20flames&amp;journal=Combust%20Explos%20Shock%20Waves&amp;volume=14&amp;issue=5&amp;pages=606-612&amp;publication_year=1978&amp;author=Abugov%2CDI&amp;author=Obrezkov%2COI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JN. Chadwick, DL. James, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8" /><p class="c-article-references__text" id="ref-CR2">Chadwick JN, James DL (2011) Animating fire with sound. ACM Trans Graph 30(4):1–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2010324.1964979" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Animating%20fire%20with%20sound&amp;journal=ACM%20Trans%20Graph&amp;volume=30&amp;issue=4&amp;pages=1-8&amp;publication_year=2011&amp;author=Chadwick%2CJN&amp;author=James%2CDL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Coenen, A. Vos, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Coenen A, De Vos A (1992) FFT-based interpolation for multipath detection in GPS/GLONASS receivers. Electron L" /><p class="c-article-references__text" id="ref-CR3">Coenen A, De Vos A (1992) FFT-based interpolation for multipath detection in GPS/GLONASS receivers. Electron Lett 28(19):1787–1788</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1049%2Fel%3A19921139" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=FFT-based%20interpolation%20for%20multipath%20detection%20in%20GPS%2FGLONASS%20receivers&amp;journal=Electron%20Lett&amp;volume=28&amp;issue=19&amp;pages=1787-1788&amp;publication_year=1992&amp;author=Coenen%2CA&amp;author=Vos%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RL. Cook, T. DeRose, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Cook RL, DeRose T (2005) Wavelet noise. ACM Trans Graph 24(3):803–811" /><p class="c-article-references__text" id="ref-CR4">Cook RL, DeRose T (2005) Wavelet noise. ACM Trans Graph 24(3):803–811</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1073204.1073264" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Wavelet%20noise&amp;journal=ACM%20Trans%20Graph&amp;volume=24&amp;issue=3&amp;pages=803-811&amp;publication_year=2005&amp;author=Cook%2CRL&amp;author=DeRose%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DG. Crighton, AP. Dowling, JE. Ffwocs Williams, MA. Heckl, FA. Leppington, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Crighton DG, Dowling AP, Ffwocs Williams JE, Heckl MA, Leppington FA (1992) Modern methods in analytical acous" /><p class="c-article-references__text" id="ref-CR5">Crighton DG, Dowling AP, Ffwocs Williams JE, Heckl MA, Leppington FA (1992) Modern methods in analytical acoustics. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modern%20methods%20in%20analytical%20acoustics&amp;publication_year=1992&amp;author=Crighton%2CDG&amp;author=Dowling%2CAP&amp;author=Ffwocs%20Williams%2CJE&amp;author=Heckl%2CMA&amp;author=Leppington%2CFA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Dobashi, T. Yamamoto, T. Nishita, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Dobashi Y, Yamamoto T, Nishita T (2003) Real-time rendering of aerodynamic sound using sound textures based on" /><p class="c-article-references__text" id="ref-CR6">Dobashi Y, Yamamoto T, Nishita T (2003) Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. ACM Trans Graph 22(3):539–545</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F882262.882339" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20rendering%20of%20aerodynamic%20sound%20using%20sound%20textures%20based%20on%20computational%20fluid%20dynamics&amp;journal=ACM%20Trans%20Graph&amp;volume=22&amp;issue=3&amp;pages=539-545&amp;publication_year=2003&amp;author=Dobashi%2CY&amp;author=Yamamoto%2CT&amp;author=Nishita%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Dobashi, T. Yamamoto, T. Nishita, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Dobashi Y, Yamamoto T, Nishita T (2004) Synthesizing sound from turbulent fields using sound textures for inte" /><p class="c-article-references__text" id="ref-CR7">Dobashi Y, Yamamoto T, Nishita T (2004) Synthesizing sound from turbulent fields using sound textures for interactive fluid simulation. Comput Graph Forum 23(3):736–744</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2004.00785.x" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthesizing%20sound%20from%20turbulent%20fields%20using%20sound%20textures%20for%20interactive%20fluid%20simulation&amp;journal=Comput%20Graph%20Forum&amp;volume=23&amp;issue=3&amp;pages=736-744&amp;publication_year=2004&amp;author=Dobashi%2CY&amp;author=Yamamoto%2CT&amp;author=Nishita%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WT. Freeman, TR. Jones, EC. Pasztor, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Freeman WT, Jones TR, Pasztor EC (2002) Example-based super-resolution. IEEE Comput Graph 22(2):56–65" /><p class="c-article-references__text" id="ref-CR8">Freeman WT, Jones TR, Pasztor EC (2002) Example-based super-resolution. IEEE Comput Graph 22(2):56–65</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.988747" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Example-based%20super-resolution&amp;journal=IEEE%20Comput%20Graph&amp;volume=22&amp;issue=2&amp;pages=56-65&amp;publication_year=2002&amp;author=Freeman%2CWT&amp;author=Jones%2CTR&amp;author=Pasztor%2CEC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hertzmann, CE. Jacobs, N. Oliver, B. Curless, D. Salesin, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hertzmann A, Jacobs CE, Oliver N, Curless B, Salesin D (2001) Image analogies. ACM Trans Graph 20(3):327–340" /><p class="c-article-references__text" id="ref-CR9">Hertzmann A, Jacobs CE, Oliver N, Curless B, Salesin D (2001) Image analogies. ACM Trans Graph 20(3):327–340</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20analogies&amp;journal=ACM%20Trans%20Graph&amp;volume=20&amp;issue=3&amp;pages=327-340&amp;publication_year=2001&amp;author=Hertzmann%2CA&amp;author=Jacobs%2CCE&amp;author=Oliver%2CN&amp;author=Curless%2CB&amp;author=Salesin%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ihme, H. Pitsch, D. Bodony, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Ihme M, Pitsch H, Bodony D (2009) Radiation of noise in turbulent non-premixed flames. Proc Combust Instit 32(" /><p class="c-article-references__text" id="ref-CR10">Ihme M, Pitsch H, Bodony D (2009) Radiation of noise in turbulent non-premixed flames. Proc Combust Instit 32(1):1545–1553</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.proci.2008.06.137" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Radiation%20of%20noise%20in%20turbulent%20non-premixed%20flames&amp;journal=Proc%20Combust%20Instit&amp;volume=32&amp;issue=1&amp;pages=1545-1553&amp;publication_year=2009&amp;author=Ihme%2CM&amp;author=Pitsch%2CH&amp;author=Bodony%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Imura M, Nakano Y, Yasumuro Y, Manabe Y, Chihara K (2007), Real-time generation of CG and sound of liquid with" /><p class="c-article-references__text" id="ref-CR11">Imura M, Nakano Y, Yasumuro Y, Manabe Y, Chihara K (2007), Real-time generation of CG and sound of liquid with bubble. In Proceedings of SIGGRAPH posters</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RG. Keys, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Keys RG (1981) Cubic convolution interpolation for digital image processing. IEEE Trans Audio Speech Lang Proc" /><p class="c-article-references__text" id="ref-CR12">Keys RG (1981) Cubic convolution interpolation for digital image processing. IEEE Trans Audio Speech Lang Process 29(6):1153–1160</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0524.65006" aria-label="View reference 12 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=642902" aria-label="View reference 12 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASSP.1981.1163711" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cubic%20convolution%20interpolation%20for%20digital%20image%20processing&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Process&amp;volume=29&amp;issue=6&amp;pages=1153-1160&amp;publication_year=1981&amp;author=Keys%2CRG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Kim, N. Thurey, D. James, M. Gross, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Kim T, Thurey N, James D, Gross M (2008) Wavelet turbulence for fluid simulation. ACM Trans Graph 27(3):50" /><p class="c-article-references__text" id="ref-CR13">Kim T, Thurey N, James D, Gross M (2008) Wavelet turbulence for fluid simulation. ACM Trans Graph 27(3):50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1360612.1360649" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Wavelet%20turbulence%20for%20fluid%20simulation&amp;journal=ACM%20Trans%20Graph&amp;volume=27&amp;issue=3&amp;publication_year=2008&amp;author=Kim%2CT&amp;author=Thurey%2CN&amp;author=James%2CD&amp;author=Gross%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Liu, T. An, Z. Gong, I. Hagiwara, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Liu S, An T, Gong Z, Hagiwara I (2012) Physically based simulation of solid objects’ burning. Trans Edutain 7:" /><p class="c-article-references__text" id="ref-CR14">Liu S, An T, Gong Z, Hagiwara I (2012) Physically based simulation of solid objects’ burning. Trans Edutain 7:110–120</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physically%20based%20simulation%20of%20solid%20objects%E2%80%99%20burning&amp;journal=Trans%20Edutain&amp;volume=7&amp;pages=110-120&amp;publication_year=2012&amp;author=Liu%2CS&amp;author=An%2CT&amp;author=Gong%2CZ&amp;author=Hagiwara%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SG. Mallat, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Mallat SG (1989) A theory for multiresolution signal decomposition: the wavelet representation. IEEE Trans Pat" /><p class="c-article-references__text" id="ref-CR15">Mallat SG (1989) A theory for multiresolution signal decomposition: the wavelet representation. IEEE Trans Pattern Anal Mach Intell 11(7):674–693</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0709.94650" aria-label="View reference 15 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.192463" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20theory%20for%20multiresolution%20signal%20decomposition%3A%20the%20wavelet%20representation&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=11&amp;issue=7&amp;pages=674-693&amp;publication_year=1989&amp;author=Mallat%2CSG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Marelli, M. Aramaki, R. Kronland-Martinet, C. Verron, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Marelli D, Aramaki M, Kronland-Martinet R, Verron C (2010) Time-frequency synthesis of noisy sounds with narro" /><p class="c-article-references__text" id="ref-CR16">Marelli D, Aramaki M, Kronland-Martinet R, Verron C (2010) Time-frequency synthesis of noisy sounds with narrow spectral components. IEEE Trans Audio Speech Lang Process 18(8):399–414</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2010.2040532" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Time-frequency%20synthesis%20of%20noisy%20sounds%20with%20narrow%20spectral%20components&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Process&amp;volume=18&amp;issue=8&amp;pages=399-414&amp;publication_year=2010&amp;author=Marelli%2CD&amp;author=Aramaki%2CM&amp;author=Kronland-Martinet%2CR&amp;author=Verron%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DP. Mitchell, AN. Netravali, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Mitchell DP, Netravali AN (1988) Reconstruction filters in computer graphics. Comput Graph 22(4):221–228" /><p class="c-article-references__text" id="ref-CR17">Mitchell DP, Netravali AN (1988) Reconstruction filters in computer graphics. Comput Graph 22(4):221–228</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F378456.378514" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20filters%20in%20computer%20graphics&amp;journal=Comput%20Graph&amp;volume=22&amp;issue=4&amp;pages=221-228&amp;publication_year=1988&amp;author=Mitchell%2CDP&amp;author=Netravali%2CAN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DQ. Nguyen, R. Fedkiw, HW. Jensen, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Nguyen DQ, Fedkiw R, Jensen HW (2002) Physically based modeling and animation of fire. ACM Trans Graph 21(3):7" /><p class="c-article-references__text" id="ref-CR18">Nguyen DQ, Fedkiw R, Jensen HW (2002) Physically based modeling and animation of fire. ACM Trans Graph 21(3):721–728</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F566654.566643" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physically%20based%20modeling%20and%20animation%20of%20fire&amp;journal=ACM%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=721-728&amp;publication_year=2002&amp;author=Nguyen%2CDQ&amp;author=Fedkiw%2CR&amp;author=Jensen%2CHW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JF. O’Brien, PR. Cook, G. Essl, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. ACM Trans Graph 20(3):529" /><p class="c-article-references__text" id="ref-CR19">O’Brien JF, Cook PR, Essl G (2001) Synthesizing sounds from physically based motion. ACM Trans Graph 20(3):529–536</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthesizing%20sounds%20from%20physically%20based%20motion&amp;journal=ACM%20Trans%20Graph&amp;volume=20&amp;issue=3&amp;pages=529-536&amp;publication_year=2001&amp;author=O%E2%80%99Brien%2CJF&amp;author=Cook%2CPR&amp;author=Essl%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Perlin, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Perlin K (1985) An image synthesizer. Comput Graph 19(3):287–296" /><p class="c-article-references__text" id="ref-CR20">Perlin K (1985) An image synthesizer. Comput Graph 19(3):287–296</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F325165.325247" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image%20synthesizer&amp;journal=Comput%20Graph&amp;volume=19&amp;issue=3&amp;pages=287-296&amp;publication_year=1985&amp;author=Perlin%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren Z, Yeh H, Lin M (2010) Synthesizing contact sounds between textured models. In: Proceedings of virtual rea" /><p class="c-article-references__text" id="ref-CR21">Ren Z, Yeh H, Lin M (2010) Synthesizing contact sounds between textured models. In: Proceedings of virtual reality (VR), pp 139–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Roads, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Roads C (2004) Microsound. The MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR22">Roads C (2004) Microsound. The MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Microsound&amp;publication_year=2004&amp;author=Roads%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Serra, J. Smith, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Serra X, Smith J III (1990) Spectral modeling synthesis: a sound analysis/synthesis system based on a determin" /><p class="c-article-references__text" id="ref-CR23">Serra X, Smith J III (1990) Spectral modeling synthesis: a sound analysis/synthesis system based on a deterministic plus stochastic decomposition. Comput Music J 14(4):12–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F3680788" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spectral%20modeling%20synthesis%3A%20a%20sound%20analysis%2Fsynthesis%20system%20based%20on%20a%20deterministic%20plus%20stochastic%20decomposition&amp;journal=Comput%20Music%20J&amp;volume=14&amp;issue=4&amp;pages=12-24&amp;publication_year=1990&amp;author=Serra%2CX&amp;author=Smith%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Smith, E. Ericson, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Smith S, Ericson E (2009) Using immersive game-based virtual reality to teach fire-safety skills to children. " /><p class="c-article-references__text" id="ref-CR24">Smith S, Ericson E (2009) Using immersive game-based virtual reality to teach fire-safety skills to children. Virtual Real 13(2):87–99</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0113-6" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20immersive%20game-based%20virtual%20reality%20to%20teach%20fire-safety%20skills%20to%20children&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=2&amp;pages=87-99&amp;publication_year=2009&amp;author=Smith%2CS&amp;author=Ericson%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RJ. Stone, A. Caird-Daley, K. Bessell, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Stone RJ, Caird-Daley A, Bessell K (2009) SubSafe: a games-based training system for submarine safety and spat" /><p class="c-article-references__text" id="ref-CR25">Stone RJ, Caird-Daley A, Bessell K (2009) SubSafe: a games-based training system for submarine safety and spatial awareness (Part 1). Virtual Real 13(1):3–12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-008-0110-1" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SubSafe%3A%20a%20games-based%20training%20system%20for%20submarine%20safety%20and%20spatial%20awareness%20%28Part%201%29&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=1&amp;pages=3-12&amp;publication_year=2009&amp;author=Stone%2CRJ&amp;author=Caird-Daley%2CA&amp;author=Bessell%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Doel, P. Kry, D. Pai, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Van Den Doel K, Kry P, Pai D (2001) Foley automatic: physically-based sound effects for interactive simulation" /><p class="c-article-references__text" id="ref-CR26">Van Den Doel K, Kry P, Pai D (2001) Foley automatic: physically-based sound effects for interactive simulation and animation. ACM Trans Graph 20(3):537–544</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Foley%20automatic%3A%20physically-based%20sound%20effects%20for%20interactive%20simulation%20and%20animation&amp;journal=ACM%20Trans%20Graph&amp;volume=20&amp;issue=3&amp;pages=537-544&amp;publication_year=2001&amp;author=Doel%2CK&amp;author=Kry%2CP&amp;author=Pai%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang J, Yang X, Lu W (2011) FFT-upsampling for smoke animation. In Proceedings of ACM/eurographics symposium o" /><p class="c-article-references__text" id="ref-CR27">Wang J, Yang X, Lu W (2011) FFT-upsampling for smoke animation. In Proceedings of ACM/eurographics symposium on computer animation poster</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Watt, SC. Maddock, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Watt A, Maddock SC (2000) Computer games technology and higher education. Virtual Real 5(4):185–194" /><p class="c-article-references__text" id="ref-CR28">Watt A, Maddock SC (2000) Computer games technology and higher education. Virtual Real 5(4):185–194</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF01408517" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20games%20technology%20and%20higher%20education&amp;journal=Virtual%20Real&amp;volume=5&amp;issue=4&amp;pages=185-194&amp;publication_year=2000&amp;author=Watt%2CA&amp;author=Maddock%2CSC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Zheng, DL. James, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Zheng C, James DL (2009) Harmonic fluids. ACM Trans Graph 28(3):37" /><p class="c-article-references__text" id="ref-CR29">Zheng C, James DL (2009) Harmonic fluids. ACM Trans Graph 28(3):37</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1531326.1531343" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Harmonic%20fluids&amp;journal=ACM%20Trans%20Graph&amp;volume=28&amp;issue=3&amp;publication_year=2009&amp;author=Zheng%2CC&amp;author=James%2CDL">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0271-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to thank the anonymous reviewers for their insightful comments. This work was supported by the Natural Science Foundation of China under grant nos. 61170118 and 60803047, and the Application Foundation Research Plan Project of Tianjin under grant no. 14JCQNJC00100.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Computer Science and Technology, Tianjin University, Tianjin, People’s Republic of China</p><p class="c-article-author-affiliation__authors-list">Shiguang Liu &amp; Zhuojun Yu</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Tianjin Key Laboratory of Cognitive Computing and Application, No. 92, Weijin Road, Nankai District, Tianjin, 300072, People’s Republic of China</p><p class="c-article-author-affiliation__authors-list">Shiguang Liu</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Shiguang-Liu"><span class="c-article-authors-search__title u-h3 js-search-name">Shiguang Liu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Shiguang+Liu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shiguang+Liu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shiguang+Liu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Zhuojun-Yu"><span class="c-article-authors-search__title u-h3 js-search-name">Zhuojun Yu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Zhuojun+Yu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zhuojun+Yu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zhuojun+Yu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0271-7/email/correspondent/c1/new">Shiguang Liu</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Sounding%20fire%20for%20immersive%20virtual%20reality&amp;author=Shiguang%20Liu%20et%20al&amp;contentID=10.1007%2Fs10055-015-0271-7&amp;publication=1359-4338&amp;publicationDate=2015-08-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0271-7" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0271-7" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Liu, S., Yu, Z. Sounding fire for immersive virtual reality.
                    <i>Virtual Reality</i> <b>19, </b>291–302 (2015). https://doi.org/10.1007/s10055-015-0271-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0271-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08-03">03 August 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-05">05 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-12">12 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11">November 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0271-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0271-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Fire sound</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Physically based simulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">GPU</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive virtual reality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0271-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=271;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

