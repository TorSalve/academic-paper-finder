<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Linking GIS with real-time visualisation for exploration of landscape "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="To allow rural communities to evaluate possible future landscape scenarios, we have created a portable environment for landscape simulation (envisioning system). The goal of this system is to give..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops"/>

    <meta name="dc.source" content="Virtual Reality 2006 9:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-03-18"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="To allow rural communities to evaluate possible future landscape scenarios, we have created a portable environment for landscape simulation (envisioning system). The goal of this system is to give communities the opportunity to plan their desired futures. Our system is designed for workshop environments and allows workshop attendees to explore and to interact with representations of virtual landscapes. We are using virtual reality technology to visualise the landscape representations, a geographic information system to allow participants to change the current landscape configuration, and mobile computing devices to allow the attendees to navigate in the virtual landscape, and give feedback and opinions on the landscape changes. Here, we describe the technology that implements the interaction between geographical information systems and real-time rendering needed to achieve real-time visualisation of landscape changes. To achieve this functionality we have programmed two software clients (a renderer and an ESRI ArcMap extension) and a server that handles message flow. The landscape has been divided into management units that each supports one land use type. Using the GIS interface, users can change the land uses associated to the units and the renderer will update the landscape correspondingly in real time."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-03-18"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="260"/>

    <meta name="prism.endingPage" content="270"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0023-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0023-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0023-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0023-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2006/04"/>

    <meta name="citation_online_date" content="2006/03/18"/>

    <meta name="citation_firstpage" content="260"/>

    <meta name="citation_lastpage" content="270"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0023-9"/>

    <meta name="DOI" content="10.1007/s10055-006-0023-9"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0023-9"/>

    <meta name="description" content="To allow rural communities to evaluate possible future landscape scenarios, we have created a portable environment for landscape simulation (envisioning sy"/>

    <meta name="dc.creator" content="Christian Stock"/>

    <meta name="dc.creator" content="Ian D. Bishop"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Environ Plann B Plann Des; citation_title=Linking modelling and visualisation for natural resources management; citation_author=ID Bishop, C Karadaglis; citation_volume=24; citation_publication_date=1997; citation_pages=345-358; citation_doi=10.1068/b240345; citation_id=CR1"/>

    <meta name="citation_reference" content="Bishop ID, Dave B (2001) Beyond the moving camera: systems development for interactive immersive exploration of urban environments. In: Kim K (ed) Proceedings for computers in urban planning and urban management, Honolulu"/>

    <meta name="citation_reference" content="citation_journal_title=Environ Model Softw; citation_title=Supporting personal world-views in an envisioning system; citation_author=ID Bishop, IV RB Hull, C Stock; citation_volume=20; citation_publication_date=2005; citation_pages=1459-1468; citation_doi=10.1016/j.envsoft.2004.06.014; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Landsc Arche; citation_title=Exploring design through 3-dimensional simulations; citation_author=JW Danahy, R Wright; citation_volume=78; citation_issue=4; citation_publication_date=1988; citation_pages=64-71; citation_id=CR4"/>

    <meta name="citation_reference" content="Duchaineau MA, Wolinsky M, Sigeti DE, Miller MC, Aldrich C, Mineev-Weinstein MB (1997) ROAMing terrain: real-time optimally adapting meshes. In: Yagel R, Hagen H (eds) Proceedings of IEEE visualization, Phoenix, pp 81&#8211;88"/>

    <meta name="citation_reference" content="citation_journal_title=GIS World; citation_title=3-D for free: toolkit expands visual dimensions in GIS; citation_author=R Hoinkes, E Lange; citation_volume=8; citation_issue=7; citation_publication_date=1995; citation_pages=54-56; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_title=Communityviz: an integrated planning support system; citation_inbook_title=Planning support systems: integrating geographic information systems and visualization tools; citation_publication_date=2001; citation_pages=285-308; citation_id=CR7; citation_author=M Kwartler; citation_author=RN Bernard; citation_publisher=ESRI Press"/>

    <meta name="citation_reference" content="citation_title=Visualization in support of public participation; citation_inbook_title=Visualization for landscape and environmental planning: technology and applications; citation_publication_date=2005; citation_pages=251-260; citation_id=CR8; citation_author=M Kwartler; citation_publisher=Taylor &amp; Francis"/>

    <meta name="citation_reference" content="citation_journal_title=Environ Plann B; citation_title=An integrated environment for urban simulation; citation_author=R Liggett, W Jepson; citation_volume=22; citation_publication_date=1995; citation_pages=291-305; citation_doi=10.1068/b220291; citation_id=CR9"/>

    <meta name="citation_reference" content="Lindstrom P, Koller D, Ribarsky W, Hodges LF, Faust N, Turner GA (1996) Real-time, continuous level of detail rendering of height fields. In: Rushmeier H (ed) Proceedings of ACM SIGGRAPH 96. New Orleans, pp 109&#8211;118"/>

    <meta name="citation_reference" content="citation_title=OpenGL programming guide: the official guide to learning OpenGL, Version 1.2; citation_publication_date=1999; citation_id=CR11; citation_author=null OpenGL Architecture Review Board; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In: Glassner A (ed) Proceedings of ACM SIGGRAPH 94, Orlando, pp 381&#8211;394"/>

    <meta name="citation_reference" content="citation_journal_title=Cartogr J; citation_title=Integration methodologies for interactive forest modelling and visualisation systems; citation_author=H Tang, ID Bishop; citation_volume=39; citation_publication_date=2002; citation_pages=27-35; citation_id=CR13"/>

    <meta name="citation_author" content="Christian Stock"/>

    <meta name="citation_author_email" content="cstock@unimelb.edu.au"/>

    <meta name="citation_author_institution" content="Department of Geomatics,  The University of Melbourne,  Melbourne, Australia"/>

    <meta name="citation_author" content="Ian D. Bishop"/>

    <meta name="citation_author_institution" content="Department of Geomatics,  The University of Melbourne,  Melbourne, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0023-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/04/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0023-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops"/>
        <meta property="og:description" content="To allow rural communities to evaluate possible future landscape scenarios, we have created a portable environment for landscape simulation (envisioning system). The goal of this system is to give communities the opportunity to plan their desired futures. Our system is designed for workshop environments and allows workshop attendees to explore and to interact with representations of virtual landscapes. We are using virtual reality technology to visualise the landscape representations, a geographic information system to allow participants to change the current landscape configuration, and mobile computing devices to allow the attendees to navigate in the virtual landscape, and give feedback and opinions on the landscape changes. Here, we describe the technology that implements the interaction between geographical information systems and real-time rendering needed to achieve real-time visualisation of landscape changes. To achieve this functionality we have programmed two software clients (a renderer and an ESRI ArcMap extension) and a server that handles message flow. The landscape has been divided into management units that each supports one land use type. Using the GIS interface, users can change the land uses associated to the units and the renderer will update the landscape correspondingly in real time."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0023-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Envisioning systems, Virtual reality, Geographic information systems, Community values","kwrd":["Envisioning_systems","Virtual_reality","Geographic_information_systems","Community_values"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0023-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0023-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=23;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0023-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0023-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0023-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-03-18" itemprop="datePublished">18 March 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christian-Stock" data-author-popup="auth-Christian-Stock" data-corresp-id="c1">Christian Stock<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content=" The University of Melbourne" /><meta itemprop="address" content="grid.1008.9, 000000012179088X, Department of Geomatics,  The University of Melbourne, Parkville,  3010,  Melbourne,  VIC, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ian_D_-Bishop" data-author-popup="auth-Ian_D_-Bishop">Ian D. Bishop</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content=" The University of Melbourne" /><meta itemprop="address" content="grid.1008.9, 000000012179088X, Department of Geomatics,  The University of Melbourne, Parkville,  3010,  Melbourne,  VIC, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">260</span>–<span itemprop="pageEnd">270</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">221 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0023-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>To allow rural communities to evaluate possible future landscape scenarios, we have created a portable environment for landscape simulation (envisioning system). The goal of this system is to give communities the opportunity to plan their desired futures. Our system is designed for workshop environments and allows workshop attendees to explore and to interact with representations of virtual landscapes. We are using virtual reality technology to visualise the landscape representations, a geographic information system to allow participants to change the current landscape configuration, and mobile computing devices to allow the attendees to navigate in the virtual landscape, and give feedback and opinions on the landscape changes. Here, we describe the technology that implements the interaction between geographical information systems and real-time rendering needed to achieve real-time visualisation of landscape changes. To achieve this functionality we have programmed two software clients (a renderer and an ESRI ArcMap extension) and a server that handles message flow. The landscape has been divided into management units that each supports one land use type. Using the GIS interface, users can change the land uses associated to the units and the renderer will update the landscape correspondingly in real time.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The concept of linking geographic information systems (GIS) functions to the presentation and exploration options of virtual reality (VR) has attracted considerable attention. Among early developers were Liggett and Jepson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Liggett R, Jepson W (1995) An integrated environment for urban simulation. Environ Plann B 22:291–305" href="/article/10.1007/s10055-006-0023-9#ref-CR9" id="ref-link-section-d94470e297">1995</a>) who used remote procedure calls from the ArcView GIS (esri.com) to change physical elements within an SGI Performer-based (Rohlf and Helman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In: Glassner A (ed) Proceedings of ACM SIGGRAPH 94, Orlando, pp 381–394" href="/article/10.1007/s10055-006-0023-9#ref-CR12" id="ref-link-section-d94470e300">1994</a>) VR. Their systems linkage was used for functions such as changing the species of street trees for urban design purposes. Furthermore, working from an urban design or landscape design perspective were a group at the Centre for Landscape Research at the University of Toronto (e.g. Danahy and Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Danahy JW, Wright R (1988) Exploring design through 3-dimensional simulations. Landsc Arche 78(4):64–71" href="/article/10.1007/s10055-006-0023-9#ref-CR4" id="ref-link-section-d94470e303">1988</a>; Hoinkes and Lange <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Hoinkes R, Lange E (1995) 3-D for free: toolkit expands visual dimensions in GIS. GIS World 8(7):54–56" href="/article/10.1007/s10055-006-0023-9#ref-CR6" id="ref-link-section-d94470e306">1995</a>) who did not link their Polytrim VR system to a GIS but did include GIS-like functions in their software.</p><p>In the forestry context, Bishop and Karadaglis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bishop ID, Karadaglis C (1997) Linking modelling and visualisation for natural resources management. Environ Plann B Plann Des 24:345–358" href="/article/10.1007/s10055-006-0023-9#ref-CR1" id="ref-link-section-d94470e312">1997</a>) also used Performer to create and interactive interface to multiple mapped GIS outputs. There was no direct communications between the software, however. Tang and Bishop (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Tang H, Bishop ID (2002) Integration methodologies for interactive forest modelling and visualisation systems. Cartogr J 39:27–35" href="/article/10.1007/s10055-006-0023-9#ref-CR13" id="ref-link-section-d94470e315">2002</a>) used the RPC approach to automate this transfer from the GIS to Performer but never tested the created system in a decision context.</p><p>Development of a system especially for community decision-making was the focus of the CommunityViz project (Kwartler and Bernard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kwartler M, Bernard RN (2001) Communityviz: an integrated planning support system. In: Brail RK, Klosterman RE (eds) Planning support systems: integrating geographic information systems and visualization tools. ESRI Press, Redlands, pp 285–308" href="/article/10.1007/s10055-006-0023-9#ref-CR7" id="ref-link-section-d94470e321">2001</a>). In this approach the 3D models are generated automatically from ArcView GIS data using Multigen Creator software (multigen.com). When the GIS environment changes, a new 3D world is created. Other modules relating to scenario creation and policy simulation were also included in the Community Viz (Kwartler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Kwartler M (2005) Visualization in support of public participation. In: Bishop ID, Lange E (eds) Visualization for landscape and environmental planning: technology and applications. Taylor &amp; Francis, London, pp 251–260" href="/article/10.1007/s10055-006-0023-9#ref-CR8" id="ref-link-section-d94470e324">2005</a>).</p><p>Like CommunityViz and its predecessors, the system described here allows a group of people to interactively explore representations of virtual landscapes using an immersive display. The main use of this system is to give community stakeholders of a region (for example, a specific catchment area) the ability to explore and discuss possible future landscape configurations. We call this system an envisioning system (EvS), since it allows users to learn about future options, without forcing them to make decisive choices or finding solutions to certain problems as expected from a decision support system (see Bishop et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bishop ID, Hull IV RB, Stock C (2005) Supporting personal world-views in an envisioning system. Environ Model Softw 20:1459–1468" href="/article/10.1007/s10055-006-0023-9#ref-CR3" id="ref-link-section-d94470e330">2005</a>). Effective linkage to a GIS to support envisioning involves more than simply technical issues of network communications. The details of the design depend also upon the specific functionality being sought.</p><p>We have developed the EvS to study community values and interactions with their landscape. The system is designed for workshop environments. It allows us to present specific future landscape scenarios, but also allows the community to change various parameters to build their own landscape configurations. The landscape renders are real time and in 3D, and the workshop participants can navigate the virtual landscape freely to view their world from different vantage points. Besides realistic looking landscapes (e.g. hills covered in trees), we can also visualise environmental impacts that come with some landscape changes, for example, changes in water quality and soil erosion. This will help the community to learn and understand how landscape changes may affect their environment. At any point, we can call for a vote, for example: “Is this a good future option for this region?”. Community members can than cast their vote, which will be recorded for later evaluation.</p><p>Here, we discuss both conceptual and technical aspects of how to link the renderer with geographical information systems (GIS), to achieve real-time changes in virtual landscapes using spatial data in a way that is meaningful for the community. In two forthcoming papers we will discuss the application of the system, and the technical details behind user interaction (via handheld computing devices) and the overall systems architecture of the EvS.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Overview of system functionality</h2><div class="c-article-section__content" id="Sec2-content"><p>The main purpose of the EvS (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig1">1</a>) is to allow community exploration of changing virtual landscape representations. The main interface component is a large screen through which the three-dimensional landscape is visible. One of our desirable features is that the community members are able to navigate through the landscape in real time, and landscape changes should also happen in near real time. Since our system is used in workshop environments, interactions with the system have to be easily accessible to workshop participants or individuals that are given control over certain functionality at different points in time. There has to be an intuitive way of changing individual parts of the virtual landscape, so the community members can configure the visible landscape to alternative landscape options they like to explore. Ideally, the landscape change input would be as flexible as possible. The landscape changes we are interested in are changes in actual land use (e.g. switching from cattle grazing to growing vineyards), and changes in land management practices (e.g. reduced level of weed control).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>This shows the setup of the envisioning system. The software clients PA-Server and PA-Render are running on a server machine. PA-Render is sending the rendered 3D images to three projectors that are connected to the server machine using a Matrox Parphelia card. The three projectors project the image onto a back-projected screen about 6 m wide and 1.8 m high. On the GIS machine there is the client software PA-GIS running. The GIS machine is connected to a plasma screen showing a 2D map that corresponds to the area visible on the 3D view. The server and GIS machines are connected with an Ethernet hub. There are also instances of PA-PDA running on several PDA devices. These devices are connected via Bluetooth to a Bluetooth access point, which in turn is connected to the Ethernet hub</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>We use a three-segmented back-projected screen to show our landscape renderings to community members. A desktop machine with a 3D graphics accelerator card is used for the landscape rendering. We use the Matrox Parphelia card since it allows splitting the rendered landscape into three parts. These three landscape segments are sent to three projectors that are placed behind the back-projected screen. The screen is about 1.8 m high and 6 m wide. The screen panels are configured with a 30° angle in between them, so they approximate a 135° field of view. This allows a high degree of immersion at a distance of up to 5 m.</p><p>For landscape change input we are using a second machine running with ESRI ArcMap 8 or 9. A map of the study area is loaded as a layer into ArcMap and shown on a 200 cm plasma screen. To change the landscape, it would be ideal to be able to ‘draw’ polygons of arbitrary size and shape onto the map in ArcMap, and then associate a land use with the polygon. The renderer would then populate the area of the polygon with 3D objects that are representative for that land use (e.g. trees for forest). However, the generation of 3D objects in space during runtime is computationally heavy and not achievable on modern desktop machines in real time. Hence, we decided early in the project to pre-generate the 3D objects before runtime and load them into the system during start-up. These models will then simply be hidden or made visible depending on land use selection. For this approach to work, a study area has to be subdivided into subsections (which we call management units), and for each management unit and land use the 3D models have to be pre-generated. Similarly, the selection of land use options has to be known when the pre-built 3D models are generated. Therefore, management units, land use options, and land management options have to be known before a workshop is conducted (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig2">2</a> shows a map of land management units). Ideally, those variables reflect the conditions in the study area (e.g. property boundaries for management units and land use options that the community considers viable).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The different coloured areas show different management units in a study area. The <i>black and grey lines</i> show roads and rivers, respectively, the <i>black dots</i> show buildings</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To allow individual community members to navigate the virtual landscape and to allow the collection of feedback via voting calls, we distribute a limited number of PDA devices during a workshop. Although it is usually expected that the number of people in a workshop exceed the number of PDA devices, the devices can be passed around so that other attendees can access them. To allow free movement of the devices within the workshop area the system is using wireless Bluetooth technology. The PDA devices are connected to a Bluetooth access point, which can handle up to six devices. The access point is connected to an Ethernet hub, which also connects the render and GIS machines.</p><p>To add the desired functionality to our system, we have written three software clients, which we called PA-Render, PA-GIS, and PA-PDA. A fourth software program, PA-Server, is running on the render machine and handles all communication between the clients. This paper describes the functionality of PA-Render and the important aspects of PA-GIS for changing landscapes in PA-Render. The other system components are described in a forthcoming paper.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">PA-Render</h2><div class="c-article-section__content" id="Sec3-content"><p>PA-Render is the renderer software that we implemented for rendering the 3D landscape views in the EvS. It is written in C++ and we used OpenGL and SGI OpenGL Performer (Rohlf and Helman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In: Glassner A (ed) Proceedings of ACM SIGGRAPH 94, Orlando, pp 381–394" href="/article/10.1007/s10055-006-0023-9#ref-CR12" id="ref-link-section-d94470e413">1994</a>) for common graphics and scene graph functionality. OpenGL Performer ships with a demonstration application called ‘perfly’ that has the basic functionality of a 3D model viewer. Perfly can load various common 3D model formats and add them to the main scene graph, which gets rendered in the main view. The viewer allows the user to change the current viewpoint and also to modify some global settings via a menu or command line parameters, such as lighting and frustum.</p><p>We extended the capabilities of perfly to create the desired rendering capabilities for the EvS. For example, we added the capability of interaction between users and objects. Additionally, objects can also interact with each other. To achieve this functionality, any object can be defined as a trigger or a target. Targets are usually linked to triggers. When a trigger is activated (for example when the active camera is within a defined proximity distance of the trigger), it will search for a linked target and a pre-defined action to perform on the target. By activating a trigger, targets can be animated, for example, the valve of an irrigation gate could act as a trigger to open and close the gate (the gate being the target). We also added the capability of animating objects along pre-defined animation paths, for example, animated car objects could continually move along pre-defined paths on a road system. Another extension is the inclusion of positional sound sources. Like 3D objects, these sound sources can be static, animated along a path, or animated by a trigger object. Apart from 3D objects and sound sources, light sources can also be added and animated. These extensions to PA-Render are explained in more detail in Bishop and Dave (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bishop ID, Dave B (2001) Beyond the moving camera: systems development for interactive immersive exploration of urban environments. In: Kim K (ed) Proceedings for computers in urban planning and urban management, Honolulu" href="/article/10.1007/s10055-006-0023-9#ref-CR2" id="ref-link-section-d94470e419">2001</a>).</p><p>OpenGL Performer extends on the basic primitive rendering capabilities of OpenGL by adding scene graph functionality. The main advantage of the use of scene graphs is that 3D objects can be spatially organised, which allows for fast culling of objects and hence faster rendering times (Rohlf and Helman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In: Glassner A (ed) Proceedings of ACM SIGGRAPH 94, Orlando, pp 381–394" href="/article/10.1007/s10055-006-0023-9#ref-CR12" id="ref-link-section-d94470e425">1994</a>). To build a scene graph in perfly, for each 3D object a node is added to the scene graph tree, ideally dependent on spatial location (the programmer is responsible for providing the spatial hierarchy). At runtime, perfly goes hierarchically through the scene graph tree and renders all visible nodes. There are several different node types with different behaviour available in perfly, e.g. translation, rotation, and level of detail nodes. To extend the features in PA-Render, we added additional nodes types to the selection of node types that are implemented in perfly (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0023-9#Tab1">1</a>).
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Scene graph nodes available in PA-Render</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0023-9/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Object nodes are used simply to load a 3D object into the scene graph. This node typically links to an external file, for example a Wavefront OBJ model. These 3D objects can optionally be translated and rotated within the scene, after they are loaded, to place them at a specific location. Light and sound nodes work similarly to object nodes, but instead load light and sound sources work. For the light source, the ambient, diffuse, and specular colours and the direction of the light (for spot lights) can also be specified. The trigger and target nodes give 3D objects trigger and target behaviour. The trigger node is usually linked to at least one target node. The target node usually includes information about the available actions to be performed on the target once a linked trigger is activated. This can include object animation or sound parameters (if the object is a sound source). The path node is used to pre-define an animation path along which an object can be animated. The terrain node specifies an elevation grid (for example an ArcInfo raster export file) and a bitmap that gets draped over the terrain (typically an aerial photograph). The terrain elevation is loaded as an ASD (active surface definition) node, which means that it uses the inbuilt OpenGL Performer continuous level of detail (CLOD) functionality. For more details on how CLOD works see, e.g. Lindstrom et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Lindstrom P, Koller D, Ribarsky W, Hodges LF, Faust N, Turner GA (1996) Real-time, continuous level of detail rendering of height fields. In: Rushmeier H (ed) Proceedings of ACM SIGGRAPH 96. New Orleans, pp 109–118" href="/article/10.1007/s10055-006-0023-9#ref-CR10" id="ref-link-section-d94470e578">1996</a>) and Duchaineau et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Duchaineau MA, Wolinsky M, Sigeti DE, Miller MC, Aldrich C, Mineev-Weinstein MB (1997) ROAMing terrain: real-time optimally adapting meshes. In: Yagel R, Hagen H (eds) Proceedings of IEEE visualization, Phoenix, pp 81–88" href="/article/10.1007/s10055-006-0023-9#ref-CR5" id="ref-link-section-d94470e581">1997</a>). The regions node is used to group objects into regions for the purpose of selecting different land cover options.</p><p>Since one the main purposes of the EvS is to display different land cover options, the system needs to be able to update the scene graph of PA-Render at runtime to display different appropriate 3D models. For our application, we have defined specific 3D objects that correspond to matching land use options. For example, for a land use option ‘forest’ we have modelled 3D trees, and for the land use option ‘vineyards’ we have modelled 3D rows of vines. PA-Render will load these pre-defined 3D objects into the scene graph during start-up and group them spatially and in time. Then, individual 3D objects can be enabled or disabled during runtime by switching nodes on and off. Consequentially, only the enabled 3D objects will be rendered.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig3">3</a> shows the scene graph in more detail. Standard objects, like 3D objects, and sound and light sources, are added to the root node during start-up. The 3D objects used for land cover representations are grouped into regions that are spatially distinct based on the management units of the study area. The management units are added to the scene graph as region nodes. In theory, the number of management units can be arbitrary. However, the number of region nodes has a dramatic impact on the overall size of the scene graph (as it will become clear in the following paragraphs) and hence rendering performance. The render system requires enough available memory to be able to load the entire scene graph, and enough processing power to load it within a reasonable time. Another factor to consider is ‘manageability’ of the management unit dataset in a workshop environment. It may be desirable to have a vast number of management units to give the workshop attendants a high degree of freedom for changing the land cover distribution. However, changing and discussing land cover changes takes a certain amount of time, and a 2-h workshop may not be sufficient to explore 500 management units in great detail. A smaller number of management units may give less freedom, but may be more effective in conveying an overall picture of future options.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Drawing (<b>a</b>) shows a simple scene graph in PA-Render. The root node has two region child nodes and can have other child nodes, such as 3D objects nodes, sound nodes, etc. Each region node corresponds to a management unit and can have several landclass/management child nodes. These nodes correspond to the available land use and land management options. Each of the landclass/management nodes has a number of child nodes (<b>b</b>). There can be variations in time and level of detail (LOD) for each land use and land management option. Finally, each LOD node points to a number of 3D model child nodes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>For each management unit (region), there are a number of land cover options defined (for example, in grazing, crop horticulture, and farm forestry). Stakeholders may also be interested in different levels of management input for a specified land cover class: for example, the differences between a ‘well managed’ grazing parcel (e.g. including a high level of weed, erosion, and soil acidity control) and a ‘poorly managed’ grazing parcel. Land cover options and land management options are pre-defined and cannot be changed during runtime. They are added to the scene graph as a ‘landclass/management’ node during startup. Land cover classes and management options are treated as unique combinations, i.e. ‘high management grazing’ is added as a separate node, as is ‘low management grazing’ and ‘farm forestry’. It is also possible for each region node to have different selections of landclass/management types. For example, it may not make sense to offer ‘crop horticulture’ as an option for a management unit that is spatially defined as a riparian zone. Offering only sensible land cover and management options for each management unit helps to reduce the size of the overall scene graph.</p><p>Land cover and management options can vary visually in time. For example, a forest farm starts with small trees that grow over time. Since the trees need more space while they grow, a number of them will be removed during growth (thinning). Hence, young farm forests contain many small trees, whereas older farm forests have a lesser number of tall trees. To be able to see those variations, we have added time nodes to the landclass/management nodes in the scene graph (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig3">3</a>b). Not every land cover and management option has temporal variations; therefore different time nodes are only added to relevant landclass/management nodes to limit the size of the scene graph.</p><p>We have also added level of detail (LOD) management to the scene graph. Since we are rendering a potentially big open space with a significant amount of detail, we have to find a way to keep frame rates at an interactive level (i.e. above 10 fps). To achieve this, we have defined different 3D models for each land use option. We could, for example, have three LOD nodes. The first LOD node defines a model that has detailed tree models at a high density. The second LOD node defines a model with crosshatched trees (trees made out of 2 textured planes intersecting each other) with medium density. The third LOD node could have no trees in it. PA-Render continuously calculates the distances of the camera viewpoint to each centre of a region (management unit). Each LOD node is defined to be visible in a certain distance range. The LOD nodes get added to the time nodes in the scene graph (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig3">3</a>b).</p><p>At the end of the scene graph tree are the 3D models for each region, land cover and management option, time, and LOD distance. These 3D models are loaded during startup and cannot be modified during runtime. Each of these 3D models is in general constructed from smaller single 3D objects. For example, a forest would best be made up from single 3D tree models. Each forest 3D model sitting at the end of the scene graph hierarchy is made up of trees, but will look different, because the model is placed at a different location (e.g. each management unit has its own defined 3D forest model). There are two ways of defining a 3D model. The scene graph can refer to an external 3D file (for example a Wavefront OBJ file). This way each end node has a corresponding OBJ file (and hence there are a lot of OBJ files). The other way is to define a scatter (placement) file that refers to external OBJ files and places them in xyz space. For a forest, for example, the scatter file would refer to an OBJ file containing a single tree and place that tree at different locations. It is possible to refer to different OBJ files in a scatter file, for example, to have several types of trees within a region. Using scatter files, OBJ files can be shared between different regions, hence only the basic OBJ files (trees, houses, etc) are required to create a land cover set for each region.</p><p>Apart from 3D objects that are assigned to a region to define a land cover option visually, one can also assign a ground texture to a region. Using a ground texture is of advantage to visualise features that are close to the ground and extend over larger areas, such as crops or weeds. Ground texture regions are defined as triangles. PA-Render usually draws an aerial photograph over the terrain. To draw a region with a different texture into the aerial photograph, we use the stencil buffer (e.g. OpenGL Architecture Review Board <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="OpenGL Architecture Review Board (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2. 3rd edn. Addison-Wesley, Reading" href="/article/10.1007/s10055-006-0023-9#ref-CR11" id="ref-link-section-d94470e638">1999</a>). This works by firstly drawing the base texture (aerial photograph) into the texture memory. Next, the polygon geometry (the area of the region) is drawn into the stencil buffer. Using the stencil buffer test, the land cover texture (for example, a texture showing weeds) is drawn into the texture memory. The land cover texture will only be drawn where the stencil buffer is set (i.e. where the polygon is located). Finally, this new combined texture is drawn over the terrain (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig4">4</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>This illustrates how the ground texture (<b>a</b>) gets modified with the help of the stencil buffer. If a region is enabled, the area of the region will be drawn into the stencil buffer (<b>b</b>). Then, a tileable ground land use texture will be drawn over the ground texture (<b>a</b>) with the stencil buffer enabled. The land use texture will only be drawn in the enabled (<i>black</i>) areas of the stencil buffer and a new ground texture is produced</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To keep frame rates at a reasonable level, we did not use the traditional crosshatched textured planes for plants in a row or grid configuration (e.g. orchard trees, vineyards). Instead, we used two long planes at an angle to each other (top to bottom) with multiple plants on one texture. To simulate growing trees (and the thinning of trees), we simply substituted the texture with taller trees with bigger spacing in between them (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig5">5</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Hardwood plantation textures. The textures were used to map elongated and two-sided rectangles to give the impression of rows of trees. The three different textures were applied at different time stages to simulate growing trees, i.e. they correspond to the age of the plantation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To visualise additional information, such as the current heading of the camera view or voting results after a voting call, we have added an overlay window to PA-Render (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig4">4</a>). In this window 2D symbols are drawn, such as icons or text. To assist with orientation, the overlay window includes the current heading in compass form. The current time step of a simulated landscape change in years is also shown. When a landscape change is advanced in time (for example a growing forest), the users will know which time step is currently visualised. The overlay window is also used to display colour-coded iconic information on non-visual outcomes of landscape changes. For example, a water quality comparison can be run between the current existing land cover of a study area and an alternative land cover configuration currently rendered in the 3D view. Furthermore, the results of a vote can be displayed in the overlay window. Thus, the result is instantly visible to all users and can be discussed. Finally, a console window can be used for text output (e.g. error messages).</p><p>Since perfly is a simple 3D object viewer application, it only needs to be able to load 3D files, and hence only recognises a selection of 3D industry formats (e.g. Wavefront OBJ, Discreet 3DS). However, we need to be able to also load specific parameters for individual 3D objects, and also link 3D objects together to achieve specific functionality (e.g. grouping objects). Therefore, we have created a new file format to load this extra information. This new file format uses XML (extensible markup language) and we named it ‘scene file’ format. The scene file is usually referencing other files that contain the 3D geometry information (i.e. files that perfly understands natively). It uses different sections, which we explain in the following.</p><p>In the global section, several global parameters get set. This includes, for example, enabling and disabling sound or the overlay and console window. The HUD (head-up display) section controls the layout of the overlay window (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig6">6</a>). The user can specify which 2D graphics file is to be loaded for the compass and which ones for the environmental impact model icons. The size and placement of the graphics elements can also be specified. This way, the overlay window can be configured differently for different scene files.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The main menu of PA-Render. In the landscape view, there are different land cover options shown, e.g. vineyards and orchards. The <i>overlay window</i> is showing a compass, a vote result from a yes/no vote, several impact icons (water quantity, water quality, soil erosion, soil salinity, soil acidity, and native bird habitat), and a time indicator (from <i>left to right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The objects section is used to specify 3D objects to be loaded and added to the scene graph (e.g. in Wavefront OBJ format). The objects can be translated and rotated, and thus added to a specific location within the scene. It can also be specified whether the object should be modified by the current lighting conditions. This is useful, for example, when loading a skybox, which should not be affected by any shadows. It is also specified in this section if an object should act as a trigger or a target. Terrain geometry has its own section. In this section, a digital terrain file that is supported natively by OpenGL Performer (e.g. ESRI arcinfo) is referenced. A reference to a 2D bitmap that is used as a base ground texture can be added.</p><p>Another section is defining the scatter objects which refer to a 3D object file and a scatter file. The scatter file is in plain ASCII format and contains xyz positions. When this section is loaded, PA-Render will add the referenced 3D object to the scene graph and place it at the positions that are specified in the scatter file. Also, a parameter can be set that forces PA-Render to calculate the <i>z</i> component for the position of each object from the underlying terrain. This way, one would only need to know <i>x</i>- and <i>y</i>-coordinates to place the objects accurately onto a given terrain. However, this option is computation intensive.</p><p>In the region section, 3D objects get grouped by location and time. This is used to define parcels that can have different kinds of land cover. The objects get divided into subsections of management units, land use, level of detail, and time. Within each subsection, either an external 3D object file or a scatter object (which has to be defined in the scatter section) is referenced. The geometry of the management unit can also be specified (as tessellated triangles), together with a ground texture for land use. This is used to blend a land use ground texture over the terrain base texture.</p><p>With help of the light section, individual lights can be added to the scene graph. For each light, the position, ambient, diffuse, and specular colour components, and light direction can be set. The sound section is used to add positional sound sources. In this section an external sound file is referenced in a format that perfly can understand natively. Sounds can be global or placed in xyz space with an amplitude falloff parameter.</p><p>In the trigger section, triggers are mapped to targets that are defined in the target section. In the target section the actions that occur when a target is triggered are defined. An action can be a pre-defined rotation, a pre-defined translation along a path, or the playback of a pre-defined sound file. The rotation, path, and sound parameters are defined in the rotation, path, and sound sections, respectively. The triggers and targets can be mapped to objects in the objects section (Bishop and Dave <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bishop ID, Dave B (2001) Beyond the moving camera: systems development for interactive immersive exploration of urban environments. In: Kim K (ed) Proceedings for computers in urban planning and urban management, Honolulu" href="/article/10.1007/s10055-006-0023-9#ref-CR2" id="ref-link-section-d94470e760">2001</a>).</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">PA-GIS</h2><div class="c-article-section__content" id="Sec4-content"><p>To link the renderer to spatial information processing and location maps, we have implemented a module that is used with ESRI ArcMap 8. The software is written in C++ using ESRI ArcObjects 8. The module is programmed as an extension and can be enabled within ArcMap. It comes with its own menu for access to the individual implemented features (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig7">7</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The main menu of PA-GIS. This menu exposes the functionality of the ArcMap extension to the user. PA-GIS allows the user to change the land use of management units, generate 3D objects, navigate through the 3D view, compare non-visual impacts between two different land use configurations, and call for votes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Users of the EvS have an overview map next to the three-dimensional view of the study area through ArcMap. Also visible are the management units, which were digitised into shape file format, so that they can be used in ArcMap as layers. Each management unit layer contains the geometries of the units in form of polygons. The current land use values and the time values are associated with each polygon. The land use value reflects the currently selected land use. The time value identifies the time after a land use change took place. This is used, for example, to simulate growing trees. We have also added a value that records if a management unit is a riparian zone. This record is used internally to allow different sets of land uses for riparian and non-riparian zones. Ideally, one would have a field that would classify a management unit as a different type (for example, riparian zone, irrigation area, etc.), and have a set of land use options for each type. In our study we only needed to identify the riparian zones, so simply distinguishing between riparian and non-riparian was sufficient. For a study area, there can be any number of management unit shape files (limited only by computer performance). Each shape file can have a different land use and time configuration representing different base scenarios. Furthermore, it is possible to export an existing management unit layer into a shape file. This way a land cover configuration can be recorded at any point during a workshop.</p><p>For each management unit set there has to be a scene file for PA-Render that matches the geometry of the management units in the shape file. Furthermore, the available land use options have to be available in the scene file, where they are linked to 3D objects that are used to represent those land uses. Also, models for land uses that vary in time have to be available in the scene file. Land uses and time steps are hard coded into PA-GIS. It would be more flexible to have the land use options and time steps in external text files that are loaded at runtime, but since we implemented the software to be applied to only one study area, our approach was more practical.</p><p>All available land uses are grouped into one list, from which users can select their preferred land use options. For each land use, the number of time steps, land management options, and environmental process parameters are hard coded. Land use and management options are mapped together to a land use number (as an example see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0023-9#Tab2">2</a>). Those numbers have to be referred to in the scene file. This value is also recorded in the management unit shape file to identify the current land use and management option. The time steps are also hard coded. Each land use may have a number of different time steps available. For example, a regenerating forest land use may have five time steps to simulate growing trees over five time intervals. When the regenerated forestland use is selected for a management unit, the time will be set to zero (which represents 0–5 years in our case). Then, the time can be advanced four times, the final time step corresponding to 20–25 years. A land use that does not change over time (e.g. pasture) has only one time step defined. Each land use also has a number of environmental process parameters defined (this is explained in more detail in a forthcoming paper). Finally, each land use option has a flag indicating management units that are defined as riparian zones.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Example IDs for land use and management options</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0023-9/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>PA-GIS can also be used to generate scene files. A management unit layer can be selected and the geometry of each management unit can be written into a scene file (via an ASCII exporter that was implemented for PA-GIS) with the available land uses and time steps. The exporter can be modified to also include level of detail options. It is possible to write scene files with management units referencing scatter objects and 3D file objects. PA-GIS can also be used for the generation of scatter files. We have implemented an option to generate a layer of randomly generated points. These points can be exported as locations into scatter files. The exporter will read the <i>z</i>-component from a DTM raster layer and export a scatter file for each management unit. For the land use type pine plantation, for example, a scatter file with specified average tree density (e.g. one tree every 10 m) can be generated (alternatively, regularly spaced trees can be exported as lines, but this does not work in conjunction with scatter files). In the scene file, the scatter object could reference several similar texture mapped pine tress. We can also automatically generate 3D object files with the help of PA-GIS. The point locations can be exported together with an ID for the management unit in which the points are located and the <i>z</i>-components taken from a raster DTM file. This file can be further processed with a small external program that reads the exported text file and converts the information into a Wavefront OBJ file. The external program can place trees and buildings at the specified locations. It generates an OBJ file for each management unit, which can be linked to the scene file. This algorithm also works for lines, which we have used to build 3D rows of vine plants and fences (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig8">8</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>This shows the dialog to generate 2D point locations as a layer. The user can specify if the locations should be spatially distributed as a grid or randomly. For a grid distribution the <i>x</i> and <i>y</i> distance between two points in metres can be specified, for a random distribution an average density per square kilometre can be specified. The height option is used for generating OBJ files and controls the height of the generated objects (e.g. to generate 30 m high trees). A newly generated point layer will be added to the layer window to the <i>left</i>. A DTM layer (dtm10) has to be available to read the <i>z</i> values of the terrain at the point locations. A management unit layer has to be available to identify in which management units the points are located. The <i>map view</i> shows a point layer (random locations), together with a management unit layer and other map features</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Once a management unit layer with a matching scene file has been built, PA-GIS can be used to control the configuration of the virtual landscape in PA-Render. We have implemented several options to change the current landscape configuration. Whenever a new scenario (management unit configuration) is selected in ArcMap, the user can click on a ‘send all land uses’ button and a message to update all land uses and time steps is sent to PA-Render. This updates the scene graph in PA-Render. For each management unit the corresponding land use object is enabled and all other land use objects are disabled. One can quickly change from one land use scenario to another, for example, turn a predominantly pasture growing valley into a valley which mainly features orchards and vineyards.</p><p>However, one can also change the land cover on individual land management units. Users can select a tool and click on a management unit they want to change. A dialog box with a drop-down list of land use options (e.g. pasture, farm forestry, orchards) and a drop down list for land management options (e.g. low level, high level) appears. After selection, PA-GIS sends a message with management unit ID and land use ID to PA-Render. PA-Render updates the scene graph by disabling the old land use object and enabling the new land use object for the selected management unit. The time step value will be set to zero to reflect that the new land use has been ‘planted’ and the ‘growing’ process can begin. Multiple management units can be selected and changed to the same land use at the same time (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0023-9#Fig9">9</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0023-9/MediaObjects/10055_2006_23_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>This shows the dialog for changing land use and management options. After selecting the second icon from the <i>left</i> in the tool bar and selecting a polygon in the management unit, this dialog opens. After selecting the preferred option, PA-GIS is sending messages to PA-Render to update the scene graph. The <i>third icon from the left</i> advances the time by one step, and the <i>icon next</i> to it sets the time back to zero. The <i>fifth icon from the left</i> sends the complete land cover configuration of the currently selected scenario (management unit layer) to PA-Render. The currently selected scenario is ‘current condition’. The layer window shows a selection of scenarios that are available in ArcMap and these scenarios can be selected in the ‘units’ drop down list in the tool bar</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0023-9/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The user can also advance the current time by clicking on a button in PA-GIS. This will add one time step to each management unit that currently has a land use that is not at its maximum time step. A land use with five time steps (e.g. forest) will advance one step if the current time step is below five. If it is at five, the time will not advance as the trees are fully grown. PA-GIS will send messages to PA-Render to disable all previous time step models and enable all new time step models. All management units record the current time step individually allowing forest plantations to be at different stages of growth. The user can also reset all time steps to zero and restart the animation cycle.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Network</h2><div class="c-article-section__content" id="Sec5-content"><p>To enable the three system components (PA-Render, PA-GIS, and PA-PDA) to interact with each other, each system component has the capability of sending messages to, and receiving messages from, another system component. Message communication and flow is handled by PA-Server.</p><p>To achieve the desired functionality of the EvS, we have implemented a group of messages that can enable and disable objects in the scene graph. Typically, objects are grouped into spatial parcels that can be switched on and off by sending corresponding messages (region enable and region disable). These messages are carrying IDs for land use and land management type, so we can switch an appropriate scene graph object on or off. It also has a region (management unit) ID to uniquely identify the region to be changed in the scene graph. Furthermore, we have a message that allows us to disable all regions at the same time. This way we can quickly build a scene from scratch without knowing what the current scene graph configuration is. In addition, we have two messages that allows us to advance the time by a number of years or set it back to zero years. All of these messages are sent from PA-GIS to PA-Server and are directly sent on to PA-Render. The ‘region enable/disable messages’ update the scene graph, as do the ‘set time’ messages. The ‘set time’ messages also update the ‘current time’ display in the overlay window to reflect the current time state the scene is displaying.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Summary and conclusions</h2><div class="c-article-section__content" id="Sec6-content"><p>The EvS allows community stakeholders to explore possible future landscape scenarios in a workshop environment. The landscape options are visualised with the help of a software renderer (PA-Render) and a large back-projected screen. A GIS system allows workshop attendants to view the landscape in an abstract 2D map view, and use this map to interactively reconfigure the current landscape.</p><p>The EvS can be used to allow residents of an area that is, for example, under economic pressure to consider possible future land management or land use settings. We could also present alternatives to non-residents of an area, seeking suggestions from specific target groups. These might be tourists needing better infrastructures to make tourist attractions more accessible, or policy developers evaluating policies on nature conservation. Also, we have used this tool to learn more about how communities interact with their local landscape and how their values relate to landscape issues.</p><p>The EvS implements a functional link that successfully allows manipulation of a rendered 3D scene via the GIS. We found that the system was reliable and stable. However, low frame rates due to high numbers of tree objects were a concern. Also, some workshop attendees commented that the scene was still too abstract and a higher degree of photorealism was desired. Other workshop attendees almost felt at home in the virtual representation of their landscape and found the EvS to be a very valuable tool. As computers and graphics cards become more powerful, we expect that a higher degree of photorealism will easily be achievable. However more resources will be required to implement a renderer using recent and future technology and for content creation (e.g. modelling high polygon trees, assembling textures of high quality). The bottleneck in real-time rendering may be in the future shift due to lack of computer resources or due to lack of human resources. Another concern that needs addressing in the future is the land change input mechanism. At the moment, there is some time and cooperation from stakeholders needed for the creating of suitable management units. Ideally, community members could change their landscape without any limitation (i.e. management units, pre-defined land uses), but suitable input mechanisms have to be explored in more detail. Additionally, it would be desirable to create a backwards link from the renderer to the GIS, to allow manipulation within the 3D world, while keeping the GIS data updated with these changes.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ID. Bishop, C. Karadaglis, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Bishop ID, Karadaglis C (1997) Linking modelling and visualisation for natural resources management. Environ P" /><p class="c-article-references__text" id="ref-CR1">Bishop ID, Karadaglis C (1997) Linking modelling and visualisation for natural resources management. Environ Plann B Plann Des 24:345–358</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fb240345" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Linking%20modelling%20and%20visualisation%20for%20natural%20resources%20management&amp;journal=Environ%20Plann%20B%20Plann%20Des&amp;volume=24&amp;pages=345-358&amp;publication_year=1997&amp;author=Bishop%2CID&amp;author=Karadaglis%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bishop ID, Dave B (2001) Beyond the moving camera: systems development for interactive immersive exploration o" /><p class="c-article-references__text" id="ref-CR2">Bishop ID, Dave B (2001) Beyond the moving camera: systems development for interactive immersive exploration of urban environments. In: Kim K (ed) Proceedings for computers in urban planning and urban management, Honolulu</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ID. Bishop, IV RB. Hull, C. Stock, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bishop ID, Hull IV RB, Stock C (2005) Supporting personal world-views in an envisioning system. Environ Model " /><p class="c-article-references__text" id="ref-CR3">Bishop ID, Hull IV RB, Stock C (2005) Supporting personal world-views in an envisioning system. Environ Model Softw 20:1459–1468</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.envsoft.2004.06.014" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20personal%20world-views%20in%20an%20envisioning%20system&amp;journal=Environ%20Model%20Softw&amp;volume=20&amp;pages=1459-1468&amp;publication_year=2005&amp;author=Bishop%2CID&amp;author=Hull%2CIV%20RB&amp;author=Stock%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Danahy, R. Wright, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Danahy JW, Wright R (1988) Exploring design through 3-dimensional simulations. Landsc Arche 78(4):64–71" /><p class="c-article-references__text" id="ref-CR4">Danahy JW, Wright R (1988) Exploring design through 3-dimensional simulations. Landsc Arche 78(4):64–71</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20design%20through%203-dimensional%20simulations&amp;journal=Landsc%20Arche&amp;volume=78&amp;issue=4&amp;pages=64-71&amp;publication_year=1988&amp;author=Danahy%2CJW&amp;author=Wright%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Duchaineau MA, Wolinsky M, Sigeti DE, Miller MC, Aldrich C, Mineev-Weinstein MB (1997) ROAMing terrain: real-t" /><p class="c-article-references__text" id="ref-CR5">Duchaineau MA, Wolinsky M, Sigeti DE, Miller MC, Aldrich C, Mineev-Weinstein MB (1997) ROAMing terrain: real-time optimally adapting meshes. In: Yagel R, Hagen H (eds) Proceedings of IEEE visualization, Phoenix, pp 81–88</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Hoinkes, E. Lange, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Hoinkes R, Lange E (1995) 3-D for free: toolkit expands visual dimensions in GIS. GIS World 8(7):54–56" /><p class="c-article-references__text" id="ref-CR6">Hoinkes R, Lange E (1995) 3-D for free: toolkit expands visual dimensions in GIS. GIS World 8(7):54–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3-D%20for%20free%3A%20toolkit%20expands%20visual%20dimensions%20in%20GIS&amp;journal=GIS%20World&amp;volume=8&amp;issue=7&amp;pages=54-56&amp;publication_year=1995&amp;author=Hoinkes%2CR&amp;author=Lange%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kwartler, RN. Bernard, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kwartler M, Bernard RN (2001) Communityviz: an integrated planning support system. In: Brail RK, Klosterman RE" /><p class="c-article-references__text" id="ref-CR7">Kwartler M, Bernard RN (2001) Communityviz: an integrated planning support system. In: Brail RK, Klosterman RE (eds) Planning support systems: integrating geographic information systems and visualization tools. ESRI Press, Redlands, pp 285–308</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Planning%20support%20systems%3A%20integrating%20geographic%20information%20systems%20and%20visualization%20tools&amp;pages=285-308&amp;publication_year=2001&amp;author=Kwartler%2CM&amp;author=Bernard%2CRN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kwartler, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Kwartler M (2005) Visualization in support of public participation. In: Bishop ID, Lange E (eds) Visualization" /><p class="c-article-references__text" id="ref-CR8">Kwartler M (2005) Visualization in support of public participation. In: Bishop ID, Lange E (eds) Visualization for landscape and environmental planning: technology and applications. Taylor &amp; Francis, London, pp 251–260</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visualization%20for%20landscape%20and%20environmental%20planning%3A%20technology%20and%20applications&amp;pages=251-260&amp;publication_year=2005&amp;author=Kwartler%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Liggett, W. Jepson, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Liggett R, Jepson W (1995) An integrated environment for urban simulation. Environ Plann B 22:291–305" /><p class="c-article-references__text" id="ref-CR9">Liggett R, Jepson W (1995) An integrated environment for urban simulation. Environ Plann B 22:291–305</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fb220291" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20integrated%20environment%20for%20urban%20simulation&amp;journal=Environ%20Plann%20B&amp;volume=22&amp;pages=291-305&amp;publication_year=1995&amp;author=Liggett%2CR&amp;author=Jepson%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lindstrom P, Koller D, Ribarsky W, Hodges LF, Faust N, Turner GA (1996) Real-time, continuous level of detail " /><p class="c-article-references__text" id="ref-CR10">Lindstrom P, Koller D, Ribarsky W, Hodges LF, Faust N, Turner GA (1996) Real-time, continuous level of detail rendering of height fields. In: Rushmeier H (ed) Proceedings of ACM SIGGRAPH 96. New Orleans, pp 109–118</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content=". OpenGL Architecture Review Board, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="OpenGL Architecture Review Board (1999) OpenGL programming guide: the official guide to learning OpenGL, Versi" /><p class="c-article-references__text" id="ref-CR11">OpenGL Architecture Review Board (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2. 3rd edn. Addison-Wesley, Reading</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=OpenGL%20programming%20guide%3A%20the%20official%20guide%20to%20learning%20OpenGL%2C%20Version%201.2&amp;publication_year=1999&amp;author=OpenGL%20Architecture%20Review%20Board%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics." /><p class="c-article-references__text" id="ref-CR12">Rohlf J, Helman J (1994) IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In: Glassner A (ed) Proceedings of ACM SIGGRAPH 94, Orlando, pp 381–394</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Tang, ID. Bishop, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Tang H, Bishop ID (2002) Integration methodologies for interactive forest modelling and visualisation systems." /><p class="c-article-references__text" id="ref-CR13">Tang H, Bishop ID (2002) Integration methodologies for interactive forest modelling and visualisation systems. Cartogr J 39:27–35</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integration%20methodologies%20for%20interactive%20forest%20modelling%20and%20visualisation%20systems&amp;journal=Cartogr%20J&amp;volume=39&amp;pages=27-35&amp;publication_year=2002&amp;author=Tang%2CH&amp;author=Bishop%2CID">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0023-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This project (UME65) was funded by Land &amp; Water Australia. We would like to thank Andrew Kudilczak, who has written most of the code behind PA-Render.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Geomatics,  The University of Melbourne, Parkville,  3010,  Melbourne,  VIC, Australia</p><p class="c-article-author-affiliation__authors-list">Christian Stock &amp; Ian D. Bishop</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Christian-Stock"><span class="c-article-authors-search__title u-h3 js-search-name">Christian Stock</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Christian+Stock&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christian+Stock" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christian+Stock%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ian_D_-Bishop"><span class="c-article-authors-search__title u-h3 js-search-name">Ian D. Bishop</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ian D.+Bishop&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ian D.+Bishop" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ian D.+Bishop%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0023-9/email/correspondent/c1/new">Christian Stock</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Linking%20GIS%20with%20real-time%20visualisation%20for%20exploration%20of%20landscape%20changes%20in%20rural%20community%20workshops&amp;author=Christian%20Stock%20et%20al&amp;contentID=10.1007%2Fs10055-006-0023-9&amp;publication=1359-4338&amp;publicationDate=2006-03-18&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Stock, C., Bishop, I.D. Linking GIS with real-time visualisation for exploration of landscape changes in rural community workshops.
                    <i>Virtual Reality</i> <b>9, </b>260–270 (2006). https://doi.org/10.1007/s10055-006-0023-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0023-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-11-16">16 November 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-02-17">17 February 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03-18">18 March 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04">April 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0023-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0023-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Envisioning systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Geographic information systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Community values</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0023-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=23;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

