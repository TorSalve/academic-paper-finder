<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A framework to design 3D interaction assistance in constraints-based v"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="
The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A framework to design 3D interaction assistance in constraints-based virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2014 18:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-06-25"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="
The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D interactions. In this paper, we propose a generic framework for designing assistance to 3D user interaction in constraints-based virtual environment that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs). This framework is applied to design assistance tools for molecular biology analysis. Evaluation shows that VF designed using our framework improve effectiveness of the manipulation task.
"/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-06-25"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="219"/>

    <meta name="prism.endingPage" content="234"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0247-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0247-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0247-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0247-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A framework to design 3D interaction assistance in constraints-based virtual environments"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2014/09"/>

    <meta name="citation_online_date" content="2014/06/25"/>

    <meta name="citation_firstpage" content="219"/>

    <meta name="citation_lastpage" content="234"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0247-z"/>

    <meta name="DOI" content="10.1007/s10055-014-0247-z"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0247-z"/>

    <meta name="description" content="
The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these cons"/>

    <meta name="dc.creator" content="Mouna Essabbah"/>

    <meta name="dc.creator" content="Guillaume Bouyer"/>

    <meta name="dc.creator" content="Samir Otmane"/>

    <meta name="dc.creator" content="Malik Mallem"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Abbott JJ, Marayong P, Okamura MA (2007) Haptic virtual fixtures for robot-assisted manipulation. Springer tracts in robotics research, vol 28, pp 49&#8211;64"/>

    <meta name="citation_reference" content="Bowman D (1999) Interaction techniques for common tasks in immersive virtual environments: design, evaluation, and application. PhD thesis, Georgia Institute of Technology"/>

    <meta name="citation_reference" content="Bowman D, Hodges L (1995) User interface constraints for immersive virtual environment applications. Graphics, Visualization, and Usability Center technical report GIT-GVU-95-26"/>

    <meta name="citation_reference" content="Bowman D, Hodges L (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 35&#8211;ff"/>

    <meta name="citation_reference" content="Bowman D, Koller D, Hodges L (1998) A methodology for the evaluation of travel techniques for immersive virtual environments. In: Virtual reality, vol 3, pp 120&#8211;131"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Virtual reality: how much immersion is enough?; citation_author=DA Bowman, RP McMahan; citation_volume=40; citation_issue=7; citation_publication_date=2007; citation_pages=36-43; citation_doi=10.1109/MC.2007.257; citation_id=CR6"/>

    <meta name="citation_reference" content="Brooks T, Ince I, Robotics H (1992) Operator vision aids for telerobotic assembly and servicing inspace. In: IEEE international conference on robotics and automation, proceedings, vol 1, pp 886&#8211;891"/>

    <meta name="citation_reference" content="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. In: Smart graphics, Third international symposium. Springer, Berlin, pp 112&#8211;122"/>

    <meta name="citation_reference" content="Castet J, Florens J (2008) A virtual reality simulator based on haptic hard constraints. In: Haptics: perception, devices and scenarios. Lecture notes in computer science, vol 5024, pp 918&#8211;923"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Predicting protein structures with a multiplayer online game; citation_author=S Cooper, F Khatib, A Treuille, J Barbero, J Lee, M Beenen, A Leaver-Fay, D Baker, Z Popovic, F Players; citation_volume=466; citation_issue=7307; citation_publication_date=2010; citation_pages=756-760; citation_doi=10.1038/nature09304; citation_id=CR10"/>

    <meta name="citation_reference" content="Essabbah M, Otmane S, H&#233;risson J, Mallem M (2009a) A new approach to design an interactive system for molecular analysis. In: Human-computer interaction. Interacting in various application domains, Lecture notes in computer science, vol 5613, pp 713&#8211;722"/>

    <meta name="citation_reference" content="Essabbah M, Otmane S, Mallem M, Herisson J (2009b) Spatial organization of DNA: from the physical data to the 3D model. In: IEEE/ACS international conference on computer systems and applications, pp 880&#8211;883"/>

    <meta name="citation_reference" content="F&#233;rey N, Delalande O, Grasseau G, Baaden M (2008) A VR framework for interacting with molecular simulations. In: ACM symposium on virtual reality software and technology, pp 91&#8211;94"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Multisensory VR interaction for protein-docking in the CoRSAIRe project; citation_author=N F&#233;rey, J Nelson, C Martin, L Picinali, G Bouyer, A Tek, P Bourdot, J Burkhardt, B Katz, M Ammi, C Etchebest, L Autin; citation_volume=13; citation_issue=4; citation_publication_date=2009; citation_pages=257-271; citation_doi=10.1007/s10055-009-0136-z; citation_id=CR14"/>

    <meta name="citation_reference" content="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST &#8217;99. ACM, New York, pp 147&#8211;154"/>

    <meta name="citation_reference" content="Gibson JJ (1977) The Theory of affordances. In: Shaw R, Bransford J (eds) Perceiving, acting, and knowing"/>

    <meta name="citation_reference" content="citation_journal_title=Structure; citation_title=Tangible interfaces for structural molecular biology; citation_author=A Gillet, M Sanner, D Stoffler, A Olson; citation_volume=13; citation_issue=3; citation_publication_date=2005; citation_pages=483-491; citation_doi=10.1016/j.str.2005.01.009; citation_id=CR17"/>

    <meta name="citation_reference" content="Gu&#233;bert C, Duriez C, Grisoni L (2008) Unified processing of constraints for interactive simulation. In: Proceedings of VRIPHYS"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Immersive structural biology: a new approach to hybrid modeling of macromolecular assemblies; citation_author=J Heyd, S Birmanns; citation_volume=13; citation_issue=4; citation_publication_date=2009; citation_pages=245-255; citation_doi=10.1007/s10055-009-0129-y; citation_id=CR19"/>

    <meta name="citation_reference" content="Jacoby R, Ferneau M, Humphries J (1994) Gestural interaction in a virtual environment. In: Proceedings of SPIE 2177(355)"/>

    <meta name="citation_reference" content="Kalawsky R (1996) Exploiting virtual reality techniques in education and training: technological issues. Technical report. Loughborough University of Technology, Advisory Group on Computer Graphics (AGOCG). 
                    http://www.agocg.ac.uk/reports/virtual/vrtech/title.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="Kuang AB, Payandeh S, Zheng B, Henigman F, MacKenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: International symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367&#8211;374"/>

    <meta name="citation_reference" content="Marayong P, Li M, Okamura AM, Hager GD (2003) Spatial motion constraints: theory and demonstrations for robot guidance using virtual fixtures. In: Proceedings of the IEEE international conference on robotics and automation, ICRA 2003, 14&#8211;19 Sept 2003, Taipei, Taiwan. IEEE, pp 1954&#8211;1959"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=A constraint manager to support virtual maintainability; citation_author=L Marcelino, N Murray, T Fernando; citation_volume=27; citation_issue=1; citation_publication_date=2003; citation_pages=19-26; citation_doi=10.1016/S0097-8493(02)00228-5; citation_id=CR24"/>

    <meta name="citation_reference" content="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill Computer Science technical report TR95-018"/>

    <meta name="citation_reference" content="Mine RM, Brooks FP Jr, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques, pp 19&#8211;26"/>

    <meta name="citation_reference" content="Oakley I, Adams A, Brewster S, Gray P (2002) Guidelines for the design of haptic widgets. In: Proceedings of British HCI, British Computer Society, pp 195&#8211;212"/>

    <meta name="citation_reference" content="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality based telemanipulation system on the internet. In: IEEE annual simulation symposium (ANSS 2000) proceedings, pp 185&#8211;191"/>

    <meta name="citation_reference" content="Ouramdane N, Otmane S, Davesne F, Mallem M (2006) Follow-me: a new 3d interaction technique based on virtual guides and granularity of interaction. In: Proceedings of the 2006 ACM international conference on virtual reality continuum and its applications, pp 137&#8211;144"/>

    <meta name="citation_reference" content="Picon F, Ammi M, Bourdot P (2008) Haptically-aided extrusion for object edition in CAD. In: Haptics: perception, devices and scenarios, Lecture notes in computer science, vol 5024, pp 736&#8211;741"/>

    <meta name="citation_reference" content="Pierce J, Forsberg A, Conway M, Hong S (1997) Image plane interaction techniques in 3d immersive environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 33&#8211;ff"/>

    <meta name="citation_reference" content="Poupyrev I, Billinghurst M, Weghorst S (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th annual ACM symposium on user interface software and technology, pp 79&#8211;80"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=On study of design and implementation of virtual fixtures; citation_author=R Prada, S Payandeh; citation_volume=13; citation_issue=2; citation_publication_date=2009; citation_pages=117-129; citation_doi=10.1007/s10055-009-0115-4; citation_id=CR33"/>

    <meta name="citation_reference" content="Ren J, Zhang H, Patel R, Peters T (2007) Haptics constrained motion for surgical intervention. In: Studies in health technology and informatics, vol 125, pp 379&#8211;384"/>

    <meta name="citation_reference" content="Rosenberg L (1992) The use of virtual fixtures as perceptual overlays to enhance operator performance in remote environments. Technical report, no A054292, USAF Amstrong"/>

    <meta name="citation_reference" content="Simard J, Ammi M (2012) Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments. In: Virtual reality, vol 16, pp 173&#8211;186"/>

    <meta name="citation_reference" content="Smelik R, Galka K, De Kraker KJ, Kuijper F, Bidarra R (2011) Semantic constraints for procedural generation of virtual worlds. In: Proceedings of the 2nd international workshop on procedural content generation in games, ACM, p 9"/>

    <meta name="citation_reference" content="Smith G, Stuerzlinger W, Salzman T (2001) 3d scene manipulation with 2d devices and constraints. In: Proceedings of graphics interface, pp 135&#8211;142"/>

    <meta name="citation_reference" content="Sternberger L, Bechmann D (2005) Deformable ray-casting interaction technique. In: IEEE young virtual reality conference"/>

    <meta name="citation_reference" content="citation_title=Cognitive load theory; citation_publication_date=2011; citation_id=CR40; citation_author=J Sweller; citation_author=P Ayres; citation_author=S Kalyuga; citation_publisher=Springer"/>

    <meta name="citation_author" content="Mouna Essabbah"/>

    <meta name="citation_author_email" content="mouna.essabbah@gmail.com"/>

    <meta name="citation_author_institution" content="French National Research Agency, Paris, France"/>

    <meta name="citation_author" content="Guillaume Bouyer"/>

    <meta name="citation_author_email" content="Guillaume.Bouyer@ibisc.fr"/>

    <meta name="citation_author_institution" content="IBISC, University of Evry-Val-d&#8217;Essonne, Evry Courcouronnes Cedex, France"/>

    <meta name="citation_author" content="Samir Otmane"/>

    <meta name="citation_author_email" content="Samir.Otmane@ibisc.fr"/>

    <meta name="citation_author_institution" content="IBISC, University of Evry-Val-d&#8217;Essonne, Evry Courcouronnes Cedex, France"/>

    <meta name="citation_author" content="Malik Mallem"/>

    <meta name="citation_author_email" content="Malik.Mallem@ibisc.fr"/>

    <meta name="citation_author_institution" content="IBISC, University of Evry-Val-d&#8217;Essonne, Evry Courcouronnes Cedex, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0247-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0247-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A framework to design 3D interaction assistance in constraints-based virtual environments"/>
        <meta property="og:description" content="The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D interactions. In this paper, we propose a generic framework for designing assistance to 3D user interaction in constraints-based virtual environment that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs). This framework is applied to design assistance tools for molecular biology analysis. Evaluation shows that VF designed using our framework improve effectiveness of the manipulation task."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A framework to design 3D interaction assistance in constraints-based virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0247-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, 3D interaction, Framework, Complex environments, Constraints, Assistance model, Virtual fixtures","kwrd":["Virtual_reality","3D_interaction","Framework","Complex_environments","Constraints","Assistance_model","Virtual_fixtures"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0247-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0247-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=247;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0247-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A framework to design 3D interaction assistance in constraints-based virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0247-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0247-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-06-25" itemprop="datePublished">25 June 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A framework to design 3D interaction assistance in constraints-based virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mouna-Essabbah" data-author-popup="auth-Mouna-Essabbah" data-corresp-id="c1">Mouna Essabbah<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="French National Research Agency" /><meta itemprop="address" content="grid.22058.3d, 000000012104254X, French National Research Agency, Paris, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guillaume-Bouyer" data-author-popup="auth-Guillaume-Bouyer">Guillaume Bouyer</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Evry-Val-d’Essonne" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, IBISC, University of Evry-Val-d’Essonne, Evry Courcouronnes Cedex, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Samir-Otmane" data-author-popup="auth-Samir-Otmane">Samir Otmane</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Evry-Val-d’Essonne" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, IBISC, University of Evry-Val-d’Essonne, Evry Courcouronnes Cedex, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Malik-Mallem" data-author-popup="auth-Malik-Mallem">Malik Mallem</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Evry-Val-d’Essonne" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, IBISC, University of Evry-Val-d’Essonne, Evry Courcouronnes Cedex, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">219</span>–<span itemprop="pageEnd">234</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">452 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0247-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>
The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D interactions. In this paper, we propose a generic framework for designing assistance to 3D user interaction in constraints-based virtual environment that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs). This framework is applied to design assistance tools for molecular biology analysis. Evaluation shows that VF designed using our framework improve effectiveness of the manipulation task.
</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Complex domains or systems such as scientific data exploration or robot tele-manipulation are ruled by a set of constraints, for example, on the tasks to be performed (e.g., time, safety, etc.) or on the behaviour of dynamic elements (e.g., physical or chemical laws, etc.). These constraints are identified, studied and modeled by the experts of each domain. Virtual Reality (VR) simulations of these systems have also to integrate these constraints in order to be credible. This integration should be at every level of the application: 3D modeling, 3D user interaction (3DI), behaviour of the virtual environment (VE), etc. This work is part of a larger project, which aims to provide an application for 3D interactive modeling of chromosomes. After an automatic generation of chromosomal 3D models from physico-chemical rules, the biologist can manually edit the 3D model to improve/correct it. However, the problem is that he/she may alter the physico-chemical reality of the 3D model. The question is then how to help the human user to maintain the initial constraints during the interaction phase.</p><p>Therefore, the purpose of our work is to provide assistance to 3DI in constraints-based VE, not only to improve user’s performance (e.g., precision or speed), but also to improve the credibility of his/her experience by ensuring the coherence between the building rules of the VE and the interactions with it. This issue arises in a variety of application domains, and it is suggested that this research addresses the more general issue in domains other than biology. Therefore, we propose a new framework for designing assistance tools from the specification of the task to achieve and the constraints imposed by the application domain. The conceptualization we suggest provides a useful guideline. The contribution of this research is as follows: (1) a new framework for designing assistance to 3D interaction called CTT that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs), (2) a new formalism of VF (guides), and (3) a case study which demonstrates the implementation of the framework and evaluates the benefits of resulting VF with non-immersive and immersive devices.</p><p>The paper is organized as follows: Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec2">2</a> presents the related work to 3DI assistance and constraints; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec3">3</a> describes our assistance model based on the relation between constraints, assistance tools and 3DI tasks; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec13">4</a> presents the application of molecular biology (MB) we chose to test our model; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec19">5</a> details the evaluations we have conducted and discusses the results.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The interaction integrity of a virtual system is its ability to conform to actions that could be done in reality. It could be equivalent to the behavioral realism. To ensure the interaction integrity in a VR system, assistance is proposed to users. We can define assistance as a set of tools that the system provides to users to help and guide them while interacting with virtual worlds. It may be in various levels from lowest to highest (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig1">1</a>). For example, assistance may be a simple transmission of information (e.g., written or vocal message, visual trajectory to specify a path to follow). Support can be made by other users either through a direct exchange (e.g., verbal instructions) or by active cooperation in the application (e.g., sharing of remote entities). Higher level assistance can be software, with predictive algorithms (e.g., to anticipate the impact of a task on the system) or multi-sensory indicators (i.e., audio, visual or haptic): force feedback to help users assemble entities or to simulate collisions between objects (Férey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Férey N, Delalande O, Grasseau G, Baaden M (2008) A VR framework for interacting with molecular simulations. In: ACM symposium on virtual reality software and technology, pp 91–94" href="/article/10.1007/s10055-014-0247-z#ref-CR13" id="ref-link-section-d57433e380">2008</a>; Férey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt J, Katz B, Ammi M, Etchebest C, Autin L (2009) Multisensory VR interaction for protein-docking in the CoRSAIRe project. Virtual Real 13(4):257–271" href="/article/10.1007/s10055-014-0247-z#ref-CR14" id="ref-link-section-d57433e383">2009</a>; Heyd and Birmanns <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Heyd J, Birmanns S (2009) Immersive structural biology: a new approach to hybrid modeling of macromolecular assemblies. Virtual Real 13(4):245–255" href="/article/10.1007/s10055-014-0247-z#ref-CR19" id="ref-link-section-d57433e386">2009</a>), etc. It can also be physical, through tangible interfaces (Gillet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gillet A, Sanner M, Stoffler D, Olson A (2005) Tangible interfaces for structural molecular biology. Structure 13(3):483–491" href="/article/10.1007/s10055-014-0247-z#ref-CR17" id="ref-link-section-d57433e389">2005</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Different assistance levels in constraint-based VE</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In some studies, constraints are implicitly incorporated into the 3DI technique. For the navigation task, a 2D map can be used to characterize a shift from the current location to next one while being aware of the constraints that compose the virtual world (Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman D (1999) Interaction techniques for common tasks in immersive virtual environments: design, evaluation, and application. PhD thesis, Georgia Institute of Technology" href="/article/10.1007/s10055-014-0247-z#ref-CR2" id="ref-link-section-d57433e414">1999</a>). A virtual deformable ray (3D curve) can represent a path, avoiding obstacles (Sternberger and Bechmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sternberger L, Bechmann D (2005) Deformable ray-casting interaction technique. In: IEEE young virtual reality conference" href="/article/10.1007/s10055-014-0247-z#ref-CR39" id="ref-link-section-d57433e417">2005</a>). In the case of manipulation, the movement of objects can be restricted by constraints (Smith et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Smith G, Stuerzlinger W, Salzman T (2001) 3d scene manipulation with 2d devices and constraints. In: Proceedings of graphics interface, pp 135–142" href="/article/10.1007/s10055-014-0247-z#ref-CR38" id="ref-link-section-d57433e420">2001</a>), so they are implicitly preserved. When the target is remote or small, specific areas can act on the effector (by attraction/repulsion) to improve the accuracy of conventional virtual hand technique (Ouramdane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ouramdane N, Otmane S, Davesne F, Mallem M (2006) Follow-me: a new 3d interaction technique based on virtual guides and granularity of interaction. In: Proceedings of the 2006 ACM international conference on virtual reality continuum and its applications, pp 137–144" href="/article/10.1007/s10055-014-0247-z#ref-CR29" id="ref-link-section-d57433e423">2006</a>).</p><p>In other works, constraints are explicitly represented and assistance tools are VF. The VF improves the performance of a human operator in tasks of remote manipulation with transmission delay (Rosenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Rosenberg L (1992) The use of virtual fixtures as perceptual overlays to enhance operator performance in remote environments. Technical report, no A054292, USAF Amstrong" href="/article/10.1007/s10055-014-0247-z#ref-CR35" id="ref-link-section-d57433e429">1992</a>), using different sensory channels (audio, visual and haptic). Also known as virtual mechanisms (Brooks et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Brooks T, Ince I, Robotics H (1992) Operator vision aids for telerobotic assembly and servicing inspace. In: IEEE international conference on robotics and automation, proceedings, vol 1, pp 886–891" href="/article/10.1007/s10055-014-0247-z#ref-CR7" id="ref-link-section-d57433e432">1992</a>), VF can enhance both the execution time, quality and precision of tele-operation tasks (Kuang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kuang AB, Payandeh S, Zheng B, Henigman F, MacKenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: International symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367–374" href="/article/10.1007/s10055-014-0247-z#ref-CR22" id="ref-link-section-d57433e435">2004</a>; Marayong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Marayong P, Li M, Okamura AM, Hager GD (2003) Spatial motion constraints: theory and demonstrations for robot guidance using virtual fixtures. In: Proceedings of the IEEE international conference on robotics and automation, ICRA 2003, 14–19 Sept 2003, Taipei, Taiwan. IEEE, pp 1954–1959" href="/article/10.1007/s10055-014-0247-z#ref-CR23" id="ref-link-section-d57433e438">2003</a>). They are used to help in the design and the assembling of objects in CAD (Picon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Picon F, Ammi M, Bourdot P (2008) Haptically-aided extrusion for object edition in CAD. In: Haptics: perception, devices and scenarios, Lecture notes in computer science, vol 5024, pp 736–741" href="/article/10.1007/s10055-014-0247-z#ref-CR30" id="ref-link-section-d57433e441">2008</a>), to support tele-manipulation in Augmented Reality (Otmane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality based telemanipulation system on the internet. In: IEEE annual simulation symposium (ANSS 2000) proceedings, pp 185–191" href="/article/10.1007/s10055-014-0247-z#ref-CR28" id="ref-link-section-d57433e445">2000</a>) or in assistance to collaborative 3D tele-work (Ouramdane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ouramdane N, Otmane S, Davesne F, Mallem M (2006) Follow-me: a new 3d interaction technique based on virtual guides and granularity of interaction. In: Proceedings of the 2006 ACM international conference on virtual reality continuum and its applications, pp 137–144" href="/article/10.1007/s10055-014-0247-z#ref-CR29" id="ref-link-section-d57433e448">2006</a>). Haptic VF can also guide the user along a specified path, prevent access in restricted areas (Abbott et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Abbott JJ, Marayong P, Okamura MA (2007) Haptic virtual fixtures for robot-assisted manipulation. Springer tracts in robotics research, vol 28, pp 49–64" href="/article/10.1007/s10055-014-0247-z#ref-CR1" id="ref-link-section-d57433e451">2007</a>), or limit surgeons’ movements in critical operations (Ren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ren J, Zhang H, Patel R, Peters T (2007) Haptics constrained motion for surgical intervention. In: Studies in health technology and informatics, vol 125, pp 379–384" href="/article/10.1007/s10055-014-0247-z#ref-CR34" id="ref-link-section-d57433e454">2007</a>). The use of haptic feedback was evaluated and shows better performance than classic audiovisual interfaces (Oakley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Oakley I, Adams A, Brewster S, Gray P (2002) Guidelines for the design of haptic widgets. In: Proceedings of British HCI, British Computer Society, pp 195–212" href="/article/10.1007/s10055-014-0247-z#ref-CR27" id="ref-link-section-d57433e457">2002</a>).</p><p>All these works do not explicitly refer to “guides” and “constraints” at the same time. They are implicitly integrated in those applications, for example, by modifying the interaction techniques or adding ad hoc elements in the VE. This lack of genericity leads to modify the entire system whenever there is a new constraint.</p><p>Therefore, some researches focused on the generic formalization of VF in order to ensure their reusability. VF has been first formalized for graphic assistance, limited to the visual modality (Otmane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality based telemanipulation system on the internet. In: IEEE annual simulation symposium (ANSS 2000) proceedings, pp 185–191" href="/article/10.1007/s10055-014-0247-z#ref-CR28" id="ref-link-section-d57433e466">2000</a>). They have recently been formalized in the case of haptic, based on mechanics (Prada and Payandeh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real 13(2):117–129" href="/article/10.1007/s10055-014-0247-z#ref-CR33" id="ref-link-section-d57433e469">2009</a>). It was applied to follow a trajectory in a virtual training application for surgeons. These two models make guides configurable and scalable: Standard VF can be configured to adapt to new situations or tasks. Though, they still disregard the constraints of the system.</p><p>Some works are interested in the concept of constraints-based environment. For example, in maintenance simulation, Marcelino et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Marcelino L, Murray N, Fernando T (2003) A constraint manager to support virtual maintainability. Comput Graph 27(1):19–26" href="/article/10.1007/s10055-014-0247-z#ref-CR24" id="ref-link-section-d57433e475">2003</a>) proposed a manager to support physical realism and interactive assembly and disassembly tasks within VE. However, this approach is limited to the geometric constraints and so restricted to the CAD-like applications. Otherwise, Simard and Ammi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Simard J, Ammi M (2012) Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments. In: Virtual reality, vol 16, pp 173–186" href="/article/10.1007/s10055-014-0247-z#ref-CR36" id="ref-link-section-d57433e478">2011</a>) used haptic-based communication approach to overcome constraints related to collaborative VE. Despite that, it is still limited to a particular domain. Our paper addresses guidelines for more general constraint management.</p><p>In conclusion, we observed that, on one hand, there are generic 3D interaction techniques for acting in a virtual world. On the other hand, there are assistance tools to 3D interaction such as VF (or even ad hoc feedbacks). What we propose is a new framework to design assistance tools adapted to the targeted VE. This framework is based on the specification of the task to achieve and the constraints imposed by the application domain to offer a tailored assistance. It explicitly integrates the constraints of the VE, to support the design of generic and multimodal VF. This formalism is based on three components: Constraints, assistance Tools and interaction Tasks (CTT).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">The CTT assistance framework</h2><div class="c-article-section__content" id="Sec3-content"><h3 class="c-article__sub-heading" id="Sec4">Assistance basis</h3><p>Such assistance covers complex areas where virtual representations must match reality. We are not looking for visual realism but behavioral one, that is why we are interested in the impact, it can have on 3DI tasks. The behavioral realism is determined by the constraints identified by the designer and that are required for 3D interaction to be sufficiently credible. Integration of assistance in the interaction process with VR environments aims to reduce the user cognitive load (Sweller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Sweller J, Ayres P, Kalyuga S (2011) Cognitive load theory. Springer, Berlin" href="/article/10.1007/s10055-014-0247-z#ref-CR40" id="ref-link-section-d57433e495">2011</a>) on how to interact, due to physical constraints or software techniques used. For the same reasons, assistance includes constraints of the application domain directly into the interaction system, transparently for user. In other words, assistance helps the users obey constraints or at least reminds their existence.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig1">1</a>, we classify different assistance types regardless of the used tool. We do not consider the technical means used for assistance but only its objective. Our goal is to understand the relationship between the required assistance level and the VF type. Thus, the VF specification will be done according to the needed assistance. For constraints-based 3DI, we observed different assistance levels: to inform of the constraints existence, to inform that user’s actions are not complying with constraints, to inform about the current state of compliance or exceeding with constraints and finally to automatically prevent exceeding constraints.</p><p>To ensure 3DI according to domain constraints, we propose the assistance model CTT. This model has three components (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig2">2</a>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Constraint: It represents the system constraints. According to our classification, it can be local and/or global. The constraint limits the interaction task (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec6">3.3</a>).</p>
                    </li>
                    <li>
                      <p>Task: It represents the possible actions in an application. The four standard 3DI tasks are as follows: navigation, selection, manipulation and system control (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec5">3.2</a>).</p>
                    </li>
                    <li>
                      <p>Tool: It is the interface between the other two model components. The assistance tool allows performing a task according to all rules imposed by the application domain (i.e., constraints). It can be tangible (instruction manual), human (co-user) or software (audio, visual and haptic VF) (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec9">3.4</a>).</p>
                    </li>
                  </ul><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig2">2</a> shows globally the relationship between the three components of our model: we identify the various considered constraints of the application domain and formalize them. Then, from this formalism, we can deduce the geometric primitives that will form the VF. Then, we must attribute to VF, the modalities we want to use. Finally, given the constraints, we have to specify for which 3D interaction tasks guides must be enabled.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Classes defining the CTT framework</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec5">Task</h3><p>3DI is an essential aspect of VR, it allows the user to act in the virtual world. Bowman defines three kinds of natural actions: navigation, selection and manipulation of objects (Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman D (1999) Interaction techniques for common tasks in immersive virtual environments: design, evaluation, and application. PhD thesis, Georgia Institute of Technology" href="/article/10.1007/s10055-014-0247-z#ref-CR2" id="ref-link-section-d57433e569">1999</a>). Finally, a fourth type of task, more specific to computer science, is the system control. They are carried out through interfaces, which may be sensory or motor, supplemented by software tools and methods called interaction techniques. These 3DI techniques can be generic or specific for each task. Here are some well-known examples:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Navigation: Moving or changing the point of view in a scene with the technique Gaze-directed steering or pointing, Grabbing the air (Mine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill Computer Science technical report TR95-018" href="/article/10.1007/s10055-014-0247-z#ref-CR25" id="ref-link-section-d57433e578">1995</a>), or a Map-based travel (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Bowman D, Koller D, Hodges L (1998) A methodology for the evaluation of travel techniques for immersive virtual environments. In: Virtual reality, vol 3, pp 120–131" href="/article/10.1007/s10055-014-0247-z#ref-CR5" id="ref-link-section-d57433e581">1998</a>).</p>
                    </li>
                    <li>
                      <p>Selection: Designation of one or more objects for a given purpose by the technique of simple virtual hand, ray casting (Jacoby et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Jacoby R, Ferneau M, Humphries J (1994) Gestural interaction in a virtual environment. In: Proceedings of SPIE 2177(355)" href="/article/10.1007/s10055-014-0247-z#ref-CR20" id="ref-link-section-d57433e590">1994</a>), sticky finger (Pierce et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Pierce J, Forsberg A, Conway M, Hong S (1997) Image plane interaction techniques in 3d immersive environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 33–ff" href="/article/10.1007/s10055-014-0247-z#ref-CR31" id="ref-link-section-d57433e593">1997</a>) or arm extension (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poupyrev I, Billinghurst M, Weghorst S (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th annual ACM symposium on user interface software and technology, pp 79–80" href="/article/10.1007/s10055-014-0247-z#ref-CR32" id="ref-link-section-d57433e596">1996</a>).</p>
                    </li>
                    <li>
                      <p>Manipulation: Changing the properties of objects by the technique of simple virtual hand, HOMER (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman D, Hodges L (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 35–ff" href="/article/10.1007/s10055-014-0247-z#ref-CR4" id="ref-link-section-d57433e605">1997</a>) or Scaled-world grab (Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Mine RM, Brooks FP Jr, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques, pp 19–26" href="/article/10.1007/s10055-014-0247-z#ref-CR26" id="ref-link-section-d57433e608">1997</a>).</p>
                    </li>
                    <li>
                      <p>Control system: Changing the system state or interaction mode with graphical menus, voice commands and gesture interaction.</p>
                    </li>
                  </ul>
                        <h3 class="c-article__sub-heading" id="Sec6">Constraints</h3><p>In VR, the concept of constraints can be viewed as the opposite of affordance<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> because it limits the potential actions of an object. The word “constraint” itself has a negative connotation because it refers to something that limits us, but constraints are necessary, both in real and virtual situations (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bowman D, Hodges L (1995) User interface constraints for immersive virtual environment applications. Graphics, Visualization, and Usability Center technical report GIT-GVU-95-26" href="/article/10.1007/s10055-014-0247-z#ref-CR3" id="ref-link-section-d57433e636">1995</a>). This concept has been applied in the field of real-time graphical simulation, interaction or haptic rendering: “constraints” are spatial or semantic relationships between virtual objects that have to be respected. These relationships constrain the position, the kinematic or the behaviour of the objects. Constraints can be resolved using either physical equation systems that are computationally intensive (Guébert et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Guébert C, Duriez C, Grisoni L (2008) Unified processing of constraints for interactive simulation. In: Proceedings of VRIPHYS" href="/article/10.1007/s10055-014-0247-z#ref-CR18" id="ref-link-section-d57433e639">2008</a>), dedicated hardware (Castet and Florens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Castet J, Florens J (2008) A virtual reality simulator based on haptic hard constraints. In: Haptics: perception, devices and scenarios. Lecture notes in computer science, vol 5024, pp 918–923" href="/article/10.1007/s10055-014-0247-z#ref-CR9" id="ref-link-section-d57433e642">2008</a>) or software architectures (e.g., Geometric Constraint Manager (Marcelino et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Marcelino L, Murray N, Fernando T (2003) A constraint manager to support virtual maintainability. Comput Graph 27(1):19–26" href="/article/10.1007/s10055-014-0247-z#ref-CR24" id="ref-link-section-d57433e645">2003</a>), Constraint Logic Programming (CLP) (Calderon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. In: Smart graphics, Third international symposium. Springer, Berlin, pp 112–122" href="/article/10.1007/s10055-014-0247-z#ref-CR8" id="ref-link-section-d57433e649">2003</a>). Smelik et al introduced a new use of the word with “semantic constraint”: a control mechanism imposed on the procedural generation of VEs in order to satisfy explicit designers intent over a specific area (e.g., a line of sight between two positions). It is composed of “feature sub-constraints” mapped to low-level operations (e.g., the height of trees) (Smelik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Smelik R, Galka K, De Kraker KJ, Kuijper F, Bidarra R (2011) Semantic constraints for procedural generation of virtual worlds. In: Proceedings of the 2nd international workshop on procedural content generation in games, ACM, p 9" href="/article/10.1007/s10055-014-0247-z#ref-CR37" id="ref-link-section-d57433e652">2011</a>).</p><p>Indeed, constraints are not always geometric or even virtual: Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab1">1</a> shows a list of possible issues, situations or requirements which can have a constraining influence on VR interactions. Some are inherent in a domain, due to the simulated environment or required specifically by the task, others are imposed by the real environment or the devices. We can then distinguish “positive constraints” that users should respect to conduct a meaningful activity and for which they need assistance from the system; and “negative constraints” that designers and users have to deal with.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Some issues in VR applications, related to the task, the VE, the real environment or the users</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0247-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Our proposed framework, which maps specified constraints with formalized VF, can apply to these types of constraints.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Proposed classification</h4><p>Constraints imposed by the environment or the task affect both virtual entities (e.g., spatial form, position, etc.) and their behaviour (e.g., possible movement, collisions, etc.). However, we also identified constraints affecting the whole system.</p><p>Contrarily to the classification proposed by Bowman and Hodges (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bowman D, Hodges L (1995) User interface constraints for immersive virtual environment applications. Graphics, Visualization, and Usability Center technical report GIT-GVU-95-26" href="/article/10.1007/s10055-014-0247-z#ref-CR3" id="ref-link-section-d57433e879">1995</a>) that identifies the constraints according to each 3DI task independently from the rest of the system, we are interested in the behaviour of one or more objects in their environment. For instance, object properties change while it is manipulated (e.g., position, orientation, etc.) and this can influence surrounding objects. Therefore, we study the impact of constraints not only on each task but also on the whole 3DI system. In parallel, we also take into account, the consequences of constraints on an object and on all the virtual entities.</p><p>We propose the following classification to identify the different constraints types:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Global: This is the main type of constraint. It concerns the final result or outcome of the application. A global constraint is related to:</p><ul class="u-list-style-bullet">
                            <li>
                              <p>the virtual objects set, such as an overall bounding volume or their relative positions.</p>
                            </li>
                            <li>
                              <p>the whole 3DI system, such as accuracy or speed expected for the task.</p>
                            </li>
                          </ul>
                                    
                      </li>
                      <li>
                        <p>Local: This type represents a sub-constraint of global type that is associated with a task (or an application function). A local constraint may concern:</p><ul class="u-list-style-bullet">
                            <li>
                              <p>a 3D object (entity of the VE), for example, by imposing a geometric shape.</p>
                            </li>
                            <li>
                              <p>3DI task, such as a confined manipulation space, or a limited distance for navigation.</p>
                            </li>
                          </ul>
                                    
                      </li>
                    </ul><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig3">3</a> represents a VR system with its specific constraints and assistance toolbox. The VE main modules are the set of virtual objects and 3DI system. The local type of constraint refers to elements of each of the previous subsystems. This type concerns 3D objects and 3DI tasks. Thus, if we consider that a constraint is a parameter of our system, so there may be sub-constraints in our application. The constraints categorization contributes to the specification of the corresponding VF. Indeed, this parameter indicates when the VF would be enabled. For example, if a VF is associated with a local constraint such as precision in selection, it will be activated only for the selection task. On the other hand, if the VF is related to a global constraint, it should be active all the time.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Example of contraints distribution (represented by <i>padlocks</i>) in a VR system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Constraint specification</h4><p>The constraint specification is based on observation we have done of different constraints in various application domains (examples in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab1">1</a>). A complex environment is often organized according to several correlated constraints. Therefore, we need to isolate each constraint separately and then determine the process that connects all of them. Thus, as in the rule-based systems, constraints can be considered as unit rules whose relationship is managed by logical operators (AND, OR, XOR, etc.). The first stage of the design is precisely to decompose the constraints into logical rules.</p><p>Thus, a constraint can be represented by a quantifiable parameter <span class="mathjax-tex">\(C_{i}\)</span> that will be compared with one or more thresholds. Depending on the value of this parameter, an associated VF is activated. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab2">2</a> provides some simplified examples of constraints specification. The goal of this specification is to implement each constraint by translating the rule in simple calculations, tests and parameters extraction, in order to execute the rendering functions of their associated VF.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Some examples of constraints specification</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0247-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec9">Tools</h3><p>The <i>tool</i> component of our assistance model results from the specification of the <i>task</i> to achieve and the imposed <i>constraint</i>. The assistance tool is at the heart of the assistance model. We choose to use VF as assistance tools.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Virtual fixtures formalism</h4><p>Our specification is based on a formalism proposed by Otmane et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality based telemanipulation system on the internet. In: IEEE annual simulation symposium (ANSS 2000) proceedings, pp 185–191" href="/article/10.1007/s10055-014-0247-z#ref-CR28" id="ref-link-section-d57433e1451">2000</a>). The Otmane’s formalism is built on a data structure composed of fields defining the properties of a VF (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig4">4</a>):</p><ul class="u-list-style-bullet">
                      <li>
                        <p>VF_ID: Allows the identification of a VF.</p>
                      </li>
                      <li>
                        <p>VF_Type: Defines the VF type (simple, composed, passive, active). If the guide is “composed” then it contains links to other VF (e.g., using a chained list). In the case of “active” guide, it informs us if it is “repulsive” or “attractive.” This field is highly dependent on the desired assistance level (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig1">1</a>): if assistance is low, the VF will be passive, but if it is high, the guide will be active.</p>
                      </li>
                      <li>
                        <p>Referential: Contains information about position and orientation (3D coordinates) of the guide in the VE.</p>
                      </li>
                      <li>
                        <p>Attachment: Specifies whether the guide is “static” (fixed in one place or attached to a particular object) or “dynamic” (appears after an event, for example, when the avatar enters the precise manipulation area of a virtual object). In the first case, it contains the name of the virtual object or the point at which it is attached (otherwise <i>NULL</i>).</p>
                      </li>
                      <li>
                        <p>Influence_Area: Contains the analytical equation (static or parameterized) defining (partially or completely) the guide shape.</p>
                      </li>
                    </ul><p>As described above, this formalism is limited to the specification of geometric VF with only a visual rendering. We expanded it to adapt the assistance in constraints-based 3DI and to encompass a broader category of VF that may have a multimodal rendering (audio and haptic). Moreover, we group the geometric primitive and the guide’s function for a clearer specification. In particular, it is essential that VF includes a reference to the constraint that it represents and to the task for which it is activated. The data structure that represents our VF formalism (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig4">4</a>) must take into account three new components:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Constraint: Contains settings related to domain constraints, such as a geometric equation defining a manipulation space. Assuming that each constraint can be modeled by a relational equation (Constraint_Equation), the guide should inform this relationship, but also its parameters.</p>
                      </li>
                      <li>
                        <p>3DI_Task: This field identifies to which 3DI task this guide is related. This can affect the activation function of a guide (e.g., VF for selection only).</p>
                      </li>
                      <li>
                        <p>Modality: In order to be perceived by the user, the VF must provide a visual, auditory or haptic rendering. Thus, used tool must include information on the type of sensory modalities that are operated.</p>
                      </li>
                    </ul>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>VF data structure for assistance in constraints-based 3DI</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>For example, the temperature of the VE is a global constraint that can be represented by a planar VF orthogonal to the view of the virtual camera. Depending on temperature’s threshold(s), the guide may change color or opacity. For instance, higher is the temperature, redder is the scene. If another constraint is to not entering a perimeter where the temperature is above <span class="mathjax-tex">\(T_{\max }\)</span>, the guide can use haptic feedback to be repulsive. Here is an example of a VF specification:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>VF_ID: temperature_guide;</p>
                      </li>
                      <li>
                        <p>VF_Type: simple, active;</p>
                      </li>
                      <li>
                        <p>Referential: root of the VE;</p>
                      </li>
                      <li>
                        <p>Attachment: static;</p>
                      </li>
                      <li>
                        <p>Influence_Area: a plan;</p>
                      </li>
                      <li>
                        <p>Constraint: <span class="mathjax-tex">\(\forall\)</span>
                                       <span class="mathjax-tex">\(T, T &lt; T_{\max }\)</span> (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab2">2</a>);</p>
                      </li>
                      <li>
                        <p>3DI_Task: manipulation; Modality: visual, haptic.</p>
                      </li>
                    </ul>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">VF operating cycle</h4><p>The operating cycle of a VF is given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig5">5</a>. In general, the activation of a VF is an event triggered by a membership test of a point to a VF (e.g., position of the effector). This is a “Pre-condition” (or activation condition). In this case, the guide properties are recovered from the data structure relative to this guide to define this pre-condition. It is also possible to activate a guide by another event such as collision detection or approach distance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>VF operating cycle</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>When the guide is enabled, we have to determine actions or operations to be performed: the “function” of the guide. Generally, this function comes from a combination of actions realized by user and renderings defining the type of the guide. The function can also be limited to the execution of an operation defined by the guide (e.g., allows the automatic execution of a task).</p><p>The guide function has no effect on the virtual world and the operator, when the application has reached a desired end state, or the end of the automatic execution of a task. This disables the guide and defines the “Post-condition.”</p><h3 class="c-article__sub-heading" id="Sec12">Model design</h3><p>Now, we are interested in the concrete implementation of the assistance model without considering the rest of the system (because the application architecture may differ).</p><p>We start from the simplified class diagram of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig2">2</a> and we focus on the <i>Tool</i> component of the CTT model. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig6">6</a> illustrates the new class diagram involving major classes that define the various properties previously defined. We propose an architecture for the CTT model based on different structures and different levels of abstractions to reflect the characteristics of some VF. The distribution of characteristics on several classes allows to distinguish on one side the general appearance and on the other side components and specific functions of each VF.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Set of classes containing attributes and methods that carry the VF functionality</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Besides the global attributes (<i>VF_id, VF_Type, referential</i>, etc.), a VF is composed of primitive and sensory modalities. The <i>Primitive</i> class represents the geometry of the guide (i.e., its influence area). It is possible that different guides have the same primitive, which justifies the composition relationship between <i>Virtual Fixture</i> and <i>Primitive</i> class.</p><p>Modalities employed by the VF are represented by the <i>Modality</i> class whose subclasses are three possible sensory channels (visual, auditory and haptic). This class represents the perceptible aspect of a guide, its concrete rendering. The aggregation relationship between <i>VirtualFixture</i> and <i>Modality</i> indicates that a guide can be expressed in one or several modalities and a modality can be used by zero or many guides. Then, the <i>Constraint</i> class is related to the <i>VirtualFixture</i> class since a guide implements a constraint and a constraint may be associated with several guides. On the other hand, a VF is active during one or more tasks. In contrast, during a task, there could be zero or several guides. VR application may be subject to various constraints related to its associated real environment. Thus, to define the appropriate VF, the first step is to model each constraint by an equation defining <i>Constraint</i> class. Then, the choice of parameters determines the state of constraint and its influence degree on the system. In addition, VF must incorporate the type of tasks (<i>Task</i>) during which they will be active, and the type of modalities (<i>Modality</i>) they will operate.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Case of study: in virtuo project</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Context</h3><p>One of the aims of MB is the study of molecules’ 3D structure. <i>In silico</i> experiments (i.e., computing simulations) for 3D modeling usually use automatic approaches. However, these approaches have limits: time-consuming computation, non-modifiable generated model, false-positive results, etc. The interactive contribution of expert knowledge, during the automatic modeling process, could overcome some limits of the usual computational methods. It involves placing the biologist in the center of virtual experiments, rather than an observer of automatic simulation results. This is what we call hybrid approach (Essabbah et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009a" title="Essabbah M, Otmane S, Hérisson J, Mallem M (2009a) A new approach to design an interactive system for molecular analysis. In: Human-computer interaction. Interacting in various application domains, Lecture notes in computer science, vol 5613, pp 713–722" href="/article/10.1007/s10055-014-0247-z#ref-CR11" id="ref-link-section-d57433e1775">2009a</a>) that combines the advantages of <i>in silico</i> experiments, human–computer interaction (HCI) and VR: natural interaction, immersion in the VE, multimodality, etc. The result of this approach is the creation of <i>in virtuo</i> experiments which have three components: the 3D modeling, the visualization and the 3DI.</p><p>We think CTT can be part of the solution to reach this hybrid approach. Indeed, it formalizes the close links between the 3DI tasks, the constraints which are the parameters of the modeling algorithms, and the assistance tools which ensure these constraints stay consistent during the 3DI.</p><p>We applied this concept of <i>in virtuo</i> human–molecule interaction to chromosome 3D modeling. The architectural (i.e., physico-chemical data) and functional (i.e., biological models) constraints dictate the chromosome spatial organization. Structures of chromosomes built with this hybrid system respect these constraints. They could be considered more credible, on the one hand, compared with the results of pure algorithmic methods and, on the other hand, compared with knowledge-based manual building.</p><h3 class="c-article__sub-heading" id="Sec15">Application of the assistance framework</h3><p>Our CTT assistance framework has been applied to the <i>in virtuo</i> project, giving the following elements.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Constraints</h4><p>Architectural constraints of the 3D model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig7">7</a>) based on physico-chemical data:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>An angular constraint, due to the curvature energy of the molecule.</p>
                      </li>
                      <li>
                        <p>A volumic constraint, due to limited volume in which the molecule evolves (chromosome territories).</p>
                      </li>
                      <li>
                        <p>A self-avoidance constraint, resulting from the diameter and the persistence length of the molecule.</p>
                      </li>
                    </ul>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Constraints in the analysis of a chromosome</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Tasks</h4><p>The application integrity must be preserved during the different 3DI tasks: navigation, selection, manipulation and system control. However, the assistance deals particularly with the manipulation task. We propose a low assistance to selection, but a mean assistance to manipulation since it is during this task that the 3D model may lose its physico-chemical integrity. Navigation is not the most commonly used task in this application, although it is possible for a visual exploration. Moreover, neither navigation nor system control is subject to constraints, in this case.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Tools</h4><p>Assistance multimodal tools vary from passive informative VF to restrictive ones. Each VF provides assistance to a 3DI task using one or more sensory modalities.</p><p>
                              <i>Selection</i> The designated cylinder is colored and its ID is displayed on the screen. After the validation of the selection, another color is attributed to the selected cylinder and the manipulation VF <i>angle_guide</i> is shown.</p><p>
                              <i>Manipulation</i> In addition to textual information, each constraint is associated with a VF in accordance with our CTT model (their structure is detailed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab3">3</a>):</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Angular constraint: When editing the 3D model, it imposes a limit angle at each joint (between two successive cylinders). We use the <i>angle_guide</i> VF which properties are in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab3">3</a>. If the angle applied during manipulation is accepted, the guide does not change. However, if there is an angle exceeding, the guide turns red, a beep is activated and a visual decoupling is applied. The visual decoupling is a modality used to dissociate the user’s action from the visual rendering. That is to say that when the user reaches the limit before exceeding a constraint, he/she remains free to move but the 3D model remains unchanged. The modification is blocked. In the following, we will use the term “angular constraint-blocked movement”.</p>
                      </li>
                      <li>
                        <p>Volumic Constraint: During manipulation, the <i>volume_guide</i> tests if the volume of the chromosome 3D model (or the diameter of its bounding box) is inferior to its own volume (or diameter). If the volume constraint is verified, the guide remains unchanged. However, once there is a volume exceeding, the guide takes a red color in addition to displaying a text message informing of the excess. A visual decorrelation and an auditory beep are also activated. In the following, we will use the term “volumic constraint-blocked movement”.</p>
                      </li>
                      <li>
                        <p>Self-avoidance constraint: The cylinders must be impenetrable. To check this constraint, we have used the <i>avoidance_guide</i> guide. This guide uses the visual modality. Although it is unseen by users, it ensures a decoupling display when the user causes a collision between any two cylinders. The transformation caused by the user is not applied to the model if it does not respect this constraint.</p>
                      </li>
                    </ul>
                              <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Formalism of three VF designed in the <i>in virtuo</i> project</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0247-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Experiments and evaluations</h2><div class="c-article-section__content" id="Sec19-content"><p>Through this experimental study, we have not fully evaluated our CTT framework but tried to evaluate the provided assistance. The main question is whether the assistance tools designed using our CTT framework provide benefits to users when interacting with constraints-based VEs. The experiments allowed us to:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Compare performance of manipulation of a constraints-based molecular model with and without assistance, i.e., with and without VF;</p>
                  </li>
                  <li>
                    <p>Compare the contribution of assistance in two different interactive platforms;</p>
                  </li>
                  <li>
                    <p>Observe users’ reaction and have a first global appreciation of the multimodal assistance tools we designed.</p>
                  </li>
                </ul>
                     <h3 class="c-article__sub-heading" id="Sec20">Experimental process</h3><p>We conducted these evaluations in two types of platforms, distinct in terms of level of visual immersion<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> and interaction devices. In what follows, to simplify, we use the terms “non-immersive” and “semi-immersive” to refer to these experiments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Non-immersive evaluations</h4><p>Hardware devices are those of a personal desktop computer: a 20-in. screen for viewing, interaction with a mouse (2D pointing) for the selection and with a keyboard for manipulation (directional arrows to change the orientation of the selected cylinder). Eleven volunteers participated: eight men and three women. Their expertise level in HCI was self-evaluated: five experts, two intermediates and four novices. The evaluation included six tests per participant, alternately three with and three without assistance, i.e., a total of 66 tests.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Semi-immersive evaluations</h4><p>Stereoscopic visualization is provided by a wide screen and stereoscopic glasses, and 3DI is provided by VR devices and techniques: a hand-held flystick, ray casting for selection and simple virtual hand for manipulation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig8">8</a>). Twelve volunteers participated: eight men, four women (four experts, four intermediates and four novices in HCI). The evaluation included ten tests per participant, alternately five with and five without assistance, i.e., a total of 120 tests.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Our semi-immersive platform: <i>1</i> flystick, <i>2</i> stereoscopic glasses, <i>3</i> infrared cameras, <i>4</i> video projector</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec23">Evaluation scenario</h3><p>The VE is composed of a simulated 3D model of chromosome and its bounding volume. The 3D model is specially built for evaluation purposes. It represents a scale-down model (i.e., chromosome segment) generated by our geometric calculations engine (Essabbah et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009b" title="Essabbah M, Otmane S, Mallem M, Herisson J (2009b) Spatial organization of DNA: from the physical data to the 3D model. In: IEEE/ACS international conference on computer systems and applications, pp 880–883" href="/article/10.1007/s10055-014-0247-z#ref-CR12" id="ref-link-section-d57433e2730">2009b</a>) that can provide a substantial database of 3D models (even whole chromosome models).</p><p>The model consists of a chain of articulated yellow cylinders, connected to each other by joining spheres (
Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig9">9</a>). A blue sphere represents the bounding volume of chromosome territory.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Experimental VE consisting of bounding volume (in blue) and chromosome model (in <i>yellow</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p> The 3D model is hierarchical which means that the manipulation of a cylinder <span class="mathjax-tex">\(i\)</span> will move the following cylinders (<span class="mathjax-tex">\(i+1\)</span> to <span class="mathjax-tex">\(n\)</span>). Previous cylinders remain static.</p><p>We model the physico-chemical constraints of our application by the previously described VF (<i>angle_guide</i> and <i>volume_guide</i>).</p><p>The experiment consists of modifying the chromosome model to bring it to a desired conformation, as the expert would do in an hybrid approach. For this purpose, the participant must reproduce a form, starting from an initial conformation. To accomplish this, the participant can select and manipulate all cylinders. The task is divided in three steps where a specific cylinder must be placed in a specific area, generally by manipulating another cylinder in the chain. A cube represents an area and the target cylinder area is indicated by a green color. The initial conditions are the same for each test: 3D model and position of areas. Validation of an area depends on a threshold distance between area and the target cylinder. The areas are dynamically displayed. Once an area is validated, it will disappear and give way to the next area. Thus, a test is to repeat three times the following steps:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Select a cylinder and manipulate it (that is to say, change its orientation taking for reference the previous cylinder). These tasks can be repeated as many times as desired.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Put the corresponding target cylinder in the current area to validate it.</p>
                      
                    </li>
                  </ol><p>This scenario can be compared with the game Foldit (Cooper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Cooper S, Khatib F, Treuille A, Barbero J, Lee J, Beenen M, Leaver-Fay A, Baker D, Popovic Z, Players F (2010) Predicting protein structures with a multiplayer online game. Nature 466(7307):756–760" href="/article/10.1007/s10055-014-0247-z#ref-CR10" id="ref-link-section-d57433e2845">2010</a>) that aims to create 3D proteins structures from a given puzzle with rules of molecular chemistry for only limits. To test the contribution of our CTT framework, we assigned two types of tests: (1) with assistance, i.e., with the possible activation of all VF designed Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0247-z#Sec18">4.2.3</a>, (2) without assistance, i.e., without displaying the constraints and without VF. Apart from that, the experiment is identical for each of these two types of tests.</p><p>Before starting the experiment and during a phase of familiarization, we explain to the participant what is required. Then he/she alternates a test with assistance and a test without assistance. The total duration of an experiment for a participant is 10 min on average. At the end of the experiment, the participants fill out a questionnaire on qualitative feedback from the experience.</p><p>Our prediction concerning assistance is that the VF will offer lesser constraints exceeding movements especially in the semi-immersive experiments.</p><h3 class="c-article__sub-heading" id="Sec24">Result analysis</h3><p>We measure at the end of each test, the following parameters:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Total duration of a test (<span class="mathjax-tex">\(T\)</span>)</p>
                    </li>
                    <li>
                      <p>Number of angular constraints exceeding movements on the <span class="mathjax-tex">\(X\)</span> axis (<span class="mathjax-tex">\(N_{b}\_dX\)</span>)</p>
                    </li>
                    <li>
                      <p>Number of angular constraints exceeding movements on the <span class="mathjax-tex">\(Y\)</span> axis (<span class="mathjax-tex">\(N_{b}\_dY\)</span>)</p>
                    </li>
                    <li>
                      <p>Number of volumic constraints exceeding movements (<span class="mathjax-tex">\(N_{b}\_dV\)</span>)</p>
                    </li>
                    <li>
                      <p>Number of validated areas (<span class="mathjax-tex">\(N_{b}\_Z\)</span>)</p>
                    </li>
                    <li>
                      <p>Failure: the task (i.e., three successive validations) has not been completed in a given <i>timeout</i>, determined by averaging over a set of preliminary tests.</p>
                    </li>
                  </ul><p>Analysis factors that we adopted are:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <span class="mathjax-tex">\(F_{1}\)</span>: participant expertise, derived from the questionnaire (novice, intermediate or expert)</p>
                    </li>
                    <li>
                      <p>
                                    <span class="mathjax-tex">\(F_{2}\)</span>: test category (with or without assistance)</p>
                    </li>
                    <li>
                      <p>
                                    <span class="mathjax-tex">\(F_{3}\)</span>: test number</p>
                    </li>
                  </ul><p>We use the analysis of variance (ANOVA) to determine the influence of these factors on the average of these variables. Analyses were conducted using the free software TANAGRA.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>
                        </p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Non-immersive evaluation results</h4><p>
                              <i>Assistance influence</i> A first indication is given by the low number of failure for tests with assistance compared with tests without assistance: three versus ten (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig10">10</a>a). This difference is significant (<span class="mathjax-tex">\(F=6,168675, p &lt; 0.01\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Non-immersive evaluation results (<span class="mathjax-tex">\(n = 11, \hbox {total tests} = 66\)</span>). <b>a</b> Total number of failures. <b>b</b> Average number of constraints exceeding movements per test. <b>c</b> Average completion time per test</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Then, the average number of users’ manipulations exceeding the constraints (<span class="mathjax-tex">\(N_{b}\_D = N_{b}\_dX + N_{b}\_dY + N_{b}\_dV\)</span>) is 43.06 for test without assistance, while it is 15.42 for test with (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig10">10</a>b). This difference is highly significant (<span class="mathjax-tex">\(F = 12.61, p = 0.0007 \ll 0.01\)</span>). By studying in detail this variation, it seems more evident for volumic constraints exceeding movements (<span class="mathjax-tex">\(F = 8.52, p = 0.004 \ll 0.01\)</span>) than for angular constraints exceeding movements on the <span class="mathjax-tex">\(X\)</span> axis (<span class="mathjax-tex">\(F = 4.21, p = 0.04 &gt; 0.01\)</span>) or on the <span class="mathjax-tex">\(Y\)</span> axis (<span class="mathjax-tex">\(F = 2.36, p = 0.12 &gt; 0.01\)</span>).</p><p>The average time <span class="mathjax-tex">\(T\)</span> is decreased with assistance: <span class="mathjax-tex">\(37.06\)</span> s versus <span class="mathjax-tex">\(53.42\)</span> s (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig10">10</a>c). The SD is lower: <span class="mathjax-tex">\(26.55\)</span> s with assistance against <span class="mathjax-tex">\(31.43\)</span> s without assistance. This difference is marginally significant (<span class="mathjax-tex">\(F = 5.21, p = 0.02\)</span>), although the execution speed of the task is not a priority.</p><p>These first results show that:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>The presence of VF allows the participants to complete the tasks at 91 % of trials.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Designed VF seem to ensure their function, namely to inform about the presence of a constraint and enforce it. The participants have a quick perception of the model state (exceeding or not a constraint) and so identify very quickly what manipulation led to a probable exceed. Thus, participants have less hesitation to perform the task and do not persist in repeating the same manipulation. That reduces <span class="mathjax-tex">\(N_{b}\_D\)</span>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Volume guide is the most used, and this could be explained by the difficulty to perceive the 3D model through a monoscopic display. Users are able to manage the angular constraint because it is local to the manipulated cylinder, whereas the volume constraint is related to the whole model and is potentially exceeded by a distant cylinder.</p>
                        
                      </li>
                    </ol><p>
                              <i>Expertise influence</i> Furthermore, we observe that the expertise level (novice, intermediate, expert) of participants influence the average time <span class="mathjax-tex">\(T\)</span> (<span class="mathjax-tex">\(F = 9.37, p = 0.0002 \ll 0.01\)</span>) with a SD almost equivalent to the three levels. Consequently, participant expertise influences the number of failures. But we note that the expertise determines the number of validated areas in a test <span class="mathjax-tex">\(N_{b}\_Z\)</span> (<span class="mathjax-tex">\(F = 4.53, p &lt; 0.01\)</span>). This means that even if in failure, an expert can validate, on average, 2.8 areas while a novice validates 2.2.</p><p>However, the expertise influence on the total number of constraints exceeding movements <span class="mathjax-tex">\(N_{b}\_D\)</span> is not highly significant (<span class="mathjax-tex">\(F = 1.96, p = 0.14\)</span>), even if the average is 13.5 for an expert, 16.4 for an intermediary and 26.9 for a novice. This confirms that obeying to constraints does not depend on any particular knowledge in interaction with 3D objects but on expertise in the model design. We find the same results for all participants. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig11">11</a> illustrates the decrease in the average number of constraints exceeding movements with assistance, while <span class="mathjax-tex">\(N_{b}\_D\)</span> remains relatively high and irregular without assistance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Non-immersive evaluation results (<span class="mathjax-tex">\(n = 11, \hbox {total tests} = 66\)</span>). Evolution of the average number of constraints exceeding movements for all participants per test</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Semi-immersive evaluation results</h4><p>
                              <i>Assistance influence</i> We keep the same analysis order as for the first evaluation. First, the failure number is greatly influenced by the assistance: 6 with versus 17 without (<span class="mathjax-tex">\(F=6.76, p = 0.01 \le 0.01\)</span>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig12">12</a>a). This observation is coherent with the previous one; it confirms that even in semi-immersive environment, the assistance reduces the failures number.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Semi-immersive evaluation results (<span class="mathjax-tex">\(n = 12, \hbox {total tests} = 120\)</span>). <b>a</b> Total number of failures. <b>b</b> Average number of constraints exceeding movements per test. <b>c</b> Average completion time per test</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Assistance also highly influences the average of constraint exceeding movements <span class="mathjax-tex">\(N_{b}\_D\)</span>: 32.61 with assistance versus 76.9 without (<span class="mathjax-tex">\(F = 24.34, p = 0.000003 \ll 0.01\)</span>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig12">12</a>b). Unlike the first evaluation, this difference is highly significant for volume constraint (<span class="mathjax-tex">\(F = 12.71, p = 0.0005 \ll 0.01\)</span>) but also for angular constraints [on the <span class="mathjax-tex">\(X\)</span> axis (<span class="mathjax-tex">\(F = 12.94, p = 0.0004 \ll 0.01\)</span>) and <span class="mathjax-tex">\(Y\)</span> axis (<span class="mathjax-tex">\(F = 11.65, p = 0.0008 \ll 0.01\)</span>)]. We assume that the semi-immersive devices improve visual perception of the participants, allowing them to recognize both the local and global constraints. This confirms that the global constraint (i.e., volume) was improperly perceived by participants with non-immersive system. The test category influences the average time <span class="mathjax-tex">\(T\)</span>: <span class="mathjax-tex">\(34.96\)</span> versus <span class="mathjax-tex">\(50.38\)</span> s (<span class="mathjax-tex">\(F=10.4, p = 0.001 &lt; 0.01\)</span>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig12">12</a>b).</p><p>
                              <i>Expertise influence</i> Concerning the relation between participant results and expertise level (novice, intermediate and expert), the conclusions are as follows:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>The expertise level of individuals influences <span class="mathjax-tex">\(T\)</span> (<span class="mathjax-tex">\(F = 7.34, p = 0.0009 \ll 0.01\)</span>), with a SD almost equivalent to the three expertise levels. Indeed, mastering 3DI techniques and devices simplifies the accomplishment of the task and consequently, reduces the execution time.</p>
                      </li>
                      <li>
                        <p>The influence of expertise on the average number of failures is not significant (<span class="mathjax-tex">\(F = 2.68, p = 0.07\)</span>), as well as the number of validated areas in each test <span class="mathjax-tex">\(N_b\_z\)</span> (<span class="mathjax-tex">\(F = 1.92, p = 0.15\)</span>). This probably results from the naturalness of 6-DOF manipulation device, which allows novices to succeed in a test (even if it takes much more time), contrary to keyboard interaction.</p>
                      </li>
                      <li>
                        <p>As well as non-immersive evaluation, the expertise influence on <span class="mathjax-tex">\(N_b\_D\)</span> is not very significant (<span class="mathjax-tex">\(F = 2.92, p = 0.05\)</span>). <span class="mathjax-tex">\(N_b\_D\)</span> for experts is 38.22, 63 for intermediates and 63.05 for novices, with a SD equal for all (roughly equal to 52). Semi-immersive experiments confirm that the constraints do not depend on any particular knowledge in 3DI.</p>
                      </li>
                    </ul><p>The use of our assistance model improves the execution time and decreases the number of failures regardless of the expertise level of participant. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig13">13</a>a illustrates on one hand, a decrease in <span class="mathjax-tex">\(N_{b}\_D\)</span> with assistance, while it remains relatively high without assistance. On the other hand, we observe a decrease in the total time <span class="mathjax-tex">\(T\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0247-z#Fig13">13</a>b). For these two parameters, <span class="mathjax-tex">\(N_{b}\_D\)</span> and <span class="mathjax-tex">\(T\)</span>, a small learning curve emerges during tests with assistance. Thus, we can see that learning the approach of constraints-based 3DI and the assistance model improves the results of a participant, from one test to another. However, it is important to note that the regular alternation between tests with and without assistance may have influenced learning. Moreover, since the initial conditions of each experiment are the same (except point of view), participants might have learned progressively manipulation to do to validate the experience. This could also explain the slight irregular decrease in <span class="mathjax-tex">\(N_{b}\_D\)</span> and <span class="mathjax-tex">\(T\)</span> for test without assistance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0247-z/MediaObjects/10055_2014_247_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Semi-immersive evaluation results (<span class="mathjax-tex">\(n = 12, \hbox {total tests} = 120\)</span>). <b>a</b> Evolution of the average time. <b>b</b> Evolution of the average exceeding number per test</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0247-z/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Comparison between non-immersive and semi-immersive assistance</h4><p>To compare results of non-immersive with desktop interactions (noted NI) and semi-immersive with VR interactions (SI) evaluations, we followed the same evaluation scenario and the same initial conditions for each test. Ten participants participated to both experiments.</p><p>
                              <i>Tests with assistance</i>
                              <span class="mathjax-tex">\(N_b\_D\)</span> is identical for both environments. So we can note that the CTT model is as effective for non-immersive environments as for semi-immersive environments. This confirms the genericity of the model and its independence from the used devices. <span class="mathjax-tex">\(T\)</span> is <span class="mathjax-tex">\(39.72\)</span> s in semi-immersive environment (SD 22.91) and slightly lower in non-immersive one, <span class="mathjax-tex">\(37.69\)</span> s (SD <span class="mathjax-tex">\(27.43\)</span>). Our analysis is that the selection is a little more difficult and time-consuming with the VR interaction (Flystick device and virtual ray technique) than with the mouse (2D picking) because of the increased DOF and the lack of physical support for the hand. It also adds an extra time for the navigation task.</p><p>
                              <span class="mathjax-tex">\(N_{b}\_D\)</span> is 16.96 for NI and 37.33 for SI. The number of constraints exceeding movements has almost doubled. Here, again we think this is due to the increased DOF with the VR device and technique for the manipulation task. Indeed, the user acts on three rotation axes while the keyboard allows a single rotation axis (up–down arrows correspond to an axis and left–right to another).</p><p>We can make two statements:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>VF is used and assist the participants in the two evaluations.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>The difference is great between assistance to the NI and SI manipulation. NI interaction is already limited by the device and the manipulation technique, while the latter offers more freedom to the user, which involves a greater risk of error.</p>
                        
                      </li>
                    </ol><p>Responses to the questionnaire of the second evaluation are coherent with what we previously suspected. To the question “do you think the assisted SI interaction mode is better than in NI?”, 70 % of the participants answered “yes” and 30 % responded “probably yes.” This seems natural because the participants judged VR interaction more intuitive and less constricting, so they try new changes, which leads to greater number of constraints exceeding attempts. Therefore, users are more reassured by the given assistance. This result justifies the need of our assistance model for the constraints-based interaction, especially for VR interactions.</p><p>
                              <i>Comparison</i> Now, we want to clearly quantify the performance gain of moving from non-immersive to semi-immersive 3DI of our CTT model. We focus on assisted and non-assisted tests of each experiment, and we calculate the performance gain for each analyzed parameter (number of failures, number of constraints exceeding movements and execution time of a test). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0247-z#Tab4">4</a> presents gains on all parameters and for each evaluation. With these global parameters, it is difficult to determine a possible comparison between the contribution of the assistance model in NI and SI because several additional factors come into play, as the complexity of the 3D selection, adding the navigation task or the system control <i>via</i> the flystick for the second evaluation. However, we can observe that:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>For both experiments, the assistance model reduces more than <span class="mathjax-tex">\(50\,\%\)</span> the average number of failures and the average number of constraints exceeding movements.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>The gain on the average time for performing a test is also comparable to the experience with NI and SI.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Although the assistance model seems to provide a performance gain greater for NI, it is nonetheless for SI. Despite the additional navigation and selection time, the gain is almost equal for both platforms.</p>
                        
                      </li>
                    </ol>
                              <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Gains of adding assistance VF for each parameter according to the experimental platform</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0247-z/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec28"><div class="c-article-section" id="Sec28-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec28">Conclusion</h2><div class="c-article-section__content" id="Sec28-content"><p>We have developed a new framework CTT to assist 3DI in constraints-based VE, thus to guarantee the integrity of users’ experiences. The assistance tools we have formalized are VF. They are generic and multimodal. Their structure allows a reference to a specific global or local constraint, a 3DI task, and a rendering on visual, audio and haptic modalities. We have implemented our CTT framework in the case of the <i>in virtuo</i> human–molecule interaction. The aim of this application is to propose a new way to analyze a chromosome by manipulating its 3D model. It requires the integration of three specific physico-chemical constraints in the interaction system. Each constraint was associated with a VF in accordance with our formalism. Then, we compared our assistance model to conventional (non-assisted) 3DI, both in non-immersive and semi-immersive environments. The two evaluations of our approach (objective, based on the parameters of time, number of failures and the number of constraints exceeding movements; subjective, based on questionnaires) are coherent and show the gain brought by the assistance we have designed. VF gives better information on constraints and makes the participant more effective. Moreover, their unified structure allows clear, logical and adaptable implementation.</p><p>Future works are the formalization of haptic feedback. Indeed, it may be interesting to naturally “feel” the limits imposed by the constraints, e.g., a deformation, a collision or an unreachable zone. First studies show that the CTT model can handle haptic VF by adding to the modality field a force profile and a direction function linked to the primitive. This will lead us to evaluate again the system on a VR platform to identify more precisely the contribution of multimodal and immersive assistance.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>From the verb “to afford”: the ability of an object to suggest his own use (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1977" title="Gibson JJ (1977) The Theory of affordances. In: Shaw R, Bransford J (eds) Perceiving, acting, and knowing" href="/article/10.1007/s10055-014-0247-z#ref-CR16" id="ref-link-section-d57433e631">1977</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>Visual immersion can be defined as the objective level of fidelity provided by the system (Bowman and McMahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" href="/article/10.1007/s10055-014-0247-z#ref-CR6" id="ref-link-section-d57433e2665">2007</a>). It depends on several factors: field of view (FOV), field of regard (FOR), display size, display resolution, stereoscopy, head-tracking, etc. Kalawsky have also adopted a techno-centered classification of immersion depending on the type of visual display and FOV: non-immersive, semi-immersive or fully-immersive VEs (Kalawsky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Kalawsky R (1996) Exploiting virtual reality techniques in education and training: technological issues. Technical report. Loughborough University of Technology, Advisory Group on Computer Graphics (AGOCG). &#xA;                    http://www.agocg.ac.uk/reports/virtual/vrtech/title.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-014-0247-z#ref-CR21" id="ref-link-section-d57433e2668">1996</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>
                                 <a href="http://eric.univ-lyon2.fr/~ricco/tanagra/fr/tanagra.html">http://eric.univ-lyon2.fr/~ricco/tanagra/fr/tanagra.html</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Abbott JJ, Marayong P, Okamura MA (2007) Haptic virtual fixtures for robot-assisted manipulation. Springer tra" /><p class="c-article-references__text" id="ref-CR1">Abbott JJ, Marayong P, Okamura MA (2007) Haptic virtual fixtures for robot-assisted manipulation. Springer tracts in robotics research, vol 28, pp 49–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D (1999) Interaction techniques for common tasks in immersive virtual environments: design, evaluation," /><p class="c-article-references__text" id="ref-CR2">Bowman D (1999) Interaction techniques for common tasks in immersive virtual environments: design, evaluation, and application. PhD thesis, Georgia Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Hodges L (1995) User interface constraints for immersive virtual environment applications. Graphics," /><p class="c-article-references__text" id="ref-CR3">Bowman D, Hodges L (1995) User interface constraints for immersive virtual environment applications. Graphics, Visualization, and Usability Center technical report GIT-GVU-95-26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Hodges L (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersiv" /><p class="c-article-references__text" id="ref-CR4">Bowman D, Hodges L (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 35–ff</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Koller D, Hodges L (1998) A methodology for the evaluation of travel techniques for immersive virtua" /><p class="c-article-references__text" id="ref-CR5">Bowman D, Koller D, Hodges L (1998) A methodology for the evaluation of travel techniques for immersive virtual environments. In: Virtual reality, vol 3, pp 120–131</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, RP. McMahan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" /><p class="c-article-references__text" id="ref-CR6">Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2007.257" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20how%20much%20immersion%20is%20enough%3F&amp;journal=Computer&amp;volume=40&amp;issue=7&amp;pages=36-43&amp;publication_year=2007&amp;author=Bowman%2CDA&amp;author=McMahan%2CRP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brooks T, Ince I, Robotics H (1992) Operator vision aids for telerobotic assembly and servicing inspace. In: I" /><p class="c-article-references__text" id="ref-CR7">Brooks T, Ince I, Robotics H (1992) Operator vision aids for telerobotic assembly and servicing inspace. In: IEEE international conference on robotics and automation, proceedings, vol 1, pp 886–891</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in" /><p class="c-article-references__text" id="ref-CR8">Calderon C, Cavazza M, Diaz D (2003) A new approach to the interactive resolution of configuration problems in virtual environments. In: Smart graphics, Third international symposium. Springer, Berlin, pp 112–122</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Castet J, Florens J (2008) A virtual reality simulator based on haptic hard constraints. In: Haptics: percepti" /><p class="c-article-references__text" id="ref-CR9">Castet J, Florens J (2008) A virtual reality simulator based on haptic hard constraints. In: Haptics: perception, devices and scenarios. Lecture notes in computer science, vol 5024, pp 918–923</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Cooper, F. Khatib, A. Treuille, J. Barbero, J. Lee, M. Beenen, A. Leaver-Fay, D. Baker, Z. Popovic, F. Players, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Cooper S, Khatib F, Treuille A, Barbero J, Lee J, Beenen M, Leaver-Fay A, Baker D, Popovic Z, Players F (2010)" /><p class="c-article-references__text" id="ref-CR10">Cooper S, Khatib F, Treuille A, Barbero J, Lee J, Beenen M, Leaver-Fay A, Baker D, Popovic Z, Players F (2010) Predicting protein structures with a multiplayer online game. Nature 466(7307):756–760</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature09304" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20protein%20structures%20with%20a%20multiplayer%20online%20game&amp;journal=Nature&amp;volume=466&amp;issue=7307&amp;pages=756-760&amp;publication_year=2010&amp;author=Cooper%2CS&amp;author=Khatib%2CF&amp;author=Treuille%2CA&amp;author=Barbero%2CJ&amp;author=Lee%2CJ&amp;author=Beenen%2CM&amp;author=Leaver-Fay%2CA&amp;author=Baker%2CD&amp;author=Popovic%2CZ&amp;author=Players%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Essabbah M, Otmane S, Hérisson J, Mallem M (2009a) A new approach to design an interactive system for molecula" /><p class="c-article-references__text" id="ref-CR11">Essabbah M, Otmane S, Hérisson J, Mallem M (2009a) A new approach to design an interactive system for molecular analysis. In: Human-computer interaction. Interacting in various application domains, Lecture notes in computer science, vol 5613, pp 713–722</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Essabbah M, Otmane S, Mallem M, Herisson J (2009b) Spatial organization of DNA: from the physical data to the " /><p class="c-article-references__text" id="ref-CR12">Essabbah M, Otmane S, Mallem M, Herisson J (2009b) Spatial organization of DNA: from the physical data to the 3D model. In: IEEE/ACS international conference on computer systems and applications, pp 880–883</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Férey N, Delalande O, Grasseau G, Baaden M (2008) A VR framework for interacting with molecular simulations. I" /><p class="c-article-references__text" id="ref-CR13">Férey N, Delalande O, Grasseau G, Baaden M (2008) A VR framework for interacting with molecular simulations. In: ACM symposium on virtual reality software and technology, pp 91–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Férey, J. Nelson, C. Martin, L. Picinali, G. Bouyer, A. Tek, P. Bourdot, J. Burkhardt, B. Katz, M. Ammi, C. Etchebest, L. Autin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt J, Katz B, Ammi M, Etchebest C," /><p class="c-article-references__text" id="ref-CR14">Férey N, Nelson J, Martin C, Picinali L, Bouyer G, Tek A, Bourdot P, Burkhardt J, Katz B, Ammi M, Etchebest C, Autin L (2009) Multisensory VR interaction for protein-docking in the CoRSAIRe project. Virtual Real 13(4):257–271</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0136-z" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multisensory%20VR%20interaction%20for%20protein-docking%20in%20the%20CoRSAIRe%20project&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=4&amp;pages=257-271&amp;publication_year=2009&amp;author=F%C3%A9rey%2CN&amp;author=Nelson%2CJ&amp;author=Martin%2CC&amp;author=Picinali%2CL&amp;author=Bouyer%2CG&amp;author=Tek%2CA&amp;author=Bourdot%2CP&amp;author=Burkhardt%2CJ&amp;author=Katz%2CB&amp;author=Ammi%2CM&amp;author=Etchebest%2CC&amp;author=Autin%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environ" /><p class="c-article-references__text" id="ref-CR15">Fernando T, Murray N, Tan K, Wimalaratne P (1999) Software architecture for a constraint-based virtual environment. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST ’99. ACM, New York, pp 147–154</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gibson JJ (1977) The Theory of affordances. In: Shaw R, Bransford J (eds) Perceiving, acting, and knowing" /><p class="c-article-references__text" id="ref-CR16">Gibson JJ (1977) The Theory of affordances. In: Shaw R, Bransford J (eds) Perceiving, acting, and knowing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Gillet, M. Sanner, D. Stoffler, A. Olson, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Gillet A, Sanner M, Stoffler D, Olson A (2005) Tangible interfaces for structural molecular biology. Structure" /><p class="c-article-references__text" id="ref-CR17">Gillet A, Sanner M, Stoffler D, Olson A (2005) Tangible interfaces for structural molecular biology. Structure 13(3):483–491</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.str.2005.01.009" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tangible%20interfaces%20for%20structural%20molecular%20biology&amp;journal=Structure&amp;volume=13&amp;issue=3&amp;pages=483-491&amp;publication_year=2005&amp;author=Gillet%2CA&amp;author=Sanner%2CM&amp;author=Stoffler%2CD&amp;author=Olson%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Guébert C, Duriez C, Grisoni L (2008) Unified processing of constraints for interactive simulation. In: Procee" /><p class="c-article-references__text" id="ref-CR18">Guébert C, Duriez C, Grisoni L (2008) Unified processing of constraints for interactive simulation. In: Proceedings of VRIPHYS</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Heyd, S. Birmanns, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Heyd J, Birmanns S (2009) Immersive structural biology: a new approach to hybrid modeling of macromolecular as" /><p class="c-article-references__text" id="ref-CR19">Heyd J, Birmanns S (2009) Immersive structural biology: a new approach to hybrid modeling of macromolecular assemblies. Virtual Real 13(4):245–255</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0129-y" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20structural%20biology%3A%20a%20new%20approach%20to%20hybrid%20modeling%20of%20macromolecular%20assemblies&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=4&amp;pages=245-255&amp;publication_year=2009&amp;author=Heyd%2CJ&amp;author=Birmanns%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacoby R, Ferneau M, Humphries J (1994) Gestural interaction in a virtual environment. In: Proceedings of SPIE" /><p class="c-article-references__text" id="ref-CR20">Jacoby R, Ferneau M, Humphries J (1994) Gestural interaction in a virtual environment. In: Proceedings of SPIE 2177(355)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kalawsky R (1996) Exploiting virtual reality techniques in education and training: technological issues. Techn" /><p class="c-article-references__text" id="ref-CR21">Kalawsky R (1996) Exploiting virtual reality techniques in education and training: technological issues. Technical report. Loughborough University of Technology, Advisory Group on Computer Graphics (AGOCG). <a href="http://www.agocg.ac.uk/reports/virtual/vrtech/title.htm">http://www.agocg.ac.uk/reports/virtual/vrtech/title.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kuang AB, Payandeh S, Zheng B, Henigman F, MacKenzie CL (2004) Assembling virtual fixtures for guidance in tra" /><p class="c-article-references__text" id="ref-CR22">Kuang AB, Payandeh S, Zheng B, Henigman F, MacKenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: International symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367–374</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marayong P, Li M, Okamura AM, Hager GD (2003) Spatial motion constraints: theory and demonstrations for robot " /><p class="c-article-references__text" id="ref-CR23">Marayong P, Li M, Okamura AM, Hager GD (2003) Spatial motion constraints: theory and demonstrations for robot guidance using virtual fixtures. In: Proceedings of the IEEE international conference on robotics and automation, ICRA 2003, 14–19 Sept 2003, Taipei, Taiwan. IEEE, pp 1954–1959</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Marcelino, N. Murray, T. Fernando, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Marcelino L, Murray N, Fernando T (2003) A constraint manager to support virtual maintainability. Comput Graph" /><p class="c-article-references__text" id="ref-CR24">Marcelino L, Murray N, Fernando T (2003) A constraint manager to support virtual maintainability. Comput Graph 27(1):19–26</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2802%2900228-5" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20constraint%20manager%20to%20support%20virtual%20maintainability&amp;journal=Comput%20Graph&amp;volume=27&amp;issue=1&amp;pages=19-26&amp;publication_year=2003&amp;author=Marcelino%2CL&amp;author=Murray%2CN&amp;author=Fernando%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill Computer Science technical report TR" /><p class="c-article-references__text" id="ref-CR25">Mine M (1995) Virtual environment interaction techniques. UNC Chapel Hill Computer Science technical report TR95-018</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine RM, Brooks FP Jr, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environm" /><p class="c-article-references__text" id="ref-CR26">Mine RM, Brooks FP Jr, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: Proceedings of the 24th annual conference on computer graphics and interactive techniques, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oakley I, Adams A, Brewster S, Gray P (2002) Guidelines for the design of haptic widgets. In: Proceedings of B" /><p class="c-article-references__text" id="ref-CR27">Oakley I, Adams A, Brewster S, Gray P (2002) Guidelines for the design of haptic widgets. In: Proceedings of British HCI, British Computer Society, pp 195–212</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality ba" /><p class="c-article-references__text" id="ref-CR28">Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guides as an apparatus for augmented reality based telemanipulation system on the internet. In: IEEE annual simulation symposium (ANSS 2000) proceedings, pp 185–191</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ouramdane N, Otmane S, Davesne F, Mallem M (2006) Follow-me: a new 3d interaction technique based on virtual g" /><p class="c-article-references__text" id="ref-CR29">Ouramdane N, Otmane S, Davesne F, Mallem M (2006) Follow-me: a new 3d interaction technique based on virtual guides and granularity of interaction. In: Proceedings of the 2006 ACM international conference on virtual reality continuum and its applications, pp 137–144</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Picon F, Ammi M, Bourdot P (2008) Haptically-aided extrusion for object edition in CAD. In: Haptics: perceptio" /><p class="c-article-references__text" id="ref-CR30">Picon F, Ammi M, Bourdot P (2008) Haptically-aided extrusion for object edition in CAD. In: Haptics: perception, devices and scenarios, Lecture notes in computer science, vol 5024, pp 736–741</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pierce J, Forsberg A, Conway M, Hong S (1997) Image plane interaction techniques in 3d immersive environments." /><p class="c-article-references__text" id="ref-CR31">Pierce J, Forsberg A, Conway M, Hong S (1997) Image plane interaction techniques in 3d immersive environments. In: Proceedings of the 1997 symposium on interactive 3D graphics, pp 33–ff</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Billinghurst M, Weghorst S (1996) The go-go interaction technique: non-linear mapping for direct m" /><p class="c-article-references__text" id="ref-CR32">Poupyrev I, Billinghurst M, Weghorst S (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the 9th annual ACM symposium on user interface software and technology, pp 79–80</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Prada, S. Payandeh, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real 13(2):117–1" /><p class="c-article-references__text" id="ref-CR33">Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real 13(2):117–129</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0115-4" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20study%20of%20design%20and%20implementation%20of%20virtual%20fixtures&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=2&amp;pages=117-129&amp;publication_year=2009&amp;author=Prada%2CR&amp;author=Payandeh%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren J, Zhang H, Patel R, Peters T (2007) Haptics constrained motion for surgical intervention. In: Studies in " /><p class="c-article-references__text" id="ref-CR34">Ren J, Zhang H, Patel R, Peters T (2007) Haptics constrained motion for surgical intervention. In: Studies in health technology and informatics, vol 125, pp 379–384</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosenberg L (1992) The use of virtual fixtures as perceptual overlays to enhance operator performance in remot" /><p class="c-article-references__text" id="ref-CR35">Rosenberg L (1992) The use of virtual fixtures as perceptual overlays to enhance operator performance in remote environments. Technical report, no A054292, USAF Amstrong</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simard J, Ammi M (2012) Haptic interpersonal communication: improvement of actions coordination in collaborati" /><p class="c-article-references__text" id="ref-CR36">Simard J, Ammi M (2012) Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments. In: Virtual reality, vol 16, pp 173–186</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smelik R, Galka K, De Kraker KJ, Kuijper F, Bidarra R (2011) Semantic constraints for procedural generation of" /><p class="c-article-references__text" id="ref-CR37">Smelik R, Galka K, De Kraker KJ, Kuijper F, Bidarra R (2011) Semantic constraints for procedural generation of virtual worlds. In: Proceedings of the 2nd international workshop on procedural content generation in games, ACM, p 9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smith G, Stuerzlinger W, Salzman T (2001) 3d scene manipulation with 2d devices and constraints. In: Proceedin" /><p class="c-article-references__text" id="ref-CR38">Smith G, Stuerzlinger W, Salzman T (2001) 3d scene manipulation with 2d devices and constraints. In: Proceedings of graphics interface, pp 135–142</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sternberger L, Bechmann D (2005) Deformable ray-casting interaction technique. In: IEEE young virtual reality " /><p class="c-article-references__text" id="ref-CR39">Sternberger L, Bechmann D (2005) Deformable ray-casting interaction technique. In: IEEE young virtual reality conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Sweller, P. Ayres, S. Kalyuga, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Sweller J, Ayres P, Kalyuga S (2011) Cognitive load theory. Springer, Berlin" /><p class="c-article-references__text" id="ref-CR40">Sweller J, Ayres P, Kalyuga S (2011) Cognitive load theory. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20load%20theory&amp;publication_year=2011&amp;author=Sweller%2CJ&amp;author=Ayres%2CP&amp;author=Kalyuga%2CS">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0247-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">French National Research Agency, Paris, France</p><p class="c-article-author-affiliation__authors-list">Mouna Essabbah</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">IBISC, University of Evry-Val-d’Essonne, Evry Courcouronnes Cedex, France</p><p class="c-article-author-affiliation__authors-list">Guillaume Bouyer, Samir Otmane &amp; Malik Mallem</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mouna-Essabbah"><span class="c-article-authors-search__title u-h3 js-search-name">Mouna Essabbah</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mouna+Essabbah&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mouna+Essabbah" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mouna+Essabbah%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Guillaume-Bouyer"><span class="c-article-authors-search__title u-h3 js-search-name">Guillaume Bouyer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guillaume+Bouyer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guillaume+Bouyer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guillaume+Bouyer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Samir-Otmane"><span class="c-article-authors-search__title u-h3 js-search-name">Samir Otmane</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Samir+Otmane&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samir+Otmane" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samir+Otmane%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Malik-Mallem"><span class="c-article-authors-search__title u-h3 js-search-name">Malik Mallem</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Malik+Mallem&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Malik+Mallem" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Malik+Mallem%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0247-z/email/correspondent/c1/new">Mouna Essabbah</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20framework%20to%20design%203D%20interaction%20assistance%20in%20constraints-based%20virtual%20environments&amp;author=Mouna%20Essabbah%20et%20al&amp;contentID=10.1007%2Fs10055-014-0247-z&amp;publication=1359-4338&amp;publicationDate=2014-06-25&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0247-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0247-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Essabbah, M., Bouyer, G., Otmane, S. <i>et al.</i> A framework to design 3D interaction assistance in constraints-based virtual environments.
                    <i>Virtual Reality</i> <b>18, </b>219–234 (2014). https://doi.org/10.1007/s10055-014-0247-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0247-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06-27">27 June 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-04-07">07 April 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06-25">25 June 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-09">September 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0247-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0247-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Framework</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Complex environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Constraints</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Assistance model</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual fixtures</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0247-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=247;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

