<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Optimized GPU-based post-processing for stereoscopic multi-projector d"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="To drive multi-projector display systems, the image content has to be post-processed to apply geometrical and photometrical correction algorithms. Since the optical path is shared for both views of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Optimized GPU-based post-processing for stereoscopic multi-projector display systems"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-06-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="To drive multi-projector display systems, the image content has to be post-processed to apply geometrical and photometrical correction algorithms. Since the optical path is shared for both views of a stereoscopic projector, we propose to eliminate redundant calculations by processing both views at once. We show that by exploiting the color similarities of stereoscopic image pairs, the cache efficiency of LUT-based color correction methods can be improved. Stereoscopic content is often transmitted in different formats where a single frame represents both views in a spatially multiplexed pattern, and display devices typically support only a subset of these formats, so that the need for conversion arises. We propose an algorithm to directly incorporate such format conversions into the post-processing and show that our combined approach can achieve significant performance improvements compared to the naive multi-pass implementation. The reduced overhead of the post-processing step increases the number of projectors which can be driven by a single GPU, which enables building larger or higher resolution displays at lower costs. The flexibility gained by the ability to process different input formats also greatly enhances the usability of powerwalls by allowing to implement the post-processing at different levels and working with unmodified software applications."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-06-13"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="45"/>

    <meta name="prism.endingPage" content="60"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0352-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0352-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0352-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0352-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Optimized GPU-based post-processing for stereoscopic multi-projector display systems"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2019/03"/>

    <meta name="citation_online_date" content="2018/06/13"/>

    <meta name="citation_firstpage" content="45"/>

    <meta name="citation_lastpage" content="60"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0352-5"/>

    <meta name="DOI" content="10.1007/s10055-018-0352-5"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0352-5"/>

    <meta name="description" content="To drive multi-projector display systems, the image content has to be post-processed to apply geometrical and photometrical correction algorithms. Since th"/>

    <meta name="dc.creator" content="Marcel Heinz"/>

    <meta name="dc.creator" content="Guido Brunnett"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Allen W, Ulichney R (2005) Wobulation: doubling the addressed resolution of projection display. In: Proceedings of SID symposium digest of technical papers (SID), Boston, MA, USA, pp 1514&#8211;1517"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Embedded entertainment with smart projectors; citation_author=O Bimber, A Emmerling, T Klemmer; citation_volume=38; citation_issue=1; citation_publication_date=2005; citation_pages=48-55; citation_doi=10.1109/MC.2005.17; citation_id=CR2"/>

    <meta name="citation_reference" content="Bourke P, Calati R (2004) Edge blending using commodity projectors. Swinburne University, Technical report"/>

    <meta name="citation_reference" content="citation_journal_title=Photogramm Eng; citation_title=Close-range camera calibration; citation_author=DC Brown; citation_volume=37; citation_issue=8; citation_publication_date=1971; citation_pages=855-866; citation_id=CR4"/>

    <meta name="citation_reference" content="Chen H, Sukthankar R, Wallace G, Li K (2002) Scalable alignment of large-format multi-projector displays using camera homography trees. In: Proceedings of the conference on Visualization &#8217;02, IEEE Computer Society, Washington, DC, USA, VIS &#8217;02, pp 339&#8211;346"/>

    <meta name="citation_reference" content="Chen M, Fan B, Song H (2014) Geometry calibration for multi-projector display automatically based on the feedback of camera algorithm. In: 2014 11th international conference on fuzzy systems and knowledge discovery (FSKD), pp 570&#8211;574. 
                    https://doi.org/10.1109/FSKD.2014.6980897
                    
                  
                        "/>

    <meta name="citation_reference" content="Fr&#246;hlich B, Hoffmann J, Klueger K, Hochstrate J (2004) Implementing multi-viewer time-sequential stereo displays based on shuttered lcd projectors. 
                    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.488.3164
                    
                  
                        "/>

    <meta name="citation_reference" content="Fr&#246;hlich B, Hochstrate J, Hoffmann J, Kl&#252;ger K, Blach R, Bues M, Stefani O (2005) Implementing multi-viewer stereo displays. In: WSCG&#8217;2005"/>

    <meta name="citation_reference" content="Gaur PK, Sarode DM, Shete PP, Venkata PK, Bose SK (2014) Achieving seamlessness in multi-projector based tiled display using camera feedback. In: 2014 international conference on contemporary computing and informatics (IC3I). IEEE, pp 293&#8211;298"/>

    <meta name="citation_reference" content="Heinz M, Brunnett G (2015) Dense sampling of 3D color transfer functions using HDR photography. In: 2015 IEEE conference on computer vision and pattern recognition workshops (CVPRW), Boston, MA, USA, pp 25&#8211;32. 
                    https://doi.org/10.1109/CVPRW.2015.7301372
                    
                  
                        "/>

    <meta name="citation_reference" content="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of the conference on Visualization &#8217;01, IEEE Computer Society, Washington, DC, USA, VIS &#8217;01, pp 175&#8211;182"/>

    <meta name="citation_reference" content="Jordan S, Greenspan M (2010) Projector optical distortion calibration using gray code patterns. In: 2010 IEEE computer society conference on computer vision and pattern recognition workshops (CVPRW), pp 72 &#8211;79"/>

    <meta name="citation_reference" content="Majumder A, Stevens R (2002) LAM: luminance attenuation map for photometric uniformity in projection based displays. In: Proceedings of the ACM symposium on Virtual reality software and technology, ACM, New York, NY, USA, VRST &#8217;02, pp 147&#8211;154"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Perceptual photometric seamlessness in projection-based tiled displays; citation_author=A Majumder, R Stevens; citation_volume=24; citation_issue=1; citation_publication_date=2005; citation_pages=118-139; citation_doi=10.1145/1037957.1037964; citation_id=CR14"/>

    <meta name="citation_reference" content="Martynov I, Kamarainen JK, Lensu L (2011) Projector calibration by &#8220;inverse camera calibration&#8221;. In: Proceedings of the 17th scandinavian conference on image analysis, SCIA&#8217;11. Springer, Berlin, pp 536&#8211;544"/>

    <meta name="citation_reference" content="Meyer C (2013) AMD FirePro display output post-processing. Advanced Micro Devices Inc, Technical report"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Easy calibration of a multi-projector display system; citation_author=T Okatani, K Deguchi; citation_volume=85; citation_issue=1; citation_publication_date=2009; citation_pages=1-18; citation_doi=10.1007/s11263-009-0242-0; citation_id=CR17"/>

    <meta name="citation_reference" content="Raij A, Gill G, Majumder A, Towles H, Fuchs H (2003) Pixelflex2: a comprehensive, automatic, casually-aligned multi-projector display. In. In Proceedings of IEEE international workshop on projector-camera systems"/>

    <meta name="citation_reference" content="Raskar R (2000) Immersive planar display using roughly aligned projectors. In: Proceedings of the IEEE virtual reality 2000 conference. IEEE Computer Society, Washington, DC, USA, VR &#8217;00, p 109"/>

    <meta name="citation_reference" content="Raskar R, Welch G, Fuchs H (1998) Seamless projection overlaps using image warping and intensity blending. In: Fourth international conference on virtual systems and multimedia"/>

    <meta name="citation_reference" content="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization &#8217;99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS &#8217;99, pp 161&#8211;168"/>

    <meta name="citation_reference" content="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2005) ilamps: geometrically aware and self-configuring projectors. In: ACM SIGGRAPH 2005 courses. SIGGRAPH &#8217;05. ACM, New York"/>

    <meta name="citation_reference" content="Sadlo F, Weyrich T, Peikert R, Gross M (2005) A practical structured light acquisition system for point-based geometry and texture. In: Proceedings of the eurographics symposium of point-based graphics"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Visual Comput Graph; citation_title=Color seamlessness in multi-projector displays using constrained gamut morphing; citation_author=B Sajadi, M Lazarov, M Gopi, A Majumder; citation_volume=15; citation_issue=6; citation_publication_date=2009; citation_pages=1317-1326; citation_doi=10.1109/TVCG.2009.124; citation_id=CR24"/>

    <meta name="citation_reference" content="Sajadi B, Lazarov M, Majumder A (2010) ADICT: accurate direct and inverse color transformation. In: Proceedings of the 11th European conference on computer vision: part IV. ECCV&#8217;10. Springer, Berlin, pp 72&#8211;86"/>

    <meta name="citation_reference" content="Stefanizzi B (2014) DirectGMA on AMD&#8217;s FirePro GPUs. Advanced Micro Devices Inc, Technical report"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Color and brightness appearance issues in tiled displays; citation_author=MC Stone; citation_volume=21; citation_issue=5; citation_publication_date=2001; citation_pages=58-66; citation_doi=10.1109/38.946632; citation_id=CR27"/>

    <meta name="citation_reference" content="Surati RJ (1999) Scalable self-calibrating display technology for seamless large-scale displays. PhD thesis, Massachusetts Institute of Technology"/>

    <meta name="citation_reference" content="Wallace G, Chen H, Li K (2003) Color gamut matching for tiled display walls. In: Proceedings of the workshop on virtual environments 2003, EGVE &#8217;03. ACM, New York, pp 293&#8211;302"/>

    <meta name="citation_reference" content="Yang R, Gotz D, Hensley J, Towles H, Brown MS (2001) Pixelflex: a reconfigurable multi-projector display system. In: Proceedings of the conference on visualization &#8217;01. VIS &#8217;01. IEEE Computer Society, Washington, pp 167&#8211;174"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A flexible new technique for camera calibration; citation_author=Z Zhang; citation_volume=22; citation_issue=11; citation_publication_date=2000; citation_pages=1330-1334; citation_doi=10.1109/34.888718; citation_id=CR31"/>

    <meta name="citation_author" content="Marcel Heinz"/>

    <meta name="citation_author_email" content="heinm@informatik.tu-chemnitz.de"/>

    <meta name="citation_author_institution" content="Chemnitz University of Technology, Chemnitz, Germany"/>

    <meta name="citation_author" content="Guido Brunnett"/>

    <meta name="citation_author_email" content="brunnett@informatik.tu-chemnitz.de"/>

    <meta name="citation_author_institution" content="Chemnitz University of Technology, Chemnitz, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0352-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0352-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Optimized GPU-based post-processing for stereoscopic multi-projector display systems"/>
        <meta property="og:description" content="To drive multi-projector display systems, the image content has to be post-processed to apply geometrical and photometrical correction algorithms. Since the optical path is shared for both views of a stereoscopic projector, we propose to eliminate redundant calculations by processing both views at once. We show that by exploiting the color similarities of stereoscopic image pairs, the cache efficiency of LUT-based color correction methods can be improved. Stereoscopic content is often transmitted in different formats where a single frame represents both views in a spatially multiplexed pattern, and display devices typically support only a subset of these formats, so that the need for conversion arises. We propose an algorithm to directly incorporate such format conversions into the post-processing and show that our combined approach can achieve significant performance improvements compared to the naive multi-pass implementation. The reduced overhead of the post-processing step increases the number of projectors which can be driven by a single GPU, which enables building larger or higher resolution displays at lower costs. The flexibility gained by the ability to process different input formats also greatly enhances the usability of powerwalls by allowing to implement the post-processing at different levels and working with unmodified software applications."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Optimized GPU-based post-processing for stereoscopic multi-projector display systems | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0352-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Projector, Display, Multi-projector, Tiled display, Calibration, Stereoscopy","kwrd":["Projector","Display","Multi-projector","Tiled_display","Calibration","Stereoscopy"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0352-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0352-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=352;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0352-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Optimized GPU-based post-processing for stereoscopic multi-projector display systems
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0352-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0352-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-06-13" itemprop="datePublished">13 June 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Optimized GPU-based post-processing for stereoscopic multi-projector display systems</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marcel-Heinz" data-author-popup="auth-Marcel-Heinz" data-corresp-id="c1">Marcel Heinz<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-0586-6981"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-0586-6981</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Chemnitz University of Technology" /><meta itemprop="address" content="0000 0001 2294 5505, grid.6810.f, Chemnitz University of Technology, Straße der Nationen 62, 09017, Chemnitz, Germany" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guido-Brunnett" data-author-popup="auth-Guido-Brunnett">Guido Brunnett</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Chemnitz University of Technology" /><meta itemprop="address" content="0000 0001 2294 5505, grid.6810.f, Chemnitz University of Technology, Straße der Nationen 62, 09017, Chemnitz, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">45</span>–<span itemprop="pageEnd">60</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">200 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0352-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>To drive multi-projector display systems, the image content has to be post-processed to apply geometrical and photometrical correction algorithms. Since the optical path is shared for both views of a stereoscopic projector, we propose to eliminate redundant calculations by processing both views at once. We show that by exploiting the color similarities of stereoscopic image pairs, the cache efficiency of LUT-based color correction methods can be improved. Stereoscopic content is often transmitted in different formats where a single frame represents both views in a spatially multiplexed pattern, and display devices typically support only a subset of these formats, so that the need for conversion arises. We propose an algorithm to directly incorporate such format conversions into the post-processing and show that our combined approach can achieve significant performance improvements compared to the naive multi-pass implementation. The reduced overhead of the post-processing step increases the number of projectors which can be driven by a single GPU, which enables building larger or higher resolution displays at lower costs. The flexibility gained by the ability to process different input formats also greatly enhances the usability of powerwalls by allowing to implement the post-processing at different levels and working with unmodified software applications.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Display systems constitute a major part of most virtual reality hardware setups. Besides head mounted displays (HMDs), powerwalls with large screens and CAVE installations can be used to immersive visualizations with a wide field of view. Multi-projector display systems are a flexible and extensible means to build such large-scale, high-resolution displays. However, image data cannot directly be sent to the projectors, but has to be specifically generated or post-processed to account for the alignment of the tiles and to compensate for the geometric and photometric properties of the projectors in order to achieve a perceptually uniform output.</p><p>The choice of eligible calibration methods will depend on the properties of the projectors in use, the shape and structure of the display surface, the available calibration equipment (i.e., cameras, colorimeters) and many other factors—there is no “one size fits all” solution. Unfortunately, there is also no general software standard for applying the required post-processing operations—which are specific to the chosen calibration method—to the output of graphical applications or to already available images or video streams.</p><p>The increase in processing power of GPUs in recent years made it feasible to drive systems with 4–6 segments with a single GPUs, so that a single computer is capable of rendering <i>and</i> applying said post-processing for all of the projectors in real time. Such setups greatly improve the usability of powerwalls and other segmented displays, since no render cluster has to be operated and maintained and no distributed VR software system is required. Furthermore, the output latency can be reduced if the video signal is not to be post-processed in a separate device, which is especially important for fast-paced VR applications.</p><h3 class="c-article__sub-heading" id="Sec2">Focus of this work</h3><p>A wide range of different calibration techniques and algorithms have been proposed over the course of the last 20 years. In this work, we do <b>not</b> address the actual calibration process, and we are <b>not</b> proposing a novel calibration method. Instead, we focus on efficiently <i>applying</i> the necessary post-processing operations to image data. To do so, we will briefly review existing calibration methods to identify the requirements and constraints that they impose on the post-processing and derive a generalized framework capable of implementing most of the existing techniques.</p><p>Since the established calibration methods were developed with monoscopic devices in mind, the usual approach to deal with stereoscopic projectors is to apply the post-processing algorithms onto each view separately. However, when using stereoscopic projectors, the same optical path is used for displaying both views. We argue that in such a scenario, the geometric calibration and parts of photometrical calibration can actually be <i>shared</i> for both views, eliminating redundant computations of the naive approach.</p><p>In this work, we propose a GPU-based framework which allows the efficient implementation and easy integration of different such calibration-related post-processing methods into graphics and VR applications. We specifically address the properties of <i>stereoscopic</i> projectors and focus on minimizing the processing time. We aim for reducing the overhead of the post-processing as much as possible, so that as many projectors as possible can be driven with a single GPU while most of the computing power can be devoted to the actual image generation.</p><p>There are different levels where this framework could be used:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>in the application itself,</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>inside an engine/library or graphical middleware,</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>at the operating system level, or</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>in a separate device</p>
                      
                    </li>
                  </ol><p>The levels 1 and 2 allow for the most efficient operation, but require source code modifications. Levels 3 and 4 can work independently of the applications and provide the functionality of using the multi-projector display just like a “big monitor,” greatly enhancing the usability, especially when working with multitude of different applications.</p><p>In the ideal case, stereoscopic content is provided in the form of separate images or video streams for each eye, and the display devices can process such content in the full resolution. Unfortunately, especially with consumer-grade devices, a single display connection is often used to transfer both views. Since the bandwidth is limited, the data amount of each view is often reduced. Using half the pixel resolution, two views can be transmitted with the same bandwidth requirements by placing the reduced views next to (or on top) of each other, or by vertically or horizontally interleaving both views in the pixel raster, or by mixing them in some checkerboard pattern (the common formats are outlined in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig1">1</a>). We will show that conversions between different such formats can be combined with the post-processing in a manner which is more efficient than the naive approach of decoding to separate images, post-processing and re-encoding into the stereoscopic output format. This is beneficial even if a multiplexed stereoscopic format is used only either on the input <i>or</i> the output side (with the other side using separated images per view).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="86" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Overview of common stereoscopic formats. Conceptual data layout in an 30 by 20 pixel raster is visualized with left view in red, right view in blue. All of these formats can be extended for an arbitrary number of views. <b>a</b> Separate views/frame sequential. <b>b</b> Side-by-side. <b>c</b> Top-bottom. <b>d</b> Horizontally interleaved. <b>e</b> Vertically interleaved. <b>f</b> Checkerboard. <b>g</b> Multiple views per pixel (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Our actual implementation takes advantage of the programmable features of modern GPUs. At the core, our framework provides a software library to post-process multiview image data according to various calibration methods, which can be integrated into existing applications. Furthermore, our framework provides a component for driving display walls by multiple video inputs, turning a standard PC into a pixel-processing device (level 4). We also adopted AMD’s “Display Output Post-Processing (D.O.P.P.)” technology (Meyer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Meyer C (2013) AMD FirePro display output post-processing. Advanced Micro Devices Inc, Technical report" href="/article/10.1007/s10055-018-0352-5#ref-CR16" id="ref-link-section-d37710e459">2013</a>) to directly apply the calibration post-processing to the Aero desktop on windows platforms (level 3), eliminating the need for a dedicated pixel-processing device while still retaining the ability to work with unmodified applications.</p><p>Being able to process stereoscopic content in different multiplexed formats proves very useful especially when implementing the post-processing at levels 3 or 4, where the image generation is decoupled from the post-processing and the choice of formats is restricted by the application software. While it is easy to see the need to support such “encoded” stereoscopic formats on the <i>output</i> side to drive the projectors, there are also use cases for using stereoscopic formats on the <i>input</i> side, for example to be able to use a standard laptop and a single display connection as the source for a stereoscopic presentation on a powerwall. In a similar way, the Windows desktop texture—which is only monoscopic—can be re-interpreted as a stereoscopic format, so by using an appropriate encoding, we can still work with stereoscopic applications, even in windowed mode. To achieve maximum practicability, it is desirable to support all of the widely used stereoscopic image formats both on input and output side, ultimately allowing for arbitrary format conversions. This will ensure maximal flexibility in working with unmodified software applications, improving the usability of multi-projector display systems in general.</p><p>By combining several active-stereoscopic projectors with passive filter systems (Fröhlich et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fröhlich B, Hoffmann J, Klueger K, Hochstrate J (2004) Implementing multi-viewer time-sequential stereo displays based on shuttered lcd projectors. &#xA;                    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.488.3164&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0352-5#ref-CR7" id="ref-link-section-d37710e474">2004</a>), or by applying hardware modifications to the projectors (Fröhlich et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Fröhlich B, Hochstrate J, Hoffmann J, Klüger K, Blach R, Bues M, Stefani O (2005) Implementing multi-viewer stereo displays. In: WSCG’2005" href="/article/10.1007/s10055-018-0352-5#ref-CR8" id="ref-link-section-d37710e477">2005</a>), <i>multiview</i>-capable displays can be built, which allow more than just two views (i.e., using two stereoscopic image pairs for two viewers simultaneously). Besides working with stereoscopic content, we also intended to keep our framework generic with respect to such multiview setups by generalizing the existing stereoscopic image formats to incorporate <i>n</i> views instead of just two.</p><h3 class="c-article__sub-heading" id="Sec3">General overview and terminology</h3><p>The term <i>view</i> shall refer to the image content which can be separately viewed in the context of the display system. For example, in a stereoscopic setup, two views (left and right) are separated to one view per eye by technical means like shutter or polarization glasses. We use the terms <i>monoscopic</i> to refer to formats which encode exactly one view, <i>stereoscopic</i> for two views, and <i>multiview</i> for formats with more than two views.</p><p>The term <i>image</i> will be used to represent a standard two-dimensional raster image consisting of pixels with one to four color channels. Images which can be stored in typical image file formats, transmitted via typical display interfaces, and represented on a GPU with typical textures formats. An image might encode one ore several views, or multiple images might be used to represent multiple views.</p><p>The input of our algorithm consists of a set of images representing one or more input views which are intended to be displayed. The output is a set of images representing one or more output views, where the image data is adjusted according to some calibration method with respect to some projector, so that when displayed by that projector, the resulting output most closely resembles the corresponding input views.</p><p>In multi-projector setups, our algorithm must be applied separately for each projector. In a tiled display, each projector is responsible for a specific subregion of the overall display area; therefore, the output of the algorithm will only contain the image data for that particular region. Furthermore, a projector might contribute only a subset of the views (e.g., with polarization filters, separate projectors may be used for each view of a stereoscopic pair), so the output of a single instance may also contain only a subset of the output views.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Related work</h2><div class="c-article-section__content" id="Sec4-content"><p>In this section, we give a brief overview of different calibration approaches which were proposed in the past. Discussing the details of the actual calibration process of the various methods is beyond the scope of this work; we instead focus on the general approaches which have been proposed as far as the post-processing is concerned, to derive requirements for the abstract interfaces of our framework, so that the required post-processing steps of the existing methods (and potential future improvements) can easily be implemented within it.</p><h3 class="c-article__sub-heading" id="Sec5">Geometric calibration</h3><p>In the most basic <i>geometric calibration</i>, an ideally flat display surface as well as an ideal projector without geometric distortion is assumed and the projected image represents a trapezoidal subregion of the display area. This can be accounted for by a projective transform matrix which in some cases can directly be incorporated into the rendering process of a typical graphical application (Raskar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Raskar R (2000) Immersive planar display using roughly aligned projectors. In: Proceedings of the IEEE virtual reality 2000 conference. IEEE Computer Society, Washington, DC, USA, VR ’00, p 109" href="/article/10.1007/s10055-018-0352-5#ref-CR19" id="ref-link-section-d37710e538">2000</a>; Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Chen H, Sukthankar R, Wallace G, Li K (2002) Scalable alignment of large-format multi-projector displays using camera homography trees. In: Proceedings of the conference on Visualization ’02, IEEE Computer Society, Washington, DC, USA, VIS ’02, pp 339–346" href="/article/10.1007/s10055-018-0352-5#ref-CR5" id="ref-link-section-d37710e541">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Chen M, Fan B, Song H (2014) Geometry calibration for multi-projector display automatically based on the feedback of camera algorithm. In: 2014 11th international conference on fuzzy systems and knowledge discovery (FSKD), pp 570–574. &#xA;                    https://doi.org/10.1109/FSKD.2014.6980897&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0352-5#ref-CR6" id="ref-link-section-d37710e544">2014</a>). However, in most cases, the geometric distortions of projectors can not be neglected. Sadlo et al. argued that projectors can be treated as “inverse cameras” (Sadlo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sadlo F, Weyrich T, Peikert R, Gross M (2005) A practical structured light acquisition system for point-based geometry and texture. In: Proceedings of the eurographics symposium of point-based graphics" href="/article/10.1007/s10055-018-0352-5#ref-CR23" id="ref-link-section-d37710e547">2005</a>; Martynov et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Martynov I, Kamarainen JK, Lensu L (2011) Projector calibration by “inverse camera calibration”. In: Proceedings of the 17th scandinavian conference on image analysis, SCIA’11. Springer, Berlin, pp 536–544" href="/article/10.1007/s10055-018-0352-5#ref-CR15" id="ref-link-section-d37710e551">2011</a>), so that analytical models for optical distortions of cameras like Brown (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Brown DC (1971) Close-range camera calibration. Photogramm Eng 37(8):855–866" href="/article/10.1007/s10055-018-0352-5#ref-CR4" id="ref-link-section-d37710e554">1971</a>), Zhang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334" href="/article/10.1007/s10055-018-0352-5#ref-CR31" id="ref-link-section-d37710e557">2000</a>) can be applied (Okatani and Deguchi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Okatani T, Deguchi K (2009) Easy calibration of a multi-projector display system. Int J Comput Vis 85(1):1–18. &#xA;                    https://doi.org/10.1007/s11263-009-0242-0&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0352-5#ref-CR17" id="ref-link-section-d37710e560">2009</a>).</p><p>A more popular approach is to project structured light patterns onto the screen and capturing the output by cameras. A geometric registration between features in the projector’s image space and the camera space allows for a piece-wise linear approximation of the optical distortions introduced by the lenses as well as non-planar display surfaces (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Raskar R, Welch G, Fuchs H (1998) Seamless projection overlaps using image warping and intensity blending. In: Fourth international conference on virtual systems and multimedia" href="/article/10.1007/s10055-018-0352-5#ref-CR20" id="ref-link-section-d37710e566">1998</a>; Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yang R, Gotz D, Hensley J, Towles H, Brown MS (2001) Pixelflex: a reconfigurable multi-projector display system. In: Proceedings of the conference on visualization ’01. VIS ’01. IEEE Computer Society, Washington, pp 167–174" href="/article/10.1007/s10055-018-0352-5#ref-CR30" id="ref-link-section-d37710e569">2001</a>; Raij et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Raij A, Gill G, Majumder A, Towles H, Fuchs H (2003) Pixelflex2: a comprehensive, automatic, casually-aligned multi-projector display. In. In Proceedings of IEEE international workshop on projector-camera systems" href="/article/10.1007/s10055-018-0352-5#ref-CR18" id="ref-link-section-d37710e572">2003</a>; Surati <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Surati RJ (1999) Scalable self-calibrating display technology for seamless large-scale displays. PhD thesis, Massachusetts Institute of Technology" href="/article/10.1007/s10055-018-0352-5#ref-CR28" id="ref-link-section-d37710e575">1999</a>; Jordan and Greenspan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Jordan S, Greenspan M (2010) Projector optical distortion calibration using gray code patterns. In: 2010 IEEE computer society conference on computer vision and pattern recognition workshops (CVPRW), pp 72 –79" href="/article/10.1007/s10055-018-0352-5#ref-CR12" id="ref-link-section-d37710e578">2010</a>; Gaur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gaur PK, Sarode DM, Shete PP, Venkata PK, Bose SK (2014) Achieving seamlessness in multi-projector based tiled display using camera feedback. In: 2014 international conference on contemporary computing and informatics (IC3I). IEEE, pp 293–298" href="/article/10.1007/s10055-018-0352-5#ref-CR9" id="ref-link-section-d37710e582">2014</a>). In the most general case, arbitrary non-planar display surfaces are used, and a three-dimensional representation of the display surface as well as the viewer’s actual or assumed position is needed to generate an undistorted image (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization ’99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS ’99, pp 161–168" href="/article/10.1007/s10055-018-0352-5#ref-CR21" id="ref-link-section-d37710e585">1999</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2005) ilamps: geometrically aware and self-configuring projectors. In: ACM SIGGRAPH 2005 courses. SIGGRAPH ’05. ACM, New York" href="/article/10.1007/s10055-018-0352-5#ref-CR22" id="ref-link-section-d37710e588">2005</a>; Bimber et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Emmerling A, Klemmer T (2005) Embedded entertainment with smart projectors. Computer 38(1):48–55" href="/article/10.1007/s10055-018-0352-5#ref-CR2" id="ref-link-section-d37710e591">2005</a>). Usually, the geometric registration is represented by triangle meshes, employing standard texture mapping techniques to implement the image warping. In the case of 3D meshes for non-planar surfaces, the viewer’s position can easily be incorporated into the rendering.</p><h3 class="c-article__sub-heading" id="Sec6">Photometric calibration and color transfer</h3><p>The luminance of areas where images of multiple projectors overlap is corrected using <i>edge blending</i> techniques where a smooth fade-out toward the projector’s borders is applied so that the total luminance stays roughly constant across the overlap areas and a smooth transition between the segments is attained (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Raskar R, Welch G, Fuchs H (1998) Seamless projection overlaps using image warping and intensity blending. In: Fourth international conference on virtual systems and multimedia" href="/article/10.1007/s10055-018-0352-5#ref-CR20" id="ref-link-section-d37710e605">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization ’99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS ’99, pp 161–168" href="/article/10.1007/s10055-018-0352-5#ref-CR21" id="ref-link-section-d37710e608">1999</a>; Bourke and Calati <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bourke P, Calati R (2004) Edge blending using commodity projectors. Swinburne University, Technical report" href="/article/10.1007/s10055-018-0352-5#ref-CR3" id="ref-link-section-d37710e611">2004</a>; Gaur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gaur PK, Sarode DM, Shete PP, Venkata PK, Bose SK (2014) Achieving seamlessness in multi-projector based tiled display using camera feedback. In: 2014 international conference on contemporary computing and informatics (IC3I). IEEE, pp 293–298" href="/article/10.1007/s10055-018-0352-5#ref-CR9" id="ref-link-section-d37710e614">2014</a>). To achieve <i>photometric uniformity</i>, inter- and intra-projector variations of luminance and color reproductions have to be taken into account. Concentrating solely on luminance, the maximum brightness of each pixel of the display can be captured by a camera to calculate per-pixel attenuation factors (Majumder and Stevens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Majumder A, Stevens R (2002) LAM: luminance attenuation map for photometric uniformity in projection based displays. In: Proceedings of the ACM symposium on Virtual reality software and technology, ACM, New York, NY, USA, VRST ’02, pp 147–154" href="/article/10.1007/s10055-018-0352-5#ref-CR13" id="ref-link-section-d37710e621">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Majumder A, Stevens R (2005) Perceptual photometric seamlessness in projection-based tiled displays. ACM Trans Graph 24(1):118–139" href="/article/10.1007/s10055-018-0352-5#ref-CR14" id="ref-link-section-d37710e624">2005</a>). This technique can be extended to preserve a common white balance across the whole display (Sajadi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Sajadi B, Lazarov M, Gopi M, Majumder A (2009) Color seamlessness in multi-projector displays using constrained gamut morphing. IEEE Trans Visual Comput Graph 15(6):1317–1326" href="/article/10.1007/s10055-018-0352-5#ref-CR24" id="ref-link-section-d37710e627">2009</a>).</p><p>It must be noted that for any kind of photometric corrections, the projectors’ <i>color transfer functions</i> must be known or assumed. This function describes the mapping from the input color values sent to the projector (typically in a RGB color space) to the actual output created by the projector (typically measured in a device-independent color space like CIE XYZ). Using the inverse of that function, the required projector input value to achieve a desired output value can be determined. Due to the way projectors (and other display devices) are built, the color transfer function can usually be decomposed into <i>n</i> color primaries (implemented via color filters or the light sources itself) and a black offset representing light which is always emitted by the device. A <i>intensity transfer function</i> describes the mapping of each input color vector to the <i>n</i>-dimensional vector of the relative intensities of each color primary in the normalized range [0, 1]:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\mathbf{c}}({\mathbf{x}}) = M \cdot {\mathbf{t}}({\mathbf{x}}) + {\mathbf{k}}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>Here, <span class="mathjax-tex">\({\mathbf {x}} \in {\mathbb {R}}^m\)</span> denotes the input color vector in the <i>m</i>-dimensional input color space, <i>M</i> denotes the <span class="mathjax-tex">\(o \times n\)</span> matrix where each column represents the measured output color for the <i>n</i> color primaries in the <i>o</i>-dimensional measurement color space, <span class="mathjax-tex">\(k \in {\mathbb {R}}^o\)</span> denotes the black offset, and <span class="mathjax-tex">\({\mathbf {t}}: {\mathbb {R}}^m \rightarrow {\mathbb {R}}^n\)</span> the intensity transfer function. In a standard RGB model (<span class="mathjax-tex">\(m=n=o=3\)</span>), <i>channel independency</i> can be assumed, so that the intensity of each output channel depends solely on the value from one input channel (and not the combination of <i>all</i> input channels). The multi-dimensional function <span class="mathjax-tex">\({\mathbf {t}}\)</span> can then be modeled by <i>n</i> independent scalar functions <i>t</i>(<i>x</i>) (Stone <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Stone MC (2001) Color and brightness appearance issues in tiled displays. IEEE Comput Graph Appl 21(5):58–66" href="/article/10.1007/s10055-018-0352-5#ref-CR27" id="ref-link-section-d37710e936">2001</a>).</p><p>In early works, deviations in the color transfer functions of the projectors were ignored. However, as the input color values are mapped to the actual physical luminances in a nonlinear way, the intensity transfer function can never be completely ignored. Raskar et al. approximate this as <span class="mathjax-tex">\(t(x)=x^\gamma\)</span> and show that with this model, the inverse gamma value can be directly applied to the edge blending masks (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization ’99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS ’99, pp 161–168" href="/article/10.1007/s10055-018-0352-5#ref-CR21" id="ref-link-section-d37710e984">1999</a>), avoiding any runtime costs for the model. However, the intensity transfer function of many projectors cannot be adequately represented with that simple model, so that more elaborate models have been proposed (Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yang R, Gotz D, Hensley J, Towles H, Brown MS (2001) Pixelflex: a reconfigurable multi-projector display system. In: Proceedings of the conference on visualization ’01. VIS ’01. IEEE Computer Society, Washington, pp 167–174" href="/article/10.1007/s10055-018-0352-5#ref-CR30" id="ref-link-section-d37710e987">2001</a>; Majumder and Stevens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Majumder A, Stevens R (2002) LAM: luminance attenuation map for photometric uniformity in projection based displays. In: Proceedings of the ACM symposium on Virtual reality software and technology, ACM, New York, NY, USA, VRST ’02, pp 147–154" href="/article/10.1007/s10055-018-0352-5#ref-CR13" id="ref-link-section-d37710e990">2002</a>; Raij et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Raij A, Gill G, Majumder A, Towles H, Fuchs H (2003) Pixelflex2: a comprehensive, automatic, casually-aligned multi-projector display. In. In Proceedings of IEEE international workshop on projector-camera systems" href="/article/10.1007/s10055-018-0352-5#ref-CR18" id="ref-link-section-d37710e993">2003</a>). Besides measuring the <i>t</i> functions with colorimeters or cameras and storing them in 1D LUTs, analytical models have also been proposed (Jaynes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of the conference on Visualization ’01, IEEE Computer Society, Washington, DC, USA, VIS ’01, pp 175–182" href="/article/10.1007/s10055-018-0352-5#ref-CR11" id="ref-link-section-d37710e1000">2001</a>). However, the RGB model is not well-suited for all types of devices. DLP projectors often work with color wheels with 4 (RGBW) or 6 (RGBCYW) segments. In such scenarios, the channel independency assumption does not hold true any more, and <span class="mathjax-tex">\({\mathbf {c}}({\mathbf {x}})\)</span> might be assumed as a generic vector function mapping <span class="mathjax-tex">\({\mathbb {R}}^n \rightarrow {\mathbb {R}}^o\)</span> (Wallace et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wallace G, Chen H, Li K (2003) Color gamut matching for tiled display walls. In: Proceedings of the workshop on virtual environments 2003, EGVE ’03. ACM, New York, pp 293–302" href="/article/10.1007/s10055-018-0352-5#ref-CR29" id="ref-link-section-d37710e1080">2003</a>; Sajadi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Sajadi B, Lazarov M, Majumder A (2010) ADICT: accurate direct and inverse color transformation. In: Proceedings of the 11th European conference on computer vision: part IV. ECCV’10. Springer, Berlin, pp 72–86" href="/article/10.1007/s10055-018-0352-5#ref-CR25" id="ref-link-section-d37710e1083">2010</a>; Heinz and Brunnett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Heinz M, Brunnett G (2015) Dense sampling of 3D color transfer functions using HDR photography. In: 2015 IEEE conference on computer vision and pattern recognition workshops (CVPRW), Boston, MA, USA, pp 25–32. &#xA;                    https://doi.org/10.1109/CVPRW.2015.7301372&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0352-5#ref-CR10" id="ref-link-section-d37710e1087">2015</a>). Analytical models as well as 3D lookup tables might be used to represent the function and its inverse.</p><p>Knowledge of the complete color transfer function is also a requirement for achieving color uniformity and color calibration. A common gamut for the whole display must be determined, usually the least common denominator between the gamuts of all projectors. If channel independency can be assumed, inter-projector color variances can be addressed by affine transformation matrices directly in the 3D RGB color space (Stone <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Stone MC (2001) Color and brightness appearance issues in tiled displays. IEEE Comput Graph Appl 21(5):58–66" href="/article/10.1007/s10055-018-0352-5#ref-CR27" id="ref-link-section-d37710e1093">2001</a>). Otherwise, a more generic approach involves transforming the image content into a device-independent color space (taking the common gamut into account), applying any color and brightness adjustments, and finally using the inverse color transfer function of each projector to generate the actual color values which must be sent to the projectors.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Algorithm</h2><div class="c-article-section__content" id="Sec7-content"><p>We do not treat the conversion of stereoscopic image formats as separated from the image post-processing, but suggest to work directly on the encoded image data. To allow for a wide range of different calibration methods and stereoscopic image formats, we propose an <i>abstract model</i> which defines the minimal requirements we impose onto the methods.</p><p>We assume there is a global coordinate space, where all the display surfaces are known. The desired output is a two-dimensional image (per view). To allow for the placement of an input image at an arbitrary location on the display surface, the affine or projective transformation matrix <span class="mathjax-tex">\(T_{\mathtt {in}}\)</span> is used, mapping from the input (pixel) space of the image to the global display space.</p><p>A geometric calibration can then be described as function <i>g</i> mapping each pixel at location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> in the projector input space to the location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> in the input image space as</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {x}}_{\mathtt {in}} = g({\mathbf {x}}_{\mathtt {prj}}, T_{\mathtt {in}}, \alpha _{\mathtt {g}}, {\mathbf {v}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(\alpha _{\mathtt {g}}\)</span> denotes the calibration parameters of the specific method (which describe the geometric registration between each projector pixel and the global coordinate space) and <span class="mathjax-tex">\({\mathbf {v}}\)</span> represents the optionally relevant viewing position (with respect to the global coordinate space).</p><p>We describe the algorithm for the case of a single projector. If this projector represents only a single tile of a tiled display, the geometric calibration will have the effect of selecting the appropriate sub-area of the input image automatically. This allows for using the same image as input for different instances of the algorithm, each for a specific projector, so that the input images are conceptually independent of the actual setup of the multi-segmented display. For optimal image quality, the effective resolution of the display should be taken into account when generating the image, though.</p><p>For photometric calibration, we propose splitting the function into two parts as follows:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \beta _{{\mathbf {x}}_{\mathtt {prj}}} &amp;= p({\mathbf {x}}_{\mathtt {prj}},\alpha _{\mathtt {p}},{\mathbf {v}})\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {c}}_{\mathtt {prj}} &amp;= q({\mathbf {c}}_{\mathtt {in}},\alpha _{\mathtt {p}},\beta _{{\mathbf {x}}_{\mathtt {prj}}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p><span class="mathjax-tex">\(\alpha _{\mathtt {p}}\)</span> denotes the parameters specific to the photometric calibration method (which might include the model of the color transfer function, masks for edge blending, data for luminance correction and so on), <span class="mathjax-tex">\({\mathbf {c}}_{\mathtt {in}}\)</span> the desired output color at <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> and <span class="mathjax-tex">\({\mathbf {c}}_{\mathtt {prj}}\)</span> the final output color which must be sent to the projector to achieve the desired result. The function <i>p</i> does not depend on the colors at all and allows us to remove redundant calculations between different views.</p><p>In the following, we assume that the input for the algorithm at each point in time consists of a set of images, where each image may represent either the image content for a specific view directly, or might encode two or more views. Let <span class="mathjax-tex">\(A=\{a_0, \ldots , a_{m_{\mathtt {d}}-1}\}\)</span> be the set of such input images. We restrict <span class="mathjax-tex">\(A\)</span> so that each image <span class="mathjax-tex">\(a_i\)</span> has to be in the same format. Thus, the total number of input views is <span class="mathjax-tex">\(n_d \ge m_d\)</span>. The set of output images <span class="mathjax-tex">\(B=\{b_0, \ldots , b_{m_{\mathtt {e}}-1}\}\)</span> is defined analogously, providing a total of <span class="mathjax-tex">\(n_e \ge m_e\)</span> output views. The format of <span class="mathjax-tex">\(A\)</span> and B can be chosen independently of each other. We use the index <i>e</i> for the <i>encoded</i> input images, meaning the raw input in the stereoscopic format, and <i>d</i> for the <i>decoded</i> images. In this context, the decoded image for a view is represented by a function which maps each input pixel location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> to a color vector that the decoded image would have at that particular input location. Note that the decoded image is never actually stored, the mapping is established by accessing the relevant pixels of the encoded image data directly. We define the function</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {c}}_{{\mathtt {d}},i} = d({\mathbf {x}}_{\mathtt {in}}, i, A, \alpha _{\mathtt {d}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>which allows for sampling the decoded input image at the input image location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> for the input view <i>i</i>. The parameter <span class="mathjax-tex">\(\alpha _{\mathtt {d}}\)</span> describes all parameters specific to the format chosen for <i>A</i>. The function’s purpose is to select the appropriate image in the set <span class="mathjax-tex">\(A\)</span>, calculate the encoded pixel coordinates, and to sample the encoded image data gathering the decoded color vector <span class="mathjax-tex">\({\mathbf {c}}_{{\mathtt {d}},i}\)</span>. As the decoded image locations are not restricted to pixel centers, interpolation of the image data of neighboring pixels might be desired. Such filtering operations are part of the decoding method and specific to the format, since neighboring pixels in the decoded view do not need to be neighbors in the encoded data.</p><p>The mapping between input view index <i>i</i> and output view index <i>j</i> is specified using a function <i>h</i> such that <span class="mathjax-tex">\(i=h(j, \alpha _{\mathtt {d}})\)</span>. In the most general case, the <i>h</i> does not have to be injective nor surjective—there is no need to enforce a one-to-one mapping of views. As trivial cases, displaying monoscopic content on a stereoscopic display can be achieved by duplicating input view 0 to all output views. On the other hand, a single view of a multiview set can be shown on a monoscopic display or can be replicated to all output views a multiview display.</p><p>When writing directly to an encoded image, the pixel location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> might represent some arbitrary location in the decoded projector space (notably, if output formats like side-by-side or top-bottom are used). The encoding method thus must specify such a mapping. Furthermore, the output encoding <i>controls</i> the decoding step in the sense that it defines the set of relevant input views for any output pixel:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {x}}_{\mathtt {prj}} &amp;= f({\mathbf {x}}_{\mathtt {e}}, \alpha _{\mathtt {e}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {c}}_{{\mathtt {in}},k} &amp; = e({\mathbf {x}}_{\mathtt {e}}, {\mathbf {x}}_{\mathtt {in}}, k, \alpha _{\mathtt {e}}, A, \alpha _\mathtt {d}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>The purpose of <i>e</i> is to determine the <i>input</i> color for decoded input location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> when writing to encoded output pixel location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> of output image <span class="mathjax-tex">\(b_k\)</span>. This requires determining the set of relevant output views <span class="mathjax-tex">\(J\)</span>, finding the corresponding input views <span class="mathjax-tex">\(I\)</span> (using the function <i>h</i>), determining the color values of the input views with function <i>d</i> and finally combining them to the single color value <span class="mathjax-tex">\({\mathbf {c}}_{{\mathtt {in}},k}\)</span>. In the typical case, only one input view is relevant for a single <i>encoded</i> output pixel, so that <span class="mathjax-tex">\(I=\{i\}\)</span> and <span class="mathjax-tex">\(J=\{j\}\)</span>. An exception is anaglyph output, where two views have to be encoded in different color channels of each pixel, so that using actual sets is still appropriate in the generic model.</p><h3 class="c-article__sub-heading" id="Sec8">Stereoscopic and multiview formats</h3><p>We categorize the stereoscopic and multiview image formats by the operations which are necessary for encoding/decoding them and identified the following three main classes of formats:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Each view is described by an ordinary monoscopic image in full resolution (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig1">1</a>a). The number of images <i>m</i> in the set directly equals the number of available views <i>n</i>. Monoscopic content is a trivial case of this category.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p><i>n</i> views are encoded into a single image (<span class="mathjax-tex">\(m=1\)</span>) Each pixel represents data from one view.</p><ol class="u-list-style-none">
                            <li>
                              <span class="u-custom-list-number">a.</span>
                              
                                <p>Each view is encoded as a rectangular sub-area inside the image (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig1">1</a>b, c). This includes the popular side-by-side and top-bottom formats, and can easily be extended for more than two views (either by adding more views along one dimension, or by aligning them in a two-dimensional grid).</p>
                              
                            </li>
                            <li>
                              <span class="u-custom-list-number">b.</span>
                              
                                <p>Neighboring pixels in the decoded view are not neighbors in the encoded image (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig1">1</a>d–f). Examples are line- or column-wise interleaved images or checkerboard formats. These formats can also be extended for an arbitrary number of views.</p>
                              
                            </li>
                          </ol>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">III.</span>
                      
                        <p>Each image pixel describes an encoded color value which represents data for more than one view (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig1">1</a>g), like anaglyph images or combining two RGB565 views into a 32 bpp image. Due to the limited practical relevance of category III formats, we restrained from reproducing the detailed description of the model functions in the following sections, but we nonetheless implemented and evaluated these formats in our framework.</p>
                      
                    </li>
                  </ol><p>Besides these three, further categories and combinations thereof could be defined. Due to the abstract definition of the model function, new categories and formats can be easily added without changing the overall structure of the algorithm.</p><p>The common frame-sequential format (where the views are transmitted after each other in a temporally multiplexed fashion, usually with doubled frame rate) is just a special case of category I as far as this work is concerned. If it is used on the input side, it can either be treated like monoscopic content (and the post-processing being run on each view separately), nor all views can be collected first, and the post-processing run on all of them at once (allowing the algorithm to remove the redundant calculations, but also adding another frame of latency if the output format is also frame sequential).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Decoding</h4><p>When dealing with content of category I, the decode function can be implemented as</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} d^{\mathtt {(I)}}({\mathbf {x}}_{\mathtt {in}}, i, A, \alpha _{\mathtt {d}})={\mathtt {sample}}(a_i, {\mathbf {x}}_{\mathtt {in}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <span class="mathjax-tex">\({\mathtt {sample}}\)</span> represents sampling the image with the desired image filter.</p><p>For content of category IIa, the pixel location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> must be mapped to the location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {d}}\)</span> in the encoded image depending on the selected view <i>i</i>. The sampling can still be done as before, as neighboring pixels in the decoded space are also neighbors in the encoded space:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {x}}_{\mathtt {d}}&amp; = d^{\mathtt {(IIa)}}_{\mathtt {coord}}({\mathbf {x}}_{\mathtt {in}}, i, \alpha _{\mathtt {d}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} d^{\mathtt {(IIa)}}({\mathbf {x}}_{\mathtt {in}}, i, A, \alpha _{\mathtt {d}}) &amp;= {\mathtt {sample}}(a_0, {\mathbf {x}}_{\mathtt {d}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>When using image filtering to sample data of the category IIa, care must be taken to prevent accessing neighboring views when processing border pixels. In case of the common bilinear filter, this can be incorporated into the <span class="mathjax-tex">\(d^{\mathtt {(IIa)}}_{\mathtt {coord}}\)</span> function so that <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {d}}\)</span> is clamped to the centers of the border pixels of the sub-image of the respective view.</p><p>In category IIb, standard image filtering capabilities cannot be used directly. If only point sampling is desired, the data can in principle be handled like in the IIa case. The <span class="mathjax-tex">\(d^{\mathtt {(IIa)}}_{\mathtt {coord}}\)</span> has to be specified so that the nearest pixel representing view <i>i</i> has to be selected. If a higher quality filtering is desired, the filtering must be adapted to the properties of the format. However, the regular structure of the content makes it possible to define simple invariants for finding neighboring pixels of the same view once one pixel of a view has been identified. For example, in vertically interleaved images, the horizontal neighborhood is unchanged, while in the vertical direction, the distance is <span class="mathjax-tex">\(n_{\mathtt {d}}\)</span> pixels. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig2">2</a> illustrates the decoding for checkerboard formats.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="488" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Two views in a checkerboard pattern can be interpreted as images with a pixel raster rotated by 45°, and scaled by a factor of <span class="mathjax-tex">\(\sqrt{2}\)</span> along each dimension. Bilinear filtering must be manually implemented with respect to that raster. However, it will not suffice to clamp the sample coordinates to the centers of the border pixels in the <i>unrotated</i> view (dotted rectangle), but selecting each of the four base texels for the filtering requires an additional clamp step</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Encoding</h4><p>For content of category I, no modifications of the pixel locations are needed, so that <span class="mathjax-tex">\(f^{\mathtt {(I)}}({\mathbf {x}}_{\mathtt {e}}, \alpha _{\mathtt {e}})={\mathbf {x}}_{\mathtt {e}}\)</span> can be used. To determine the input color relevant for the output pixel, the decode function can be directly used, resorting to the view mapping function <i>h</i> considering the fact that the output image index corresponds to the output view index <i>j</i>:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} e^{\mathtt {(I)}}({\mathbf {x}}_{\mathtt {e}}, {\mathbf {x}}_{\mathtt {in}}, k, \alpha _{\mathtt {e}}, A, \alpha _{\mathtt {d}}) = d({\mathbf {x}}_{\mathtt {in}}, h(k, \alpha _{\mathtt {d}}), A, \alpha _{\mathtt {d}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>In the case of content of category II, the view index <i>j</i> depends on the pixel location in encoded output space, and <i>e</i> can thus be modeled as</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} e^{\mathtt {(II)}}({\mathbf {x}}_{\mathtt {e}}, {\mathbf {x}}_{\mathtt {in}}, k, \alpha _{\mathtt {e}}, A, \alpha _{\mathtt {d}}) = d({\mathbf {x}}_{\mathtt {in}},h( e^{\mathtt {(II)}}_{\mathtt {view}}({\mathbf {x}}_{\mathtt {e}}, \alpha _{\mathtt {e}}), \alpha _{\mathtt {d}}),\, A, \alpha _{\mathtt {d}}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>The functions <span class="mathjax-tex">\(e^{\mathtt {(II)}}_{\mathtt {view}}\)</span> and <i>f</i> have to be chosen to match the specific format, <span class="mathjax-tex">\(e^{\mathtt {(II)}}_{\mathtt {view}}\)</span> can typically be modeled using some modulo operations on <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> for interleaved formats (category IIb) and some divisions and rounding operations for category IIa formats like side-by-side and top-bottom. In these cases, <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> inside one specific sub-image in the encoded space must be scaled and translated so that the sub-image is completely filling the decoded image space.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Figa_HTML.png?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Figa_HTML.png" alt="figurea" loading="lazy" width="685" height="464" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div><h3 class="c-article__sub-heading" id="Sec11">Combined algorithm</h3><p>The algorithm for combined stereoscopic format conversion and image post-processing for calibration of multi-projector display systems is outlined in Algorithm 1, and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig3">3</a> shows a schematic overview for it.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="312" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Schematic of the combined algorithm. Black arrows denote coordinates, red dotted arrows denote color data and green dashed arrows denote color-independent color calibration intermediate results. Geometry processing and color-independent parts of color processing are applied only once per output pixel <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span>, while color processing is done in the inner loop per output image <i>k</i>. For improved clarity, the method-specific parameters for each model function have been omitted in this schematic view (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The algorithm works with two nested loops. The outer loop is iterating over all output pixel locations <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span>. Since pixels are not points in the mathematical sense but represent some area, we use the pixel centers. Note that the number of output pixel locations is independent of the number of output images specified by the stereoscopic format—the algorithm will visit each output pixel location just once, and the inner loop does iterate over all output images at this location to generate the pixel color values for each separate image.</p><p>In the first step, the output pixel position <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> is transformed to the decoded projector output space position <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> for which the geometric calibration data is present. This transformation is done with the <i>f</i> model function specific to the output format. This mapping is required for class IIa formats where a view is represented by a sub-rectangle of the output image (like side-by-side or top-bottom). The projector firmware will eventually scale each view to fit to the full image size, shifting the effective pixel location which is required for the image warping. (We assume that the geometric calibration parameters were obtained when the projector was in monoscopic mode to make use of the full native resolution. The geometric calibration might as well be gathered directly in side-by-side or top-bottom mode, so that an identity mapping could be assumed for this step.)</p><p>After the actual projector image space position <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> is determined, the geometric calibration model function <i>g</i> is called to obtain the normalized input pixel location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span>. Furthermore, all photometric calculations which do depend on the pixel location, but are independent of the actual color values to be displayed at that location, are carried out only once in the outer loop. This is done by the model function <i>p</i>, and the results are stored in the method-specific parameter <span class="mathjax-tex">\(\beta _{{\mathbf {x}}_{\mathtt {prj}}}\)</span>. Typical operations for this stage are determining values for edge blending, luminance control, white balance and local black offset.</p><p>After the coordinate transformations and color pre-calculations are carried out, the algorithm enters the inner loop which is responsible for the actual color processing. The inner loop iterates over all <span class="mathjax-tex">\(m_{\mathtt {e}}\)</span> output images defined by the selected output format. For each output image <i>k</i>, a color value is determined according to the model of the combined encoding and decoding method for stereoscopic formats, as described in the previous section.</p><p>The function <i>e</i> will internally determine the set of output views which are relevant for pixel position <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> in output image <i>k</i>, use the view map function <i>h</i> to determine the relevant input views, and will use the decode model function <i>d</i> to sample the corresponding input images from set <span class="mathjax-tex">\(A\)</span> at input position <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span>. The resulting color value <span class="mathjax-tex">\({\mathbf {c}}_{\mathtt {in}}\)</span> represents the actual image data which is desired to be displayed at the selected input location. These color values are used as input to the final part of the photometric calibration, the model function <i>q</i> which is responsible for determining the actual output values which will result in the desired output of the projector at this particular location. This output color is finally written to the co output image <span class="mathjax-tex">\(b_k\)</span> at location <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span>.</p><p>Note that the photometric calibration is applied after the encoding of the output image. In the stereoscopic formats considered so far, this is only relevant for anaglyph outputs, where the anaglyph images could be calibrated to match the used filters as close as possible. Some other stereoscopic and multiview formats could take advantage of the <i>sub-pixel</i> structure of the displays, using different color channels for different views. In these cases, the stereoscopic encoding would have to be applied after the photometric calibration. However, since projectors typically do not work with sub-pixels, but either use separate panels/DMD chips per color channel or work in a time-multiplexing manner by projecting the different color channels sequentially using the same panel or DMD chip, we refrained from adding specific support for sub-pixel view multiplexing here. If such output formats are really desired, the model can be easily extended to add another per-format encoding step which is applied after the photometric calibration.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">GPU-based implementation</h2><div class="c-article-section__content" id="Sec12-content"><p>The <i>Calibrator Image Processor</i> (<i>CIP</i>) constitutes the central component of our framework, where a CIP context encapsulates a single instance of the combined algorithm for a specific projector. For our prototype implementation, we used the OpenGL API, but the general concepts are valid for other rendering APIs as well. The CIP module assumes that an OpenGL context already exists (i.e., from the application which is rendering the image content in the first place, and which is to be modified to render to a texture for post-processing with our framework) and will create additional GL objects (mainly textures and buffers) in this context.</p><p>The general architecture of the CIP module is outlined in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig4">4</a>a. The algorithm from Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec7">3</a> is split into the 5 processing stages <span class="u-monospace">DECODE</span> (format decoding function <i>d</i>), <span class="u-monospace">ENCODE</span> (format encoding functions <i>e</i> and <i>f</i>), <span class="u-monospace">MAP</span> (view map function <i>h</i>), <span class="u-monospace">WARP</span> (geometric calibration function <i>g</i>) and <span class="u-monospace">COLOR</span> (color calibration functions <i>p</i> and <i>q</i>). We use a plugin-based design, so for each stage, a specific implementation can be selected.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="792" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>In the CIP module, the algorithm is divided into 5 stages which are implemented as exchangeable plugins. Each stage provides parts of vertex and fragment shader codes. The draw calls are invoked by the <span class="u-monospace">WARP</span> plugin, which also defines the geometric primitives which are rendered. Each plugin might register additional texture objects, which are globally managed by the texture manager. The input image set <span class="mathjax-tex">\(A\)</span> is represented by texture objects, the output image set <span class="mathjax-tex">\(B\)</span> by the frame buffer. The selected plugins and their configuration options can be changed at runtime. <b>a</b> Architecture overview. <b>b</b> Conceptual data flow in a configuration with two output views</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The <i>Render Manager</i> provides the general framework for the plugins and defines the interfaces between them. For each stage, we specified a set of GLSL functions representing the model functions from the algorithm, and each plugin must provide GLSL source code to implement them. The Render Manager combines the shader code parts of all selected plugins with some base shaders which implement the combined algorithm from <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec11">3.2</a> to create the final vertex and fragment shaders and GL program objects. Since each plugin may need a set of own textures to store some parameters (i.e., luminance and blend masks, color LUTs), the Render Manager also consist of a <i>Texture Manager</i>, where all plugins have to register the textures they are going to use. The Texture Manager will then assign the actual textures to the GL texture units, and establish the according texture bindings when rendering. Furthermore, the Render Manager stores some state: the name of the <i>input texture</i>, the number of render passes (<i>PassCnt</i>) and the number of draw buffers (<i>BufCnt</i>).</p><p>The configuration of the CIP module is controlled by the <i>Config Block</i>. For each plugin, a <i>Mode</i> string stores the name of the selected plugin, and an <i>Opts</i> string contains plugin-specific configuration options (list of key-value-pairs). Furthermore, the input and output resolution and view counts are stored, as well as the view map. The CIP module does provide <span class="u-monospace">Set*</span> methods to update the configuration at runtime. Furthermore, we also use a <span class="u-monospace">UpdateParam</span> method which provides a well-defined set of parameters which might be changed at runtime (like the name of the input texture), but plugins can also define their own parameters.</p><p>To do the actual post-processing, a draw call must be issued, and some input primitives for the render pipeline have to be provided. Most parts of the combined algorithm are implemented in the fragment shader. The outer loop of the algorithm is replaced by the rasterizer of the GPU, and each fragment shader invocation represents one iteration of the outer loop.</p><p>Not all parts of the algorithm are implemented in the fragment shader, though. Since a 2D or 3D mesh might be directly used for representing a geometric warping, the vertex data and the vertex shader might be used to implement the geometric distortions. Therefore, we decided that the <span class="u-monospace">WARP</span> plugin is responsible for managing the input mesh. If no vertex-based warping is needed, the plugin may just provide a full-screen quad as geometry. Since the draw call must match the vertex input (number and type of primitives, indexing or not, VAOs and VBOs), The <span class="u-monospace">WARP</span> plugin itself also provides the actual GL draw call. The Render Manager will rely on that whenever a post-processing pass is to be applied.</p><p>For formats of category IIa, the same geometric distortion has to be applied to each view, but the views will be located in distinct regions of the output image. This can be implemented in a single pass by replicating the geometric warping for each output region, or by a multi-pass approach, where each pass renders to one output region. To be able to implement both strategies, the <span class="u-monospace">ENCODE</span> plugin controls the number of passes and registers them with the Render Manager, which will compile a separate shader for each pass, where the pass index is injected as a source code constant.</p><p>Furthermore, the <span class="u-monospace">ENCODE</span> plugin also controls the number of render targets to be used simultaneously (i.e., writing to the left and right buffer of a quad-buffered framebuffer in one pass, or to more than one layer of an array texture). This number of draw buffers resembles the output image count <span class="mathjax-tex">\(m_e\)</span> of the abstract model and is also injected into the shader sources by the Render Manager.</p><h3 class="c-article__sub-heading" id="Sec13">General data flow</h3><p>The general data flow is outlined in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig4">4</a>b. The functions have to be implemented by the shader code parts provided by the respective plugins. The general interface and the global skeleton implementing the algorithm is provided by the CIP module and resembles the combined algorithm from Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec11">3.2</a>.</p><p>The set of input images is represented by the texture object <i>A</i> (an array texture if more than one image is required). The combined <span class="u-monospace">encode_decode</span> method must be provided by the <span class="u-monospace">ENCODE</span> stage and will internally use the <span class="u-monospace">decode</span> method of the <span class="u-monospace">DECODE</span> plugin (which actually samples the input texture) and the <span class="u-monospace">map</span> function supplied by the <span class="u-monospace">MAP</span> plugin. This will yield the color vector <span class="mathjax-tex">\({\mathbf {c}}_{in}\)</span> which is input to the <span class="u-monospace">color</span> functions provided by the <span class="u-monospace">COLOR</span> plugin. The resulting color value is written as the fragment shader output to the output image <span class="mathjax-tex">\(b_k\)</span>.</p><p>The <span class="u-monospace">encode_decode</span> and <span class="u-monospace">color</span> sequence represents the inner loop of our algorithm, it is repeated for all <span class="mathjax-tex">\(m_e\)</span> output image indices in <i>B</i> (which is just the framebuffer in a multiple render target configuration, if multiple output images are required). As the number of output views is known at shader compile time, the inner loop of the algorithm can be completely unrolled. This again enables further optimization with respect to the encode and decode methods.</p><p>The outer loop of the algorithm is represented by the fragment shader itself. The <span class="u-monospace">WARP</span> stage must calculate the coordinates <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> and <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {in}}\)</span> according to the geometric calibration, which provide the input coordinates for <span class="u-monospace">encode_decode</span> as well as <span class="u-monospace">params</span>, the position-dependent but color-independent part of the <span class="u-monospace">COLOR</span> calibration (only executed once per fragment, with the results stored in the plugin-specific <span class="mathjax-tex">\(\beta\)</span> variables). The geometry warping function <i>g</i> of the model is split across vertex and fragment shader as the functions <span class="u-monospace">vs_warp</span> and <span class="u-monospace">fs_warp</span>. The plugin can add its own varyings to transmit data from the vertex shader to the fragment shader, so that an arbitrary combination of per-vertex and per-fragment warping can be implemented.</p><p>In deviation from the abstract model, the function <i>f</i> is not used in the GPU-based implementation, but is replaced by</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {x}}_{\mathtt {e}} = \tilde{f}({\mathbf {x}}_{\mathtt {prj}},\, j,\, \alpha _{\mathtt {e}}). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>In a way, <span class="mathjax-tex">\(\tilde{f}\)</span> represents the reversion of <i>f</i>, but not the inverse in the strict mathematical sense: <i>f</i> maps a set of different <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {e}}\)</span> coordinates to the same projector space location, while <span class="mathjax-tex">\(\tilde{f}\)</span> maps <span class="mathjax-tex">\({\mathbf {x}}_{\mathtt {prj}}\)</span> to a specific encoded image location for a <i>known view index</i>. This function is implemented as <span class="u-monospace">encode_vertex</span> function of the <span class="u-monospace">ENCODE</span> plugin and is usually just the identity mapping excepts for formats of category IIa, where it is used to map the geometry to the sub-rectangle of the output image for the corresponding view.</p><h3 class="c-article__sub-heading" id="Sec14">Stereoscopic de-/encoding</h3><p>We implemented plugins for 8 distinct stereoscopic formats to support a wide range of image content: monoscopic (<span class="mathjax-tex">\({\mathtt {mono}}\)</span>), separate images per view (<span class="mathjax-tex">\({\mathtt {multiTA}}\)</span> and <span class="mathjax-tex">\({\mathtt {multiT3D}}\)</span> using an array texture or 3D texture with one view per layer), side-by-side/top-bottom (<span class="mathjax-tex">\({\mathtt {sbs}}\)</span>/<span class="mathjax-tex">\({\mathtt {tb}}\)</span>), horizontally or vertically interleaved views (<span class="mathjax-tex">\({\mathtt {ivh}}\)</span>/<span class="mathjax-tex">\({\mathtt {ivv}}\)</span>), checkerboard (<span class="mathjax-tex">\({\mathtt {chkb}}\)</span>) and two anaglyph variants (<span class="mathjax-tex">\({\mathtt {ag}}\)</span> / <span class="mathjax-tex">\({\mathtt {agbw}}\)</span>).</p><p>When sampling the input texture, one can take advantage of the GPU’s hardware capabilities for bilinear filtering. Special care needs to be taken for formats of class IIb. For horizontal and vertical interleaving, the bilinear texture filter can be used to filter along one axis, restricting the coordinate along the other axis to a texel center to avoid filtering between texels of different views. To get a fully filtered result, two texture samples have to be fetched and manually mixed in the shader. For formats like checkerboard patterns, the filtering capabilities of the GPU cannot be used at all. To re-implement it in the shader itself, it is advisable to directly use the <span class="u-monospace">texelFetch</span> operation to bypass any filtering the GPU might otherwise do. For the IIb formats with manual filtering, we did implement two variants of the plugins to support both nearest and bilinear filtering. We use suffix <span class="mathjax-tex">\({\mathtt {n}}\)</span> and <span class="mathjax-tex">\({\mathtt {b}}\)</span> to denote these variants in the plugin name.</p><p>When producing category IIa output, either a single render pass can be used, amplifying the geometry for each view, or a multi-pass approach may be chosen. Usually, single-pass rendering algorithms are superior regarding the overall performance. However, we argue that in our special case, the multi-pass strategy is more efficient. In the single-pass variant, the view index <i>j</i> depends on the instance and varies between shader invocations. In the multi-pass variant, we can use the view index as <i>compile time constant</i>, using a separate shader for each pass, allowing further compile time optimizations. This is especially relevant if <i>input</i> formats of class II or III are used.</p><p>To implement the view mapping function <i>h</i>, a uniform array can be used. However, changes to the view map typically occur not very often, so that it can be injected as a constant array into the shader source (triggering a shader recompilation every time the view map is updated). Depending on the selected image formats, this enables further optimizations on the encode and decode equations. The flexibility provided by using an array is not always necessary. We identified the following mappings as especially important in practice: <span class="mathjax-tex">\({\mathtt {direct}}\)</span>: <span class="mathjax-tex">\(h(j, \alpha _{\mathtt {d}})=j\)</span>, <span class="mathjax-tex">\({\mathtt {inverse}}\)</span>: <span class="mathjax-tex">\(h(j, \alpha _{\mathtt {d}})=n_{\mathtt {d}}-j\)</span> and <span class="mathjax-tex">\({\mathtt {const}}\)</span>: <span class="mathjax-tex">\(h(j, \alpha _{\mathtt {d}})=l\)</span>. In a stereoscopic setup, these cover both switching the views between eyes and selecting a view for monoscopic output.</p><h3 class="c-article__sub-heading" id="Sec15">Post-processing for calibration</h3><p>To implement the image warping, we identified three different general approaches. The first one is using an analytical model, the distortion can directly be calculated in a fragment shader. Second, the distortion can be described by a geometric representation. Using two sets of coordinates, a piece-wise linear approximation of the two-dimensional mapping between distorted and undistorted projector space can be established. Furthermore, a 3D representation of the shape of the projection surface can be used. The third method involves using a 2D lookup table, effectively storing a distortion map in a texture. To allow the implementation of these different image warping methods in the context of our framework, the <span class="u-monospace">WARP</span> plugin is responsible for managing a geometric mesh (using a full-screen quad if no geometric representation is used).</p><p>We implemented the <span class="mathjax-tex">\({\mathtt {mesh}}\)</span> (warping based on a tessellated 2D grid) and <span class="mathjax-tex">\({\mathtt {texrel}}\)</span> (using a texture with a per-pixel displacement vector) and <span class="mathjax-tex">\({\mathtt {texabs}}\)</span> (which uses a texture to directly look up the warped coordinates in a 16 bit per channel texture) plugins.</p><p>To apply photometric calibration, the <i>linearization</i> of the color space is required. Once in linear space, further adjustments such as changing the brightness to compensate for inter- and intra-projector variations and to implement soft-edge blending can be made. The input color values are to be interpreted in some method-specific color space. To achieve photometric uniformity, that color space is constrained by the common gamut of all segments forming the multi-projector display. The linearization and de-linearization can either be analytically modeled or stored as 1D or 3D lookup textures. 2D textures can be used for per-pixel parameters like attenuation or blend factors, black levels or white balance.</p><p>For the <span class="u-monospace">COLOR</span> stage, we tested 3 different variants. The plugin <span class="mathjax-tex">\({\mathtt {raskar99}}\)</span> implements the model from Raskar et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization ’99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS ’99, pp 161–168" href="/article/10.1007/s10055-018-0352-5#ref-CR21" id="ref-link-section-d37710e5705">1999</a>). This assumes an ideal RGB model, ignores black offset and directly applies the inverse gamma value to the correction mask. That approach is only suitable for luminance correction, and does not address color modifications at all. To implement the latter, we first transform the input color into some linear, device-independent color space, apply the luminance correction, and finally use the inverse of the projector’s color transfer function to find the output value which should be sent to the projector. We implemented the plugins <span class="mathjax-tex">\({\mathtt {2LUT1D}}\)</span> and <span class="mathjax-tex">\({\mathtt {2LUT3D}}\)</span> for this. Both variants use <span class="mathjax-tex">\(4\times 4\)</span> matrices to transform from input RGB space to XYZ and from XYZ to the RGB basis of the projector and LUT textures are used to represent the nonlinear <i>intensity</i> transfer. To ensure a suitable distribution of the sample points, we apply a gamma correction to the fetched value in the forward transform (so that we can work with 8 bit per channel) or to the texture <i>coordinates</i> in the inverse case (so that resolution of the texture is reduced while still providing the highest sample density near the black point). To put it simply, the textures are used to represent the <i>deviations</i> to an ideal RGB model with some chosen gamma value. In the <span class="mathjax-tex">\({\mathtt {2LUT1D}}\)</span> case, the LUTs are represented as a single-channel texture array with 3 layers. The plugin <span class="mathjax-tex">\({\mathtt {2LUT3D}}\)</span> does not presume the channel independency of the classical RGB model and uses 3D textures to model the 3D vector functions.</p><h3 class="c-article__sub-heading" id="Sec16">Acquiring applications’ output</h3><p>The CIP module needs access to the graphical output of the applications which are to be displayed on the multi-projector display system. The framework can be directly integrated into the application or the engine or rendering library the application is based on (the levels 1 and 2 we discussed in the introduction).</p><p>We also implemented level 3 by employing AMD’s DOPP technology, which represents the windows Aero desktop as a single texture, and allows to define another texture as the output used for presentation. While this allows to work with completely unmodified applications, it also has the drawback that those textures are completely monoscopic. However, our framework’s and most projectors’ abilities to use spatially multiplexed input formats offer a way to circumvent that restriction. As far as the operation system is concerned, only monoscopic content must be handled, and we can configure the CIP module to interpret the desktop texture in a stereoscopic format. Formats of class IIb are particularly suitable input formats for that use case due to the locality of pixel data between the views. For example, a checkerboard input can be used so that a single window can contain stereoscopic content (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig5">5</a>), whereas in a side-by-side format, two separate windows with exact placement (with respect to the side-by-side partitioning of the whole desktop) would have to be used. (Moving the window in the checkerboard case by an odd number of pixels will result in the stereo content being flipped, though.) On the output side, side-by-side or top-bottom appears to be the best choice, since it is supported by most projectors.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="596" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Post-processing of the Windows desktop using AMD DOPP on a powerwall with <span class="mathjax-tex">\(3 \times 2\)</span> FullHD segments. The desktop texture is only monoscopic, but our framework allows to interpret it as two spatially multiplexed views. <b>a</b> The input texture with a single window containing stereoscopic content encoded in a checkerboard pattern (enlarged in the circle). For illustration purposes, two completely different test images have been used as the left (red background) and right (blue background) view. <b>b</b> The calibrated output for our setup where the projectors are driven in side-by-side stereoscopic mode (projector IDs and markers for left/right view were added for illustration purposes). The projectors will internally scale the views back to original width and present both views using their native stereo separation technology (in our case, frame sequential for use with shutter glasses) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Note that even though DOPP does not support stereoscopy directly, we can still employ the quad-buffer stereo mode of the GPU in this setup. AMD’s driver supports quad-buffer output modes like “Auto-Stereo Horizontal Interleaved,” in which the multiplexed views are actually written to the desktop texture, so that they can be accessed and post-processed by the CIP DOPP module. This means that we don’t require the applications to support multiplexing different views on their own, but can use all quad-buffered applications natively.</p><p>For implementing a dedicated pixel-processing device (level 4), we use Datapath VisionRGB grabber boards which are available as DVI and DisplayPort variants. To achieve optimal throughput and minimal latency, we make use of AMD’s DirectGMA technology (Stefanizzi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Stefanizzi B (2014) DirectGMA on AMD’s FirePro GPUs. Advanced Micro Devices Inc, Technical report" href="/article/10.1007/s10055-018-0352-5#ref-CR26" id="ref-link-section-d37710e5942">2014</a>) to directly transfer the image data between the grabbers and the GPU via PCI-Express bus-mastered DMA transfers (similar mechanisms also exist for NVidia GPUs). Using a separate device is conceptually very similar to the desktop capture case, but it is more flexible: Outputs of different systems can be combined, mixed, or switched. Stereoscopic content can be transmitted as separate signals (which can be generated by the passive stereo output mode of workstation GPUs), or as spatially or temporally multiplexed signals. There is no special decoding step required for active stereo signals, two consecutive frames must just be collected and forwarded as a stereoscopic image pair to the CIP module. Conceptually, the CIP framework is also able to process combined spatial and temporal multiplexed that way (which might be a worthwhile format for multiview setups).</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Experimental results</h2><div class="c-article-section__content" id="Sec17-content"><p>We used a PC with an Intel Xeon E5 1620v2 (<span class="mathjax-tex">\(4 \times 3,\;7\;{\text{GHz}}\)</span>), 32 GB RAM, Windows 7 Pro SP1, 64 Bit and an AMD FirePRO W9000 GPU with 6 GB VRAM to drive a planar powerwall based on <span class="mathjax-tex">\(3\times 2\)</span> Epson EH-TW8100 FullHD LCD projectors.</p><p>In the context of a VR application, one important metric is the overall latency between detection an action by the input devices/sensors and the actual display of the reaction of the system. The post-processing considered in this work introduces additional latencies. In scenarios where the output is synchronized to the display refresh cycle (classical “VSync” setup), there is a fixed time budget for processing a frame, and the post-processing will consume a part of that budget. In non-VSync scenarios, the post-processing time may add more directly to the overall latency. In both scenarios, it is desirable to reduce the post-processing time as much as possible, because this will reduce the additional latency and leave a bigger share of the GPU processing power to the actual rendering. Therefore, we did focus on minimizing the absolute post-processing times, and also use the post-processing times as the metric for evaluation. All measurements were taken directly on the GPU, employing OpenGL timer query objects.</p><p>With all variants described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec14">4.2</a>, a total of 13 <span class="u-monospace">DECODE</span> and 9 <span class="u-monospace">ENCODE</span> plugins are available. Furthermore, we tested with the <span class="mathjax-tex">\({\mathtt {mesh}}\)</span> and <span class="mathjax-tex">\({\mathtt {texrel}}\)</span><span class="u-monospace">WARP</span> plugins introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec15">4.3</a> (we also tested <span class="mathjax-tex">\({\mathtt {texabs}}\)</span>, but the performance was nearly identical to the <span class="mathjax-tex">\({\mathtt {texrel}}\)</span> plugin, so we did not include these measurements here). As basis for the image warping, we used a triangle mesh on a <span class="mathjax-tex">\(64 \times 36\)</span> point grid. We used the same data as basis for the texture-based warping by interpolating the displacement vectors for every pixel of the device, so that both variants represent the same distortion. We tested all 3 <span class="u-monospace">COLOR</span> plugins introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0352-5#Sec15">4.3</a>. In the <span class="mathjax-tex">\({\mathtt {2LUT1D}}\)</span> case, the LUTs are represented as a single-channel 1D texture array with 3 layers with 256 values for the forward transform and 1024 for the inverse. For <span class="mathjax-tex">\({\mathtt {2LUT3D}}\)</span>, the LUTs were obtained using a DLSR camera following the method described in Heinz and Brunnett (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Heinz M, Brunnett G (2015) Dense sampling of 3D color transfer functions using HDR photography. In: 2015 IEEE conference on computer vision and pattern recognition workshops (CVPRW), Boston, MA, USA, pp 25–32. &#xA;                    https://doi.org/10.1109/CVPRW.2015.7301372&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0352-5#ref-CR10" id="ref-link-section-d37710e6215">2015</a>). We used a <span class="mathjax-tex">\(65^3\)</span> texture with linear filtering in the forward case and <span class="mathjax-tex">\(128^3\)</span> with linear filtering for the inverse transform. In all cases, a 2D texture with a per-pixel luminance attenuation factor was used for luminance control and blending.</p><h3 class="c-article__sub-heading" id="Sec18">Single segment</h3><p>We compared our algorithm with the naive approach (decoding, applying the post-processing to each view separately, re-encoding to final format). However, we always assumed a reasonable implementation of the naive approach, so that the decoding or encoding passes were omitted when the input or output area defined by the respective formats already consists of a contiguous rectangular area (formats of class IIa), so that a view can be directly sampled from or rendered into.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig6">6</a> shows the post-processing times we achieved in our setup, and Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0352-5#Tab1">1</a> lists the absolute processing times for a selected subset of the possible combinations of the plugins. The choice of the <span class="u-monospace">COLOR</span> plugin exerts the highest influence on the overall processing times. With <span class="mathjax-tex">\({\mathtt {raskar99}}\)</span>, the baseline for processing a monoscopic image is 100 μs, while using the more flexible <span class="mathjax-tex">\({\mathtt {2LUT1D}}\)</span> plugin results in a twofold increase. Using 3D LUTs is substantially more expensive. As the texels fetched depend on the colors in the input image, the image content significantly influences the cache efficiency of the operation. We chose a stereoscopic pair of rather noisy photographs to yield realistic measurements of the performance. In such a scenario, the post-processing took 480 μs per projector in the monoscopic case and went up to 836 μs when processing two stereoscopic views.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Processing times [μs] of the CIP module for various combinations of plugins applied to one or two (<span class="mathjax-tex">\({\mathtt {multiTA}}\)</span>) FullHD images</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0352-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="275" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Absolute processing times of the CIP module for all implemented format conversions and 3 different <span class="u-monospace">COLOR</span> plugins (in all cases, a <span class="mathjax-tex">\(64 \times 36\)</span> mesh was used for the image warping) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Due to our optimized single-pass approach, the influence of the format conversion is quite low. There is a noticeable cost for manually bilinear filtering in the shader, especially in the <span class="mathjax-tex">\({\mathtt {chkb}}\)</span> case, where 4 separate texel fetches with individual coordinate clamping (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig2">2</a>) per fetch. It must be noted that when using the <span class="mathjax-tex">\({\mathtt {multi}}\)</span> output, the effective number of produced pixels is twice as high as in all other cases, which naturally results in higher processing times.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0352-5#Fig7">7</a> shows the relative <i>speedup</i> over the naive implementation achieved by our proposed algorithm for all combinations of input/output format conversions, <span class="u-monospace">COLOR</span> plugins, in conjunction with <span class="mathjax-tex">\({\mathtt {mesh}}\)</span> warping. (The figures were very similar for the texture-based warping, so we restrained from reproducing them in this paper.) We found that our algorithm is significantly faster in most cases, as fast as naive approach in some cases, and only slower in one specific corner case: duplicating a single input view to both views of a class IIb output format. We implemented the naive approach for this case so that the post-processing to the input image is applied only once, and the result is simply copied to the second half of the output image. The combined algorithm performs worse, because the input data duplication conceptually happens before the post-processing step, so that the post-processing is done twice. However, even if the algorithm is suboptimal in that case, our framework can easily be reconfigured to emulate the optimal approach by chaining two CIP modules, the first one to apply the post-processing of the input image to an image with half the resolution, and a second one to duplicate the monoscopic image to all views of the output format, avoiding to apply the expensive post-processing twice.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0352-5/MediaObjects/10055_2018_352_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="289" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Relative speedup factors of the combined algorithm compared to the naive implementation (separate passes for decoding, calibration, encoding) for mesh based warping and 3 different color calibration methods. The combined approach is superior in most cases, even when separate images are used as input and output formats (<span class="mathjax-tex">\({\mathtt {multiTA}}\)</span><span class="mathjax-tex">\(\rightarrow\)</span><span class="mathjax-tex">\({\mathtt {multi}}\)</span>) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0352-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As expected, the highest improvements of our algorithm can be achieved when using formats of class IIb, either on the input or the output side (or both). The combined algorithm performs best for checkerboard output, achieving speedup factors from 1.5 to 4.5. This format actually has some relevance in practice, as it represents the native format for 3D DLP projectors which work with the “wobulation” technology to sequentially draw two half-images with diagonally shifted pixel locations (Allen and Ulichney <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Allen W, Ulichney R (2005) Wobulation: doubling the addressed resolution of projection display. In: Proceedings of SID symposium digest of technical papers (SID), Boston, MA, USA, pp 1514–1517" href="/article/10.1007/s10055-018-0352-5#ref-CR1" id="ref-link-section-d37710e7866">2005</a>).</p><p>In practice, the <span class="mathjax-tex">\({\mathtt {multiTA}}\rightarrow {\mathtt {multi}}\)</span> format combination appears as the most relevant one, as this covers using the full resolution per view as well as the native quad-buffered mode of workstation GPUs. One could assume that the performance improvement of the combined algorithm is close to zero as the complexity of the color post-processing increases, since the actual color calibration steps have to be carried out for each view separately in any case. We found that in such situations, the combined algorithm still performs better, as it exploits the color coherency of stereoscopic image pairs. The image data at a specific location is sampled for both views sequentially, so that the texture cache for the LUTs is not only used for neighboring locations, but also for the other views. As result, we see an improvement of about 20% (<span class="mathjax-tex">\({\mathtt {2LUT1D}}\)</span>) or 15% (<span class="mathjax-tex">\({\mathtt {2LUT3D}}\)</span>) in this important use case. With the processing times approaching 1ms <i>per segment</i>, these improvements become actually relevant when driving multiple segments with just one GPU.</p><h3 class="c-article__sub-heading" id="Sec19">Whole powerwall</h3><p>To drive our powerwall, we use a separate CIP module per projector, and sequentially apply the post-processing for each projector to shared input images of a resolution of 5760 by 2160 pixels. We focused on two use cases for measuring the performance in the full wall setup: First, we tested using two separate views in full resolution, both on input and output side. This appears as the most relevant use case, and also the one with the highest amount of data to be processed.</p><p>Second, we created a setup which does not require any stereoscopy-aware components at all, besides the projector itself. As output format, we chose side-by-side, since this is supported by our (and most other) stereoscopic projectors. We did benchmark this setup with the bilinearly filtered horizontal interleave <span class="mathjax-tex">\({\mathtt {ivhb}}\)</span> input format (suitable for DOPP post-processing of quad-buffered applications with “Auto-Stereo (Horizontal Interleaved)” output mode), as well as bilinearly filtered checkerboard pattern as the input format (which more evenly distributes the loss of resolution both horizontally and vertically).</p><p>Our results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0352-5#Tab2">2</a>. The processing times roughly scaled linearly with the number of segments used, so the single-segment measurements from the previous section were confirmed. When using no format conversions at all, our algorithm is still able to reduce overall post-processing time by 15%, translating into a benefit of about 0.5–1 ms per frame. In combination with the format conversion, we see improvements between 20 and 100%, again translating into saving roughly 0.5–1 ms of post-processing time per frame—time which is better spent on generating the actual image content.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Processing times [ms] achieved by our combined algorithm (column “cmb.”) compared to the naive multi-pass approach (column “na.”) when driving our 6 segment powerwall</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0352-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Summary and conclusions</h2><div class="c-article-section__content" id="Sec20-content"><p>We propose a novel algorithm to apply calibration algorithms for multi-projector display systems directly to input data which contains one or several views in one or several images, and which directly creates output data in such formats, and where the input and output formats might differ. Furthermore, we suggest splitting the algorithm for photometric calibrations into two parts, so that position-dependent but color-independent calculations are separated. Our algorithm guarantees that for each pixel location in the output image space, the image warping operations and the position-dependent but color-independent calculations of the photometric calibration are applied only once. Moreover, for each output pixel, only the input pixels potentially affecting the output color have to be fetched. For all but class IIa output formats, the algorithm reduces the number of input pixels which are decoded, or avoids redundant calculations for the image warping and parts of the photometric calibration—or both. We have shown that our proposed algorithm can reduce the post-processing times significantly, in some cases by two thirds. The flexibility of the algorithm and our frameworks allows implementing 1-, 2- or 3-pass strategies with arbitrary combinations of the stages. When dealing with <span class="mathjax-tex">\({\mathtt {mono}}\)</span> to class IIa conversions, optimal performance can still be achieved by a 2-pass strategy, so that there is no conversion case where our framework is slower than the naive implementation.</p><p>For integrating the post-processing into existing VR and graphics applications, providing each view as a separate image turned out to be the most useful approach. This also maps directly to the quad-buffered stereoscopic rendering modes supported by OpenGL, so that such applications can easily be modified to write the output into a texture array with two layers instead. We have shown that our algorithm constitutes an 15–20% improvement even in that use case, due to the improved cache efficiency when processing stereoscopic image pairs. We were able to reduce the post-processing time for a 6 segment powerwall by 1ms, so that more GPU processing time can be spent on rendering the actual image content.</p><p>When using a dedicated PC as pixel-processing device and working with unmodified applications, the various input formats proved very useful. For example, a laptop with a single display connector can be used for stereoscopic presentations. If the application output or image/video content is available in interleaved formats, an arbitrary subregion of the total image transmitted can be used for stereoscopic content, like a single window on a desktop. Such formats are also useful for working with AMD’s DOPP technology, which provides an easy, non-invasive way to apply post-processing operations to the OS desktop and all applications running on it. Our combined one-pass algorithm was able to cut processing times in half in some scenarios and usually reduces them by at least a third compared to the naive multi-pass implementation.</p><p>In practical use, we found that the ability to process different formats, and to easily switch between them on-line, greatly simplifies the handling of powerwalls, especially when working with different software applications. Since the shaders for the conversions are built on demand, there is also no runtime overhead in situations where they are not needed.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Allen W, Ulichney R (2005) Wobulation: doubling the addressed resolution of projection display. In: Proceeding" /><p class="c-article-references__text" id="ref-CR1">Allen W, Ulichney R (2005) Wobulation: doubling the addressed resolution of projection display. In: Proceedings of SID symposium digest of technical papers (SID), Boston, MA, USA, pp 1514–1517</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Bimber, A. Emmerling, T. Klemmer, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bimber O, Emmerling A, Klemmer T (2005) Embedded entertainment with smart projectors. Computer 38(1):48–55" /><p class="c-article-references__text" id="ref-CR2">Bimber O, Emmerling A, Klemmer T (2005) Embedded entertainment with smart projectors. Computer 38(1):48–55</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2005.17" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Embedded%20entertainment%20with%20smart%20projectors&amp;journal=Computer&amp;volume=38&amp;issue=1&amp;pages=48-55&amp;publication_year=2005&amp;author=Bimber%2CO&amp;author=Emmerling%2CA&amp;author=Klemmer%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bourke P, Calati R (2004) Edge blending using commodity projectors. Swinburne University, Technical report" /><p class="c-article-references__text" id="ref-CR3">Bourke P, Calati R (2004) Edge blending using commodity projectors. Swinburne University, Technical report</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DC. Brown, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="Brown DC (1971) Close-range camera calibration. Photogramm Eng 37(8):855–866" /><p class="c-article-references__text" id="ref-CR4">Brown DC (1971) Close-range camera calibration. Photogramm Eng 37(8):855–866</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Close-range%20camera%20calibration&amp;journal=Photogramm%20Eng&amp;volume=37&amp;issue=8&amp;pages=855-866&amp;publication_year=1971&amp;author=Brown%2CDC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen H, Sukthankar R, Wallace G, Li K (2002) Scalable alignment of large-format multi-projector displays using" /><p class="c-article-references__text" id="ref-CR5">Chen H, Sukthankar R, Wallace G, Li K (2002) Scalable alignment of large-format multi-projector displays using camera homography trees. In: Proceedings of the conference on Visualization ’02, IEEE Computer Society, Washington, DC, USA, VIS ’02, pp 339–346</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen M, Fan B, Song H (2014) Geometry calibration for multi-projector display automatically based on the feedb" /><p class="c-article-references__text" id="ref-CR6">Chen M, Fan B, Song H (2014) Geometry calibration for multi-projector display automatically based on the feedback of camera algorithm. In: 2014 11th international conference on fuzzy systems and knowledge discovery (FSKD), pp 570–574. <a href="https://doi.org/10.1109/FSKD.2014.6980897">https://doi.org/10.1109/FSKD.2014.6980897</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fröhlich B, Hoffmann J, Klueger K, Hochstrate J (2004) Implementing multi-viewer time-sequential stereo displa" /><p class="c-article-references__text" id="ref-CR7">Fröhlich B, Hoffmann J, Klueger K, Hochstrate J (2004) Implementing multi-viewer time-sequential stereo displays based on shuttered lcd projectors. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.488.3164">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.488.3164</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fröhlich B, Hochstrate J, Hoffmann J, Klüger K, Blach R, Bues M, Stefani O (2005) Implementing multi-viewer st" /><p class="c-article-references__text" id="ref-CR8">Fröhlich B, Hochstrate J, Hoffmann J, Klüger K, Blach R, Bues M, Stefani O (2005) Implementing multi-viewer stereo displays. In: WSCG’2005</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gaur PK, Sarode DM, Shete PP, Venkata PK, Bose SK (2014) Achieving seamlessness in multi-projector based tiled" /><p class="c-article-references__text" id="ref-CR9">Gaur PK, Sarode DM, Shete PP, Venkata PK, Bose SK (2014) Achieving seamlessness in multi-projector based tiled display using camera feedback. In: 2014 international conference on contemporary computing and informatics (IC3I). IEEE, pp 293–298</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heinz M, Brunnett G (2015) Dense sampling of 3D color transfer functions using HDR photography. In: 2015 IEEE " /><p class="c-article-references__text" id="ref-CR10">Heinz M, Brunnett G (2015) Dense sampling of 3D color transfer functions using HDR photography. In: 2015 IEEE conference on computer vision and pattern recognition workshops (CVPRW), Boston, MA, USA, pp 25–32. <a href="https://doi.org/10.1109/CVPRW.2015.7301372">https://doi.org/10.1109/CVPRW.2015.7301372</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. " /><p class="c-article-references__text" id="ref-CR11">Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of the conference on Visualization ’01, IEEE Computer Society, Washington, DC, USA, VIS ’01, pp 175–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jordan S, Greenspan M (2010) Projector optical distortion calibration using gray code patterns. In: 2010 IEEE " /><p class="c-article-references__text" id="ref-CR12">Jordan S, Greenspan M (2010) Projector optical distortion calibration using gray code patterns. In: 2010 IEEE computer society conference on computer vision and pattern recognition workshops (CVPRW), pp 72 –79</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Majumder A, Stevens R (2002) LAM: luminance attenuation map for photometric uniformity in projection based dis" /><p class="c-article-references__text" id="ref-CR13">Majumder A, Stevens R (2002) LAM: luminance attenuation map for photometric uniformity in projection based displays. In: Proceedings of the ACM symposium on Virtual reality software and technology, ACM, New York, NY, USA, VRST ’02, pp 147–154</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Majumder, R. Stevens, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Majumder A, Stevens R (2005) Perceptual photometric seamlessness in projection-based tiled displays. ACM Trans" /><p class="c-article-references__text" id="ref-CR14">Majumder A, Stevens R (2005) Perceptual photometric seamlessness in projection-based tiled displays. ACM Trans Graph 24(1):118–139</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1037957.1037964" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20photometric%20seamlessness%20in%20projection-based%20tiled%20displays&amp;journal=ACM%20Trans%20Graph&amp;volume=24&amp;issue=1&amp;pages=118-139&amp;publication_year=2005&amp;author=Majumder%2CA&amp;author=Stevens%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martynov I, Kamarainen JK, Lensu L (2011) Projector calibration by “inverse camera calibration”. In: Proceedin" /><p class="c-article-references__text" id="ref-CR15">Martynov I, Kamarainen JK, Lensu L (2011) Projector calibration by “inverse camera calibration”. In: Proceedings of the 17th scandinavian conference on image analysis, SCIA’11. Springer, Berlin, pp 536–544</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Meyer C (2013) AMD FirePro display output post-processing. Advanced Micro Devices Inc, Technical report" /><p class="c-article-references__text" id="ref-CR16">Meyer C (2013) AMD FirePro display output post-processing. Advanced Micro Devices Inc, Technical report</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Okatani, K. Deguchi, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Okatani T, Deguchi K (2009) Easy calibration of a multi-projector display system. Int J Comput Vis 85(1):1–18." /><p class="c-article-references__text" id="ref-CR17">Okatani T, Deguchi K (2009) Easy calibration of a multi-projector display system. Int J Comput Vis 85(1):1–18. <a href="https://doi.org/10.1007/s11263-009-0242-0">https://doi.org/10.1007/s11263-009-0242-0</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-009-0242-0" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Easy%20calibration%20of%20a%20multi-projector%20display%20system&amp;journal=Int%20J%20Comput%20Vis&amp;doi=10.1007%2Fs11263-009-0242-0&amp;volume=85&amp;issue=1&amp;pages=1-18&amp;publication_year=2009&amp;author=Okatani%2CT&amp;author=Deguchi%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raij A, Gill G, Majumder A, Towles H, Fuchs H (2003) Pixelflex2: a comprehensive, automatic, casually-aligned " /><p class="c-article-references__text" id="ref-CR18">Raij A, Gill G, Majumder A, Towles H, Fuchs H (2003) Pixelflex2: a comprehensive, automatic, casually-aligned multi-projector display. In. In Proceedings of IEEE international workshop on projector-camera systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R (2000) Immersive planar display using roughly aligned projectors. In: Proceedings of the IEEE virtual" /><p class="c-article-references__text" id="ref-CR19">Raskar R (2000) Immersive planar display using roughly aligned projectors. In: Proceedings of the IEEE virtual reality 2000 conference. IEEE Computer Society, Washington, DC, USA, VR ’00, p 109</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Welch G, Fuchs H (1998) Seamless projection overlaps using image warping and intensity blending. In:" /><p class="c-article-references__text" id="ref-CR20">Raskar R, Welch G, Fuchs H (1998) Seamless projection overlaps using image warping and intensity blending. In: Fourth international conference on virtual systems and multimedia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays usin" /><p class="c-article-references__text" id="ref-CR21">Raskar R, Brown MS, Yang R, Chen WC, Welch G, Towles H, Seales B, Fuchs H (1999) Multi-projector displays using camera-based registration. In: Proceedings of the conference on Visualization ’99: celebrating ten years, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS ’99, pp 161–168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2005) ilamps: geometrically aware and self" /><p class="c-article-references__text" id="ref-CR22">Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2005) ilamps: geometrically aware and self-configuring projectors. In: ACM SIGGRAPH 2005 courses. SIGGRAPH ’05. ACM, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sadlo F, Weyrich T, Peikert R, Gross M (2005) A practical structured light acquisition system for point-based " /><p class="c-article-references__text" id="ref-CR23">Sadlo F, Weyrich T, Peikert R, Gross M (2005) A practical structured light acquisition system for point-based geometry and texture. In: Proceedings of the eurographics symposium of point-based graphics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Sajadi, M. Lazarov, M. Gopi, A. Majumder, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Sajadi B, Lazarov M, Gopi M, Majumder A (2009) Color seamlessness in multi-projector displays using constraine" /><p class="c-article-references__text" id="ref-CR24">Sajadi B, Lazarov M, Gopi M, Majumder A (2009) Color seamlessness in multi-projector displays using constrained gamut morphing. IEEE Trans Visual Comput Graph 15(6):1317–1326</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2009.124" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color%20seamlessness%20in%20multi-projector%20displays%20using%20constrained%20gamut%20morphing&amp;journal=IEEE%20Trans%20Visual%20Comput%20Graph&amp;volume=15&amp;issue=6&amp;pages=1317-1326&amp;publication_year=2009&amp;author=Sajadi%2CB&amp;author=Lazarov%2CM&amp;author=Gopi%2CM&amp;author=Majumder%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sajadi B, Lazarov M, Majumder A (2010) ADICT: accurate direct and inverse color transformation. In: Proceeding" /><p class="c-article-references__text" id="ref-CR25">Sajadi B, Lazarov M, Majumder A (2010) ADICT: accurate direct and inverse color transformation. In: Proceedings of the 11th European conference on computer vision: part IV. ECCV’10. Springer, Berlin, pp 72–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stefanizzi B (2014) DirectGMA on AMD’s FirePro GPUs. Advanced Micro Devices Inc, Technical report" /><p class="c-article-references__text" id="ref-CR26">Stefanizzi B (2014) DirectGMA on AMD’s FirePro GPUs. Advanced Micro Devices Inc, Technical report</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MC. Stone, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Stone MC (2001) Color and brightness appearance issues in tiled displays. IEEE Comput Graph Appl 21(5):58–66" /><p class="c-article-references__text" id="ref-CR27">Stone MC (2001) Color and brightness appearance issues in tiled displays. IEEE Comput Graph Appl 21(5):58–66</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.946632" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color%20and%20brightness%20appearance%20issues%20in%20tiled%20displays&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=5&amp;pages=58-66&amp;publication_year=2001&amp;author=Stone%2CMC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Surati RJ (1999) Scalable self-calibrating display technology for seamless large-scale displays. PhD thesis, M" /><p class="c-article-references__text" id="ref-CR28">Surati RJ (1999) Scalable self-calibrating display technology for seamless large-scale displays. PhD thesis, Massachusetts Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wallace G, Chen H, Li K (2003) Color gamut matching for tiled display walls. In: Proceedings of the workshop o" /><p class="c-article-references__text" id="ref-CR29">Wallace G, Chen H, Li K (2003) Color gamut matching for tiled display walls. In: Proceedings of the workshop on virtual environments 2003, EGVE ’03. ACM, New York, pp 293–302</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang R, Gotz D, Hensley J, Towles H, Brown MS (2001) Pixelflex: a reconfigurable multi-projector display syste" /><p class="c-article-references__text" id="ref-CR30">Yang R, Gotz D, Hensley J, Towles H, Brown MS (2001) Pixelflex: a reconfigurable multi-projector display system. In: Proceedings of the conference on visualization ’01. VIS ’01. IEEE Computer Society, Washington, pp 167–174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhang, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):133" /><p class="c-article-references__text" id="ref-CR31">Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.888718" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20flexible%20new%20technique%20for%20camera%20calibration&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=11&amp;pages=1330-1334&amp;publication_year=2000&amp;author=Zhang%2CZ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0352-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Chemnitz University of Technology, Straße der Nationen 62, 09017, Chemnitz, Germany</p><p class="c-article-author-affiliation__authors-list">Marcel Heinz &amp; Guido Brunnett</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Marcel-Heinz"><span class="c-article-authors-search__title u-h3 js-search-name">Marcel Heinz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marcel+Heinz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marcel+Heinz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marcel+Heinz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Guido-Brunnett"><span class="c-article-authors-search__title u-h3 js-search-name">Guido Brunnett</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guido+Brunnett&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guido+Brunnett" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guido+Brunnett%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0352-5/email/correspondent/c1/new">Marcel Heinz</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Optimized%20GPU-based%20post-processing%20for%20stereoscopic%20multi-projector%20display%20systems&amp;author=Marcel%20Heinz%20et%20al&amp;contentID=10.1007%2Fs10055-018-0352-5&amp;publication=1359-4338&amp;publicationDate=2018-06-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0352-5" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0352-5" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Heinz, M., Brunnett, G. Optimized GPU-based post-processing for stereoscopic multi-projector display systems.
                    <i>Virtual Reality</i> <b>23, </b>45–60 (2019). https://doi.org/10.1007/s10055-018-0352-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0352-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-06-21">21 June 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-06-06">06 June 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-06-13">13 June 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-03-05">05 March 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0352-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0352-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Projector</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Display</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multi-projector</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tiled display</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Calibration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Stereoscopy</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0352-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=352;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

