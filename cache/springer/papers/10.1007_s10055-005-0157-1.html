<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Analysis of composite gestures with a coherent probabilistic graphical"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Traditionally, gesture-based interaction in virtual environments is composed of either static, posture-based gesture primitives or temporally analyzed dynamic primitives. However, it would be ideal..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Analysis of composite gestures with a coherent probabilistic graphical model"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-08-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Traditionally, gesture-based interaction in virtual environments is composed of either static, posture-based gesture primitives or temporally analyzed dynamic primitives. However, it would be ideal to incorporate both static and dynamic gestures to fully utilize the potential of gesture-based interaction. To that end, we propose a probabilistic framework that incorporates both static and dynamic gesture primitives. We call these primitives Gesture Words (GWords). Using a probabilistic graphical model (PGM), we integrate these heterogeneous GWords and a high-level language model in a coherent fashion. Composite gestures are represented as stochastic paths through the PGM. A gesture is analyzed by finding the path that maximizes the likelihood on the PGM with respect to the video sequence. To facilitate online computation, we propose a greedy algorithm for performing inference on the PGM. The parameters of the PGM can be learned via three different methods: supervised, unsupervised, and hybrid. We have implemented the PGM model for a gesture set of ten GWords with six composite gestures. The experimental results show that the PGM can accurately recognize composite gestures."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-08-12"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="242"/>

    <meta name="prism.endingPage" content="252"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0157-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0157-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0157-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0157-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Analysis of composite gestures with a coherent probabilistic graphical model"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2005/09"/>

    <meta name="citation_online_date" content="2005/08/12"/>

    <meta name="citation_firstpage" content="242"/>

    <meta name="citation_lastpage" content="252"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0157-1"/>

    <meta name="DOI" content="10.1007/s10055-005-0157-1"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0157-1"/>

    <meta name="description" content="Traditionally, gesture-based interaction in virtual environments is composed of either static, posture-based gesture primitives or temporally analyzed dyna"/>

    <meta name="dc.creator" content="Jason J. Corso"/>

    <meta name="dc.creator" content="Guangqi Ye"/>

    <meta name="dc.creator" content="Gregory D. Hager"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="3rd Tech. Hiball-3100 sensor, 
                    http://www.3rdtech.com/HiBall.htm
                    
                  "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=A computational framework and an algorithm for the measurement of visual motion; citation_author=P Anandan; citation_volume=2; citation_issue=3; citation_publication_date=1989; citation_pages=283-310; citation_doi=10.1007/BF00158167; citation_id=CR2"/>

    <meta name="citation_reference" content="Ascension technology corporation. Flock of birds, 
                    http://www.ascension-tech.com/products/flockofbirds.php
                    
                  "/>

    <meta name="citation_reference" content="citation_journal_title=In Proceedings of Comput Vis Pattern Recognit; citation_title=Estimating 3D hand pose from a cluttered image; citation_author=V Athitsos, S Sclaroff; citation_volume=2; citation_publication_date=2003; citation_pages=432-439; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Teleoperators Virtual Environments; citation_title=A survey of augmented reality. Presence; citation_author=Azuma RT; citation_volume=6; citation_issue=11; citation_publication_date=1997; citation_pages=1-38; citation_id=CR5"/>

    <meta name="citation_reference" content="Belongie S, Malik J, Puzicha J (2000) Shape context: a new descriptor for shape matching and object recognition. In: Neural Information Processing, pp 831&#8211;837"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=The recognition of human movement using temporal templates; citation_author=A Bobick, J Davis; citation_volume=23; citation_issue=3; citation_publication_date=2001; citation_pages=257-267; citation_doi=10.1109/34.910878; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A state-based approach to the representation and recognition of gesture; citation_author=A Bobick, A Wilson; citation_volume=19; citation_issue=12; citation_publication_date=1997; citation_pages=1325-1337; citation_doi=10.1109/34.643892; citation_id=CR8"/>

    <meta name="citation_reference" content="Brand M, Oliver N, Pentland AP (1997) Coupled hidden Markov models for complex action recognition. In: Proceedings of the 1997 Computer Vision Pattern Recognition, pp 994&#8211;999"/>

    <meta name="citation_reference" content="Bregler C (1997) Learning and recognizing human dynamics in video sequences. In: IEEE Conference on Computer Vision Pattern Recognition"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Vis Image Understand; citation_title=Human motion analysis: a review; citation_author=Q Cai, JK Aggarwal; citation_volume=73; citation_issue=3; citation_publication_date=1999; citation_pages=428-440; citation_doi=10.1006/cviu.1998.0744; citation_id=CR11"/>

    <meta name="citation_reference" content="Corso JJ (2004) Vision-based techniques for dynamic, collaborative mixed-realities. In: Thompson BJ (eds) Research papers of the Link Foundation fellows. University of Rochestor Press in association with the Link Foundation, vol 4"/>

    <meta name="citation_reference" content="Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer Interaction at Conference on Computer Vision and Pattern Recognition"/>

    <meta name="citation_reference" content="citation_title=Three-dimensional computer vision; citation_publication_date=1993; citation_id=CR14; citation_author=O Faugeras; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Understand; citation_title=Learning variable-length Markov models of behavior; citation_author=A Galata, N Johnson, D Hogg; citation_volume=83; citation_issue=1; citation_publication_date=2001; citation_pages=398-413; citation_doi=10.1006/cviu.2000.0894; citation_id=CR15"/>

    <meta name="citation_reference" content="Insko B, Meehan M, Whitton M, Brooks F (2001) Passive haptics significantly enhances virtual environments. Technical Report 01&#8211;10, Department of Computer Science, UNC Chapel Hill"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Recognition of visual activities and interactions by stochastic parsing; citation_author=YA Ivanov, AF Bobick; citation_volume=22; citation_issue=8; citation_publication_date=2000; citation_pages=852-872; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_title=Statistical methods for speech recognition; citation_publication_date=1999; citation_id=CR18; citation_author=F Jelinek; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Distinctive image features from scale-invariant keypoints; citation_author=D Lowe; citation_volume=60; citation_issue=2; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR19"/>

    <meta name="citation_reference" content="Malassiotis S, Aifanti N, Strintzis M (2002) A gesture recognition system using 3D data. In: Proceedings of the First International Symposium on 3D Data Processing Visualization and Transmisssion, pp 190&#8211;193"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=A comparison of skin history and trajectory-based representation schemes for the recognition of user-specific gestures; citation_author=SJ Mckenna, K Morrison; citation_volume=37; citation_publication_date=2004; citation_pages=999-1009; citation_doi=10.1016/j.patcog.2003.09.007; citation_id=CR21"/>

    <meta name="citation_reference" content="Moon TK (1996) The expectation-maximization algorithm. IEEE Signal Processing Magazine, pp 47&#8211;60"/>

    <meta name="citation_reference" content="Nickel K, Stiefelhagen R (2003) Pointing gesture recognition based on 3D-tracking of face, hands and head orientation. In: Workshop on Perceptive User Interfaces, pp 140&#8211;146"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Real-time fingertip tracking and gesture recognition; citation_author=K Oka, Y Sato, H Koike; citation_volume=22; citation_issue=6; citation_publication_date=2002; citation_pages=64-71; citation_doi=10.1109/MCG.2002.1046630; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=In Proceedings of Comput Vis Pattern Recognit; citation_title=View invariants for human action recognition; citation_author=V Parameswaran, R Chellappa; citation_volume=2; citation_publication_date=2003; citation_pages=613-619; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Visual interpretation of hand gestures for human-computer interaction: a review; citation_author=VI Pavlovic, R Sharma, TS Huang; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=677-695; citation_doi=10.1109/34.598226; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput; citation_title=Modeling and prediciton of human behavior; citation_author=A Pentland, A Liu; citation_volume=11; citation_issue=1; citation_publication_date=1999; citation_pages=229-242; citation_doi=10.1162/089976699300016890; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimedia; citation_title=Unnencumbered gesture interaction; citation_author=F Quek; citation_volume=3; citation_issue=3; citation_publication_date=1996; citation_pages=36-47; citation_doi=10.1109/93.556459; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=A tutorial on hidden Markov models and selected applications in speech recognition; citation_author=L Rabiner; citation_volume=77; citation_issue=2; citation_publication_date=1989; citation_pages=257-286; citation_doi=10.1109/5.18626; citation_id=CR29"/>

    <meta name="citation_reference" content="Salada M, Colgate JE, Lee M, Vishton P (2002) Validating a novel approach to rendering fingertip contact sensations. In: Proceedings of the 10th IEEE Virtual Reality Haptics Symposium, pp 217&#8211;224"/>

    <meta name="citation_reference" content="citation_title=Artificial Neural Networks; citation_publication_date=1997; citation_id=CR31; citation_author=RJ Schalkoff; citation_publisher=McGraw-Hill"/>

    <meta name="citation_reference" content="citation_journal_title=In Proceedings of Comput Vis Pattern Recognit; citation_title=Propagation networks for recognition of partially ordered sequential action; citation_author=Y Shi, Y Huang, D Minnen, A Bobick, I Essa; citation_volume=2; citation_publication_date=2004; citation_pages=862-869; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Gesture recognition using bezier curvers for visualization navigation from registered 3-D data; citation_author=MC Shin, LV Tsap, DB Goldgof; citation_volume=37; citation_issue=0; citation_publication_date=2004; citation_pages=1011-1024; citation_doi=10.1016/j.patcog.2003.11.007; citation_id=CR33"/>

    <meta name="citation_reference" content="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. Technical Report TR-375, M.I.T. Media Laboratory"/>

    <meta name="citation_reference" content="Tomasi C, Petrov S, Sastry A (2003) 3D Tracking = Classification + Interpolation. In: Proceeding International Conference Computer Vision, pp 1441&#8211;1448"/>

    <meta name="citation_reference" content="von Hardenberg C, Berard F (2001) Bare-hand human-computer interaction. In: Workshop on Perceptive User Interfaces"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Parametric hidden Markov models for gesture recognition; citation_author=A Wilson, A Bobick; citation_volume=21; citation_issue=9; citation_publication_date=1999; citation_pages=884-900; citation_doi=10.1109/34.790429; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Pfinder: real-time tracking of the human body; citation_author=C Wren, A Azarbayejani, T Darrell, AP Pentland; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=780-784; citation_doi=10.1109/34.598236; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=In Proceedings of Comput Vis Pattern Recognit; citation_title=View-independent recognition of hand postures; citation_author=Y Wu, TS Huang; citation_volume=2; citation_publication_date=2000; citation_pages=88-94; citation_id=CR39"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Processing Magazine; citation_title=Hand modeling, analysis, and recognition; citation_author=Y Wu, TS Huang; citation_volume=18; citation_issue=3; citation_publication_date=2001; citation_pages=51-60; citation_doi=10.1109/79.924889; citation_id=CR40"/>

    <meta name="citation_reference" content="Yamato J, Ohya J, Ishii K (1992) Recognizing human actions in time-sequential images using hidden Markov model. In: Proceedings of the 1992 IEEE Conference on Computer Vision Pattern Recognition, pp 379&#8211;385"/>

    <meta name="citation_reference" content=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications"/>

    <meta name="citation_reference" content="Ye G, Corso JJ, Hager GD (2004) Gesture recognition using 3D appearance and motion features. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction"/>

    <meta name="citation_reference" content="Ye G, Corso JJ, Hager GD, Okamura AM (2003) VisHap: augmented reality combining haptics and vision. In: Proceedings of IEEE International Conference on Systems, Man and Cybernetics, pp 3425&#8211;3431"/>

    <meta name="citation_reference" content="Yokokohji Y, Kinoshita J, Yoshikawa T (2001) Path planning for encountered-type haptic devices that render multiple objects in 3D space. In: Proceedings of IEEE Virtual Reality, pp 271&#8211;278"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A touch/force display system for haptic interface; citation_author=T Yoshikawa, A Nagura; citation_volume=10; citation_issue=2; citation_publication_date=2001; citation_pages=225-235; citation_id=CR46"/>

    <meta name="citation_reference" content="Zhang Z, Wu T, Shan Y, Shafer S (2001) Visual panel: virtual mouse keyboard and 3D controller with an ordinary piece of paper. In: Workshop on Perceptive User Interfaces"/>

    <meta name="citation_reference" content="Zhou H, Lin DJ, Huang TS (2004) Static hand postures recognition based on local orientation histogram feature distribution model. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction"/>

    <meta name="citation_author" content="Jason J. Corso"/>

    <meta name="citation_author_email" content="jcorso@cs.jhu.edu"/>

    <meta name="citation_author_institution" content="Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, USA"/>

    <meta name="citation_author" content="Guangqi Ye"/>

    <meta name="citation_author_institution" content="Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, USA"/>

    <meta name="citation_author" content="Gregory D. Hager"/>

    <meta name="citation_author_institution" content="Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0157-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0157-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Analysis of composite gestures with a coherent probabilistic graphical model"/>
        <meta property="og:description" content="Traditionally, gesture-based interaction in virtual environments is composed of either static, posture-based gesture primitives or temporally analyzed dynamic primitives. However, it would be ideal to incorporate both static and dynamic gestures to fully utilize the potential of gesture-based interaction. To that end, we propose a probabilistic framework that incorporates both static and dynamic gesture primitives. We call these primitives Gesture Words (GWords). Using a probabilistic graphical model (PGM), we integrate these heterogeneous GWords and a high-level language model in a coherent fashion. Composite gestures are represented as stochastic paths through the PGM. A gesture is analyzed by finding the path that maximizes the likelihood on the PGM with respect to the video sequence. To facilitate online computation, we propose a greedy algorithm for performing inference on the PGM. The parameters of the PGM can be learned via three different methods: supervised, unsupervised, and hybrid. We have implemented the PGM model for a gesture set of ten GWords with six composite gestures. The experimental results show that the PGM can accurately recognize composite gestures."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Analysis of composite gestures with a coherent probabilistic graphical model | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0157-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Human computer interaction, Gesture recognition, Hand postures, Vision-based interaction, Probabilistic graphical model","kwrd":["Human_computer_interaction","Gesture_recognition","Hand_postures","Vision-based_interaction","Probabilistic_graphical_model"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0157-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0157-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=157;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0157-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Analysis of composite gestures with a coherent probabilistic graphical model
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0157-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0157-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-08-12" itemprop="datePublished">12 August 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Analysis of composite gestures with a coherent probabilistic graphical model</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jason_J_-Corso" data-author-popup="auth-Jason_J_-Corso" data-corresp-id="c1">Jason J. Corso<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The Johns Hopkins University" /><meta itemprop="address" content="grid.21107.35, 0000000121719311, Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, MD, 21218, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guangqi-Ye" data-author-popup="auth-Guangqi-Ye">Guangqi Ye</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The Johns Hopkins University" /><meta itemprop="address" content="grid.21107.35, 0000000121719311, Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, MD, 21218, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gregory_D_-Hager" data-author-popup="auth-Gregory_D_-Hager">Gregory D. Hager</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The Johns Hopkins University" /><meta itemprop="address" content="grid.21107.35, 0000000121719311, Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, MD, 21218, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">242</span>–<span itemprop="pageEnd">252</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">96 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0157-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Traditionally, gesture-based interaction in virtual environments is composed of either static, posture-based gesture primitives or temporally analyzed dynamic primitives. However, it would be ideal to incorporate both static and dynamic gestures to fully utilize the potential of gesture-based interaction. To that end, we propose a probabilistic framework that incorporates both static and dynamic gesture primitives. We call these primitives Gesture Words (GWords). Using a probabilistic graphical model (PGM), we integrate these heterogeneous GWords and a high-level language model in a coherent fashion. Composite gestures are represented as stochastic paths through the PGM. A gesture is analyzed by finding the path that maximizes the likelihood on the PGM with respect to the video sequence. To facilitate online computation, we propose a greedy algorithm for performing inference on the PGM. The parameters of the PGM can be learned via three different methods: supervised, unsupervised, and hybrid. We have implemented the PGM model for a gesture set of ten GWords with six composite gestures. The experimental results show that the PGM can accurately recognize composite gestures.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Recently, the development in virtual reality (VR) technologies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environments 6(11):1–38" href="/article/10.1007/s10055-005-0157-1#ref-CR5" id="ref-link-section-d36321e294">5</a>] has taken us to 3D virtual worlds and prompted us to develop new human-computer interaction (HCI) techniques. Many of the current VR applications employ such traditional HCI media as joysticks, wands, or other tracking technologies (magnetic trackers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Ascension technology corporation. Flock of birds, &#xA;                    http://www.ascension-tech.com/products/flockofbirds.php&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0157-1#ref-CR3" id="ref-link-section-d36321e297">3</a>], optical trackers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="3rd Tech. Hiball-3100 sensor, &#xA;                    http://www.3rdtech.com/HiBall.htm&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0157-1#ref-CR1" id="ref-link-section-d36321e300">1</a>], etc.). However, many of these techniques encumber the user with hardware that can potentially reduce the realism (and effect) of the simulation. These limitations have presented us with the challenge to design and implement new HCI techniques that are natural and intuitive. Spoken language [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Jelinek F (1999) Statistical methods for speech recognition. MIT Press, Cambridge" href="/article/10.1007/s10055-005-0157-1#ref-CR18" id="ref-link-section-d36321e303">18</a>], haptics [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Insko B, Meehan M, Whitton M, Brooks F (2001) Passive haptics significantly enhances virtual environments. Technical Report 01–10, Department of Computer Science, UNC Chapel Hill" href="/article/10.1007/s10055-005-0157-1#ref-CR16" id="ref-link-section-d36321e306">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Salada M, Colgate JE, Lee M, Vishton P (2002) Validating a novel approach to rendering fingertip contact sensations. In: Proceedings of the 10th IEEE Virtual Reality Haptics Symposium, pp 217–224" href="/article/10.1007/s10055-005-0157-1#ref-CR30" id="ref-link-section-d36321e310">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Ye G, Corso JJ, Hager GD, Okamura AM (2003) VisHap: augmented reality combining haptics and vision. In: Proceedings of IEEE International Conference on Systems, Man and Cybernetics, pp 3425–3431" href="/article/10.1007/s10055-005-0157-1#ref-CR44" id="ref-link-section-d36321e313">44</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Yoshikawa T, Nagura A (2001) A touch/force display system for haptic interface. Presence 10(2):225–235" href="/article/10.1007/s10055-005-0157-1#ref-CR46" id="ref-link-section-d36321e316">46</a>] and vision [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Cai Q, Aggarwal JK (1999) Human motion analysis: a review. J Comput Vis Image Understand 73(3):428–440" href="/article/10.1007/s10055-005-0157-1#ref-CR11" id="ref-link-section-d36321e319">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: a review. IEEE Trans Pattern Anal Mach Intell 19(7):677–695" href="/article/10.1007/s10055-005-0157-1#ref-CR26" id="ref-link-section-d36321e322">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Wu Y, Huang TS (2001) Hand modeling, analysis, and recognition. IEEE Signal Processing Magazine 18(3):51–60" href="/article/10.1007/s10055-005-0157-1#ref-CR40" id="ref-link-section-d36321e325">40</a>] have been popular choices to replace traditional interaction media. Computer vision holds great promise: vision-based interfaces would allow unencumbered, large-scale spatial motion. Furthermore, rich visual information provides strong cues to infer the motion and configuration of the human hands and arms.</p><p>Many gesture-based visual interfaces have been developed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer Interaction at Conference on Computer Vision and Pattern Recognition" href="/article/10.1007/s10055-005-0157-1#ref-CR13" id="ref-link-section-d36321e331">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Oka K, Sato Y,Koike H (2002) Real-time fingertip tracking and gesture recognition. IEEE Comput Graph Appl 22(6):64–71" href="/article/10.1007/s10055-005-0157-1#ref-CR24" id="ref-link-section-d36321e334">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="von Hardenberg C, Berard F (2001) Bare-hand human-computer interaction. In: Workshop on Perceptive User Interfaces" href="/article/10.1007/s10055-005-0157-1#ref-CR36" id="ref-link-section-d36321e337">36</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Wren C, Azarbayejani A, Darrell T, Pentland AP (1997) Pfinder: real-time tracking of the human body. IEEE Trans Pattern Anal Mach Intell 19(7):780–784" href="/article/10.1007/s10055-005-0157-1#ref-CR38" id="ref-link-section-d36321e340">38</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Ye G, Corso JJ, Hager GD (2004) Gesture recognition using 3D appearance and motion features. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction" href="/article/10.1007/s10055-005-0157-1#ref-CR43" id="ref-link-section-d36321e343">43</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Zhang Z, Wu T, Shan Y, Shafer S (2001) Visual panel: virtual mouse keyboard and 3D controller with an ordinary piece of paper. In: Workshop on Perceptive User Interfaces" href="/article/10.1007/s10055-005-0157-1#ref-CR47" id="ref-link-section-d36321e347">47</a>]. According to the nature of the gestures in the vocabulary, the gestures in existing interfaces can be classified into two categories: static hand postures and dynamic gestures. Static postures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Athitsos V, Sclaroff S (2003) Estimating 3D hand pose from a cluttered image. In Proceedings of Comput Vis Pattern Recognit 2:432–439" href="/article/10.1007/s10055-005-0157-1#ref-CR4" id="ref-link-section-d36321e350">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Malassiotis S, Aifanti N, Strintzis M (2002) A gesture recognition system using 3D data. In: Proceedings of the First International Symposium on 3D Data Processing Visualization and Transmisssion, pp 190–193" href="/article/10.1007/s10055-005-0157-1#ref-CR20" id="ref-link-section-d36321e353">20</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Nickel K, Stiefelhagen R (2003) Pointing gesture recognition based on 3D-tracking of face, hands and head orientation. In: Workshop on Perceptive User Interfaces, pp 140–146" href="/article/10.1007/s10055-005-0157-1#ref-CR23" id="ref-link-section-d36321e356">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Quek F (1996) Unnencumbered gesture interaction. IEEE Multimedia 3(3):36–47" href="/article/10.1007/s10055-005-0157-1#ref-CR28" id="ref-link-section-d36321e359">28</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Tomasi C, Petrov S, Sastry A (2003) 3D Tracking = Classification + Interpolation. In: Proceeding International Conference Computer Vision, pp 1441–1448" href="/article/10.1007/s10055-005-0157-1#ref-CR35" id="ref-link-section-d36321e362">35</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Wu Y, Huang TS (2000) View-independent recognition of hand postures. In Proceedings of Comput Vis Pattern Recognit 2:88–94" href="/article/10.1007/s10055-005-0157-1#ref-CR39" id="ref-link-section-d36321e366">39</a>] model the gesture as a single key frame, thus discarding any dynamic characteristics. For example, in recent research on American sign language (ASL) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. Technical Report TR-375, M.I.T. Media Laboratory" href="/article/10.1007/s10055-005-0157-1#ref-CR34" id="ref-link-section-d36321e369">34</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Zhou H, Lin DJ, Huang TS (2004) Static hand postures recognition based on local orientation histogram feature distribution model. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction" href="/article/10.1007/s10055-005-0157-1#ref-CR48" id="ref-link-section-d36321e372">48</a>], static hand configuration is the only cue used to recognize a subset of the ASL consisting of alphabetical letters and numerical digits. The advantage of this approach is the efficiency of recognizing those gestures that display explicit static spatial configuration. However, it has an inherent shortcoming in handling dynamic gestures whose temporal patterns play a more important role than their static spatial arrangement.</p><p>Dynamic gestures contain both spatial and temporal characteristics, thus providing more challenges for modeling. Many models have been proposed to characterize the temporal structure of dynamic gestures: including temporal template matching [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Bobick A, Davis J (2001) The recognition of human movement using temporal templates. IEEE Trans Pattern Anal Mach Intell 23(3):257–267" href="/article/10.1007/s10055-005-0157-1#ref-CR7" id="ref-link-section-d36321e378">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Mckenna SJ, Morrison K (2004) A comparison of skin history and trajectory-based representation schemes for the recognition of user-specific gestures. Pattern Recognit 37:999–1009" href="/article/10.1007/s10055-005-0157-1#ref-CR21" id="ref-link-section-d36321e381">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Parameswaran V, Chellappa R (2003) View invariants for human action recognition. In Proceedings of Comput Vis Pattern Recognit 2:613–619" href="/article/10.1007/s10055-005-0157-1#ref-CR25" id="ref-link-section-d36321e384">25</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using bezier curvers for visualization navigation from registered 3-D data. Pattern Recognit 37(0):1011–1024" href="/article/10.1007/s10055-005-0157-1#ref-CR33" id="ref-link-section-d36321e387">33</a>], rule-based and state-based approaches [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Bobick A, Wilson A (1997) A state-based approach to the representation and recognition of gesture. IEEE Trans Pattern Anal Mach Intell 19(12):1325–1337" href="/article/10.1007/s10055-005-0157-1#ref-CR8" id="ref-link-section-d36321e390">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Quek F (1996) Unnencumbered gesture interaction. IEEE Multimedia 3(3):36–47" href="/article/10.1007/s10055-005-0157-1#ref-CR28" id="ref-link-section-d36321e394">28</a>], hidden Markov models (HMM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Oka K, Sato Y,Koike H (2002) Real-time fingertip tracking and gesture recognition. IEEE Comput Graph Appl 22(6):64–71" href="/article/10.1007/s10055-005-0157-1#ref-CR24" id="ref-link-section-d36321e397">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Rabiner L (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE 77(2):257–286" href="/article/10.1007/s10055-005-0157-1#ref-CR29" id="ref-link-section-d36321e400">29</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. Technical Report TR-375, M.I.T. Media Laboratory" href="/article/10.1007/s10055-005-0157-1#ref-CR34" id="ref-link-section-d36321e403">34</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Yamato J, Ohya J, Ishii K (1992) Recognizing human actions in time-sequential images using hidden Markov model. In: Proceedings of the 1992 IEEE Conference on Computer Vision Pattern Recognition, pp 379–385" href="/article/10.1007/s10055-005-0157-1#ref-CR41" id="ref-link-section-d36321e406">41</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e409">42</a>] and its variations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Brand M, Oliver N, Pentland AP (1997) Coupled hidden Markov models for complex action recognition. In: Proceedings of the 1997 Computer Vision Pattern Recognition, pp 994–999" href="/article/10.1007/s10055-005-0157-1#ref-CR9" id="ref-link-section-d36321e413">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Oka K, Sato Y,Koike H (2002) Real-time fingertip tracking and gesture recognition. IEEE Comput Graph Appl 22(6):64–71" href="/article/10.1007/s10055-005-0157-1#ref-CR24" id="ref-link-section-d36321e416">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Wilson A, Bobick A (1999) Parametric hidden Markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900" href="/article/10.1007/s10055-005-0157-1#ref-CR37" id="ref-link-section-d36321e419">37</a>], and Bayesian networks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Shi Y, Huang Y, Minnen D, Bobick A, Essa I (2004) Propagation networks for recognition of partially ordered sequential action. In Proceedings of Comput Vis Pattern Recognit 2:862–869" href="/article/10.1007/s10055-005-0157-1#ref-CR32" id="ref-link-section-d36321e422">32</a>]. These models combine spatial and temporal cues to infer gestures that span a stochastic trajectory in a high-dimensional spatio-temporal space.</p><p>Most current systems model dynamic gestures qualitatively. That is, they represent the identity of the gesture, but they do not incorporate any quantitative, parametric information about the geometry or dynamics of the motion involved. To overcome this limitation, a parametric HMM (PHMM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Wilson A, Bobick A (1999) Parametric hidden Markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900" href="/article/10.1007/s10055-005-0157-1#ref-CR37" id="ref-link-section-d36321e428">37</a>] has been proposed. The PHMM includes a global parameter that carries an extra quantitative representation of each gesture. This parameter is included as an additional variable in the output probabilities of each state of the traditional HMM.</p><p>It seems clear that to fully harness the representative power of human gestures, static postures and non-parametric and parametric, dynamic gestures must be integrated into a single coherent gesture model. For example, visual modeling of ASL is still limited by the lack of capabilities to handle the composite nature of gestures. To that end, we present a novel framework that integrates static postures, unparameterized dynamic gestures and dynamic parameterized gestures into a coherent model.</p><p>In this framework, a graphical model is used to model the semantics and temporal patterns of different parts of a complex gesture; essentially, the graphical model is a high-level language (or behavioral) model. In the model, each stage of the gesture is represented as a basic language unit, which we call a Gesture Word (GWord). A GWord can be modeled as either a static posture, unparameterized dynamic gesture or a parameterized gesture. A composite gesture is composed of one or more GWords with semantic constraints. These constraints are represented in the graphical model, with nodes denoting GWords and edges describing the temporal and linguistic relationship between GWords. The parameters of the model can be learned based on heuristics or via a probabilistic framework based on recorded training data. Online gesture recognition is carried out via greedy inference on the graphical model. Here, online means that the algorithm does not have access to future video frames.</p><p>Our proposed framework is related to work in the field of activity modeling. Bregler [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Bregler C (1997) Learning and recognizing human dynamics in video sequences. In: IEEE Conference on Computer Vision Pattern Recognition" href="/article/10.1007/s10055-005-0157-1#ref-CR10" id="ref-link-section-d36321e439">10</a>] abstracted human activity in a three-layered model. In the data-driven approach, regions of coherent motion are used as low-level features. Dynamic models capture simple movements at the mid-level, and HMMs model the high-level complex actions. Pentland and Liu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Pentland A, Liu A (1999) Modeling and prediciton of human behavior. Neural Comput 11(1):229–242" href="/article/10.1007/s10055-005-0157-1#ref-CR27" id="ref-link-section-d36321e442">27</a>] proposed Markov dynamic models which couple multiple linear dynamic models (e.g. Kalman filters) with a high-level Markov model. Ivanov and Bobick [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Trans Pattern Anal Mach Intell 22(8):852–872" href="/article/10.1007/s10055-005-0157-1#ref-CR17" id="ref-link-section-d36321e445">17</a>] proposed a probabilistic syntactive approach to activity modeling. In their two-layered model, a discrete symbol stream is generated from continuous low-level detectors and then parsed with a context-free grammar. Galata et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Galata A, Johnson N, Hogg D (2001) Learning variable-length Markov models of behavior. Comput Vis Image Understand 83(1):398–413" href="/article/10.1007/s10055-005-0157-1#ref-CR15" id="ref-link-section-d36321e448">15</a>] proposed an approach to learn the size structure of the stochastic model for high-level activity recognition.</p><p>The main contribution of this work is to investigate a high-level language model to integrate the three different low-level gesture forms in a coherent manner. We extend the state-of-the-art in gesture modeling by relaxing the assumption that the low-level gesture primitives have a homogeneous form: e.g. all can be modeled with a HMM.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Modeling composite gestures</h2><div class="c-article-section__content" id="Sec2-content"><p>Probabilistic graphical models (PGM) are a tool for modeling the spatial and temporal characteristics of dynamic processes. For example, HMMs and Bayesian networks are commonly used to model such dynamic phenomena as speech and activity. PGMs provide a mathematically sound framework for learning and probabilistic inference.</p><p>However, most previous work in gesture and activity recognition assume a consistent model for all low-level processes (GWords). We propose to use PGMs to integrate multiple, heterogeneous low-level gesture processes into a high-level composite gesture. Intuitively, we combine multiple GWords to form a <i>gesture sentence</i> that corresponds to a complete interaction task. For example, grasping a virtual object → moving it → dropping the object (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Composite gesture example with a corresponding graphical model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In the remainder of this section, we define notation in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec3">2.1</a> and present our construction of the composite gestures using PGMs in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec4">2.2</a>. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec5">2.3</a>, we discuss different types of GWords. We formulate the learning of the PGM in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec9">2.4</a>. The gesture inference is discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec13">2.5</a>.</p><h3 class="c-article__sub-heading" id="Sec3">Definitions</h3><p>Let the image <span class="mathjax-tex">\({\bf I}\doteq\{\mathcal{I},I,t\}\)</span> be a finite set of pixel locations <span class="mathjax-tex">\(\mathcal{I}\)</span> (points in R<sup>2</sup>) together with a map <span class="mathjax-tex">\(I:\mathcal{I} \rightarrow \mathcal{X},\)</span> where <span class="mathjax-tex">\(\mathcal{X}\)</span> is some arbitrary value space, and <i>t</i> is a time parameter. Define <span class="mathjax-tex">\(\mathcal{S} = \{{\mathbf I}_1 \ldots {\mathbf I}_m\}\)</span> to be a sequence of images with length <i>m</i> ≥ 1. Let <span class="mathjax-tex">\(G = \{\mathcal{V},\mathcal{E}\}\)</span> be a directed graph representing the gesture language model. Each node <span class="mathjax-tex">\(v \in \mathcal{V}\)</span> in the graph corresponds to a GWord which belongs to a vocabulary <span class="mathjax-tex">\(\mathcal{V}\)</span> of size <span class="mathjax-tex">\(|\mathcal{V}|.\)</span> Associated with each node <i>v</i> is a probability function <span class="mathjax-tex">\(P(\mathcal{S}|v),\)</span> which measures the observation likelihood of <span class="mathjax-tex">\(\mathcal{S}\)</span> for a given GWord <i>v</i>. Each edge <span class="mathjax-tex">\(e \in \mathcal{E}\)</span> is a probability function <i>P</i>(<i>v</i><sub>
                    <i>j</i>
                  </sub>|<i>v</i><sub>
                    <i>i</i>
                  </sub>), where <i>v</i><sub>
                    <i>j</i>
                  </sub>, <i>v</i><sub>
                    <i>i</i>
                  </sub> ∈<i>V</i>. Intuitively, the edge models the temporal relationship between successive gesture units in the composite gesture.</p><h3 class="c-article__sub-heading" id="Sec4">The gesture language</h3><p>We use a<i> bigram model</i> to capture the dynamic nature of the gesture language. The bigram model represents the linguistic relationship between pairs of GWords. Formally, given a vocabulary <span class="mathjax-tex">\(\mathcal{V},\)</span> define a GWord sequence <span class="mathjax-tex">\(\mathcal{W}=\{s, v_1, \ldots, v_k, t\}\)</span> where <span class="mathjax-tex">\(v_i \in \mathcal{V}\)</span> and <i>s</i>, <i>t</i> are two special nodes (dummy gestures) that act as the graph source and sink. Thus, a gesture is a path through the PGM starting at the source node and ending at the sink node. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig2">2</a>, we give an example PGM that can model six gestures. For example, the path <i>s</i> → 1 → 3 → 6 → <i>t</i> is a candidate gesture sentence.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Example PGM used to represent the gesture language model. Each path beginning at node <i>s</i> and ending at node <i>t</i> is a valid gesture sentence</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We embed the bigram language model into the PGM by associating nodes with individual GWords and assigning transition probabilities from the bigram model. For convenience, let<span class="mathjax-tex">\(P(v_1) \doteq P(v_1|s),\)</span> which can be considered as the priors of a GWord. Then the probability of observing the sequence in the bigram model is </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P(\mathcal{W}) \doteq P(s,v_{1} , \ldots ,v_{k} ,t) = P(v_{1} ){\prod\limits_{i = 2}^k {P(v_{i} |v_{{i - 1}} )} }. $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>As defined in the previous section, each node of the PGM models a specific GWord with its corresponding observation likelihood. Given an image sequence <span class="mathjax-tex">\(\mathcal{S}\)</span> we can construct a candidate segmentation (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec13">2.5</a>) that splits the sequence into <i>p</i> subsequences <span class="mathjax-tex">\(\{\mathcal{S}_1 \ldots \mathcal{S}_p\}.\)</span> We correspond each of the subsequences to a GWord thus creating a gesture sentence <span class="mathjax-tex">\(\mathcal{W}.\)</span> Assuming conditional independence of the subsequences given the segmentation and the observation likelihood of a subsequence only depends the corresponding GWord, the observation likelihood of the sequence is </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P(\mathcal{S}|\mathcal{W}) = \prod_{i=1}^p P(\mathcal{S}_i|v_i).$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Then, the overall probability of observing the gesture sentence is </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} P(\mathcal{W}|\mathcal{S}) \propto &amp; P(\mathcal{W}) \cdot P(\mathcal{S}|\mathcal{W}) \\ = &amp; P(v_{1} ){\prod\limits_{i = 2}^p {P(v_{i} |v_{{i - 1}} ) \cdot {\prod\limits_{i = 1}^p {P(\mathcal{S}_{i} |v_{i} )} }} } \\ \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> with the special source and sink node probabilities defined as <span class="mathjax-tex">\(P(s)=1, P(t|v \in \mathcal{V})=1, P(v \in \mathcal{V}|s)=P(v).\)</span></p><h3 class="c-article__sub-heading" id="Sec5">The three low-level gesture processes</h3><p>As discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec1">1</a>, there are three main approaches to modeling gestures in current virtual reality systems: static postures, non-parametric dynamic, or parametric dynamic. In the PGM presented earlier, each node corresponds to one of these three types of gesture processes.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Static posture</h4><p>Static postures are based on the recognition of single discriminative frames of video. Hence, static postures simplify gesture processing by discarding all temporal information. For example, in the current literature, most alphanumeric symbols in ASL are represented as static postures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Zhou H, Lin DJ, Huang TS (2004) Static hand postures recognition based on local orientation histogram feature distribution model. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction" href="/article/10.1007/s10055-005-0157-1#ref-CR48" id="ref-link-section-d36321e955">48</a>]. Commonly used approaches to model the postures include appearance-based templates, shape-based models, and 3D model-based methods.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Non-parametric dynamic</h4><p>Non-parametric dynamic gestures capture temporal processes that carry only qualitative information; no quantitative information (e.g. length of hand-wave) is present. Hence, these gestures are potentially more discriminative than static postures because of the additional temporal dimension. For example, the ‘j’ and ‘z’ letters in the ASL have a temporal signature; i.e. the spatial trajectory of the finger over time is used to discriminate between the ‘i’ and the ‘j’. Hidden Markov models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Rabiner L (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE 77(2):257–286" href="/article/10.1007/s10055-005-0157-1#ref-CR29" id="ref-link-section-d36321e966">29</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. Technical Report TR-375, M.I.T. Media Laboratory" href="/article/10.1007/s10055-005-0157-1#ref-CR34" id="ref-link-section-d36321e969">34</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Yamato J, Ohya J, Ishii K (1992) Recognizing human actions in time-sequential images using hidden Markov model. In: Proceedings of the 1992 IEEE Conference on Computer Vision Pattern Recognition, pp 379–385" href="/article/10.1007/s10055-005-0157-1#ref-CR41" id="ref-link-section-d36321e972">41</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e975">42</a>] and motion history images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Bobick A, Davis J (2001) The recognition of human movement using temporal templates. IEEE Trans Pattern Anal Mach Intell 23(3):257–267" href="/article/10.1007/s10055-005-0157-1#ref-CR7" id="ref-link-section-d36321e978">7</a>] are common methods used to model non-parametric dynamic gestures.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Parametric dynamic</h4><p>Parametric dynamic gestures are the most complex among the three types because they not only incorporate a temporal dimension but also encode a set of quantitative parameters. For example, in explaining the height of a person using an outstretched hand, the distance between the ground and the hand gives a height estimate. PHMM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Wilson A, Bobick A (1999) Parametric hidden Markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900" href="/article/10.1007/s10055-005-0157-1#ref-CR37" id="ref-link-section-d36321e989">37</a>] have been proposed to model a single spatial variable. However, most of the techniques are based on visual tracking.</p><p>The parametric dynamic gestures bring an added degree of difficulty to the recognition process because they can have too high a degree of temporal variability to be captured by a standard model like an HMM. For example, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig1">1</a> shows a composite gesture for grabbing, moving, and dropping a virtual object. In general, the moving gesture will appear quite arbitrary because the user has the freedom to navigate the entire workspace and also pause for variable amounts of time before dropping the object.</p><h3 class="c-article__sub-heading" id="Sec9">Learning the PGM</h3><p>In this paper, we assume that the learning and the implementation of the individual low-level gesture units are handled separately (in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec14">3</a> we discuss our implementations) and the observation probabilities of these units are normalized on the same scale. Here, we address the problem of learning and inference on the high-level gesture model. Specifically, we learn the parameters of the bigram language model (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0157-1#Equ1">1</a>). We describe three basic techniques to learn the bigram model: supervised, unsupervised, and hybrid.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Supervised learning</h4><p>Given a set of <i>n</i> labeled GWord sequences <span class="mathjax-tex">\(\mathcal{L} = \{\mathcal{W}_1 \ldots \mathcal{W}_n\}\)</span> with <span class="mathjax-tex">\(\mathcal{W}_i = \{s, v_{(i,1)}, \ldots, v_{(i,m_i)}, t\}\)</span> where <i>m</i><sub>
                      <i>i</i>
                    </sub>+2 is the length of sequence <span class="mathjax-tex">\(\mathcal{W}_i\)</span> and <span class="mathjax-tex">\(v_{(i,j)} \in \mathcal{V}.\)</span> The GWord prior is given by </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$P(v_k) = \frac{{\sum^n_{i=1}\delta(v_k,v_{(i,1)})}}{{n}},$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p> where δ(·) is the Kronecker delta function and <span class="mathjax-tex">\(v_k \in \mathcal{V}.\)</span> The prior computes the probability that a gesture sentence begins with a certain GWord. The bigram transition probability is given by the following equation. </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$P(v_l|v_k) = \frac{{\sum^n_{i=1} \sum^{m_i-1}_{j=1} \delta(v_k,v_{(i,j)}) \cdot \delta(v_l,v_{(i,j+1)})}}{{ \sum^n_{i=1} \sum^{m_i-1}_{j=1}\delta(v_k,v_{(i,j)}) }}.$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>Intuitively, (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0157-1#Equ5">5</a>) measures the transition probability from a GWord <span class="mathjax-tex">\({v_k}\)</span> to another GWord <span class="mathjax-tex">\(v_l \in \mathcal{V}\)</span> by accumulating the number of bigram pairs <span class="mathjax-tex">\({v_k}\)</span>→ <span class="mathjax-tex">\({v_l}\)</span> and normalizing by the number of bigrams beginning with <span class="mathjax-tex">\({v_k}\)</span>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Unsupervised learning</h4><p>Given a set of <i>n</i> unlabeled image sequences <span class="mathjax-tex">\(\mathcal{U} = \{U_1 \ldots U_n\}.\)</span> We generate an initial bigram model <i>M</i><sub>0</sub> in a uniform fashion based on the PGM. We can use additional heuristics based on the specific application to refine the uniform initialization. We train the bigram model using an EM-like [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Moon TK (1996) The expectation-maximization algorithm. IEEE Signal Processing Magazine, pp 47–60" href="/article/10.1007/s10055-005-0157-1#ref-CR22" id="ref-link-section-d36321e1226">22</a>] iterative algorithm.</p>
                    <ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p><i>M</i> ← <i>M</i><sub>0</sub></p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Compute the best labeling (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec13">2.5</a>) for each sequence in <span class="mathjax-tex">\(\mathcal{U}\)</span> based on the current bigram model <i>M</i>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Using the supervised learning algorithm (discussed previously), refine the bigram model <i>M</i>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Repeat until a fixed number of iterations is reached or the change of the bigram model in successive iterations is small.</p>
                        
                      </li>
                    </ol>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Hybrid learning</h4><p>Given a set of labeled GWord sequences <span class="mathjax-tex">\(\mathcal{L}\)</span> and a set of unlabeled image sequences <span class="mathjax-tex">\(\mathcal{U}.\)</span> We generate an initial bigram model <i>M</i><sub>0</sub> using the labeled sequences with the supervised learning algorithm discussed above. Then, we refine the bigram model in an iterative manner similar to the one used in unsupervised learning. </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p><i>M</i> ← <i>M</i><sub>0</sub></p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Compute the best labeling (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec13">2.5</a>) for each sequence in <span class="mathjax-tex">\(\mathcal{U}\)</span> based on the current bigram model <i>M</i>. Call the labeled sequences <span class="mathjax-tex">\(\hat{\mathcal{U}}.\)</span></p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>
                            <span class="mathjax-tex">\(\mathcal{T} = \bigcup(\mathcal{L},\hat{\mathcal{U}}).\)</span>
                          </p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Using the data <span class="mathjax-tex">\(\mathcal{T}\)</span> perform the supervised learning algorithm (discussed previously) to refine the bigram model <i>M</i>.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Repeat until a fixed number of iterations is reached or the change of the bigram model in successive iterations is small.</p>
                        
                      </li>
                    </ol><h3 class="c-article__sub-heading" id="Sec13">Inference on the PGM</h3><p>Given an image sequence <span class="mathjax-tex">\(\mathcal{S}\)</span> of length <i>m</i> and a PGM with an embedded bigram model, we construct the inference problem as the search for the best labeling <span class="mathjax-tex">\(\mathcal{L}\)</span> of <span class="mathjax-tex">\(\mathcal{S}\)</span> that maximizes the overall probability given in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0157-1#Equ3">3</a>). Formally, the inference problem is stated as </p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \{v_1^* \ldots v_p^*\} = \arg \max_{\mathcal{W} = f(\mathcal{S} )} P(\mathcal{W}) \cdot P(\mathcal{S}|\mathcal{W}),$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <span class="mathjax-tex">\(\mathcal{S} \doteq \{\mathcal{S}_1 \dots \mathcal{S}_p\}, f(\mathcal{S}) = \{v_1 \ldots v_p\}\)</span> is a one-to-one mapping from a sequence segmentation to a gesture sentence, and <i>p</i> is unknown. Let <i>g</i>(·) be the mapping from subsequence <span class="mathjax-tex">\(\mathcal{S}_i\)</span> to a GWord <span class="mathjax-tex">\({v_l}\)</span> ; it is computed using the maximum-likelihood criterion: </p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$g(\mathcal{S}_i) = \arg \max_{v_j \in \mathcal{V}} P(\mathcal{S}_i|v_j). $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Theoretically, the inference problem in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0157-1#Equ6">6</a>) could be solved by an exhaustive search. However, the combinatorial complexity is prohibitive. Furthermore, the fundamental differences in the three types of low-level gesture processes makes the optimization more difficult. In addition, online processing is a prerequisite for human-computer interfaces. Thus, we propose a sub-optimal, greedy algorithm.</p><p>Initialize the algorithm by setting <span class="mathjax-tex">\({v}_0\)</span>=<i>s</i> and <span class="mathjax-tex">\(\mathcal{S}_0 = \emptyset.\)</span> At stage <i>t</i> in the algorithm processing, we search for the best transition from <span class="mathjax-tex">\({v}_t\)</span> to <span class="mathjax-tex">\(v_{t+1} \)</span> which maximizes path probability, defined as the product of the transition probability <i>P</i>(<span class="mathjax-tex">\(v_{t+1} \)</span>|<span class="mathjax-tex">\({v}_t\)</span>) and the observation probability <span class="mathjax-tex">\(P(\mathcal{S}_{t+1}|v_{t+1}).\)</span> The beginning of subsequence <span class="mathjax-tex">\(\mathcal{S}_{t+1}\)</span> is set as the end of <span class="mathjax-tex">\(\mathcal{S}_t.\)</span> To determine the end of the subsequence <span class="mathjax-tex">\(S_{t+1} \)</span> and thus make the greedy path choice, we incrementally increase the length of the subsequence until the path to one of the children <i>c</i> meet both of the following two conditions. </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>The observation probability of the child passes a threshold τ<sub>
                            <i>c</i>
                          </sub>. We discuss a supervised technique for learning the node thresholds below.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>The path probability of <i>c</i> is highest among all of the children of node <i>v</i><sub>
                            <i>t</i>
                          </sub>. Formally, <span class="mathjax-tex">\(c = \arg \max_{v_{t+1}} P(v_{t+1}|v_t) \cdot P(\mathcal{S}_{t+1}|v_{t+1}).\)</span></p>
                      
                    </li>
                  </ol><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig3">3</a> we show a graphical depiction of a stage in the middle of the greedy algorithm. In the figure, at stage <i>t</i>+1, child <i>c</i><sub>2</sub> of node <span class="mathjax-tex">\({v}_t\)</span> is chosen. We see that at the end of stage <i>t</i>+1 the end of sequence <span class="mathjax-tex">\(\mathcal{S}_{t+1}\)</span> has been determined.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Graphical depiction of two stages of the proposed greedy algorithm for computing the inference on the PGM. Black nodes are not on the best path and are disregarded, and dark gray represents past objects on the best path</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We learn the individual node thresholds using a supervised technique. Given a set of labeled GWord sequences and segmented image sequence pairs<span class="mathjax-tex">\((\mathcal{W}_i,\mathcal{S}_i) \in \mathcal{D}.\)</span> we pose the problem of determining the threshold <span class="mathjax-tex">\(\tau_v\)</span> for GWord <span class="mathjax-tex">\(v \in \mathcal{V}\)</span> as finding the minimum observation probability for all occurrences of <span class="mathjax-tex">\(v\)</span>: </p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\tau_v = \min_{(\mathcal{W}_i,\mathcal{S}_i) \in \mathcal{D} } \quad \min_{v_i \in \mathcal{W}_i \;\hbox {and}\; \delta(v_i,v) } { P(\mathcal{S}_i|v)}.$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>First, we initialize all the thresholds to 0, <span class="mathjax-tex">\(\tau_v = 0, \forall v \in \mathcal{V},\)</span> to handle the case where <span class="mathjax-tex">\(v\)</span> does not occur in <span class="mathjax-tex">\(\mathcal{L}.\)</span> Then, for all GWords <span class="mathjax-tex">\(v \in \mathcal{V}\)</span> we compute <span class="mathjax-tex">\(\tau_v\)</span> according to (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0157-1#Equ8">8</a>).</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Experimental setup</h2><div class="c-article-section__content" id="Sec14-content"><p>We analyze the proposed model for recognizing composite gestures by constructing a gesture set and the corresponding PGM. We employ the Visual Interaction Cues (VICs) paradigm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e2013">42</a>] (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec15">3.1</a>) to structure the vision processing and use the 4D Touchpad [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer Interaction at Conference on Computer Vision and Pattern Recognition" href="/article/10.1007/s10055-005-0157-1#ref-CR13" id="ref-link-section-d36321e2019">13</a>] (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec16">3.2</a>) as the experimental platform.</p><h3 class="c-article__sub-heading" id="Sec15">The visual interaction cues paradigm</h3><p>The VICs paradigm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e2032">42</a>] is a methodology for vision-based interaction operating on the fundamental premise that, in general vision-based human computer interaction (VBI) settings, global user modeling and tracking are not necessary. As discussed earlier, typical vision-based interaction methods attempt to perform continuous, global user tracking to model the interaction. Such techniques are computationally expensive, prone to error and the re-initialization problem, prohibit the inclusion of arbitrary numbers of users, and often require a complex gesture language the user must learn. However, under the VICs paradigm, we focus on the components of the interface itself instead of on the user. We motivate the paradigm with a simple, real-world example. When a person presses the keys of a telephone while making a telephone-call, the telephone maintains no notion of the user. Instead, it only recognizes the result of a key on the keypad being pressed. In contrast, typical methods for VBI would attempt to construct a model of the user’s finger, track it through space, and perform some action recognition as the user pressed the keys on the telephone. It is likely that in such processing, the computer system would also have to be aware of the real world geometric structure of the telephone itself. We claim that this processing is not necessary.</p><p>Let <span class="mathjax-tex">\(\mathcal{W}\)</span> be the space in which the components of the interface reside. In general, <span class="mathjax-tex">\(\mathcal{W}\)</span> is the 3D Euclidean space R<sup>3</sup> but it can be the Projective plane<i> P</i><sup>2</sup> or the Euclidean plane R<sup>2</sup>. Define an interface component mapping <span class="mathjax-tex">\(M : \mathcal{C} \rightarrow \mathcal{X},\)</span> where <span class="mathjax-tex">\(\mathcal{C} \subset \mathcal{W}\)</span> and <span class="mathjax-tex">\(\mathcal{X} \doteq \{\mathcal{I} \vee A(\mathcal{I})\}\)</span> with <span class="mathjax-tex">\(\mathcal{I} \)</span> the image as defined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec3">2.1</a> and A(·) being an arbitrary function, <i>A</i>:<i>P</i><i></i><sup>2</sup> → <i>P</i><sup>2</sup>. Intuitively, the mapping defines a region in the image to which an interface component projects (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1flb4.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1flb4.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Schematic explaining the principle of local image analysis for the VICs paradigm: <i>M</i> is the component mapping that yields a region of interest in the image <span class="mathjax-tex">\(\mathcal{I}\)</span> for analyzing actions on component <span class="mathjax-tex">\(\mathcal{C}\)</span></p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>If, for each interface component and the current image, a map is known, detecting a user action reduces to analyzing a local region in the image. This fundamental idea of local image analysis is the first principle of the VICs paradigm.</p><p>The second principle of the VICs paradigm concerns the computational methods involved in analyzing the image(s). Each interface component defines a function-specific set of image processing components that are ordered in a simple-to-complex fashion such that each level of increasing interaction-detection precision (and increasing computational cost) is executed only if the previous levels have validated the likely existence of an expected object in this ROI. Such a notion of simple-to-complex processing is not novel; for example, in early image processing, pyramidal schemes were invented that perform coarse-to-fine analysis of images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Anandan P (1989) A computational framework and an algorithm for the measurement of visual motion. Int J Comput Vis 2(3):283–310" href="/article/10.1007/s10055-005-0157-1#ref-CR2" id="ref-link-section-d36321e2195">2</a>]. However, it is integral to the VICs paradigm.</p><h3 class="c-article__sub-heading" id="Sec16">The 4D Touchpad</h3><p>In this section, we explain a VICs platform [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer Interaction at Conference on Computer Vision and Pattern Recognition" href="/article/10.1007/s10055-005-0157-1#ref-CR13" id="ref-link-section-d36321e2206">13</a>] that has been constructed based on the 3D–2D Projection interaction mode [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e2209">42</a>]. Here, a pair of wide-baseline cameras is directed at a flat-panel display. This setup is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig5">5</a> (left). The platform incorporates four dimensions of data: two for the physical screen, a third from the binocular vision, and a fourth from the temporal VICs processing.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>(<i>left</i>) 4D Touchpad Platform. (<i>right</i>) Example rectification process for the 4D Touchpad. Upper row contains the original images with the rectified images below</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Since the cameras <i>c</i>=1, 2 are fixed, the interface component mapping for the system can be computed during an initial calibration stage. We assume the optics can be modeled by perspective projection. Let<span class="mathjax-tex">\(\mathcal{F}\subset \hbox{P}^{2}\)</span> be the coordinate frame of the flat-panel screen. Define <span class="mathjax-tex">\(H_{c} : \mathcal{I}_{c} \rightarrow \mathcal{F}\)</span> the mapping from each input image <span class="mathjax-tex">\(\mathcal{I}_{c}\)</span> to the flat-panel frame <span class="mathjax-tex">\(\mathcal{F}\)</span> We employ a homography [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Faugeras O (1993) Three-dimensional computer vision. MIT Press, Cambridge" href="/article/10.1007/s10055-005-0157-1#ref-CR14" id="ref-link-section-d36321e2298">14</a>] for the mapping. Since the VICons exist in frame <span class="mathjax-tex">\(\mathcal{F}\)</span> each interface component mapping is simply the identity. This transformation process is known as rectification, and we show an example of it in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig5">5</a> (right). Radial distortion is evident in the rectified images; in the current system, we do not include any radial distortion correction. While doing so would complete the rectification procedure, in practice we find it unnecessary.</p><p>The rectification process warps both camera images in a way that all points in the plane of the flat-panel screen appear at the same position in both camera images. This can be used for stereo calculation; the resulting space is P<sup>2</sup> × Z with 0 disparity being in the plane of the flat-panel. Disparity is defined as the absolute distance between corresponding points in the two rectified images.</p><h3 class="c-article__sub-heading" id="Sec17">Gesture set</h3><p>The goal of the proposed framework is to facilitate the integration of different types of gestures (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec14">3</a>) and thus, natural interaction in virtual environments. To that end, we present an experimental gesture set with ten elements (GWords) with each of the three gesture types represented.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Low-level GWords</h4><p>The gesture set is designed to be used in general manipulative interfaces where actions such as selecting, grasping, and translating are required. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0157-1#Tab1">1</a> contains graphical depictions of each GWord. For dynamic gestures, we show three example images during the progress of the gesture.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Example images of basic GWords</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0157-1/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <ul class="u-list-style-none">
                      <li>
                        <p><b>Press</b>: Press is the static posture of a single finger activating the interface component.</p>
                      </li>
                      <li>
                        <p><b>Left</b>: Left is a dynamic, non-parametric motion of a finger to the left with respect to the interface component.</p>
                      </li>
                      <li>
                        <p><b>Right</b>: Right is a dynamic, non-parametric motion of a finger to the right with respect to the interface component.</p>
                      </li>
                      <li>
                        <p><b>Back</b>: Back is a dynamic, non-parametric retraction of the finger off the interface component.</p>
                      </li>
                      <li>
                        <p><b>Twist</b>: Twist is a clockwise twisting motion of a finger atop the interface component (dynamic, non-parametric).</p>
                      </li>
                      <li>
                        <p><b>Grab 1</b>: The first grabbing gesture is the dynamic, non-parametric motion of two fingers approaching the interface component open and closing once they have reached it.</p>
                      </li>
                      <li>
                        <p><b>Grab 2</b>: The second grabbing gesture is the dynamic, non-parametric motion of two fingers approaching the interface component open and remaining open upon reaching it.</p>
                      </li>
                      <li>
                        <p><b>Track</b>: Track is a parametric gesture that tracks two translational degrees-of-freedom.</p>
                      </li>
                      <li>
                        <p><b>Rotate</b>: Rotate is a parametric gesture that tracks one rotational degree-of-freedom.</p>
                      </li>
                      <li>
                        <p><b>Stop</b>: Stop is a static posture represented by an open hand atop the interface component.</p>
                      </li>
                    </ul>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Probabilistic graphical model</h4><p>With the algorithms presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec2">2</a>, we construct and train a probabilistic graphical model to be the interaction language. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig6">6</a> is a graphical depiction of the PGM; for clarity, we have not drawn any edges with zero probability in the bigram language model from supervised learning. A simple gesture sentence is thus Press → Left: the user approaches an interface component with an outstretched finger and then swipes his or her finger to the left. For example, such a composite gesture could be used to delete an interaction component. A more complex gesture sentence involving all three types of low-level GWords is Grab 1 → Track → Stop. This gesture sentence could be widely used in VR to grab and move virtual objects.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0157-1/MediaObjects/s10055-005-0157-1fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The probabilistic graphical model we constructed for our experimental setup. Edges with zero probability are not drawn. The nodes are labeled as per the discussion in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec17">3.3</a>. Additionally, each node is labeled as either<b> P</b>arametric,<b> D</b>ynamic, non-parametric, or<b> S</b>tatic posture.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0157-1/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Implementation of low-level GWords</h4><p>As discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec5">2.3</a>, we include three types of low-level gesture processing: static posture, non-parametric dynamic, or parametric dynamic. In this section we discuss the construction of these low-level processors for our experimental setup. However, from the perspective of the PGM framework, the specific construction of the low-level processors is arbitrary.</p><p><i>Static posture</i>. Static postures are based on the recognition of single discriminative frames of video. A multitude of potential methods exist in the literature for such recognition: SIFT keys [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110" href="/article/10.1007/s10055-005-0157-1#ref-CR19" id="ref-link-section-d36321e2681">19</a>] and Shape Contexts [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Belongie S, Malik J, Puzicha J (2000) Shape context: a new descriptor for shape matching and object recognition. In: Neural Information Processing, pp 831–837" href="/article/10.1007/s10055-005-0157-1#ref-CR6" id="ref-link-section-d36321e2684">6</a>] for example. Exploiting the principle of local image analysis from the VICs paradigm (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec15">3.1</a>), we use a common technique from machine learning called neural network processing [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Schalkoff RJ (1997) Artificial neural networks. McGraw-Hill, New York" href="/article/10.1007/s10055-005-0157-1#ref-CR31" id="ref-link-section-d36321e2690">31</a>]. We train a standard three-layer binary (on/off) network. We fix a local image neighborhood of 128×128 pixels corresponding to the VICon region in the image defined by its interface component mapping. As input to the network, we choose a coarse sub-sampling (16×16) and take non-overlapping pixel-neighborhood averages. We employ the intensity only (Y channel in YUV images).</p><p><i>Non-parametric dynamic</i>. We model the dynamics of the motion of the finger using discrete forward HMMs. For a complete discussion of our technique, refer to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications" href="/article/10.1007/s10055-005-0157-1#ref-CR42" id="ref-link-section-d36321e2698">42</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Ye G, Corso JJ, Hager GD (2004) Gesture recognition using 3D appearance and motion features. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction" href="/article/10.1007/s10055-005-0157-1#ref-CR43" id="ref-link-section-d36321e2701">43</a>]. Instead of directly tracking the hand, we take an object-centered approach that efficiently computes the 3D appearance using a region-based coarse stereo matching algorithm in a volume around the interaction component. The appearance feature is represented as a discrete volume with each cell describing the similarity between corresponding image patches of the stereo pair. The motion cue is captured via differentiating the appearance feature between frames. A K-means based vector quantization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Jelinek F (1999) Statistical methods for speech recognition. MIT Press, Cambridge" href="/article/10.1007/s10055-005-0157-1#ref-CR18" id="ref-link-section-d36321e2704">18</a>] algorithm is used to learn the cluster structure of these raw visual features. Then, the image sequence of a gesture is converted to a series of symbols that indicate the cluster identities of each image pair. A 6-state forward HMM is used to model the dynamics of each gestures. The parameters of the HMM are learned via the standard forward-backward algorithm based on the recorded gesture sequences. The gesture recognition is based on the probability that each HMM generates the given gesture image sequence.</p><p><i>Parametric dynamic</i>. The implementation of a parametric, dynamic processor is dependent on the task for which it is to be used. For example, in our gesture set, we require both a translational and a rotational processor. Again, many potential techniques exist for tracking the local motion of an image patch or pair of image patches. In our experiments, we used a filtered-detection algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Corso JJ (2004) Vision-based techniques for dynamic, collaborative mixed-realities. In: Thompson BJ (eds) Research papers of the Link Foundation fellows. University of Rochestor Press in association with the Link Foundation, vol 4" href="/article/10.1007/s10055-005-0157-1#ref-CR12" id="ref-link-section-d36321e2712">12</a>]: for each frame of video, we detect the feature(s) of interest and use a linear Kalman filter to model the dynamics of motion. For example, in the case of the translational processor, we detect the image point where the two grasping fingertips (thumb and index finger) meet. Assuming we can detect the same exact point every frame, tracking this grasping-point provides the two translational degrees-of-freedom. While it is difficult (or impossible) to detect exactly the same point every frame, in practice, the Kalman filter handles small variations in the point detection.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Experimental results</h2><div class="c-article-section__content" id="Sec21-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0157-1#Fig6">6</a> shows our vocabulary of six possible composite gestures. To quantitatively analyze the PGM, we recorded a training set of 100 video sequences each corresponding to one of the six gestures. The length of the sequences vary from 30 to 90 frames (at 10 frames-per-second). These sequences were not used in training the low-level gesture units. For the supervised training, we manually labeled each frame of the video sequences with a GWord. For unsupervised learning, we initialized a uniform language model and used the algorithm in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0157-1#Sec11">2.4.2</a> to refine the model. After two iterations, the bigram model converged.</p><p>We compare the language models after supervised and unsupervised learning in Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0157-1#Tab2">2</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0157-1#Tab3">3</a>, respectively. The bigram models are presented as adjacency matrices such that each row represents the probability of transitioning from a GWord (leftmost column) to other GWords (or itself). It can be seen that the 2 PGM bigram models have similar structure. It shows that even without good heuristics or labeled data, our unsupervised learning algorithm can still capture the underlying language model from raw gesture sequences.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Language model (Priors and bigram) using supervised learning</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0157-1/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Language model (Priors and bigram) using unsupervised learning</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0157-1/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>However, there are differences worth mentioning. For example, the prior for Stop from unsupervised learning is 0.03, but there are no sequences in the training corpus that begin with it. This is caused by the failure of the inference algorithm given a uniform bigram language model. Second, we see a difference in the self-transition probability for the Press GWord. In the labeled data, we fixed the duration of Press to one frame, but with a uniform bigram model, a static posture can last for several consecutive frame via self-transition.</p><p>During testing, we used the proposed greedy inference algorithm to analyze the video sequences. In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0157-1#Tab4">4</a>, we present the recognition accuracy for the gestures for both language models. For each sequence, we compared its known composite gesture identity with the GWord output of the PGM. We consider the output correct if it matches the GWord sentence at every stage.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Recognition accuracy of the PGM used in our experimentation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0157-1/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We can see from the results that the proposed high-level gesture language modeling can recognize compositions of heterogeneous low-level gestures. These composite gestures would be impossible to recognize using traditional unimodal techniques, while the PGM formulation takes advantage of high-level linguistic constraints to integrate fundamentally different low-level gesture units in a coherent probabilistic model.</p><p>However, the recognition accuracy for gesture Press → Right and gesture Press → Back are relatively poor. From visual inspection of the recognition algorithm’s output, we find that this is due to the greedy algorithm. The Left, Right, and Back are modeled with HMMs and trained with relatively long sequences (e.g. 20 frames). However, during inference, the greedy algorithm jumps to a conclusion based on an shorter subsequences (e.g. 7 frames). In our experiments, we see a bias toward the Left GWord for these incomplete subsequences.</p><p>The recognition results from the supervised and the unsupervised learning are comparable. This suggests that our linguistic approach to gesture recognition can perform well without a heuristic prior or manually labeled data. Hence, our method is less susceptible to the curse of dimensionality which, in our case, is that the amount of data (labeled, for supervised learning) required for learning generally increases exponentially with the number of GWords.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Conclusion</h2><div class="c-article-section__content" id="Sec22-content"><p>We have presented a linguistic approach to recognize composite gestures. The composite gestures consist of three different types of low-level units (GWords): static, posture-based primitives; non-parametric dynamic gestures; and parametric, dynamic gestures. We construct a coherent model by combining the GWords and a high-level language model in a probabilistic framework which is defined as a graphical model. We have proposed unsupervised and supervised learning algorithms; our results show that even with a random initialization, the PGM can learn the underlying gesture language model. By combining the PGM and the greedy inference algorithm, our method can model gestures composed of heterogeneous primitives.</p><p>Our approach allows the inference of composite gestures as paths through the PGM and uses the high-level linguistic constraints to guide the recognition of composite gestures. However, the proposed greedy inference algorithm will make locally optimal decisions since it is operating online. Furthermore, even in the off-line case, the heterogeneous, low-level gesture processes make an exhaustive search through all composite gesture sequences computationally prohibitive.</p><p>The experiments in this paper include a relatively small gesture vocabulary of 10 low-level GWords and six composite gestures. While we have found the bigram model sufficient to capture the linguistic constraints of this vocabulary, it is unclear if it will scale well with larger gesture vocabularies. In future work, we intend to investigate its scalability and other more context-rich language models. In addition, we are currently integrating the gesture model into a VR system. We plan to perform human factors experiments to analyze the efficacy of our gestural modeling in the system.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="3rd Tech. Hiball-3100 sensor, http://www.3rdtech.com/HiBall.htm" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">3rd Tech. Hiball-3100 sensor, <a href="http://www.3rdtech.com/HiBall.htm">http://www.3rdtech.com/HiBall.htm</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Anandan, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Anandan P (1989) A computational framework and an algorithm for the measurement of visual motion. Int J Comput" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Anandan P (1989) A computational framework and an algorithm for the measurement of visual motion. Int J Comput Vis 2(3):283–310</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00158167" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20computational%20framework%20and%20an%20algorithm%20for%20the%20measurement%20of%20visual%20motion&amp;journal=Int%20J%20Comput%20Vis&amp;volume=2&amp;issue=3&amp;pages=283-310&amp;publication_year=1989&amp;author=Anandan%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ascension technology corporation. Flock of birds, http://www.ascension-tech.com/products/flockofbirds.php" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Ascension technology corporation. Flock of birds, <a href="http://www.ascension-tech.com/products/flockofbirds.php">http://www.ascension-tech.com/products/flockofbirds.php</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Athitsos, S. Sclaroff, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Athitsos V, Sclaroff S (2003) Estimating 3D hand pose from a cluttered image. In Proceedings of Comput Vis Pat" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Athitsos V, Sclaroff S (2003) Estimating 3D hand pose from a cluttered image. In Proceedings of Comput Vis Pattern Recognit 2:432–439</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimating%203D%20hand%20pose%20from%20a%20cluttered%20image&amp;journal=In%20Proceedings%20of%20Comput%20Vis%20Pattern%20Recognit&amp;volume=2&amp;pages=432-439&amp;publication_year=2003&amp;author=Athitsos%2CV&amp;author=Sclaroff%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Azuma. RT, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environments 6(11):1–38" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environments 6(11):1–38</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality.%20Presence&amp;journal=Teleoperators%20Virtual%20Environments&amp;volume=6&amp;issue=11&amp;pages=1-38&amp;publication_year=1997&amp;author=RT%2CAzuma">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Belongie S, Malik J, Puzicha J (2000) Shape context: a new descriptor for shape matching and object recognitio" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Belongie S, Malik J, Puzicha J (2000) Shape context: a new descriptor for shape matching and object recognition. In: Neural Information Processing, pp 831–837</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Bobick, J. Davis, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Bobick A, Davis J (2001) The recognition of human movement using temporal templates. IEEE Trans Pattern Anal M" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Bobick A, Davis J (2001) The recognition of human movement using temporal templates. IEEE Trans Pattern Anal Mach Intell 23(3):257–267</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.910878" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20recognition%20of%20human%20movement%20using%20temporal%20templates&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=23&amp;issue=3&amp;pages=257-267&amp;publication_year=2001&amp;author=Bobick%2CA&amp;author=Davis%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Bobick, A. Wilson, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Bobick A, Wilson A (1997) A state-based approach to the representation and recognition of gesture. IEEE Trans " /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Bobick A, Wilson A (1997) A state-based approach to the representation and recognition of gesture. IEEE Trans Pattern Anal Mach Intell 19(12):1325–1337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.643892" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20state-based%20approach%20to%20the%20representation%20and%20recognition%20of%20gesture&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=12&amp;pages=1325-1337&amp;publication_year=1997&amp;author=Bobick%2CA&amp;author=Wilson%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brand M, Oliver N, Pentland AP (1997) Coupled hidden Markov models for complex action recognition. In: Proceed" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Brand M, Oliver N, Pentland AP (1997) Coupled hidden Markov models for complex action recognition. In: Proceedings of the 1997 Computer Vision Pattern Recognition, pp 994–999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bregler C (1997) Learning and recognizing human dynamics in video sequences. In: IEEE Conference on Computer V" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Bregler C (1997) Learning and recognizing human dynamics in video sequences. In: IEEE Conference on Computer Vision Pattern Recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Cai, JK. Aggarwal, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cai Q, Aggarwal JK (1999) Human motion analysis: a review. J Comput Vis Image Understand 73(3):428–440" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Cai Q, Aggarwal JK (1999) Human motion analysis: a review. J Comput Vis Image Understand 73(3):428–440</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcviu.1998.0744" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20motion%20analysis%3A%20a%20review&amp;journal=J%20Comput%20Vis%20Image%20Understand&amp;volume=73&amp;issue=3&amp;pages=428-440&amp;publication_year=1999&amp;author=Cai%2CQ&amp;author=Aggarwal%2CJK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Corso JJ (2004) Vision-based techniques for dynamic, collaborative mixed-realities. In: Thompson BJ (eds) Rese" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Corso JJ (2004) Vision-based techniques for dynamic, collaborative mixed-realities. In: Thompson BJ (eds) Research papers of the Link Foundation fellows. University of Rochestor Press in association with the Link Foundation, vol 4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Corso JJ, Burschka D, Hager GD (2003) The 4DT: unencumbered HCI with VICs. In: IEEE workshop on Human Computer Interaction at Conference on Computer Vision and Pattern Recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="O. Faugeras, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Faugeras O (1993) Three-dimensional computer vision. MIT Press, Cambridge" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Faugeras O (1993) Three-dimensional computer vision. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Three-dimensional%20computer%20vision&amp;publication_year=1993&amp;author=Faugeras%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Galata, N. Johnson, D. Hogg, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Galata A, Johnson N, Hogg D (2001) Learning variable-length Markov models of behavior. Comput Vis Image Unders" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Galata A, Johnson N, Hogg D (2001) Learning variable-length Markov models of behavior. Comput Vis Image Understand 83(1):398–413</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcviu.2000.0894" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20variable-length%20Markov%20models%20of%20behavior&amp;journal=Comput%20Vis%20Image%20Understand&amp;volume=83&amp;issue=1&amp;pages=398-413&amp;publication_year=2001&amp;author=Galata%2CA&amp;author=Johnson%2CN&amp;author=Hogg%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Insko B, Meehan M, Whitton M, Brooks F (2001) Passive haptics significantly enhances virtual environments. Tec" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Insko B, Meehan M, Whitton M, Brooks F (2001) Passive haptics significantly enhances virtual environments. Technical Report 01–10, Department of Computer Science, UNC Chapel Hill</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YA. Ivanov, AF. Bobick, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Tran" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Ivanov YA, Bobick AF (2000) Recognition of visual activities and interactions by stochastic parsing. IEEE Trans Pattern Anal Mach Intell 22(8):852–872</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20visual%20activities%20and%20interactions%20by%20stochastic%20parsing&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=8&amp;pages=852-872&amp;publication_year=2000&amp;author=Ivanov%2CYA&amp;author=Bobick%2CAF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="F. Jelinek, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Jelinek F (1999) Statistical methods for speech recognition. MIT Press, Cambridge" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Jelinek F (1999) Statistical methods for speech recognition. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20methods%20for%20speech%20recognition&amp;publication_year=1999&amp;author=Jelinek%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int%20J%20Comput%20Vis&amp;volume=60&amp;issue=2&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Malassiotis S, Aifanti N, Strintzis M (2002) A gesture recognition system using 3D data. In: Proceedings of th" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Malassiotis S, Aifanti N, Strintzis M (2002) A gesture recognition system using 3D data. In: Proceedings of the First International Symposium on 3D Data Processing Visualization and Transmisssion, pp 190–193</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Mckenna, K. Morrison, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Mckenna SJ, Morrison K (2004) A comparison of skin history and trajectory-based representation schemes for the" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Mckenna SJ, Morrison K (2004) A comparison of skin history and trajectory-based representation schemes for the recognition of user-specific gestures. Pattern Recognit 37:999–1009</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2003.09.007" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparison%20of%20skin%20history%20and%20trajectory-based%20representation%20schemes%20for%20the%20recognition%20of%20user-specific%20gestures&amp;journal=Pattern%20Recognit&amp;volume=37&amp;pages=999-1009&amp;publication_year=2004&amp;author=Mckenna%2CSJ&amp;author=Morrison%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moon TK (1996) The expectation-maximization algorithm. IEEE Signal Processing Magazine, pp 47–60" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Moon TK (1996) The expectation-maximization algorithm. IEEE Signal Processing Magazine, pp 47–60</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nickel K, Stiefelhagen R (2003) Pointing gesture recognition based on 3D-tracking of face, hands and head orie" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Nickel K, Stiefelhagen R (2003) Pointing gesture recognition based on 3D-tracking of face, hands and head orientation. In: Workshop on Perceptive User Interfaces, pp 140–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Oka, Y. Sato, H. Koike, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Oka K, Sato Y,Koike H (2002) Real-time fingertip tracking and gesture recognition. IEEE Comput Graph Appl 22(6" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Oka K, Sato Y,Koike H (2002) Real-time fingertip tracking and gesture recognition. IEEE Comput Graph Appl 22(6):64–71</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2002.1046630" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20fingertip%20tracking%20and%20gesture%20recognition&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=22&amp;issue=6&amp;pages=64-71&amp;publication_year=2002&amp;author=Oka%2CK&amp;author=Sato%2CY&amp;author=Koike%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Parameswaran, R. Chellappa, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Parameswaran V, Chellappa R (2003) View invariants for human action recognition. In Proceedings of Comput Vis " /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Parameswaran V, Chellappa R (2003) View invariants for human action recognition. In Proceedings of Comput Vis Pattern Recognit 2:613–619</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=View%20invariants%20for%20human%20action%20recognition&amp;journal=In%20Proceedings%20of%20Comput%20Vis%20Pattern%20Recognit&amp;volume=2&amp;pages=613-619&amp;publication_year=2003&amp;author=Parameswaran%2CV&amp;author=Chellappa%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VI. Pavlovic, R. Sharma, TS. Huang, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: " /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: a review. IEEE Trans Pattern Anal Mach Intell 19(7):677–695</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598226" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20interpretation%20of%20hand%20gestures%20for%20human-computer%20interaction%3A%20a%20review&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=677-695&amp;publication_year=1997&amp;author=Pavlovic%2CVI&amp;author=Sharma%2CR&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Pentland, A. Liu, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Pentland A, Liu A (1999) Modeling and prediciton of human behavior. Neural Comput 11(1):229–242" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Pentland A, Liu A (1999) Modeling and prediciton of human behavior. Neural Comput 11(1):229–242</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F089976699300016890" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9950731" aria-label="View reference 27 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20and%20prediciton%20of%20human%20behavior&amp;journal=Neural%20Comput&amp;volume=11&amp;issue=1&amp;pages=229-242&amp;publication_year=1999&amp;author=Pentland%2CA&amp;author=Liu%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Quek, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Quek F (1996) Unnencumbered gesture interaction. IEEE Multimedia 3(3):36–47" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Quek F (1996) Unnencumbered gesture interaction. IEEE Multimedia 3(3):36–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F93.556459" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unnencumbered%20gesture%20interaction&amp;journal=IEEE%20Multimedia&amp;volume=3&amp;issue=3&amp;pages=36-47&amp;publication_year=1996&amp;author=Quek%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Rabiner, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Rabiner L (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Rabiner L (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE 77(2):257–286</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5.18626" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial%20on%20hidden%20Markov%20models%20and%20selected%20applications%20in%20speech%20recognition&amp;journal=Proc%20IEEE&amp;volume=77&amp;issue=2&amp;pages=257-286&amp;publication_year=1989&amp;author=Rabiner%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salada M, Colgate JE, Lee M, Vishton P (2002) Validating a novel approach to rendering fingertip contact sensa" /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Salada M, Colgate JE, Lee M, Vishton P (2002) Validating a novel approach to rendering fingertip contact sensations. In: Proceedings of the 10th IEEE Virtual Reality Haptics Symposium, pp 217–224</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="RJ. Schalkoff, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Schalkoff RJ (1997) Artificial neural networks. McGraw-Hill, New York" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Schalkoff RJ (1997) Artificial neural networks. McGraw-Hill, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20Neural%20Networks&amp;publication_year=1997&amp;author=Schalkoff%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Shi, Y. Huang, D. Minnen, A. Bobick, I. Essa, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Shi Y, Huang Y, Minnen D, Bobick A, Essa I (2004) Propagation networks for recognition of partially ordered se" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Shi Y, Huang Y, Minnen D, Bobick A, Essa I (2004) Propagation networks for recognition of partially ordered sequential action. In Proceedings of Comput Vis Pattern Recognit 2:862–869</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Propagation%20networks%20for%20recognition%20of%20partially%20ordered%20sequential%20action&amp;journal=In%20Proceedings%20of%20Comput%20Vis%20Pattern%20Recognit&amp;volume=2&amp;pages=862-869&amp;publication_year=2004&amp;author=Shi%2CY&amp;author=Huang%2CY&amp;author=Minnen%2CD&amp;author=Bobick%2CA&amp;author=Essa%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MC. Shin, LV. Tsap, DB. Goldgof, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using bezier curvers for visualization navigation from" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Shin MC, Tsap LV, Goldgof DB (2004) Gesture recognition using bezier curvers for visualization navigation from registered 3-D data. Pattern Recognit 37(0):1011–1024</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2003.11.007" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gesture%20recognition%20using%20bezier%20curvers%20for%20visualization%20navigation%20from%20registered%203-D%20data&amp;journal=Pattern%20Recognit&amp;volume=37&amp;issue=0&amp;pages=1011-1024&amp;publication_year=2004&amp;author=Shin%2CMC&amp;author=Tsap%2CLV&amp;author=Goldgof%2CDB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov model" /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Starner T, Pentland A (1996) Real-time american sign language recognition from video using hidden Markov models. Technical Report TR-375, M.I.T. Media Laboratory</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tomasi C, Petrov S, Sastry A (2003) 3D Tracking = Classification + Interpolation. In: Proceeding International" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Tomasi C, Petrov S, Sastry A (2003) 3D Tracking = Classification + Interpolation. In: Proceeding International Conference Computer Vision, pp 1441–1448</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="von Hardenberg C, Berard F (2001) Bare-hand human-computer interaction. In: Workshop on Perceptive User Interf" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">von Hardenberg C, Berard F (2001) Bare-hand human-computer interaction. In: Workshop on Perceptive User Interfaces</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Wilson, A. Bobick, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Wilson A, Bobick A (1999) Parametric hidden Markov models for gesture recognition. IEEE Trans Pattern Anal Mac" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Wilson A, Bobick A (1999) Parametric hidden Markov models for gesture recognition. IEEE Trans Pattern Anal Mach Intell 21(9):884–900</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.790429" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parametric%20hidden%20Markov%20models%20for%20gesture%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=21&amp;issue=9&amp;pages=884-900&amp;publication_year=1999&amp;author=Wilson%2CA&amp;author=Bobick%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Wren, A. Azarbayejani, T. Darrell, AP. Pentland, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Wren C, Azarbayejani A, Darrell T, Pentland AP (1997) Pfinder: real-time tracking of the human body. IEEE Tran" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">Wren C, Azarbayejani A, Darrell T, Pentland AP (1997) Pfinder: real-time tracking of the human body. IEEE Trans Pattern Anal Mach Intell 19(7):780–784</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598236" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pfinder%3A%20real-time%20tracking%20of%20the%20human%20body&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=780-784&amp;publication_year=1997&amp;author=Wren%2CC&amp;author=Azarbayejani%2CA&amp;author=Darrell%2CT&amp;author=Pentland%2CAP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Wu, TS. Huang, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Wu Y, Huang TS (2000) View-independent recognition of hand postures. In Proceedings of Comput Vis Pattern Reco" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">Wu Y, Huang TS (2000) View-independent recognition of hand postures. In Proceedings of Comput Vis Pattern Recognit 2:88–94</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=View-independent%20recognition%20of%20hand%20postures&amp;journal=In%20Proceedings%20of%20Comput%20Vis%20Pattern%20Recognit&amp;volume=2&amp;pages=88-94&amp;publication_year=2000&amp;author=Wu%2CY&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Wu, TS. Huang, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Wu Y, Huang TS (2001) Hand modeling, analysis, and recognition. IEEE Signal Processing Magazine 18(3):51–60" /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Wu Y, Huang TS (2001) Hand modeling, analysis, and recognition. IEEE Signal Processing Magazine 18(3):51–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F79.924889" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20modeling%2C%20analysis%2C%20and%20recognition&amp;journal=IEEE%20Signal%20Processing%20Magazine&amp;volume=18&amp;issue=3&amp;pages=51-60&amp;publication_year=2001&amp;author=Wu%2CY&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yamato J, Ohya J, Ishii K (1992) Recognizing human actions in time-sequential images using hidden Markov model" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Yamato J, Ohya J, Ishii K (1992) Recognizing human actions in time-sequential images using hidden Markov model. In: Proceedings of the 1992 IEEE Conference on Computer Vision Pattern Recognition, pp 379–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content=" Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Mac" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42"> Ye G, Corso JJ, Burschka D, Hager GD (2004) VICs: a modular hci framework using spatio-temporal dynamics. Machine Vision and Applications</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ye G, Corso JJ, Hager GD (2004) Gesture recognition using 3D appearance and motion features. In: Proceedings o" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Ye G, Corso JJ, Hager GD (2004) Gesture recognition using 3D appearance and motion features. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ye G, Corso JJ, Hager GD, Okamura AM (2003) VisHap: augmented reality combining haptics and vision. In: Procee" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">Ye G, Corso JJ, Hager GD, Okamura AM (2003) VisHap: augmented reality combining haptics and vision. In: Proceedings of IEEE International Conference on Systems, Man and Cybernetics, pp 3425–3431</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yokokohji Y, Kinoshita J, Yoshikawa T (2001) Path planning for encountered-type haptic devices that render mul" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Yokokohji Y, Kinoshita J, Yoshikawa T (2001) Path planning for encountered-type haptic devices that render multiple objects in 3D space. In: Proceedings of IEEE Virtual Reality, pp 271–278</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Yoshikawa, A. Nagura, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Yoshikawa T, Nagura A (2001) A touch/force display system for haptic interface. Presence 10(2):225–235" /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Yoshikawa T, Nagura A (2001) A touch/force display system for haptic interface. Presence 10(2):225–235</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20touch%2Fforce%20display%20system%20for%20haptic%20interface&amp;journal=Presence&amp;volume=10&amp;issue=2&amp;pages=225-235&amp;publication_year=2001&amp;author=Yoshikawa%2CT&amp;author=Nagura%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang Z, Wu T, Shan Y, Shafer S (2001) Visual panel: virtual mouse keyboard and 3D controller with an ordinary" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">Zhang Z, Wu T, Shan Y, Shafer S (2001) Visual panel: virtual mouse keyboard and 3D controller with an ordinary piece of paper. In: Workshop on Perceptive User Interfaces</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhou H, Lin DJ, Huang TS (2004) Static hand postures recognition based on local orientation histogram feature " /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Zhou H, Lin DJ, Huang TS (2004) Static hand postures recognition based on local orientation histogram feature distribution model. In: Proceedings of CVPR workshop on Real-Time Vision for Human-Computer Interaction</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0157-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank Darius Burschka for his help with the Visual Interaction Cues project. This work was in part funded by a Link Foundation Fellowship in Simulation and Training and by the National Science Foundation under Grant No. 0112882.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Computational Interaction and Robotics Lab, The Johns Hopkins University, Baltimore, MD, 21218, USA</p><p class="c-article-author-affiliation__authors-list">Jason J. Corso, Guangqi Ye &amp; Gregory D. Hager</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jason_J_-Corso"><span class="c-article-authors-search__title u-h3 js-search-name">Jason J. Corso</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jason J.+Corso&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jason J.+Corso" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jason J.+Corso%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Guangqi-Ye"><span class="c-article-authors-search__title u-h3 js-search-name">Guangqi Ye</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guangqi+Ye&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guangqi+Ye" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guangqi+Ye%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gregory_D_-Hager"><span class="c-article-authors-search__title u-h3 js-search-name">Gregory D. Hager</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gregory D.+Hager&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gregory D.+Hager" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gregory D.+Hager%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0157-1/email/correspondent/c1/new">Jason J. Corso</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>An erratum to this article is available at <a href="http://dx.doi.org/10.1007/s10055-005-0007-1">http://dx.doi.org/10.1007/s10055-005-0007-1</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Analysis%20of%20composite%20gestures%20with%20a%20coherent%20probabilistic%20graphical%20model&amp;author=Jason%20J.%20Corso%20et%20al&amp;contentID=10.1007%2Fs10055-005-0157-1&amp;publication=1359-4338&amp;publicationDate=2005-08-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Corso, J.J., Ye, G. &amp; Hager, G.D. Analysis of composite gestures with a coherent probabilistic graphical model.
                    <i>Virtual Reality</i> <b>8, </b>242–252 (2005). https://doi.org/10.1007/s10055-005-0157-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0157-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-08-12">12 August 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-09">September 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0157-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0157-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Human computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gesture recognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand postures</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Vision-based interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Probabilistic graphical model</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0157-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=157;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

