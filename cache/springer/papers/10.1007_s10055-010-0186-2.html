<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Augmenting aerial earth maps with dynamic information from videos"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We introduce methods for augmenting aerial visualizations of Earth (from tools such as Google Earth or Microsoft Virtual Earth) with dynamic information obtained from videos. Our goal is to make..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Augmenting aerial earth maps with dynamic information from videos"/>

    <meta name="dc.source" content="Virtual Reality 2011 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-01-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We introduce methods for augmenting aerial visualizations of Earth (from tools such as Google Earth or Microsoft Virtual Earth) with dynamic information obtained from videos. Our goal is to make Augmented Earth Maps that visualize plausible live views of dynamic scenes in a city. We propose different approaches to analyze videos of pedestrians and cars in real situations, under differing conditions to extract dynamic information. Then, we augment an Aerial Earth Maps (AEMs) with the extracted live and dynamic content. We also analyze natural phenomenon (skies, clouds) and project information from these to the AEMs to add to the visual reality. Our primary contributions are: (1) Analyzing videos with different viewpoints, coverage, and overlaps to extract relevant information about view geometry and movements, with limited user input. (2) Projecting this information appropriately to the viewpoint of the AEMs and modeling the dynamics in the scene from observations to allow inference (in case of missing data) and synthesis. We demonstrate this over a variety of camera configurations and conditions. (3) The modeled information from videos is registered to the AEMs to render appropriate movements and related dynamics. We demonstrate this with traffic flow, people movements, and cloud motions. All of these approaches are brought together as a prototype system for a real-time visualization of a city that is alive and engaging."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-01-11"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="185"/>

    <meta name="prism.endingPage" content="200"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0186-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0186-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0186-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0186-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Augmenting aerial earth maps with dynamic information from videos"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2011/01/11"/>

    <meta name="citation_firstpage" content="185"/>

    <meta name="citation_lastpage" content="200"/>

    <meta name="citation_article_type" content="SI: Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0186-2"/>

    <meta name="DOI" content="10.1007/s10055-010-0186-2"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0186-2"/>

    <meta name="description" content="We introduce methods for augmenting aerial visualizations of Earth (from tools such as Google Earth or Microsoft Virtual Earth) with dynamic information ob"/>

    <meta name="dc.creator" content="Kihwan Kim"/>

    <meta name="dc.creator" content="Sangmin Oh"/>

    <meta name="dc.creator" content="Jeonggyu Lee"/>

    <meta name="dc.creator" content="Irfan Essa"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bouguet J-Y (2003) Pyramidal implementation of the lucas kanade feature tracker. In: Intel Corporation"/>

    <meta name="citation_reference" content="citation_title=Radial basis functions: theory and implementations; citation_publication_date=2003; citation_id=CR2; citation_author=MD Buhmann; citation_author=MJ Ablowitz; citation_publisher=Cambridge University"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Interactive procedural street modeling; citation_author=G Chen, G Esch, P Wonka, P M&#252;ller, E Zhang; citation_volume=27; citation_issue=3; citation_publication_date=2008; citation_pages=1-10; citation_doi=10.1145/1360612.1360702; citation_id=CR3"/>

    <meta name="citation_reference" content="Efros AA, Berg EC, Mori G, Malik J (2003) Recognizing action at a distance. In: ICCV03, pp 726&#8211;733"/>

    <meta name="citation_reference" content="Frey B, MacKay D (1998) A revolution: belief propagation in graphs with cycles. In: Neural information processing systems, pp 479&#8211;485"/>

    <meta name="citation_reference" content="Girgensohn A, Kimber D, Vaughan J, Yang T, Shipman F, Turner T, Rieffel E, Wilcox L, Chen F, Dunnigan T (2007) Dots: support for effective video surveillance. In: ACM MULTIMEDIA &#8217;07. ACM, New York, pp 423&#8211;432"/>

    <meta name="citation_reference" content="Harris MJ (2005) Real-time cloud simulation and rendering. In: ACM SIGGRAPH 2005 Courses. New York, p. 222"/>

    <meta name="citation_reference" content="citation_journal_title=PAMI Int J Pattern Anal Mach Intell; citation_title=In defense of the eight-point algorithm; citation_author=RI Hartley; citation_volume=19; citation_issue=6; citation_publication_date=1997; citation_pages=580-593; citation_doi=10.1109/34.601246; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_title=Multiple view geometry in computer vision; citation_publication_date=2000; citation_id=CR9; citation_author=R Hartley; citation_author=A Zisserman; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="Horry Y, Anjyo K-I, Arai K (1997) Tour into the picture: using a spidery mesh interface to make animation from a single image. In: Proceedings of ACM SIGGRAPH, New York, pp 225&#8211;232"/>

    <meta name="citation_reference" content="Kanade T (2001) Eyevision system at super bowl 2001. 
                    http://www.ri.cmu.edu/events/sb35/tksuperbowl.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR&#8217;07)"/>

    <meta name="citation_reference" content="Koller-meier EB, Ade F (2001) Tracking multiple objects using the condensation algorithm. JRAS"/>

    <meta name="citation_reference" content="Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476&#8211;490"/>

    <meta name="citation_reference" content="Lewis JP (ed) (1989) Algorithms for solid noise synthesis"/>

    <meta name="citation_reference" content="Man P (2006) Generating and real-time rendering of clouds. In: Central European seminar on computer graphics, pp 1&#8211;9"/>

    <meta name="citation_reference" content="citation_title=Probabilistic reasoning in intelligent systems: networks of plausible inference; citation_publication_date=1988; citation_id=CR20; citation_author=J Pearl; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="citation_journal_title=SIGGRAPH Comput Graph; citation_title=An image synthesizer; citation_author=K Perlin; citation_volume=19; citation_issue=3; citation_publication_date=1985; citation_pages=287-296; citation_doi=10.1145/325165.325247; citation_id=CR21"/>

    <meta name="citation_reference" content="Rabaud V, Belongie S (2006) Counting crowded moving objects. In: CVPR &#8217;06: Proceedings of IEEE computer vision and pattern recognition. IEEE Computer Society, pp 17&#8211;22"/>

    <meta name="citation_reference" content="Ramanan D, Forsyth DA (2003) Automatic annotation of everyday movements. In: NIPS. MIT Press, Cambridge"/>

    <meta name="citation_reference" content="Reynolds CW (1987) Flocks, herds and schools: a distributed behavioral model. In: ACM SIGGRAPH 1987. ACM Press, New York, pp 25&#8211;34"/>

    <meta name="citation_reference" content="Reynolds CW (1999) Steering behaviors for autonomous characters. In: GDC &#8217;99: Proceedings of game developers conference. Miller Freeman Game Group, pp 768&#8211;782"/>

    <meta name="citation_reference" content="Sawhney HS, Arpa A, Kumar R, Samarasekera S, Aggarwal M, Hsu S, Nister D, Hanna K (2002) Video flashlights: real time rendering of multiple videos for immersive model visualization. In: 13th Eurographics workshop on Rendering. Eurographics Association, pp 157&#8211;168"/>

    <meta name="citation_reference" content="Sebe IO, Hu J, You S, Neumann U (2003) 3d video surveillance with augmented virtual environments. In: IWVS &#8217;03: First ACM SIGMM international workshop on Video surveillance. ACM, New York, pp 107&#8211;112"/>

    <meta name="citation_reference" content="Seitz SM, Dyer CR (1996) View morphing. In: SIGGRAPH &#8217;96. ACM, New York, pp 21&#8211;30"/>

    <meta name="citation_reference" content="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: IEEE CVPR &#8217;06, pp 519&#8211;528"/>

    <meta name="citation_reference" content="citation_journal_title=Graph Models; citation_title=Autonomous pedestrians; citation_author=W Shao, D Terzopoulos; citation_volume=69; citation_issue=5; citation_publication_date=2007; citation_pages=246-274; citation_doi=10.1016/j.gmod.2007.09.001; citation_id=CR30"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE CVPR. IEEE computer society, pp 593&#8211;600"/>

    <meta name="citation_reference" content="Smart J, Cascio J, Paffendorf J (2007) Metaverse roadmap: pathways to the 3d web. Metaverse: a cross-industry public foresight project"/>

    <meta name="citation_reference" content="Snavely N, Seitz SM, Szeliski R (2006) Photo tourism: exploring photo collections in 3d. In: Proceedings of ACM SIGGRAPH&#8217;06. ACM Press, New York, pp 835&#8211;846"/>

    <meta name="citation_reference" content="Treuille A, Cooper S, Popovi&#263; Z (2006) Continuum crowds. In: ACM SIGGRAPH 2006 papers, pp 1160&#8211;1168"/>

    <meta name="citation_reference" content="Turk G, O&#8217;Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH &#8217;99. New York, NY, pp 335&#8211;342"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Resolving motion correspondence for densely moving points; citation_author=CJ Veenman, MJT Reinders, E Backer; citation_volume=23; citation_publication_date=2001; citation_pages=54-72; citation_doi=10.1109/34.899946; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=J Graph Tools; citation_title=Realistic and fast cloud rendering; citation_author=N Wang; citation_volume=9; citation_issue=3; citation_publication_date=2004; citation_pages=21-40; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=Abdom Imag; citation_title=Contextualized videos: combining videos with environment models to support situational understanding; citation_author=Y Wang, DM Krum, EM Coelho, DA Bowman; citation_volume=13; citation_publication_date=2007; citation_pages=1568-1575; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_title=A report on the surveillance society; citation_publication_date=2006; citation_id=CR39; citation_author=DM Wood; citation_author=K Ball; citation_author=D Lyon; citation_author=C Norris; citation_author=C Raab; citation_publisher=Surveillance Studies Network"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Comput Surv; citation_title=Object tracking: a survey; citation_author=A Yilmaz, O Javed, M Shah; citation_volume=38; citation_issue=4; citation_publication_date=2006; citation_pages=13; citation_doi=10.1145/1177352.1177355; citation_id=CR40"/>

    <meta name="citation_reference" content="Zotti G, Groller ME (2005) A sky dome visualisation for identification of astronomical orientations. In: INFOVIS &#8217;05. IEEE Computer Society, Washington, DC, p 2"/>

    <meta name="citation_author" content="Kihwan Kim"/>

    <meta name="citation_author_email" content="kihwan23@cc.gatech.edu"/>

    <meta name="citation_author_institution" content="Georgia Institute of Technology, Atlanta, USA"/>

    <meta name="citation_author" content="Sangmin Oh"/>

    <meta name="citation_author_email" content="sangmin.oh@kitware.com"/>

    <meta name="citation_author_institution" content="Georgia Institute of Technology, Atlanta, USA"/>

    <meta name="citation_author_institution" content="Kitware Inc., Clifton Park, USA"/>

    <meta name="citation_author" content="Jeonggyu Lee"/>

    <meta name="citation_author_email" content="jeonggyu.lee@intel.com"/>

    <meta name="citation_author_institution" content="Georgia Institute of Technology, Atlanta, USA"/>

    <meta name="citation_author_institution" content="Intel Corporation, Hillsboro, USA"/>

    <meta name="citation_author" content="Irfan Essa"/>

    <meta name="citation_author_email" content="irfan@cc.gatech.edu"/>

    <meta name="citation_author_institution" content="Georgia Institute of Technology, Atlanta, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0186-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0186-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Augmenting aerial earth maps with dynamic information from videos"/>
        <meta property="og:description" content="We introduce methods for augmenting aerial visualizations of Earth (from tools such as Google Earth or Microsoft Virtual Earth) with dynamic information obtained from videos. Our goal is to make Augmented Earth Maps that visualize plausible live views of dynamic scenes in a city. We propose different approaches to analyze videos of pedestrians and cars in real situations, under differing conditions to extract dynamic information. Then, we augment an Aerial Earth Maps (AEMs) with the extracted live and dynamic content. We also analyze natural phenomenon (skies, clouds) and project information from these to the AEMs to add to the visual reality. Our primary contributions are: (1) Analyzing videos with different viewpoints, coverage, and overlaps to extract relevant information about view geometry and movements, with limited user input. (2) Projecting this information appropriately to the viewpoint of the AEMs and modeling the dynamics in the scene from observations to allow inference (in case of missing data) and synthesis. We demonstrate this over a variety of camera configurations and conditions. (3) The modeled information from videos is registered to the AEMs to render appropriate movements and related dynamics. We demonstrate this with traffic flow, people movements, and cloud motions. All of these approaches are brought together as a prototype system for a real-time visualization of a city that is alive and engaging."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Augmenting aerial earth maps with dynamic information from videos | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0186-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Augmented virtual reality, Video analysis, Computer vision, Computer graphics, Tracking, View synthesis, Procedural rendering","kwrd":["Augmented_reality","Augmented_virtual_reality","Video_analysis","Computer_vision","Computer_graphics","Tracking","View_synthesis","Procedural_rendering"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0186-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0186-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=186;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0186-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Augmenting aerial earth maps with dynamic information from videos
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0186-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0186-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-01-11" itemprop="datePublished">11 January 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Augmenting aerial earth maps with dynamic information from videos</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kihwan-Kim" data-author-popup="auth-Kihwan-Kim" data-corresp-id="c1">Kihwan Kim<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Georgia Institute of Technology" /><meta itemprop="address" content="grid.213917.f, 0000000120974943, Georgia Institute of Technology, Atlanta, GA, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sangmin-Oh" data-author-popup="auth-Sangmin-Oh">Sangmin Oh</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Georgia Institute of Technology" /><meta itemprop="address" content="grid.213917.f, 0000000120974943, Georgia Institute of Technology, Atlanta, GA, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Kitware Inc." /><meta itemprop="address" content="grid.32348.3e, Kitware Inc., Clifton Park, NY, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jeonggyu-Lee" data-author-popup="auth-Jeonggyu-Lee">Jeonggyu Lee</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Georgia Institute of Technology" /><meta itemprop="address" content="grid.213917.f, 0000000120974943, Georgia Institute of Technology, Atlanta, GA, USA" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Intel Corporation" /><meta itemprop="address" content="grid.419318.6, 0000000412177655, Intel Corporation, Hillsboro, OR, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Irfan-Essa" data-author-popup="auth-Irfan-Essa">Irfan Essa</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Georgia Institute of Technology" /><meta itemprop="address" content="grid.213917.f, 0000000120974943, Georgia Institute of Technology, Atlanta, GA, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">185</span>–<span itemprop="pageEnd">200</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">229 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0186-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We introduce methods for <i>augmenting</i> aerial visualizations of Earth (from tools such as Google Earth or Microsoft Virtual Earth) with <i>dynamic information</i> obtained from videos. Our goal is to make <i>Augmented Earth Maps</i> that visualize plausible live views of dynamic scenes in a city. We propose different approaches to analyze videos of pedestrians and cars in real situations, under differing conditions to extract dynamic information. Then, we augment an <i>Aerial Earth Maps</i> (AEMs) with the extracted live and dynamic content. We also analyze natural phenomenon (skies, clouds) and project information from these to the AEMs to add to the visual reality. Our primary contributions are: (1) Analyzing videos with different viewpoints, coverage, and overlaps to extract relevant information about view geometry and movements, with limited user input. (2) Projecting this information appropriately to the viewpoint of the AEMs and modeling the dynamics in the scene from observations to allow inference (in case of missing data) and synthesis. We demonstrate this over a variety of camera configurations and conditions. (3) The modeled information from videos is registered to the AEMs to render appropriate movements and related dynamics. We demonstrate this with traffic flow, people movements, and cloud motions. All of these approaches are brought together as a prototype system for a real-time visualization of a city that is alive and engaging.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Various places on Earth can be visualized on the Internet using online Aerial Earth Maps (AEMs) services (e.g., Google Earth, Microsoft Virtual Earth, <i>etc</i>.). We can visually browse through cities across the globe from our desktops or mobile devices and see 3D models of buildings, street views and topologies. Additionally, information such as traffic, restaurants, and other services are also provided within a geo-spatial database. However, such visualizations, while rich in information, are static and do not showcase the dynamism of the real world. Such dynamism captured from the city and then mirrored on these AEMS would give us a more immersive experience and directly interpretable visual information. We are motivated to add such dynamic information to these online visualizations of the globe using distributed video resources. Imagine seeing the city with traffic flowing, people moving around, sports being played, clouds moving, all visualized by Earth/Globe Visualization systems.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig1">1</a> shows a still of such an augmented visualization being driven by the analysis of 36 video sources. Such an augmentation of static visuals of cities is possible because many sensory sources that capture the dynamic information within a city are now available. Among such sensors, there is huge growth of cameras in our environment, may these be traffic cameras, web cams and aerial videos or even handheld cameras by people in the context of crowdsourcing with images (see Snavely et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Snavely N, Seitz SM, Szeliski R (2006) Photo tourism: exploring photo collections in 3d. In: Proceedings of ACM SIGGRAPH’06. ACM Press, New York, pp 835–846" href="/article/10.1007/s10055-010-0186-2#ref-CR33" id="ref-link-section-d39778e415">2006</a> for a compelling example of crowdsourcing with personal photographs). The direct use of such resources are limited due to (1) narrow field of view (FOV) of each camera, and (2) limited number of cameras compared to the size of the city.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>An overview of the “Augmented Earth Map” generated by our system. We make use of 36 videos to add dynamic information to city visualization. Dynamism in each input video of traffic, people, and clouds is extracted, then it is mapped onto the Earth map in real time. Each video patch in the figure represents the input sources. See the video on our site for a fly-around</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In this paper, we present an approach to generate plausible visualizations of ALIVE cities that one can browse interactively in the form of dynamic and live Aerial Earth Maps. The rendition of these Alive Aerial Earth Maps is accomplished by spatio-temporal analysis of videos from different sources around the city. To achieve this goal, we address several technical challenges. We are specifically interested in using video data to augment these AEMs with dynamics. To achieve this, <span class="mathjax-tex">\({\sl first}\)</span>, we need to develop a framework which extracts information about the geometry of the scene and also the movements in the environment from video. <span class="mathjax-tex">\({\sl Second}\)</span>, we need to register the view from the given video to a view in the AEMs. In cases where we have multiple instances of views, but still not full coverage due to limited FOV, we need to infer what is happening in between the views, in a domain-specific manner. This requires designing models of dynamics from observed data. <span class="mathjax-tex">\({\sl Third}\)</span>, we need to generate visualizations from the observed data onto the AEMs. This includes synthesizing behaviors based on videos, procedural information captured from them and updating views as they are manipulated in the AEMs.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig2">2</a> shows an overview of our framework to take (1) visual measurements and adapting them for (2) registration onto the AEMs and subsequently (3) visualizing motions and viewpoints. It is important to note that in our framework, in addition to extracting viewpoints and registering them to new views, we also model the dynamic information for the purpose of synthesizing it on AEMs. We use a variety of view synthesis, behavioral simulation and procedural rendering approaches to achieve this. At present, the rendered alive cities are visualized in real time based on off-line analysis of the captured video data. As we synthesize behaviors driven by visual measurements, our visualization shows a plausible viewpoint of the state of the environment from aerial views with live dynamic information. Additionally, our system allows manual user manipulation and control to address issues such as mis-alignments and viewpoint mis-registration, if needed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overview of our approach, highlighting the three main stages of observation, registration, and simulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Scenarios under study for AEMs</h2><div class="c-article-section__content" id="Sec2-content"><p>Extracting dynamic information from video feeds for augmenting AEMs is a challenging problem, primarily due to wide variety of conditions/configurations that we will have to deal with. For example, we want to be able to take videos of clouds moving and project them onto AEMS for visualization. We want to see people moving in different situations, and also show traffic motions. Furthermore, in each of the above instances, we have to deal with different viewpoints of videos and in many cases with incomplete information. Finally, we have to track moving objects and determine coherence between different viewpoints.</p><p>To address these issues of variation across different domains of interest to us, we divide our study into four scenarios that address the distribution of cameras and motion of objects. We acknowledge that accurate tracking and registration from video still remains an open research question. In our work, we have found satisfactory solutions that help us to get to our goal of generating new augmented visualizations from real data (video). We leverage analysis within a specific context, with domain knowledge as needed, to help our attempt to build these AEMs. We do allow for simple user interaction to help to remove errors in registration.</p><p>Now, we describe in brief the various cases we concentrate on and will discuss them in detail in the rest of the paper. The results will be used to showcase the variations in configurations and how we generate Augmented Visualizations of Live Cities.</p><ul class="u-list-style-none">
                  <li>
                    <p><i>1 Direct Mapping (DM)</i>. Video is analyzed directly and tracked. Data is projected onto the limited regions covered by camera’s field of view. We showcase several examples of people walking around.</p>
                  </li>
                  <li>
                    <p><i>2 Overlapping Cameras with Complex Motion (OCCM)</i>. Several cameras with overlapping views observe a relatively small region concurrently. The motion within the area is complex. We demonstrate this with people playing sports, and this can be considered valid for other CCTV domains.</p>
                  </li>
                  <li>
                    <p><i>3 Sparse Cameras with Simple Motions (SCSM).</i> Sparsely distributed cameras cover a wide area but dynamic information is simple. For example, traffic cameras are separately observing a highway and the motion of vehicles is relatively simple and is predictable between nearby regions.</p>
                  </li>
                  <li>
                    <p><i>4 Sparse Cameras and Complex Motion (SCCM).</i> Cameras are sparsely distributed and each of them observes a different part in a larger area. Moreover, the motion of the target object is complex. This is the case where we observe and model natural phenomena such as clouds in the sky.</p>
                  </li>
                </ul>
                     </div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Related and motivating previous work</h2><div class="c-article-section__content" id="Sec3-content"><p>Our work builds on existing efforts in computer vision on object-tracking and multi-view registration. We rely on behavioral animation approaches from the graphics community for our simulations. We briefly describe some of the efforts that guided and motivated us before discussing the technical details of our work.</p><p>Multi-view geometry research (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-010-0186-2#ref-CR9" id="ref-link-section-d39778e553">2000</a>) has established techniques to compute the geometric properties between two planes. Using these approaches, we need match the 2D video scenes from our videos to the planes in the virtual 3D environment. We focus primarily on extracting coarse geometry information from monocular images rather than reconstructing exact 3D models. To aid the correspondence selection process, Spidery mesh (Horry et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Horry Y, Anjyo K-I, Arai K (1997) Tour into the picture: using a spidery mesh interface to make animation from a single image. In: Proceedings of ACM SIGGRAPH, New York, pp 225–232" href="/article/10.1007/s10055-010-0186-2#ref-CR10" id="ref-link-section-d39778e556">1997</a>) interface and a modified version of a vanishing point estimation algorithm (Kosecka and Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476–490" href="/article/10.1007/s10055-010-0186-2#ref-CR14" id="ref-link-section-d39778e559">2002</a>) are employed as necessary (the direction of the moving objects from tracking step is used to determine the most dominant vanishing points).</p><p>Some of our works on view synthesis builds on the work of Seitz and Dyer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Seitz SM, Dyer CR (1996) View morphing. In: SIGGRAPH ’96. ACM, New York, pp 21–30" href="/article/10.1007/s10055-010-0186-2#ref-CR28" id="ref-link-section-d39778e565">1996</a>) and Seitz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: IEEE CVPR ’06, pp 519–528" href="/article/10.1007/s10055-010-0186-2#ref-CR29" id="ref-link-section-d39778e568">2006</a>), which uses morphing with stereo camera models to generate in-between views. We are also inspired by the EyeVision System (Kanade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kanade T (2001) Eyevision system at super bowl 2001. &#xA;                    http://www.ri.cmu.edu/events/sb35/tksuperbowl.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0186-2#ref-CR11" id="ref-link-section-d39778e571">2001</a>). While those approaches reconstruct compelling intermediate views, they need precise stereo pairs and local correspondences. Those are not suitable for a practical application where a large amount of videos, from the crowd-resources, need to be registered in real time. Thus, we used global blending approaches to register a number of views and visualize them immediately in the AEMs.</p><p>Object tracking is widely studied in computer vision (Yilmaz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Yilmaz A, Javed O, Shah M (2006) Object tracking: a survey. ACM Comput Surv 38(4):13" href="/article/10.1007/s10055-010-0186-2#ref-CR40" id="ref-link-section-d39778e577">2006</a>). We adopt a low-level feature tracking method, namely KLT feature tracker (Bouguet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bouguet J-Y (2003) Pyramidal implementation of the lucas kanade feature tracker. In: Intel Corporation" href="/article/10.1007/s10055-010-0186-2#ref-CR1" id="ref-link-section-d39778e580">2003</a>; Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE CVPR. IEEE computer society, pp 593–600" href="/article/10.1007/s10055-010-0186-2#ref-CR31" id="ref-link-section-d39778e583">1994</a>). It provides necessary information to compute the position of pedestrians or cars and average velocities reliably and in real time for a large range of videos recorded under varying weather and viewpoint conditions.</p><p>To simulate a large number of moving objects (cars in our traffic example), we employ the flock simulation approach of Reynolds (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Reynolds CW (1987) Flocks, herds and schools: a distributed behavioral model. In: ACM SIGGRAPH 1987. ACM Press, New York, pp 25–34" href="/article/10.1007/s10055-010-0186-2#ref-CR24" id="ref-link-section-d39778e590">1987</a>). In our work, we developed a parameterized version of Reynolds’ steering behavioral model (Reynolds <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Reynolds CW (1999) Steering behaviors for autonomous characters. In: GDC ’99: Proceedings of game developers conference. Miller Freeman Game Group, pp 768–782" href="/article/10.1007/s10055-010-0186-2#ref-CR25" id="ref-link-section-d39778e593">1999</a>). Our behavioral models are dynamically adjusted during the simulation to reflect the incoming traffic information obtained from videos. While there are many approaches for simulating behaviors of autonomous agents, we adopt recent work (Shao and Terzopoulos <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Shao W, Terzopoulos D (2007) Autonomous pedestrians. Graphi Models 69(5):246–274" href="/article/10.1007/s10055-010-0186-2#ref-CR30" id="ref-link-section-d39778e596">2007</a>; Treuille et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Treuille A, Cooper S, Popović Z (2006) Continuum crowds. In: ACM SIGGRAPH 2006 papers, pp 1160–1168" href="/article/10.1007/s10055-010-0186-2#ref-CR34" id="ref-link-section-d39778e599">2006</a>) on crowd simulation, which produces good results reliably across various behavioral models.</p><p>There are a number of approaches for rendering sky and clouds. Harris (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Harris MJ (2005) Real-time cloud simulation and rendering. In: ACM SIGGRAPH 2005 Courses. New York, p. 222" href="/article/10.1007/s10055-010-0186-2#ref-CR7" id="ref-link-section-d39778e605">2005</a>) and Wang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wang N (2004) Realistic and fast cloud rendering. J Graph Tools 9(3):21–40" href="/article/10.1007/s10055-010-0186-2#ref-CR37" id="ref-link-section-d39778e608">2004</a>) introduce methods for rendering realistic clouds using imposters and sprites generated procedurally for real-time games. While extremely realistic, these approaches do not suit our purposes as we are interested in driving clouds from video data. Turk and O’Brien (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Turk G, O’Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH ’99. New York, NY, pp 335–342" href="/article/10.1007/s10055-010-0186-2#ref-CR35" id="ref-link-section-d39778e611">1999</a>) use radial basis function (RBF) interpolation to find an implicit surface from scattered data using constraint points. Building on this, we use RBF to interpolate global distribution of clouds density using video data, in a manner similar to Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen G, Esch G, Wonka P, Müller P, Zhang E (2008) Interactive procedural street modeling. ACM Trans Graph 27(3):1–10" href="/article/10.1007/s10055-010-0186-2#ref-CR3" id="ref-link-section-d39778e614">2008</a>), where it is employed for generating city blocks. We also rely on Perlin noise (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Perlin K (1985) An image synthesizer. SIGGRAPH Comput Graph 19(3):287–296" href="/article/10.1007/s10055-010-0186-2#ref-CR21" id="ref-link-section-d39778e617">1985</a>) for generating clouds volume or sprites for generating cloud maps.</p><p>A closely related system work is the Video Flashlight system (Sawhney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Sawhney HS, Arpa A, Kumar R, Samarasekera S, Aggarwal M, Hsu S, Nister D, Hanna K (2002) Video flashlights: real time rendering of multiple videos for immersive model visualization. In: 13th Eurographics workshop on Rendering. Eurographics Association, pp 157–168" href="/article/10.1007/s10055-010-0186-2#ref-CR26" id="ref-link-section-d39778e623">2002</a>), a surveillance application, which tracks people in a fixed region using multiple cameras and maps the results to the observed region. Several other efforts have also appeared in the same thread and are relevant to our work (Girgensohn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Girgensohn A, Kimber D, Vaughan J, Yang T, Shipman F, Turner T, Rieffel E, Wilcox L, Chen F, Dunnigan T (2007) Dots: support for effective video surveillance. In: ACM MULTIMEDIA ’07. ACM, New York, pp 423–432" href="/article/10.1007/s10055-010-0186-2#ref-CR6" id="ref-link-section-d39778e626">2007</a>; Sebe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sebe IO, Hu J, You S, Neumann U (2003) 3d video surveillance with augmented virtual environments. In: IWVS ’03: First ACM SIGMM international workshop on Video surveillance. ACM, New York, pp 107–112" href="/article/10.1007/s10055-010-0186-2#ref-CR27" id="ref-link-section-d39778e629">2003</a>; Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wang Y, Krum DM, Coelho EM, Bowman DA (2007) Contextualized videos: Combining videos with environment models to support situational understanding. Abdom Imag 13:1568–1575" href="/article/10.1007/s10055-010-0186-2#ref-CR38" id="ref-link-section-d39778e632">2007</a>).</p><p>A worthwhile effort to mention is proposed by Snavely et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Snavely N, Seitz SM, Szeliski R (2006) Photo tourism: exploring photo collections in 3d. In: Proceedings of ACM SIGGRAPH’06. ACM Press, New York, pp 835–846" href="/article/10.1007/s10055-010-0186-2#ref-CR33" id="ref-link-section-d39778e638">2006</a>) to reconstruct 3D scenes from large set of images taken at different views. To some extent, we share similar goals, but we work with videos instead of images and synthesize dynamic information extracted from those videos.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Tracking and geometry extraction</h2><div class="c-article-section__content" id="Sec4-content"><p>We rely on the vast body of works in computer vision to extract geometric information and track the movement of objects from videos. Subsequently, they are registered and projected onto AEMs. We brief the details of each here, and then we will describe variations of these (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec5">5</a>) as we employ them for the different scenarios discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec2">2</a>.</p><p>While most methods used in augmented reality applications need a structure of the real world to find an adequate plane to put a virtual object onto it, our methods do not need to use Structure from Motion (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’07)" href="/article/10.1007/s10055-010-0186-2#ref-CR12" id="ref-link-section-d39778e658">2007</a>). This is because the plane of interest in AEM can be easily defined as a patch of the virtual earth environment. Thus, in our case, to register the tracked objects from video onto the aerial view, we only need to find a homography between video scene and the corresponding observed area in the aerial view. Consider the relationship of two viewpoints <b>f</b>
                        <sub>1</sub> and <b>f</b>
                        <sub>2</sub> with respect to a 3 × 3 homography matrix <b>H</b> such that <b>x</b>′ = <b>Hx</b>, where <b>x</b> and <b>x</b>′ are homogeneous point positions at scenes observed from viewpoints <b>f</b>
                        <sub>1</sub> and <b>f</b>
                        <sub>2</sub> each. Using the matrix <b>H</b>, we can project objects in one view to another view. In our case, the matrix <b>H</b> is a homography induced from a patch in the plane of the Earth Map environment and the region in the screen space of the video frame corresponding to the patch. Then, we can project the location of a point in the video onto the patch in the Earth Map environment. Since the matrix <b>H</b> has 8 DOF, the 8 unknowns can be solved for using the Direct Linear Transform (DLT) with a pair of four points, if the correspondences are exactly matched on both images. To obtain stable results, we normalize the coordinates of correspondences in the same way as to compute the fundamental matrix in Hartley (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hartley RI (1997) In defense of the eight-point algorithm. PAMI Int J Pattern Anal Mach Intell 19(6):580–593" href="/article/10.1007/s10055-010-0186-2#ref-CR8" id="ref-link-section-d39778e708">1997</a>).</p><p>In the video scene, a user can manually assign four points that correspond to a vertex in the texture map by comparing both scenes. Estimating the homography by manually selecting these points may be unreliable because even small difference between points cause unstable results. For this reason, we first estimate the vanishing points (VP) of the video scene. We used an algorithm for estimating vanishing points as described in Kosecka and Zhang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476–490" href="/article/10.1007/s10055-010-0186-2#ref-CR14" id="ref-link-section-d39778e714">2002</a>) shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig3">3</a>. This estimation finds two horizontal vanishing points for the video scene and then the user can select four points by assigning a rectangle area same as the one in aerial view image. But if the estimation algorithm does not find two vanishing points or the estimation result is not exact, we use spidery mesh interface, integrated into our system, to locate the four points (Horry et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Horry Y, Anjyo K-I, Arai K (1997) Tour into the picture: using a spidery mesh interface to make animation from a single image. In: Proceedings of ACM SIGGRAPH, New York, pp 225–232" href="/article/10.1007/s10055-010-0186-2#ref-CR10" id="ref-link-section-d39778e720">1997</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Vanishing point estimation:<b> a</b> Two horizontal vanishing points are estimated using expectation maximization which is used in Kosecka and Zhang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476–490" href="/article/10.1007/s10055-010-0186-2#ref-CR14" id="ref-link-section-d39778e736">2002</a>).<b> b</b> Spidery mesh is applied to define a plane of interest</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Now, we briefly summarize the tracking algorithm used in our work. The same algorithm is used for every scenario with different settings. First, we make a few simplifying assumptions to help us in this directions. We assume that most scenes used in our work are taken from a high vantage point. This allows limited occlusions between moving objects and the tracked objects do not deform too much across views. We do not want to rely on videos from the top view because such videos are hard to obtain. We require the orientation and position of the video camera to be fixed after initialization of the geometry. On the other hand, we do require that the tracking be automatically initialized. For our tracking, at present we are using the well-known KLT trackers (Bouguet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bouguet J-Y (2003) Pyramidal implementation of the lucas kanade feature tracker. In: Intel Corporation" href="/article/10.1007/s10055-010-0186-2#ref-CR1" id="ref-link-section-d39778e754">2003</a>, Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE CVPR. IEEE computer society, pp 593–600" href="/article/10.1007/s10055-010-0186-2#ref-CR31" id="ref-link-section-d39778e757">1994</a>) as an entry step for finding tracks. However, any tracking algorithm can be used if the algorithm meets the conditions above.</p><p>KLT trackers are aimed at determining a motion vector <b>d</b> between a local window <i>W</i> from one frame to a consecutive frame with a minimal residual function ɛ,</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \varepsilon =     \int\!\!\!\int\limits_W {[J({\bf A}x + {\bf d}) - I({{\bf x}})]^2     \omega ({{\bf x}}){\rm d}{{\bf x}}} , $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>I</i> is the image from a video frame and <i>J</i> is the next frame, <b>A</b> an affine parameters of a deformation matrix, ω a weighting function that defines weights in the local window (it can be 1 or a normal distribution). After the tracker finds features to track, we merge groups of features as an object by using deterministic settings (Rabaud and Belongie <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rabaud V, Belongie S (2006) Counting crowded moving objects. In: CVPR ’06: Proceedings of IEEE computer vision and pattern recognition. IEEE Computer Society, pp 17–22" href="/article/10.1007/s10055-010-0186-2#ref-CR22" id="ref-link-section-d39778e793">2006</a>; Veenman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Veenman CJ, Reinders MJT, Backer E (2001) Resolving motion correspondence for densely moving points. IEEE Trans Pattern Anal Mach Intell 23:54–72" href="/article/10.1007/s10055-010-0186-2#ref-CR36" id="ref-link-section-d39778e796">2001</a>). However, in the case of tracking pedestrians in DM (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec6">5.1</a>), where the number of initialization is relatively small, we partly adopt a particle filter (Koller-meier and Ade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Koller-meier EB, Ade F (2001) Tracking multiple objects using the condensation algorithm. JRAS" href="/article/10.1007/s10055-010-0186-2#ref-CR13" id="ref-link-section-d39778e802">2001</a>) to track the ground trajectories.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Video to augmented aerial maps</h2><div class="c-article-section__content" id="Sec5-content"><p>Now we provide some technical details of our approaches and also present how the configurations of cameras and dynamic information described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec2">2</a> are analyzed and then visualized. Specifically, we first start with the simplest scenario, then we introduce more complex situations with different approaches.</p><h3 class="c-article__sub-heading" id="Sec6">Direct mapping from single video: pedestrians</h3><p>Our first scenario is where we capture a video from a single viewpoint and we are interested in projecting it and the related motions onto an aerial view from an AEM. Specifically, we put a virtual characters onto the AEM to visualize pedestrians. This scenario requires direct mapping (DM) of tracked objects in a scene frame onto the virtual plane. This is the basic scenario and an essential building block for all of the other scenario cases described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec2">2</a>. Our approaches here build on previous efforts aimed at surveillance-type applications (Girgensohn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Girgensohn A, Kimber D, Vaughan J, Yang T, Shipman F, Turner T, Rieffel E, Wilcox L, Chen F, Dunnigan T (2007) Dots: support for effective video surveillance. In: ACM MULTIMEDIA ’07. ACM, New York, pp 423–432" href="/article/10.1007/s10055-010-0186-2#ref-CR6" id="ref-link-section-d39778e826">2007</a>; Sawhney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Sawhney HS, Arpa A, Kumar R, Samarasekera S, Aggarwal M, Hsu S, Nister D, Hanna K (2002) Video flashlights: real time rendering of multiple videos for immersive model visualization. In: 13th Eurographics workshop on Rendering. Eurographics Association, pp 157–168" href="/article/10.1007/s10055-010-0186-2#ref-CR26" id="ref-link-section-d39778e829">2002</a>; Sebe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sebe IO, Hu J, You S, Neumann U (2003) 3d video surveillance with augmented virtual environments. In: IWVS ’03: First ACM SIGMM international workshop on Video surveillance. ACM, New York, pp 107–112" href="/article/10.1007/s10055-010-0186-2#ref-CR27" id="ref-link-section-d39778e832">2003</a>; Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wang Y, Krum DM, Coelho EM, Bowman DA (2007) Contextualized videos: Combining videos with environment models to support situational understanding. Abdom Imag 13:1568–1575" href="/article/10.1007/s10055-010-0186-2#ref-CR38" id="ref-link-section-d39778e835">2007</a>).</p><p>We rely on direct mapping to visualize pedestrians in videos. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig4">4</a>, we first track pedestrians and extract screen-space coordinates and velocities. These measurements are directly registered onto the plane space of the virtual environment. If the homogeneous coordinates in a video frame are <span class="mathjax-tex">\({\bf p_{x,y}=\left[x,y,1\right]^{T}}\)</span>, the new 2D location at planar space on virtual environment <span class="mathjax-tex">\(\hat{{\bf p}}\)</span> is simply calculated by <span class="mathjax-tex">\(\hat{{\bf p}}={\bf H}{\bf p_{x,y}}\)</span>. Subsequently, if the objects (pedestrians) are moving in the video with the velocity <b>v</b>, we can also project the velocity onto the earth map plane by <span class="mathjax-tex">\(\hat{{\bf v}}= {\bf H}{\bf v_{x,y}}\)</span>. This velocity is used to match simple motion data gathered off-line to visualize moving pedestrians. We used motion capture data from (CMU Graphics Lab Motion Capture Database <a href="http://mocap.cs.cmu.edu/">http://mocap.cs.cmu.edu/</a>) to generate similar character animations for our stand-in avatars. We first sample the trajectories of objects, then insert exactly one cycle of walking data onto them and interpolate the positions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Example of DM:<b> a</b> Pedestrians are tracked from single video.<b> b</b> Tracked pedestrians are matched to motion capture data, then mapped onto virtual plane space, then rendered only within a coverage of the given field of view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>There is no accounting for visualizing movements that are outside the camera view and we are dependent on tracking approaches. This can be problematic when we have crowds in the scene and tracking fails due to occlusion. Some of these issues are addressed in the other scenarios, discussed next.</p><p>Note that at present, we are not interested in classifying objects or recognizing their states in the scene, which is beyond the scope of this work. In this scenario, we assume moving objects on a sidewalk are only pedestrians and they are engaged in simple walking motion and other similarly simple linear actions. On the road, we can assume the object of interest is a car.</p><p>In summary, DM can be used when (1) a region of interest covered by a single viewpoint (2) the motions of objects have to be simple. In Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec7">5.2</a> to <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec10">5.4</a>, we will introduce methods to handle more complex situation in different scenarios.</p><h3 class="c-article__sub-heading" id="Sec7">Overlapping cameras, complex motions: sports</h3><p>We now move to the domain where we have overlapping views and motions that have some structure, but there are several motions at the same time. While we have employed this case for a variety of scenarios, we are going to demonstrate this in the domain of sports (other examples are also shown in video and in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>c). Sport is an interesting domain as we usually do have multiple views and in most instances we can rely on the field markings to help with registration. The overlapping views in this domain also require additional types of modeling and synthesis beyond the direct mapping from a single view (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec6">5.1</a>). Particularly, we also need to employ techniques to support view blending as the viewpoints are changed from one to the other, as we visualize the AEMs.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Observations and estimation</h4><p>We start by obtaining field of views (FOVs) <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub> and camera homographies <b>H</b>
                              <sub>
                      <i>i</i>
                    </sub> (from each view to a corresponding patch in the virtual plane) from the videos as described earlier. Then, the videos are rectified to top-views based on the extracted homographies and registered onto the corresponding regions of the virtual earth environment. Additionally, we also calculate camera locations via calibration. We rely on the fact that (1) we have multiple videos, and (2) ground yard commonly provides good features, <i>e.g</i>., lines and known distances on the field.</p><p>Once the videos are registered, the rectified top views are used as a texture on the AEM plane. Then, this textured video is re-projected to a virtual view based on the model view matrix in the AEM environment. We refer to this view as <i>Back-projected view</i> and the angle between the original and the back-projected views as θ. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig5">5</a>a shows the relationship between back-projected virtual view <b>f</b>
                              <sub>
                      <i>v</i>
                    </sub>, rectified view <span class="mathjax-tex">\(\hat{{\bf f}}_{i}\)</span>, and original video <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p><b>a</b> Virtual view <b>f</b><sub>
                              <i>v</i>
                            </sub>, Sample view <b>f</b><sub>
                              <i>i</i>
                            </sub> and the angle between them θ.<b> b</b> Two consecutive views <b>f</b><sub>
                              <i>i</i>
                            </sub>, <b>f</b><sub><i>i</i>+1</sub>, Back-projected virtual view <b>f</b><sub>
                              <i>v</i>
                            </sub> and blended rectified view <span class="mathjax-tex">\(\hat{{\bf f}}_{v}\)</span>. Note that θ<sub>
                              <i>i</i>
                            </sub> is an angle between <b>f</b><sub>
                              <i>i</i>
                            </sub> and <b>f</b><sub>
                              <i>v</i>
                            </sub> and θ<sub><i>i</i>+1</sub> is an angle between <b>f</b><sub><i>i</i>+1</sub> and <b>f</b><sub>
                              <i>v</i>
                            </sub>.<b> c</b> Source video having view <b>f</b><sub>
                              <i>i</i>
                            </sub>.<b> d</b> Back-projected virtual view <b>f</b><sub>
                              <i>v</i>
                            </sub>.<b> e</b> Rectified view <span class="mathjax-tex">\(\hat{{\bf f}}_{v}\)</span>: Note that<b> c</b> and<b> d</b> look almost similar view since the θ is almost zero</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">View synthesis and blending</h4><p>Now, we address the issue of view synthesis as we move from one view to another within the AEM environment. Past approaches use billboard-style methods that change the orientation of the original view so that it always look at the virtual view (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wang Y, Krum DM, Coelho EM, Bowman DA (2007) Contextualized videos: Combining videos with environment models to support situational understanding. Abdom Imag 13:1568–1575" href="/article/10.1007/s10055-010-0186-2#ref-CR38" id="ref-link-section-d39778e1189">2007</a>). While commonly used in practice, this approach only shows rough segment of video streams at a given 3D environment. To visualize sports games, LiberoVision system (LiberoVision, <a href="http://www.liberovision.com/">http://www.liberovision.com/</a>) uses segmentation of players and foreground warping. But it requires extensive batch-process time to make realistic view transition and only used in static scene. Seitz (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Seitz SM, Dyer CR (1996) View morphing. In: SIGGRAPH ’96. ACM, New York, pp 21–30" href="/article/10.1007/s10055-010-0186-2#ref-CR28" id="ref-link-section-d39778e1199">1996</a>) proposed view morphing methods where they reconstruct virtual views from the a pair of images under the assumption that the cameras have well-calibrated epipolar geometry. This can be done when we have precise stereo pairs and have exact pair sets of local correspondence. Thus, it is not the case for us where the camera center of the virtual views is often out of the epipolar line and finding the correspondences at every frame is computationally demanding (if not impossible).</p><p>We propose a more flexible global view blending method that can incorporate views and their subtracted backgrounds with overlapping regions and does not require an extensive pre/post-processing. This approach is adequate for the scenario of our system in which the rapid registration of crowd-sourced videos is needed.</p><p>Once multiple views covering the same region are registered onto the AEM plane, their rectified views also overlap. Our goal is to generate virtual views based on the two consecutive views that exhibit the most similar viewing angle θ. First, we search for the pair of the closest two views exhibiting small θ’s. Let these consecutive views and the corresponding angles be denoted by <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub>, <b>f</b>
                              <sub><i>i</i>+1</sub> and θ<sub>
                      <i>i</i>
                    </sub>, θ<sub><i>i</i>+1</sub>, respectively (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig5">5</a>b). Then, we compute the two blending weights for both views based on the angle differences giving larger weight to the smaller angle: <span class="mathjax-tex">\(\omega_{i}= \frac{\theta_{i+1}}{\theta_{i}+\theta_{i+1}}\)</span> and <span class="mathjax-tex">\(\omega_{i+1}= \frac{\theta_{i}}{\theta_{i}+\theta_{i+1}}\)</span>. If the ω<sub>
                      <i>i</i>
                    </sub> is close to one, which indicates that the angle θ<sub>
                      <i>i</i>
                    </sub> becomes zero, so that the viewing vector of virtual view and the given view <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub> are almost identical. This is the case where the virtual view is almost similar to <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub>, so that the virtually back-projected scene seems approximately invariant to distortions (See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig5">5</a>c, d). Even when ω<sub>
                      <i>i</i>
                    </sub> is less than 1.0, the distortion of the back-projected view is still negligible within some ranges (noted as gray arrows in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig7">7</a>c). This example is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The range of approximately invariant to distortion:<b> a</b> and<b> b</b> both are back-projected scenes from same video (notice that within small change of viewing angle it still looks reasonable)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>View blending with different interpolations where ω<sub>
                              <i>i</i>
                            </sub> is 0.6:<b> a</b> Linear<b> b</b> Bicubic<b> c</b> Bicubic and Background blending: <i>f</i> is <i>f</i>(ω<sub>
                              <i>i</i>
                            </sub>), <i>g</i> is <i>g</i>(ω<sub><i>i</i>+1</sub>) and <i>h</i> is <span class="mathjax-tex">\(\omega_{{\rm bkg}}\)</span> which is a weight for subtracted background image.<b> d</b>–<b>f</b> The back-projected views based on each interpolation. The<i> gray arrows</i> in<b> c</b> show the ranges of ω where the distortion is minimized, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig6">6</a>a, b</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In case we cannot find a very similar view, we use interpolation and blending approach based on the weights computed from the angles between two adjacent views. In blending approach, if we blend <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub>, <b>f</b>
                              <sub><i>i</i>+1</sub> using a linear interpolation, the blended region near the moving objects suffer from the ghosting artifacts, especially when both ω<sub>
                      <i>i</i>
                    </sub> and ω<sub><i>i</i>+1</sub> become near 0.5, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig7">7</a>d for an example. This is because the virtual view <b>f</b>
                              <sub>
                      <i>v</i>
                    </sub> is placed out of range from both <b>f</b>
                              <sub><i>i</i>+1</sub> and <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub>, and the camera center is out of the line <span class="mathjax-tex">\(\overleftrightarrow{{\bf C_{i}}{\bf C_{i+1}}}\)</span> in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig5">5</a>b. Even if we use bicubic interpolation, the same problem occurs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig7">7</a>e) although view transition becomes smoother.</p><p>In our solution, we blend not only a pair of rectified scenes (<span class="mathjax-tex">\(\hat{{\bf f}}_{i}\)</span>,<span class="mathjax-tex">\(\hat{{\bf f}}_{i+1}\)</span>) but also the subtracted background scenes generated from a pair of view videos. Now, suppose that <i>f</i>(<i>t</i>) and <i>g</i>(<i>t</i>) is a bicubic function for both views as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig7">7</a>c. Then, we blend each pixel in the virtual view as follows:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf p}_{v} = f(\omega_{i}){\bf p}_{i}+g(\omega_{i}){\bf p}_{i+1}+ \omega_{{\rm bkg}}{\bf p}_{{\rm bkg}} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(\omega_{{\rm bkg}}=1-(f(\omega_{i})+g(\omega_{i+1}))\)</span>, <span class="mathjax-tex">\({\bf p}_{{\rm bkg}} = \omega_{i}{\bf p}_{i}^{{\rm bkg}}+\omega_{i+1}{\bf p}_{i+1}^{{\rm bkg}}\)</span>, and <span class="mathjax-tex">\({\bf p}_{i}^{{\rm bkg}}\)</span> and <span class="mathjax-tex">\({\bf p}_{i+1}^{{\rm bkg}}\)</span> are the pixels of the subtracted background computed separately from <span class="mathjax-tex">\(\hat{{\bf f}}_{i}\)</span> and <span class="mathjax-tex">\(\hat{{\bf f}}_{i+1}\)</span>, respectively (See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig7">7</a>c).</p><p>Now, the virtual view is almost identical to the background if the target view is out of range from the both views. If the target view approaches a view to some extent, the synthesized view smoothly transit to the back-projected view of the closest viewpoint. Examples of smooth transition according to the change of virtual viewpoint is shown in the video. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec10">5.3</a>, we introduce an approach to visualize moving objects when multiple videos are not overlapped and each camera is distributed sparsely.</p><h3 class="c-article__sub-heading" id="Sec10">Sparse cameras with simple motion: traffic</h3><p>We demonstrate this scenario in the setting of analyzing videos of traffic and synthesizing traffic movements dynamically on AEMs. We are working in this domain following the ever-growing number of traffic cameras being installed all over the world for traffic monitoring. While the videos we are using are not recorded by actual traffic cameras installed by a department of transportation or the city government, our views are practically similar. The biggest technical challenge with this scenario is that as we have sparsely distributed, non-overlapping cameras, we do not have the advantage of knowing how the geometry or the movement from each camera is related to the other. While the topology of the camera networks still needs to be specified by user input, we do want to undertake the analysis of both registration of views and movement as automatically as possible.</p><p>To deal with the fact that we do not have any measurements in between the camera views, we also need to model the movements in each view in general and connect the observations between cameras, i.e. to model the flow from one view to another. To achieve this, in our framework, we add two functionalities. (1) Modeling the flow using a graph-based representation. This allows using the information obtained from observable regions and propagate it to infer a plausible traffic flow across unobservable regions. (2) Develop a synthesis framework that can be driven from such data. We employ a behavior-based animation approach to synthesize flow across view and onto the AEM. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig8">8</a>a shows the topology of traffic nodes. Each node is a patch of the road and has a traffic state <i>X</i>
                           <sub>
                    <i>i</i>
                  </sub> and a synthesized state <i>Z</i>
                           <sub>
                    <i>i</i>
                  </sub>. A measurement <i>Y</i>
                           <sub>
                    <i>i</i>
                  </sub> is defined only in observable nodes monitored by videos.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Graph-based representation of traffic nodes:<i> Red nodes</i> indicate observable region <b>M</b>(<b>X</b>) and<i> Green nodes</i> are unobserved regions <span class="mathjax-tex">\( { \tilde{\bf M}}({\bf X})\)</span>.<b> a</b> The middle chain corresponds to the traffic conditions on the graph which represents the traffic system. Node index <i>i</i> is 0 to 2 in this example<b>. b</b> Split: outgoing regions <b>O</b>(<b>X_i</b>) from node <i>X</i>
                                       <sub>
                            <i>i</i>
                          </sub> are marked.<b> c</b> Merging: incoming regions <b>I</b>(<b>X_i</b>) to node <i>X</i>
                                       <sub>
                            <i>i</i>
                          </sub> are marked (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Observation and measurements</h4><p>Using the tracking approach mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0186-2#Sec4">4</a>, we can get estimates of position and velocities of objects from the video. The measurement of <i>i</i>th node <i>Y</i>
                              <sub>
                      <i>i</i>
                    </sub> consists of the positions and the velocities of the low-level motion features and the detected cars in the corresponding video frames. Once the low-level features within an observable node <i>i</i> are obtained, they are merged to form a set of objects <span class="mathjax-tex">\({\bf r}_{i}=\{r_{i,j}\} \forall j \)</span>—cars, with <i>j</i> denoting an index of each car, based on the similarity and the connectivity formed from the features using deterministic approach (Rabaud and Belongie <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rabaud V, Belongie S (2006) Counting crowded moving objects. In: CVPR ’06: Proceedings of IEEE computer vision and pattern recognition. IEEE Computer Society, pp 17–22" href="/article/10.1007/s10055-010-0186-2#ref-CR22" id="ref-link-section-d39778e1799">2006</a>; Veenman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Veenman CJ, Reinders MJT, Backer E (2001) Resolving motion correspondence for densely moving points. IEEE Trans Pattern Anal Mach Intell 23:54–72" href="/article/10.1007/s10055-010-0186-2#ref-CR36" id="ref-link-section-d39778e1802">2001</a>). Experimental results demonstrate that it is sufficient for visualization and meet the overall purpose of the system. In summary, the measurement is defined to be the pair of the information on the low-level features <b>f</b>
                              <sub>
                      <i>i</i>
                    </sub> and the detected cars <b>r</b>
                              <sub>
                      <i>i</i>
                    </sub>: <span class="mathjax-tex">\(Y_{i}=\{{\bf f}_{i},{\bf r}_{i}\}\)</span>.</p><p>The entire traffic system is also denoted as <span class="mathjax-tex">\({\bf X}=\{X_{i}|1\leq i\leq k_{{\bf X}}\}\)</span>, where <i>k</i>
                              <sub>
                      <b>X</b>
                    </sub> is the number of nodes. Additionally, the physical length of every <i>i</i>th node is obtained from the available geo-spatial database and is denoted by <i>d</i>
                              <sub>
                      <i>i</i>
                    </sub>. Once the traffic system is decomposed into a graph manually, a set of <i>observable</i> regions <span class="mathjax-tex">\({\bf M}({\bf X})\subset{\bf X}\)</span> with available video measurements are identified. On the other hand, the <i>unobservable</i> regions are denoted by <span class="mathjax-tex">\({ \tilde{\bf M}}({\bf X})\)</span>, where <span class="mathjax-tex">\({\bf X} = {\bf M}({{\bf X}}) \cup { \tilde{\bf M}}({\bf X})\)</span>. Note that every measurement <i>Y</i>
                              <sub>
                      <i>i</i>
                    </sub> is a set of low-level information computed from a video, rather than being the raw video itself.</p><p>The state <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> is designed to capture the essential information necessary to visualize traffic flow: (1) the rate of traffic flow <i>n</i>
                              <sub>
                      <i>i</i>
                    </sub>, and (2) the average velocity <i>v</i>
                              <sub>
                      <i>i</i>
                    </sub> of cars, i.e., <span class="mathjax-tex">\(X_{i}=(n_{i},v_{i})\)</span>. By average flow, we mean the average number of cars passing through a region in unit time, whereas <i>v</i>
                              <sub>
                      <i>i</i>
                    </sub> denotes the average velocity of the cars.</p><p>The obtained measurement information <i>Y</i>
                              <sub>
                      <i>i</i>
                    </sub> is used to estimate an observable state <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> ∈ <b>M</b>(<b>X</b>), after the projection onto the virtual map plane using the available homography <b>H</b>
                              <sub>
                      <i>i</i>
                    </sub> for that region. First, the average speed <span class="mathjax-tex">\(\hat{v}_{i}\)</span> of the cars is estimated as an average of projected speeds of the low-level features w.r.t. the homography <b>H</b>
                              <sub>
                      <i>i</i>
                    </sub>. Secondly, the flow of the cars passing through the <i>i</i>th region, <span class="mathjax-tex">\(\hat{n}_{i}\)</span>, can be computed using the fact that the number of cars in the region <span class="mathjax-tex">\(N_{{\bf r}_{i}}\)</span> is the product of the flow multiplied by the average time <span class="mathjax-tex">\(d_{i}/v_{i}\)</span> for cars to pass a region, i.e., <span class="mathjax-tex">\(N_{{\bf r}_{i}} =\hat{n}_{i}\cdot(d_{i}/\hat{v}_{i})\)</span>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Modeling from data: spatial correlation to infer missing data</h4><p>Once the set of states <span class="mathjax-tex">\({\bf M}({\hat{{\bf X}}})\)</span> are estimated for the observable regions, they are used to estimate the unobserved states <span class="mathjax-tex">\({ \tilde{\bf M}}({\bf X})\)</span>. We adopt the Bayesian networks (Pearl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Pearl J (1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, Massachusetts" href="/article/10.1007/s10055-010-0186-2#ref-CR20" id="ref-link-section-d39778e2106">1988</a>) formalism to exploit the fact that the unknown traffic conditions <span class="mathjax-tex">\({ \tilde{\bf M}}({\bf X})\)</span> can be estimated by propagating the observed information from the spatial correlation models. The whole traffic graph is a directed graph where an edge from a region <i>X</i>
                              <sub>
                      <i>j</i>
                    </sub> to another region <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> exists whenever traffic can move from <i>X</i>
                              <sub>
                      <i>j</i>
                    </sub> to <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>. For every node <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>, a local spatial model <span class="mathjax-tex">\(P(X_{i}|{\bf I}(X_{i}))\)</span> between the node <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> and the incoming nodes <b>I</b>(<i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>) is specified (See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig8">8</a>). Once all the local spatial models are defined, the posterior traffic conditions <span class="mathjax-tex">\(P(X_{i}|{\bf M}({\hat{X}}_{i})),\forall X_{i}\in{ \tilde{\bf M}}({\bf X})\)</span> at the unobserved nodes are inferred using belief propagation (Frey and MacKay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Frey B, MacKay D (1998) A revolution: belief propagation in graphs with cycles. In: Neural information processing systems, pp 479–485" href="/article/10.1007/s10055-010-0186-2#ref-CR5" id="ref-link-section-d39778e2214">1998</a>, Pearl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Pearl J (1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, Massachusetts" href="/article/10.1007/s10055-010-0186-2#ref-CR20" id="ref-link-section-d39778e2218">1988</a>).</p><p>In detail, we make an assumption that the average flow <span class="mathjax-tex">\(X_{i}|_{n}\)</span> of the cars in a region <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> matches the sum of the average flow of the cars from the incoming regions <b>I</b>(<i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>) with a slight variation <i>w</i>
                              <sub>
                      <i>i</i>
                    </sub> which follows white Gaussian noise: <span class="mathjax-tex">\(X_{i}|_{n}=\sum_{j\in {\bf I}(X_{i})}X_{j}|_{n}+w_{i}\)</span>. For velocity, we assume that the average speed in a region matches the average speed of the cars with a slight variation <i>q</i>
                              <sub>
                      <i>i</i>
                    </sub> which is again a white Gaussian noise: <span class="mathjax-tex">\(X_{i}|_{v}=(\sum_{j\in {\bf I}(X_{i})}X_{j}|_{v})/ N_{{\bf I}(X_{j})}+q_{i}\)</span>. The variance of the Gaussian noises, both <i>w</i>
                              <sub>
                      <i>i</i>
                    </sub> and <i>q</i>
                              <sub>
                      <i>i</i>
                    </sub>, are set to be proportional to the length <i>d</i>
                              <sub>
                      <i>i</i>
                    </sub> of the target region <i>i</i>.</p><p>The above assumptions are applied in case the road configuration is straight or merging (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig8">8</a>a,c). For the case of splitting (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig8">8</a>b), we make a different assumption on the dependencies for the flow <span class="mathjax-tex">\(X_{i}|_{n}\)</span>. Let us denote the outgoing regions (children) of a node <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> by <b>O</b>(<i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>). To maintain a tractable solution for the overall inference phase using belief propagation, the dependencies <span class="mathjax-tex">\(P(X_{j}|X_{i})\)</span> between the parent node <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> and every child node <span class="mathjax-tex">\(X_{j}\in{\bf O}(X_{i})\)</span> are encoded individually in a Gaussian form. The new assumption is that there is a known ratio on the traffic portion <span class="mathjax-tex">\(0\leq\alpha_{j}\leq1\)</span> (<span class="mathjax-tex">\(\sum_{\forall j}\alpha_{j}=1\)</span>) flowing from the parent node <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub> to an outgoing node <i>X</i>
                              <sub>
                      <i>j</i>
                    </sub>. Then, the individual functional dependency is modeled as follows: <span class="mathjax-tex">\(X_{j}|_{n}=\alpha_{j}X_{i}|_{n}+u_{j}\)</span> where a slight white Gaussian noise is denoted by <i>u</i>
                              <sub>
                      <i>j</i>
                    </sub>, which is again proportional to the region length <i>d</i>
                              <sub>
                      <i>j</i>
                    </sub>. Note that the flow estimate at an outgoing region <span class="mathjax-tex">\(X_{j}|_{n}\)</span> is estimated based on not only the propagated information <span class="mathjax-tex">\(\alpha_{j}X_{i}\)</span> from its parent, but also on the information propagated back from its descendants in the directed traffic region graph.</p><p>The posteriors <span class="mathjax-tex">\(P(X_{i}|{\bf M}({\hat{X}}_{i}))\)</span> for the unobserved regions are concise Gaussians as all the spatial correlations are Gaussians. In the simplest case, e.g., straight configurations, the maxima (mean) of the resulting posterior estimates <span class="mathjax-tex">\(\hat{X}_{i}\)</span> is analogous to the linear interpolation of the nearest observable regions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Synthesis: parameterized traffic simulation</h4><p>To visualize traffic flow based on the estimated traffic states <span class="mathjax-tex">\(\hat{{\bf X}}\)</span>, we developed a parameterized version of Reynolds’ behavior simulation approach (Reynolds <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Reynolds CW (1987) Flocks, herds and schools: a distributed behavioral model. In: ACM SIGGRAPH 1987. ACM Press, New York, pp 25–34" href="/article/10.1007/s10055-010-0186-2#ref-CR24" id="ref-link-section-d39778e2541">1987</a>) where we adopted the steering behavior (Reynolds <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Reynolds CW (1999) Steering behaviors for autonomous characters. In: GDC ’99: Proceedings of game developers conference. Miller Freeman Game Group, pp 768–782" href="/article/10.1007/s10055-010-0186-2#ref-CR25" id="ref-link-section-d39778e2544">1999</a>) for our implementation. By parameterized behavior simulation, we mean that the cars are controlled by the associated behavior-based controller, but the behaviors are parameterized by the current traffic condition. The controller of a car in the <i>i</i>-th region is parameterized by <i>X</i>
                              <sub>
                      <i>i</i>
                    </sub>. Hence, the behavior of a car varies if the estimated traffic condition within a region changes or if the car moves onto adjacent traffic regions.</p><p>The flock simulation status within the <i>i</i>-th region is denoted by <span class="mathjax-tex">\(Z_{i}=\{s_{i,k}\}_{k=1}^{N_{Z_{i}}}\)</span> which comprises of a set of simulated cars <i>s</i>
                              <sub><i>i</i>,<i>k</i></sub> with corresponding position <span class="mathjax-tex">\(s_{i,k}|_{p}\)</span> and velocity <span class="mathjax-tex">\(s_{i,k}|_{v}\)</span> attributes, where <span class="mathjax-tex">\(N_{Z_{i}}\)</span> denotes the total number of cars. The synthesis of the flock status <i>Z</i>
                              <span class="c-stack">
                      <sup><i>t</i></sup><sub>
                        <i>i</i>
                      </sub>
                      
                    </span> at time <i>t</i> should be consistent with both the already existing cars from the past scene <i>Z</i>
                              <span class="c-stack">
                      <sup><i>t</i>−1</sup><sub>
                        <i>i</i>
                      </sub>
                      
                    </span> and the newly added cars coming in from the virtual scenes corresponding to the incoming regions {<i>Z</i>
                              <sub><i>i</i> ∈ <b>I</b>(<i>j</i>)</sub>
                              <sup><i>t</i>−1</sup>}. In terms of the adjustments of the speed <span class="mathjax-tex">\(s_{i,k}|_{v}\)</span> between the frames, the cars adjust their speeds to meet the estimated average speed of the region <span class="mathjax-tex">\(\hat{X}_{i}|_{v}\)</span> where their accelerations are set in such a way that the speeds are approximately linearly interpolated within the region. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig9">9</a> shows an example of successful simulation in (a) normal and (b) heavily loaded traffic.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Simulated cars:<b> a</b> Each object (car) acts as an autonomous agent controlled by (1) local reaction based on their own perception range, (2) global dynamics extracted from video.<b> b</b> Cars reaction when average speed of upper-left corner is forced to be very slow, while other corners are controlled by real data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec12">Sparse cameras with complex motion: clouds</h3><p>Our final scenario is aimed at using videos of natural phenomenon, primarily clouds and adding them to AEMs for an additional sense of reality. This is a hardest case, since clouds consist of particles, thus it does not have a specific shape. Moreover, the coverage of field of view from each observing camera is relatively small compared to the region displayed as sky. In this scenario, we use video of moving clouds to extract procedural models of motion, which are then interpolated and rendered onto the AEM, with a compelling effect. Some of the AEMs are now moving towards rendering clouds from satellite imagery and our work is similar to their goals, but we use sparse video and use procedural modeling to generate the clouds, rather than playback what was captured directly.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Observation and extraction</h4><p>In this scenario, cameras are spatially distributed and only a small subset of the targeted sky area that is to be synthesized is visible by the FOVs of the cameras. The entire sky scene is synthesized by: (1) the extracted local clouds density within the observed regions based on the video, (2) the interpolated cloud density information within the unobserved regions from the radial basis function (RBF) technique, and (3) entire cloud imagery synthesis based on the cloud template obtained by procedural synthesis techniques and the computed cloud density map. For measuring dynamic movement of clouds, we also extract velocities from videos. We assume that the video used to extract velocity is always taken by camera having 90 degree of elevation, and zero degree azimuth. We call this video an <i>anchor video</i>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Cloud density interpolation from multiple sky videos</h4><p>We use a radial basis function (RBF) (Buhmann and Ablowitz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Buhmann MD, Ablowitz MJ (2003) Radial basis functions: theory and implementations. Cambridge University, Cambridge" href="/article/10.1007/s10055-010-0186-2#ref-CR2" id="ref-link-section-d39778e2755">2003</a>) to globally interpolate density of clouds in unobserved sky region based on multiple videos. The main concept of our interpolation follows a method described in Turk and O’Brien (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Turk G, O’Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH ’99. New York, NY, pp 335–342" href="/article/10.1007/s10055-010-0186-2#ref-CR35" id="ref-link-section-d39778e2758">1999</a>). They interpolate an implicit surface from a given set of scattered data points. We use this method to interpolate density of unobservable region in the sky using densities extracted from given set of clouds videos. In our work, constraint points are the location of feature points(<span class="mathjax-tex">\(x_i, y_i\)</span>) extracted from each input video, and the basis vector is defined as <span class="mathjax-tex">\({\bf G}=\left[G_{1}(x_{1},y_{1}),\ldots,G_{n}(x_{n},y_{n})\right]^{\top}\)</span> encoded by strong gradient and velocity vectors representing density of clouds. Now, a basis function <i>d</i>
                              <sub>
                      <i>ij</i>
                    </sub> between any constraints points is chosen as <span class="mathjax-tex">\(||(x_{i}-x_{j})^{2}+(y_{i}-y_{j})^{2}||\)</span>.</p><p>Using these measurements, we can globally interpolate the cloud density of any points in unobserved sky region by weighted sum of basis function. The weights for the interpolated density is obtained by solving following linear system:
</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[ \begin{array}{ccccccc} d_{11} &amp; \cdots &amp; d_{1n} &amp; 1 &amp; x_{1} &amp; y_{1}\\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ d_{n1} &amp; &amp; d_{nn} &amp; 1 &amp; x_{n} &amp; y_{n}\\ 1 &amp; \cdots &amp; 1 &amp; 0 &amp; 0 &amp; 0\\ x_{1} &amp; \cdots &amp; x_{n} &amp; 0 &amp; 0 &amp; 0\\ y_{1} &amp; \cdots &amp; y_{n} &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\left[ \begin{array}{c} \lambda_{1}\\ \vdots\\ \lambda_{n}\\ c_{1}\\ c_{2}\\ c_{3} \end{array}\right]=\left[\begin{array}{c} G_{1}\\ \vdots\\ G_{n}\\ 0\\ 0\\ 0 \end{array}\right] $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                           <p>where the λ<sub>
                      <i>i</i>
                    </sub> are weights, and <i>c</i>
                              <sub>
                      <i>i</i>
                    </sub> is a degree one polynomial that accounts for the linear and constant portions of density. More details of underlying basis for RBF interpolation is described at Turk and O’Brien (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Turk G, O’Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH ’99. New York, NY, pp 335–342" href="/article/10.1007/s10055-010-0186-2#ref-CR35" id="ref-link-section-d39778e2838">1999</a>).</p><p>Now, the generated density map is used as a density function for procedural modeling of cloud textures. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig10">10</a>a, b shows an example of density map generated from four videos. However, if the generated density map is not appropriate due to mis-detection of feature vectors, the system provides an user interface to edit the density map by adding additional basis vectors.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Generating clouds layers procedurally using videos:<b> a</b> 4 input videos (<i>green region</i> is an unobserved region).<b> b</b> Globally interpolated density map from RBF. <b>c</b> Resultant cloud layer. <b>d</b> Registered clouds in the sky dome in Earth Map environment  (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Synthesis: procedurally generated clouds</h4><p>A procedural texture is a synthetic image generated based on random functions (Lewis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Lewis JP (ed) (1989) Algorithms for solid noise synthesis" href="/article/10.1007/s10055-010-0186-2#ref-CR16" id="ref-link-section-d39778e2889">1989</a>; Perlin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Perlin K (1985) An image synthesizer. SIGGRAPH Comput Graph 19(3):287–296" href="/article/10.1007/s10055-010-0186-2#ref-CR21" id="ref-link-section-d39778e2892">1985</a>). We used the 2D version of the approach suggested by (Man <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Man P (2006) Generating and real-time rendering of clouds. In: Central European seminar on computer graphics, pp 1–9" href="/article/10.1007/s10055-010-0186-2#ref-CR18" id="ref-link-section-d39778e2895">2006</a>) to synthesize a 2D cloud template. In detail, the function <i>f</i> below is used to generate a cloud template containing a set of weighted sub-templates at multiple scales. A random number generator <i>g</i> is applied to each scale with different seeds in such a way that the details of each particular template increases as follows:
</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$  f(x,y) = \left|\sum_{i=0}^{K-1}P^{i}\cdot g\left(2^{i}\cdot x,2^{i}\cdot y\right)\right|\hbox { where }P\subset [0,1]  $$</span></div></div><p>where <i>K</i> is the number of scales and <i>P</i> denotes the persistence constant. Note that the sub-template for the lower scale having smoother characteristics has larger weight compared to the sub-template for the higher scale. While there are many options for the random number generator <i>g</i>, we used the function suggested by Man (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Man P (2006) Generating and real-time rendering of clouds. In: Central European seminar on computer graphics, pp 1–9" href="/article/10.1007/s10055-010-0186-2#ref-CR18" id="ref-link-section-d39778e2925">2006</a>).</p><p>We also generate multiple cloud templates with different persistent levels and seeds for realistic sky movement. Once cloud textures are generated, we make additional layers having different effects to generate realistic sky animation. Similar approaches are in use for real-time sky-rendering systems (Harris <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Harris MJ (2005) Real-time cloud simulation and rendering. In: ACM SIGGRAPH 2005 Courses. New York, p. 222" href="/article/10.1007/s10055-010-0186-2#ref-CR7" id="ref-link-section-d39778e2931">2005</a>; Wang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wang N (2004) Realistic and fast cloud rendering. J Graph Tools 9(3):21–40" href="/article/10.1007/s10055-010-0186-2#ref-CR37" id="ref-link-section-d39778e2934">2004</a>). The sun mask and glow mask are generated based on the time (i.e., At noon, sun would be appeared on the center of texture). A sky map is used to make color of sky to be brighter if the location is near the ground. Then, the finally generated sky textures are mapped onto the sky domes (Zotti and Groller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Zotti G, Groller ME (2005) A sky dome visualisation for identification of astronomical orientations. In: INFOVIS ’05. IEEE Computer Society, Washington, DC, p 2" href="/article/10.1007/s10055-010-0186-2#ref-CR41" id="ref-link-section-d39778e2937">2005</a>). To visualize sky, representing a dynamic of current sky, the mapped sky textures moves based on the velocity captured from anchor video.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Discussion of experiments and results</h2><div class="c-article-section__content" id="Sec16-content"><p>To validate our approaches, we undertook a series of experiments for different scenarios on a variety of challenging domains, under varying conditions. For the direct mapping scenario, cars and pedestrians are animated based on single videos where we used available motion capture data to animate individual pedestrians. For the view blending examples of sports and other instances, videos are collected from three different testing locations where a total of 10 cameras were used. For the football game visualization in the stadium, we used the video footage provided by the Georgia Tech Athletic Association (made available for research purposes). For applications of traffic visualization, 6 different data sets are collected with number of video feeds varying from 2 to 8 each, where 3 data sets were captured at an identical site but at different time and under different weather conditions. Finally, for the cloud visualization, we collected four different data sets where 3 to 6 cameras were used each. For each cloud data set, a separate anchor video was captured to measure the velocity of clouds. Since a variety of the above experiments are showcased in the enclosed video, we would briefly highlight the results in this section.</p><p>The prototype system is developed using C++/OpenGL on a computer with Quad-core 2.5 GHz, 2 GB RAM, and NVidia Quadro FX770M graphics card. The resulting visualizations are rendered in real time at approximately 20 frames per second where 500 targets can be tracked at maximum for the traffic flow scenario.</p><p>Through our experiments, the scenario (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>a–c) that requires view and background blending demonstrates view transitions that are smooth and provides dynamic visualizations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Various results of sky rendering based on multiple videos at different times</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Results from our prototype system using 36 videos: <i>1. OCCM (View Blending):</i> (<b>a</b>). 5 Cameras for soccer game (<b>b</b>). Two broadcasting footages of NCAA Football game (<b>c</b>). Three surveillance cameras. <i>2. SCSM (Traffic):</i> (<b>d</b>). Merging Lanes (<b>e</b>). The steering behaviors of cars are demonstrated in the slightly curved way (<b>f</b>). 8 Cameras for larger-scale traffic simulation including merge and split (<b>g</b>). Rendered traffic scene and corresponding simulated scene <i>3. DM (Pedestrians):</i> (<b>h</b>). Direct mapping of pedestrian having simple motion (<b>i</b>). Visualization of pedestrians and cars in the street <i>4. SCCM (Clouds):</i> (<b>j</b>). Four videos for clouds and sky generation <i>5. Customized visualization using user interface:</i> (<b>k</b>). Traffic flow are visualized in a line map by adjusting the scale of moving objects (<b>l</b>). The game scenes can be augmented into different stadium. Please see the video on the project website</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In the traffic scenario, the visualized traffic closely matches the average speeds of the real traffic system. However, it was noted that the quality of the estimated traffic flow deteriorates in proportion to the distance between the cameras. The cameras used in our results are placed no more than 0.5 miles away and provide qualitatively plausible visualizations. Nonetheless, it can be observed that our results show fairly accurate <i>global tendency</i>, which reflects real-world average flows (velocities). However, it is important to note that it does not guarantee the exact individual position of every car in unobserved regions because the positions of cars in the unobserved region are designed to be driven by behavioral model where target velocities are computed through interpolation of observed ones.</p><p>To evaluate the parameterized flock behavior, we artificially forced a very slow speed only to the left-top observable node, as already shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig9">9</a>b, while the other nodes still received live information, to mimic a traffic congestion. The objects approaching the congested region changed their velocity sharply but realistically under the poised behavioral rules.</p><p>Our system produces reliable results under diverse lighting and weather conditions for traffic visualization (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig13">13</a>). In terms of the test results on a challenging road topology, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>d, e depicts a scene with curved roads that merge into a speedy highway. The cars simulated based on the live videos successfully adjust their speed on the curved roads (approximated by a series of rectangular regions) after which they accelerate into the highway to catch up with the average speed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0186-2/MediaObjects/10055_2010_186_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p><i>Qualitative evaluation of traffic in different lighting and weather conditions</i> Rendered Dynamic scenes based on (<i>Top</i>) video data captured at night: Resulting scene demonstrates sparse traffic distribution with high velocity across almost entire nodes, (<i>Bottom</i>) video data captured during snowing weather condition: Despite heavy noises in the video due to snow, the result shows flows that realistically demonstrate real-world traffic; one lane (marked as<i> red</i>) shows densely populated heavy traffic, while the other lane (<i>yellow</i>) is relatively sparser (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0186-2/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In the challenging large-scale experiments shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig8">8</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>f cameras are installed at different locations where the visualized road system consists of 13 observed and 26 unobserved regions. Some cameras observe one-way road, while others observe two-way, and the traffic topology include merge, exits and bridge crossings.</p><p>The results for the DM scenario (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>h, i) demonstrate that pedestrians and cars are animated properly based on the successful tracking results.</p><p>Various styles of sky and clouds are generated as shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig10">10</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>j. Although the visualization results do not capture the exact shape of the individual clouds or the exact sky atmosphere, movement and density of the distributed clouds reflect the characteristics of the input videos plausibly.</p><p>Finally, it is worth noting that the interface developed in our prototype system allows us to make the customized visualizations with ease. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>k demonstrates the traffic visualization on the line map (i.e. google map), and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0186-2#Fig12">12</a>l shows an example where we map a football game onto a different stadium far away from the original location.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Summary, limitations, and discussion</h2><div class="c-article-section__content" id="Sec17-content"><p>In this paper, we introduced methods to augment Aerial Earth Maps (AEMs) with diverse types of real-time information, namely pedestrians, sports scenes, traffic flows, and skies. The proposed set of solutions are targeted to address different types of scenes in terms of camera network configuration/density and the dynamism presented by the scenes. The prototype system which integrates all the components run in real time and demonstrates that our work provides a novel, more vivid, and more engaging virtual environment through which the users would browse the cities of now. Our work showcases a unique approach that provides improved experience for users to visually consume “what is happening where”, to augment the conventional text-web-browsing experience currently provided by static AEMs.</p><p>It is important to note a few of our limitations. First, the direct mapping method only visualizes the object moving within its view, and movement modeling outside is only possible when overlapping views are available. Second, the tracking within the direct mapping and the traffic flow estimation assume that the occlusion among targets is modest, and the target association can be computed relatively accurately. Accordingly, none of our current approaches for tracking will work for dense crowds or for low-frequency videos (which are often the case for many surveillance cameras), due to significant occlusions. Third, the tracking algorithms used in our work are not able to recognize the detailed postures of complex targets, e.g., human postures. While efforts have been made to handle this task in computer vision (e.g., Efros et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Efros AA, Berg EC, Mori G, Malik J (2003) Recognizing action at a distance. In: ICCV03, pp 726–733" href="/article/10.1007/s10055-010-0186-2#ref-CR4" id="ref-link-section-d39778e3146">2003</a>; Ramanan and Forsyth <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ramanan D, Forsyth DA (2003) Automatic annotation of everyday movements. In: NIPS. MIT Press, Cambridge" href="/article/10.1007/s10055-010-0186-2#ref-CR23" id="ref-link-section-d39778e3149">2003</a>), practical performance still depends on specific constraints. Fourth, we cannot directly apply our traffic flow approaches to the scenes with higher-level controls and behaviors, e.g., intersections with traffic lights, which is an interesting avenue for future research. Finally, our solutions do not support modeling or analysis of high-level semantic information, e.g., car accidents or street burglaries.</p><p>We do note that the widespread distribution of cameras, which are used by our approach, rightfully brings up serious privacy issues. This is due to the concerns of direct exposure of individuals (and their privacy) to massive distribution of public surveillance cameras (Wood et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Wood DM, Ball K, Lyon D, Norris C, Raab C (2006) A report on the surveillance society. Surveillance Studies Network, UK" href="/article/10.1007/s10055-010-0186-2#ref-CR39" id="ref-link-section-d39778e3155">2006</a>). <i>Street Views</i> by Google has similarly raised privacy concerns because it sometimes displays people.</p><p>In our current work, we are intentionally “abstracting” individuals and our camera distance to individuals is large enough that personal identity is hard to ascertain. We believe our approach of symbolizing moving objects by tracking without identification can alleviate the privacy concerns. However, we do see the concerns raised and propose that the data sources and the entire system can be managed properly. In the present prototype system, they are explicitly controlled.</p><p>It is also worthwhile to discuss a bit more on the potential of our system. <i>First</i>, OCCM application can be used as the interactive sport broadcasting where multiple broadcasting cameras or crowd-sourced videos incorporate to visualize the sport event on virtual environment. It will allow users to actively watch sports games by selecting favorite views and regions of interest. <i>Secondly</i>, new types of advertisement could be brought into our system. For instance, crowd events within the field of views of public cameras can be used as the novel types of marketing methods by immediate visualization in the virtual environment or map. <i>Third</i>, our approach provides visually fused information that integrates individual sensor observations and is potentially useful for surveillance, security, and military applications. <i>Finally</i>, our system can be used as a mirror that one can interact between real worlds and virtual worlds. This is possible because <i>Augmented Reality</i> (AR) techniques allow us put the virtual objects onto the screen, which displays the real-world scene, while our approach puts the present dynamic information onto virtual world. Imagine your avatar in the virtual world application such as Second Life (Linden Research, <a href="http://www.liberovision.com/">http://www.liberovision.com/</a>) looking at the moving objects that are actually reflecting the movements in the real world and vice versa. We believe such interactions can be an important milestone for the future of virtual reality as well as new types of cyber-cultures (Smart et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Smart J, Cascio J, Paffendorf J (2007) Metaverse roadmap: pathways to the 3d web. Metaverse: a cross-industry public foresight project" href="/article/10.1007/s10055-010-0186-2#ref-CR32" id="ref-link-section-d39778e3190">2007</a>).</p><p>In summary, we have presented a novel prototype system to augment dynamic virtual cities based on real-world observations and spatio-temporal analysis of them. We feel that our approach opens doors for interesting forms of new augmented virtual realism.</p><p>In our future work, we aim to overcome the above limitations and incorporate even more types of additional dynamic information such as river, swinging forest, sun, weather patterns, environmental condition and even aerial objects like birds and airplanes. Additionally, we would like to apply our system onto various types of possible applications mentioned above.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bouguet J-Y (2003) Pyramidal implementation of the lucas kanade feature tracker. In: Intel Corporation" /><p class="c-article-references__text" id="ref-CR1">Bouguet J-Y (2003) Pyramidal implementation of the lucas kanade feature tracker. In: Intel Corporation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="MD. Buhmann, MJ. Ablowitz, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Buhmann MD, Ablowitz MJ (2003) Radial basis functions: theory and implementations. Cambridge University, Cambr" /><p class="c-article-references__text" id="ref-CR2">Buhmann MD, Ablowitz MJ (2003) Radial basis functions: theory and implementations. Cambridge University, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Radial%20basis%20functions%3A%20theory%20and%20implementations&amp;publication_year=2003&amp;author=Buhmann%2CMD&amp;author=Ablowitz%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Chen, G. Esch, P. Wonka, P. Müller, E. Zhang, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chen G, Esch G, Wonka P, Müller P, Zhang E (2008) Interactive procedural street modeling. ACM Trans Graph 27(3" /><p class="c-article-references__text" id="ref-CR3">Chen G, Esch G, Wonka P, Müller P, Zhang E (2008) Interactive procedural street modeling. ACM Trans Graph 27(3):1–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1360612.1360702" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20procedural%20street%20modeling&amp;journal=ACM%20Trans%20Graph&amp;volume=27&amp;issue=3&amp;pages=1-10&amp;publication_year=2008&amp;author=Chen%2CG&amp;author=Esch%2CG&amp;author=Wonka%2CP&amp;author=M%C3%BCller%2CP&amp;author=Zhang%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Efros AA, Berg EC, Mori G, Malik J (2003) Recognizing action at a distance. In: ICCV03, pp 726–733" /><p class="c-article-references__text" id="ref-CR4">Efros AA, Berg EC, Mori G, Malik J (2003) Recognizing action at a distance. In: ICCV03, pp 726–733</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Frey B, MacKay D (1998) A revolution: belief propagation in graphs with cycles. In: Neural information process" /><p class="c-article-references__text" id="ref-CR5">Frey B, MacKay D (1998) A revolution: belief propagation in graphs with cycles. In: Neural information processing systems, pp 479–485</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Girgensohn A, Kimber D, Vaughan J, Yang T, Shipman F, Turner T, Rieffel E, Wilcox L, Chen F, Dunnigan T (2007)" /><p class="c-article-references__text" id="ref-CR6">Girgensohn A, Kimber D, Vaughan J, Yang T, Shipman F, Turner T, Rieffel E, Wilcox L, Chen F, Dunnigan T (2007) Dots: support for effective video surveillance. In: ACM MULTIMEDIA ’07. ACM, New York, pp 423–432</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harris MJ (2005) Real-time cloud simulation and rendering. In: ACM SIGGRAPH 2005 Courses. New York, p. 222" /><p class="c-article-references__text" id="ref-CR7">Harris MJ (2005) Real-time cloud simulation and rendering. In: ACM SIGGRAPH 2005 Courses. New York, p. 222</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RI. Hartley, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hartley RI (1997) In defense of the eight-point algorithm. PAMI Int J Pattern Anal Mach Intell 19(6):580–593" /><p class="c-article-references__text" id="ref-CR8">Hartley RI (1997) In defense of the eight-point algorithm. PAMI Int J Pattern Anal Mach Intell 19(6):580–593</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.601246" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20defense%20of%20the%20eight-point%20algorithm&amp;journal=PAMI%20Int%20J%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=6&amp;pages=580-593&amp;publication_year=1997&amp;author=Hartley%2CRI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Hartley, A. Zisserman, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR9">Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple%20view%20geometry%20in%20computer%20vision&amp;publication_year=2000&amp;author=Hartley%2CR&amp;author=Zisserman%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Horry Y, Anjyo K-I, Arai K (1997) Tour into the picture: using a spidery mesh interface to make animation from" /><p class="c-article-references__text" id="ref-CR10">Horry Y, Anjyo K-I, Arai K (1997) Tour into the picture: using a spidery mesh interface to make animation from a single image. In: Proceedings of ACM SIGGRAPH, New York, pp 225–232</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kanade T (2001) Eyevision system at super bowl 2001. http://www.ri.cmu.edu/events/sb35/tksuperbowl.html&#xA;      " /><p class="c-article-references__text" id="ref-CR11">Kanade T (2001) Eyevision system at super bowl 2001. <a href="http://www.ri.cmu.edu/events/sb35/tksuperbowl.html">http://www.ri.cmu.edu/events/sb35/tksuperbowl.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE " /><p class="c-article-references__text" id="ref-CR12">Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’07)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koller-meier EB, Ade F (2001) Tracking multiple objects using the condensation algorithm. JRAS" /><p class="c-article-references__text" id="ref-CR13">Koller-meier EB, Ade F (2001) Tracking multiple objects using the condensation algorithm. JRAS</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476–490" /><p class="c-article-references__text" id="ref-CR14">Kosecka J, Zhang W (2002) Video compass. In: Proceedings of ECCV. Springer, London, pp 476–490</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lewis JP (ed) (1989) Algorithms for solid noise synthesis" /><p class="c-article-references__text" id="ref-CR16">Lewis JP (ed) (1989) Algorithms for solid noise synthesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Man P (2006) Generating and real-time rendering of clouds. In: Central European seminar on computer graphics, " /><p class="c-article-references__text" id="ref-CR18">Man P (2006) Generating and real-time rendering of clouds. In: Central European seminar on computer graphics, pp 1–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Pearl, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Pearl J (1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufman" /><p class="c-article-references__text" id="ref-CR20">Pearl J (1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, Massachusetts</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Probabilistic%20reasoning%20in%20intelligent%20systems%3A%20networks%20of%20plausible%20inference&amp;publication_year=1988&amp;author=Pearl%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Perlin, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Perlin K (1985) An image synthesizer. SIGGRAPH Comput Graph 19(3):287–296" /><p class="c-article-references__text" id="ref-CR21">Perlin K (1985) An image synthesizer. SIGGRAPH Comput Graph 19(3):287–296</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F325165.325247" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image%20synthesizer&amp;journal=SIGGRAPH%20Comput%20Graph&amp;volume=19&amp;issue=3&amp;pages=287-296&amp;publication_year=1985&amp;author=Perlin%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rabaud V, Belongie S (2006) Counting crowded moving objects. In: CVPR ’06: Proceedings of IEEE computer vision" /><p class="c-article-references__text" id="ref-CR22">Rabaud V, Belongie S (2006) Counting crowded moving objects. In: CVPR ’06: Proceedings of IEEE computer vision and pattern recognition. IEEE Computer Society, pp 17–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ramanan D, Forsyth DA (2003) Automatic annotation of everyday movements. In: NIPS. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR23">Ramanan D, Forsyth DA (2003) Automatic annotation of everyday movements. In: NIPS. MIT Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reynolds CW (1987) Flocks, herds and schools: a distributed behavioral model. In: ACM SIGGRAPH 1987. ACM Press" /><p class="c-article-references__text" id="ref-CR24">Reynolds CW (1987) Flocks, herds and schools: a distributed behavioral model. In: ACM SIGGRAPH 1987. ACM Press, New York, pp 25–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reynolds CW (1999) Steering behaviors for autonomous characters. In: GDC ’99: Proceedings of game developers c" /><p class="c-article-references__text" id="ref-CR25">Reynolds CW (1999) Steering behaviors for autonomous characters. In: GDC ’99: Proceedings of game developers conference. Miller Freeman Game Group, pp 768–782</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sawhney HS, Arpa A, Kumar R, Samarasekera S, Aggarwal M, Hsu S, Nister D, Hanna K (2002) Video flashlights: re" /><p class="c-article-references__text" id="ref-CR26">Sawhney HS, Arpa A, Kumar R, Samarasekera S, Aggarwal M, Hsu S, Nister D, Hanna K (2002) Video flashlights: real time rendering of multiple videos for immersive model visualization. In: 13th Eurographics workshop on Rendering. Eurographics Association, pp 157–168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sebe IO, Hu J, You S, Neumann U (2003) 3d video surveillance with augmented virtual environments. In: IWVS ’03" /><p class="c-article-references__text" id="ref-CR27">Sebe IO, Hu J, You S, Neumann U (2003) 3d video surveillance with augmented virtual environments. In: IWVS ’03: First ACM SIGMM international workshop on Video surveillance. ACM, New York, pp 107–112</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seitz SM, Dyer CR (1996) View morphing. In: SIGGRAPH ’96. ACM, New York, pp 21–30" /><p class="c-article-references__text" id="ref-CR28">Seitz SM, Dyer CR (1996) View morphing. In: SIGGRAPH ’96. ACM, New York, pp 21–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stere" /><p class="c-article-references__text" id="ref-CR29">Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: IEEE CVPR ’06, pp 519–528</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Shao, D. Terzopoulos, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Shao W, Terzopoulos D (2007) Autonomous pedestrians. Graphi Models 69(5):246–274" /><p class="c-article-references__text" id="ref-CR30">Shao W, Terzopoulos D (2007) Autonomous pedestrians. Graphi Models 69(5):246–274</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.gmod.2007.09.001" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Autonomous%20pedestrians&amp;journal=Graph%20Models&amp;volume=69&amp;issue=5&amp;pages=246-274&amp;publication_year=2007&amp;author=Shao%2CW&amp;author=Terzopoulos%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE CVPR. IEEE computer society, pp 593–600" /><p class="c-article-references__text" id="ref-CR31">Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE CVPR. IEEE computer society, pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smart J, Cascio J, Paffendorf J (2007) Metaverse roadmap: pathways to the 3d web. Metaverse: a cross-industry " /><p class="c-article-references__text" id="ref-CR32">Smart J, Cascio J, Paffendorf J (2007) Metaverse roadmap: pathways to the 3d web. Metaverse: a cross-industry public foresight project</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Snavely N, Seitz SM, Szeliski R (2006) Photo tourism: exploring photo collections in 3d. In: Proceedings of AC" /><p class="c-article-references__text" id="ref-CR33">Snavely N, Seitz SM, Szeliski R (2006) Photo tourism: exploring photo collections in 3d. In: Proceedings of ACM SIGGRAPH’06. ACM Press, New York, pp 835–846</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Treuille A, Cooper S, Popović Z (2006) Continuum crowds. In: ACM SIGGRAPH 2006 papers, pp 1160–1168" /><p class="c-article-references__text" id="ref-CR34">Treuille A, Cooper S, Popović Z (2006) Continuum crowds. In: ACM SIGGRAPH 2006 papers, pp 1160–1168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turk G, O’Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH ’99. New Yor" /><p class="c-article-references__text" id="ref-CR35">Turk G, O’Brien JF (1999) Shape transformation using variational implicit functions. In: SIGGRAPH ’99. New York, NY, pp 335–342</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CJ. Veenman, MJT. Reinders, E. Backer, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Veenman CJ, Reinders MJT, Backer E (2001) Resolving motion correspondence for densely moving points. IEEE Tran" /><p class="c-article-references__text" id="ref-CR36">Veenman CJ, Reinders MJT, Backer E (2001) Resolving motion correspondence for densely moving points. IEEE Trans Pattern Anal Mach Intell 23:54–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.899946" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Resolving%20motion%20correspondence%20for%20densely%20moving%20points&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=23&amp;pages=54-72&amp;publication_year=2001&amp;author=Veenman%2CCJ&amp;author=Reinders%2CMJT&amp;author=Backer%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Wang, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Wang N (2004) Realistic and fast cloud rendering. J Graph Tools 9(3):21–40" /><p class="c-article-references__text" id="ref-CR37">Wang N (2004) Realistic and fast cloud rendering. J Graph Tools 9(3):21–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Realistic%20and%20fast%20cloud%20rendering&amp;journal=J%20Graph%20Tools&amp;volume=9&amp;issue=3&amp;pages=21-40&amp;publication_year=2004&amp;author=Wang%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Wang, DM. Krum, EM. Coelho, DA. Bowman, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Wang Y, Krum DM, Coelho EM, Bowman DA (2007) Contextualized videos: Combining videos with environment models t" /><p class="c-article-references__text" id="ref-CR38">Wang Y, Krum DM, Coelho EM, Bowman DA (2007) Contextualized videos: Combining videos with environment models to support situational understanding. Abdom Imag 13:1568–1575</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Contextualized%20videos%3A%20combining%20videos%20with%20environment%20models%20to%20support%20situational%20understanding&amp;journal=Abdom%20Imag&amp;volume=13&amp;pages=1568-1575&amp;publication_year=2007&amp;author=Wang%2CY&amp;author=Krum%2CDM&amp;author=Coelho%2CEM&amp;author=Bowman%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DM. Wood, K. Ball, D. Lyon, C. Norris, C. Raab, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Wood DM, Ball K, Lyon D, Norris C, Raab C (2006) A report on the surveillance society. Surveillance Studies Ne" /><p class="c-article-references__text" id="ref-CR39">Wood DM, Ball K, Lyon D, Norris C, Raab C (2006) A report on the surveillance society. Surveillance Studies Network, UK</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20report%20on%20the%20surveillance%20society&amp;publication_year=2006&amp;author=Wood%2CDM&amp;author=Ball%2CK&amp;author=Lyon%2CD&amp;author=Norris%2CC&amp;author=Raab%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Yilmaz, O. Javed, M. Shah, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Yilmaz A, Javed O, Shah M (2006) Object tracking: a survey. ACM Comput Surv 38(4):13" /><p class="c-article-references__text" id="ref-CR40">Yilmaz A, Javed O, Shah M (2006) Object tracking: a survey. ACM Comput Surv 38(4):13</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1177352.1177355" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Object%20tracking%3A%20a%20survey&amp;journal=ACM%20Comput%20Surv&amp;volume=38&amp;issue=4&amp;publication_year=2006&amp;author=Yilmaz%2CA&amp;author=Javed%2CO&amp;author=Shah%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zotti G, Groller ME (2005) A sky dome visualisation for identification of astronomical orientations. In: INFOV" /><p class="c-article-references__text" id="ref-CR41">Zotti G, Groller ME (2005) A sky dome visualisation for identification of astronomical orientations. In: INFOVIS ’05. IEEE Computer Society, Washington, DC, p 2</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0186-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This project was in part funded by a Google Research Award. We would like to thank Nick Diakopoulos, Matthias Grundmann, Myungcheol Doo and Dongryeol Lee for their help and comments on the work. Thanks also to the Georgia Tech Athletic Association (GTAA) for sharing with us videos of the college football games for research purposes. Finally, thanks to the reviewers for their valuable comments.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Georgia Institute of Technology, Atlanta, GA, USA</p><p class="c-article-author-affiliation__authors-list">Kihwan Kim, Sangmin Oh, Jeonggyu Lee &amp; Irfan Essa</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Kitware Inc., Clifton Park, NY, USA</p><p class="c-article-author-affiliation__authors-list">Sangmin Oh</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Intel Corporation, Hillsboro, OR, USA</p><p class="c-article-author-affiliation__authors-list">Jeonggyu Lee</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Kihwan-Kim"><span class="c-article-authors-search__title u-h3 js-search-name">Kihwan Kim</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kihwan+Kim&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kihwan+Kim" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kihwan+Kim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Sangmin-Oh"><span class="c-article-authors-search__title u-h3 js-search-name">Sangmin Oh</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sangmin+Oh&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sangmin+Oh" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sangmin+Oh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jeonggyu-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Jeonggyu Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jeonggyu+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jeonggyu+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jeonggyu+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Irfan-Essa"><span class="c-article-authors-search__title u-h3 js-search-name">Irfan Essa</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Irfan+Essa&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Irfan+Essa" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Irfan+Essa%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0186-2/email/correspondent/c1/new">Kihwan Kim</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>This work was done while authors Sangmin Oh and Jeonggyu Lee were with Georgia Institute of Technology.</p><p>Project homepage URL: <a href="http://www.cc.gatech.edu/cpl/projects/augearth">http://www.cc.gatech.edu/cpl/projects/augearth</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Augmenting%20aerial%20earth%20maps%20with%20dynamic%20information%20from%20videos&amp;author=Kihwan%20Kim%20et%20al&amp;contentID=10.1007%2Fs10055-010-0186-2&amp;publication=1359-4338&amp;publicationDate=2011-01-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kim, K., Oh, S., Lee, J. <i>et al.</i> Augmenting aerial earth maps with dynamic information from videos.
                    <i>Virtual Reality</i> <b>15, </b>185–200 (2011). https://doi.org/10.1007/s10055-010-0186-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0186-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-17">17 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-12-11">11 December 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-01-11">11 January 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0186-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0186-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Video analysis</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Computer vision</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Computer graphics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">View synthesis</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Procedural rendering</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0186-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=186;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

