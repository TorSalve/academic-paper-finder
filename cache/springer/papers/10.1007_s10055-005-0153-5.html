<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Mixed feelings: expression of non-basic emotions in a muscle-based tal"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We present an algorithm for generating facial expressions for a continuum of pure and mixed emotions of varying intensity. Based on the observation that in natural interaction among humans, shades..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Mixed feelings: expression of non-basic emotions in a muscle-based talking head"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-08-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We present an algorithm for generating facial expressions for a continuum of pure and mixed emotions of varying intensity. Based on the observation that in natural interaction among humans, shades of emotion are much more frequently encountered than expressions of basic emotions, a method to generate more than Ekman&#8217;s six basic emotions (joy, anger, fear, sadness, disgust and surprise) is required. To this end, we have adapted the algorithm proposed by Tsapatsoulis et al. [1] to be applicable to a physics-based facial animation system and a single, integrated emotion model. A physics-based facial animation system was combined with an equally flexible and expressive text-to-speech synthesis system, based upon the same emotion model, to form a talking head capable of expressing non-basic emotions of varying intensities. With a variety of life-like intermediate facial expressions captured as snapshots from the system we demonstrate the appropriateness of our approach."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-08-11"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="201"/>

    <meta name="prism.endingPage" content="212"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0153-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0153-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0153-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0153-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Mixed feelings: expression of non-basic emotions in a muscle-based talking head"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2005/09"/>

    <meta name="citation_online_date" content="2005/08/11"/>

    <meta name="citation_firstpage" content="201"/>

    <meta name="citation_lastpage" content="212"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0153-5"/>

    <meta name="DOI" content="10.1007/s10055-005-0153-5"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0153-5"/>

    <meta name="description" content="We present an algorithm for generating facial expressions for a continuum of pure and mixed emotions of varying intensity. Based on the observation that in"/>

    <meta name="dc.creator" content="Irene Albrecht"/>

    <meta name="dc.creator" content="Marc Schr&#246;der"/>

    <meta name="dc.creator" content="J&#246;rg Haber"/>

    <meta name="dc.creator" content="Hans-Peter Seidel"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation&#8212;the standard implementations applications. Wiley, Hillsdale, pp 141&#8211;167"/>

    <meta name="citation_reference" content="Andr&#233; E, Dybkyaer L, Minker W, Heisterkamp P (eds) (2004) In: Proceedings of the tutorial and research workshop on affective dialogue systems (ADS04), vol 3068 of lecture notes in artificial intelligence, Kloster Irsee, Germany. Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="citation_journal_title=Speech Commun Spec Issue Speech Emotion; citation_title=Describing the emotional states that are expressed in speech; citation_author=R Cowie, R Cornelius; citation_volume=40; citation_issue=1&#8211;2; citation_publication_date=2003; citation_pages=5-32; citation_id=CR3"/>

    <meta name="citation_reference" content="Scherer K (2000) Psychological models of emotion. Neuropsychology of emotion. Oxford University Press, Oxford, pp 137&#8211;162"/>

    <meta name="citation_reference" content="The HUMAINE network portal. 
                    http://emotion-research.net
                    
                  "/>

    <meta name="citation_reference" content="Schr&#246;der M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions in view of speech synthesis. In: Proceedings of Eurospeech&#8217;01, vol 1, pp 87-90"/>

    <meta name="citation_reference" content="Schr&#246;der M (2004) Dimensional emotion representation as a basis for speech synthesis with non-extreme emotions. In: Proceedings of the workshop on affective dialogue systems, Kloster Irsee, Germany, pp 209&#8211;220"/>

    <meta name="citation_reference" content="citation_title=An Introduction to text-to-speech synthesis; citation_publication_date=1997; citation_id=CR8; citation_author=Th Dutoit; citation_publisher=Kluwer"/>

    <meta name="citation_reference" content="citation_journal_title=Speech synthesis development made; citation_author=null Klabbers; citation_volume=easy; citation_publication_date=2001; citation_pages=the; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Speech Technology; citation_author=null Schr; citation_volume=6; citation_publication_date=2003; citation_pages=365; citation_doi=10.1023/A:1025708916924; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Soc Psychol; citation_title=Acoustic profiles in vocal emotion expression; citation_author=R Banse, K Scherer; citation_volume=70; citation_issue=3; citation_publication_date=1996; citation_pages=614-636; citation_doi=10.1037//0022-3514.70.3.614; citation_id=CR11"/>

    <meta name="citation_reference" content="Yang L (2001) Prosody as expression of emotion. In: Cav&#233; Ch (ed) Proceedings of ORAGE 2001, Oralit&#233; et gestualit&#233;, pp 209&#8211;212"/>

    <meta name="citation_reference" content="citation_journal_title=Emotional speech; citation_author=null Schr; citation_volume=synthesis; citation_publication_date=2001; citation_pages=A; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_title=From text to speech: the MITalk system; citation_publication_date=1987; citation_id=CR14; citation_author=J Allen; citation_author=S Hunnicutt; citation_author=DH Klatt; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=J Am Voice I/O Soc; citation_title=The generation of affect in synthesized speech; citation_author=J Cahn; citation_volume=8; citation_publication_date=1990; citation_pages=1-19; citation_id=CR15"/>

    <meta name="citation_reference" content="Black AW, Campbell N (1995) Optimising selection of units from speech databases for concatenative synthesis. In: Proceedings of Eurospeech 1995, Madrid, pp 581&#8211;584"/>

    <meta name="citation_reference" content="Johnson W, Narayanan S, Whitney R, Das R, Bulut M, LaBore C (2002) Limited domain synthesis of expressive military speech for animated characters. In: Proceedings of the 7th international conference on spoken language processing, Denver"/>

    <meta name="citation_reference" content="citation_journal_title=The MBROLA; citation_author=null Dutoit; citation_volume=project; citation_publication_date=1996; citation_pages=towards; citation_id=CR18"/>

    <meta name="citation_reference" content="Schr&#246;der M, Grice M (2003) Expressing vocal effort in concatenative synthesis. In: Proceedings of the 15th international conference of phonetic sciences, Barcelona"/>

    <meta name="citation_reference" content="Lee Y, Terzopoulos D, Waters K (1995) Realistic face modeling for animation. In: Proceedings of SIGGRAPH&#8217;95, pp 55&#8211;62"/>

    <meta name="citation_reference" content="K&#228;hler K, Haber J, Seidel HP (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of Graphics Interface, pp 37&#8211;46"/>

    <meta name="citation_reference" content="citation_journal_title=Video; citation_author=null Bregler; citation_volume=rewrite; citation_publication_date=1997; citation_pages=driving; citation_id=CR22"/>

    <meta name="citation_reference" content="Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH &#8217;99, pp 21&#8211;28"/>

    <meta name="citation_reference" content="Ezzat T, Geiger G, Poggio T (2002) Trainable videorealistic speech animation. In: Proceedings of SIGGRAPH&#8217;02, pp 388&#8211;398"/>

    <meta name="citation_reference" content="citation_title=A parametric model for human faces; citation_publication_date=1974; citation_id=CR25; citation_author=F Parke; citation_publisher= University of Utah"/>

    <meta name="citation_reference" content="Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalmann D (eds) Models and techniques in computer animation, pp 139&#8211;156"/>

    <meta name="citation_reference" content="Pelachaud C, Badler N, Steedman M (1991) Linguistic issues in facial animation. In: Magnenat-Thalmann N, Thalmann D (eds) Computer animation&#8217;91"/>

    <meta name="citation_reference" content="Kalberer G, M&#252;ller P, Van Gool L (2003) A visual speech generator. In: Proceedings of Videometrics VII. IS&amp;SPIE, pp 173&#8211;183"/>

    <meta name="citation_reference" content="Lee S, Badler J, Badler N (2002) Eyes alive. In: Proceedings of SIGGRAPH&#8217;02, pp 637&#8211;644"/>

    <meta name="citation_reference" content="citation_journal_title=Speech and; citation_author=null Pearce; citation_volume=expression; citation_publication_date=1986; citation_pages=a; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graphics; citation_title=Script-based facial gesture and speech animation using a NURBS based face model; citation_author=H Ip, C Chan; citation_volume=20; citation_issue=6; citation_publication_date=1996; citation_pages=881-891; citation_doi=10.1016/S0097-8493(96)00058-1; citation_id=CR31"/>

    <meta name="citation_reference" content="Kalra P, Mangili A, Magnenat-Thalmann N, Thalmann D (1991) SMILE: a multilayered facial animation system. In: Proceedings of IFIP WG 5.10, Tokyo, pp 189&#8211;198"/>

    <meta name="citation_reference" content="citation_journal_title=ANIMATED; citation_author=null Cassell; citation_volume=CONVERSATION; citation_publication_date=1994; citation_pages=rule; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Sci; citation_title=Generating facial expressions for speech; citation_author=C Pelachaud, N Badler, M Steedman; citation_volume=20; citation_issue=1; citation_publication_date=1996; citation_pages=1-46; citation_doi=10.1016/S0364-0213(99)80001-9; citation_id=CR34"/>

    <meta name="citation_reference" content="Albrecht I, Haber J, Seidel H-P (2002) Automatic generation of non-verbal facial expressions from speech. In: Proceedings of CGI, pp 283&#8211;293"/>

    <meta name="citation_reference" content="Albrecht I, Haber J, K&#228;hler K, Schr&#246;der M, Seidel H-P (2002) May I talk to you? :-)&#8212;facial animation from text. In: Proceedings of Pacific Graphics, pp 77&#8211;86"/>

    <meta name="citation_reference" content="Ekman P, Keltner D (1997) Universal facial expressions of emotion: an old controversy and new findings. In: segerstr&#248;le U, Moln&#225;r P (eds) Nonverbal communication: where nature meets culture. Lawrence Erlbaum Associates Inc., Mahwah, pp 27&#8211;46"/>

    <meta name="citation_reference" content="Byun M, Badler N (2002) FacEMOTE: qualitative parametric modifiers for facial animations. In: Proceedings of SCA&#8217;02, pp 65&#8211;71"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graphics Forum; citation_title=Emotion disc and emotion squares: tools to explore the facial expression space; citation_author=Z Ruttkay, H Noot, P ten Hagen; citation_volume=22; citation_issue=1; citation_publication_date=2003; citation_pages=49-53; citation_doi=10.1111/1467-8659.t01-1-00645; citation_id=CR39"/>

    <meta name="citation_reference" content="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113&#8211;131"/>

    <meta name="citation_reference" content="citation_title=Emotions: a psychoevolutionary synthesis; citation_publication_date=1980; citation_id=CR41; citation_author=R Plutchik; citation_publisher= Harper &amp; Row"/>

    <meta name="citation_reference" content="Bui T, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of CGI&#8217;04, pp 284&#8211;291"/>

    <meta name="citation_reference" content="citation_journal_title=Speech and emotion; citation_author=null Schr; citation_volume=research; citation_publication_date=2004; citation_pages=an; citation_id=CR43"/>

    <meta name="citation_reference" content="Cowie R, Douglas-Cowie E, Savvidou S, McMahon E, Sawey M, Schr&#246;der M (2000) &#8216;FEELTRACE&#8217;: an instrument for recording perceived emotion in real time. In: Proceedings of the ISCA workshop on speech and emotion, Northern Ireland, pp 19&#8211;24. 
                    http://www.qub.ac.uk/en/isca/proceedings
                    
                  , pp 19&#8211;24"/>

    <meta name="citation_reference" content="citation_journal_title=Speech Commun Spec Issue Speech Emotion; citation_title=Emotional speech: towards a new generation of databases; citation_author=E Douglas-Cowie, N Campbell, R Cowie, P Roach; citation_volume=40; citation_issue=1&#8211;2; citation_publication_date=2003; citation_pages=33-60; citation_id=CR45"/>

    <meta name="citation_reference" content="Cowie R, Douglas-Cowie E, Appolloni B, Taylor J, Romano A, Fellenz W (1999) What a neural net needs to know about emotion words. In: Mastorakis N (ed) Computational intelligence and applications. World Scientific &amp; Engineering Society Press, pp 109&#8211;114"/>

    <meta name="citation_reference" content="Krenn B, Pirker H, Grice M, Piwek P, van Deemter K, Schr&#246;der M, Klesen M, Gstrein E (2002) Generation of multimodal dialogue for net environments. In: Proceedings of Konvens, Saarbr&#252;cken.
                    http://www.ai.univie.ac.at/NECA
                    
                  , Saarbr&#252;cken"/>

    <meta name="citation_reference" content="Schr&#246;der M, Breuer S (2004) XML representation languages as a way of interconnecting TTS modules. In: Proceedings of ICSLP&#8217;04, Jeju"/>

    <meta name="citation_reference" content="citation_journal_title=Head; citation_author=null K; citation_volume=shop; citation_publication_date=2002; citation_pages=generating; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Semiotica; citation_title=The repertoire of nonverbal behavior: categories origins usage and coding; citation_author=P Ekman, W Wallace; citation_volume=1; citation_publication_date=1969; citation_pages=49-98; citation_id=CR50"/>

    <meta name="citation_author" content="Irene Albrecht"/>

    <meta name="citation_author_email" content="albrecht@mpi-sb.mpg.de"/>

    <meta name="citation_author_institution" content="MPI Informatik, Saarbr&#252;cken, Germany"/>

    <meta name="citation_author" content="Marc Schr&#246;der"/>

    <meta name="citation_author_email" content="schroed@dfki.de"/>

    <meta name="citation_author_institution" content="DFKI GmbH, Saarbr&#252;cken, Germany"/>

    <meta name="citation_author" content="J&#246;rg Haber"/>

    <meta name="citation_author_email" content="haberj@mpi-sb.mpg.de"/>

    <meta name="citation_author_institution" content="MPI Informatik, Saarbr&#252;cken, Germany"/>

    <meta name="citation_author" content="Hans-Peter Seidel"/>

    <meta name="citation_author_email" content="hpseidel@mpi-sb.mpg.de"/>

    <meta name="citation_author_institution" content="MPI Informatik, Saarbr&#252;cken, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0153-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0153-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Mixed feelings: expression of non-basic emotions in a muscle-based talking head"/>
        <meta property="og:description" content="We present an algorithm for generating facial expressions for a continuum of pure and mixed emotions of varying intensity. Based on the observation that in natural interaction among humans, shades of emotion are much more frequently encountered than expressions of basic emotions, a method to generate more than Ekman’s six basic emotions (joy, anger, fear, sadness, disgust and surprise) is required. To this end, we have adapted the algorithm proposed by Tsapatsoulis et al. [1] to be applicable to a physics-based facial animation system and a single, integrated emotion model. A physics-based facial animation system was combined with an equally flexible and expressive text-to-speech synthesis system, based upon the same emotion model, to form a talking head capable of expressing non-basic emotions of varying intensities. With a variety of life-like intermediate facial expressions captured as snapshots from the system we demonstrate the appropriateness of our approach."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Mixed feelings: expression of non-basic emotions in a muscle-based talking head | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0153-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Continuous emotions, Emotional speech synthesis, Facial animation","kwrd":["Continuous_emotions","Emotional_speech_synthesis","Facial_animation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0153-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0153-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=153;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0153-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Mixed feelings: expression of non-basic emotions in a muscle-based talking head
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0153-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0153-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-08-11" itemprop="datePublished">11 August 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Mixed feelings: expression of non-basic emotions in a muscle-based talking head</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Irene-Albrecht" data-author-popup="auth-Irene-Albrecht" data-corresp-id="c1">Irene Albrecht<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="MPI Informatik" /><meta itemprop="address" content="grid.419528.3, 0000 0004 0491 9823, MPI Informatik, Saarbrücken, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marc-Schr_der" data-author-popup="auth-Marc-Schr_der">Marc Schröder</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="DFKI GmbH" /><meta itemprop="address" content="grid.17272.31, 0000 0004 0621 750X, DFKI GmbH, Saarbrücken, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J_rg-Haber" data-author-popup="auth-J_rg-Haber">Jörg Haber</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="MPI Informatik" /><meta itemprop="address" content="grid.419528.3, 0000 0004 0491 9823, MPI Informatik, Saarbrücken, Germany" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hans_Peter-Seidel" data-author-popup="auth-Hans_Peter-Seidel">Hans-Peter Seidel</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="MPI Informatik" /><meta itemprop="address" content="grid.419528.3, 0000 0004 0491 9823, MPI Informatik, Saarbrücken, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">201</span>–<span itemprop="pageEnd">212</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">497 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">46 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0153-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We present an algorithm for generating facial expressions for a continuum of pure and mixed emotions of varying intensity. Based on the observation that in natural interaction among humans, shades of emotion are much more frequently encountered than expressions of basic emotions, a method to generate more than Ekman’s six basic emotions (joy, anger, fear, sadness, disgust and surprise) is required. To this end, we have adapted the algorithm proposed by Tsapatsoulis et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e284">1</a>] to be applicable to a physics-based facial animation system and a single, integrated emotion model. A physics-based facial animation system was combined with an equally flexible and expressive text-to-speech synthesis system, based upon the same emotion model, to form a talking head capable of expressing non-basic emotions of varying intensities. With a variety of life-like intermediate facial expressions captured as snapshots from the system we demonstrate the appropriateness of our approach.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The naturalness of a talking head depends on a considerable number of factors related to the proper integration of visual and audio channel, i.e., of the (audible) synthetic speech and the (visible) facial model. One important factor is the generation of adequate lip movement in synchronisation to speech. Another factor is speech-related non-verbal facial expression, such as raised eyebrows or blinks related to the structure of the spoken utterance. If none of these are present in the synthesised animation, it is perceived as soulless, highly artificial and plainly boring.</p><p>A third important factor for naturalness in a talking head is the expression of emotions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="André E, Dybkyaer L, Minker W, Heisterkamp P (eds) (2004) In: Proceedings of the tutorial and research workshop on affective dialogue systems (ADS04), vol 3068 of lecture notes in artificial intelligence, Kloster Irsee, Germany. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-005-0153-5#ref-CR2" id="ref-link-section-d22556e311">2</a>]. Unfortunately, the “toolbox” for modelling emotions and their expression is not yet very well developed. While attempts are under way to organise the vocabulary and models used for the description of affective states [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Cowie R, Cornelius R (2003) Describing the emotional states that are expressed in speech. Speech Commun Spec Issue Speech Emotion 40(1–2):5–32" href="/article/10.1007/s10055-005-0153-5#ref-CR3" id="ref-link-section-d22556e314">3</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Scherer K (2000) Psychological models of emotion. Neuropsychology of emotion. Oxford University Press, Oxford, pp 137–162" href="/article/10.1007/s10055-005-0153-5#ref-CR4" id="ref-link-section-d22556e317">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="The HUMAINE network portal. &#xA;                    http://emotion-research.net&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR5" id="ref-link-section-d22556e320">5</a>], much work on the expression of emotion has been limited to simple representations such as basic emotions (see, e.g., [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Schröder M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions in view of speech synthesis. In: Proceedings of Eurospeech’01, vol 1, pp 87-90" href="/article/10.1007/s10055-005-0153-5#ref-CR6" id="ref-link-section-d22556e323">6</a>]). Only recently, more flexible emotion representations have started to be explored in the domain of speech synthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Schröder M (2004) Dimensional emotion representation as a basis for speech synthesis with non-extreme emotions. In: Proceedings of the workshop on affective dialogue systems, Kloster Irsee, Germany, pp 209–220" href="/article/10.1007/s10055-005-0153-5#ref-CR7" id="ref-link-section-d22556e327">7</a>] and MPEG-4-based facial animation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e330">1</a>].</p><p>The present paper follows this line of development in proposing a model for the integrated generation of speech and facial expression using an expressive text-to-speech (TTS) system in combination with a photo-realistic, muscle-based facial animation model. A representation of emotional states combining categorical and dimensional aspects is used for the prediction of vocal and facial expressions of non-basic emotions, i.e., of low-intensity and intermediate emotional states.</p><p>The paper is structured as follows: we first refer to the relevant background related to speech synthesis, facial animation and emotion representations. After that, the building blocks required for our work are described, and our method for expressing emotions based on a dimensional representation of emotions is presented. Finally, we describe our plans to extend the work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Text-to-speech systems</h3><p>Text-to-Speech synthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Dutoit Th (1997) An Introduction to text-to-speech synthesis. Kluwer, Dordrecht" href="/article/10.1007/s10055-005-0153-5#ref-CR8" id="ref-link-section-d22556e349">8</a>] is a method for converting written text into audible speech. It consists of a text analysis part, generating a symbolic representation of a spoken utterance including a phonetic transcription of the words, followed by the actual speech synthesis part, in which the symbolic representation is converted into audible speech.</p><p>Speech synthesis systems that are to be used in conjunction with facial animation need to provide intermediate processing results such as timing information in addition to the resulting speech. New systems using XML-based internal data representations, such as BOSS [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Klabbers E, Støber K, Veldhuis R, Wagner P, Stefan Breuer S (2001) Speech synthesis development made easy: the Bonn Open Synthesis System. In: Proceedings of Eurospeech 2001, Aalborg, pp 521–524" href="/article/10.1007/s10055-005-0153-5#ref-CR9" id="ref-link-section-d22556e355">9</a>] and Mary [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schröder M, Trouvain J (2003) The German text-to-speech synthesis system MARY: a tool for research development and teaching. Int J Speech Technology 6:365–377. &#xA;                    http://mary.dfki.de&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR10" id="ref-link-section-d22556e358">10</a>], make the output of partial processing results a straightforward task. The XML data can be further analysed by subsequent processing components using standard XML parsers. Emotions influence the speech audio signal to a great extent [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Schröder M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions in view of speech synthesis. In: Proceedings of Eurospeech’01, vol 1, pp 87-90" href="/article/10.1007/s10055-005-0153-5#ref-CR6" id="ref-link-section-d22556e361">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Banse R, Scherer K (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70(3):614–636" href="/article/10.1007/s10055-005-0153-5#ref-CR11" id="ref-link-section-d22556e364">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Yang L (2001) Prosody as expression of emotion. In: Cavé Ch (ed) Proceedings of ORAGE 2001, Oralité et gestualité, pp 209–212" href="/article/10.1007/s10055-005-0153-5#ref-CR12" id="ref-link-section-d22556e367">12</a>]. People are able to identify emotions from the audio signal alone with an accuracy well above chance level [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Banse R, Scherer K (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70(3):614–636" href="/article/10.1007/s10055-005-0153-5#ref-CR11" id="ref-link-section-d22556e371">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Yang L (2001) Prosody as expression of emotion. In: Cavé Ch (ed) Proceedings of ORAGE 2001, Oralité et gestualité, pp 209–212" href="/article/10.1007/s10055-005-0153-5#ref-CR12" id="ref-link-section-d22556e374">12</a>]. Hence an expressive talking head should not only include emotions in the facial animation, but must also be capable of emotional audible speech.</p><p>The modelling of emotion in speech synthesis relies on a number of parameters like, among others, fundamental frequency (F0) level, voice quality or articulatory precision [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Schröder M (2001) Emotional speech synthesis: a review. In: Proceedings of Eurospeech 2001, Aalborg, pp 561–564. &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR13" id="ref-link-section-d22556e380">13</a>]. Different synthesis techniques provide control over these parameters to very different degrees. Formant synthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Allen J, Hunnicutt S, Klatt DH (1987) From text to speech: the MITalk system. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-005-0153-5#ref-CR14" id="ref-link-section-d22556e383">14</a>], the most parameterisable synthesis technique, has been extensively used in emotional speech synthesis research [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Cahn J (1990) The generation of affect in synthesized speech. J Am Voice I/O Soc 8:1–19" href="/article/10.1007/s10055-005-0153-5#ref-CR15" id="ref-link-section-d22556e386">15</a>], but is nowadays rarely used because of its low degree of naturalness. Unit selection synthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Black AW, Campbell N (1995) Optimising selection of units from speech databases for concatenative synthesis. In: Proceedings of Eurospeech 1995, Madrid, pp 581–584" href="/article/10.1007/s10055-005-0153-5#ref-CR16" id="ref-link-section-d22556e389">16</a>], relying on the re-sequencing of units from large speech corpora, is the most recent and natural-sounding speech synthesis technique, but lacks the flexibility required for a general-purpose emotional expression tool. Only a small number of expressive categories can be modelled using this technique [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Johnson W, Narayanan S, Whitney R, Das R, Bulut M, LaBore C (2002) Limited domain synthesis of expressive military speech for animated characters. In: Proceedings of the 7th international conference on spoken language processing, Denver" href="/article/10.1007/s10055-005-0153-5#ref-CR17" id="ref-link-section-d22556e392">17</a>]. Diphone synthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Dutoit Th, Pagel V, Pierret N, Bataille F, van der Vrecken O (1996) The MBROLA project: towards a set of high quality speech synthesisers free of use for non commercial purposes. In: Proceedings of the 4th international conference of spoken language processing, Philadelphia, pp 1393–1396" href="/article/10.1007/s10055-005-0153-5#ref-CR18" id="ref-link-section-d22556e396">18</a>] is a compromise between naturalness and flexibility, and can be used for emotion expression if the diphone units concatenated are recorded in several voice qualities [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Schröder M, Grice M (2003) Expressing vocal effort in concatenative synthesis. In: Proceedings of the 15th international conference of phonetic sciences, Barcelona" href="/article/10.1007/s10055-005-0153-5#ref-CR19" id="ref-link-section-d22556e399">19</a>].</p><h3 class="c-article__sub-heading" id="Sec4">Facial animation systems</h3><p>Approaches to facial animation in general can be divided into the following classes: physics-based [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Lee Y, Terzopoulos D, Waters K (1995) Realistic face modeling for animation. In: Proceedings of SIGGRAPH’95, pp 55–62" href="/article/10.1007/s10055-005-0153-5#ref-CR20" id="ref-link-section-d22556e410">20</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Kähler K, Haber J, Seidel HP (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of Graphics Interface, pp 37–46" href="/article/10.1007/s10055-005-0153-5#ref-CR21" id="ref-link-section-d22556e413">21</a>], example-based [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bregler Ch, Covell M, Slaney M (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH ’97. ACM Press, Palo Alto, pp 353–360" href="/article/10.1007/s10055-005-0153-5#ref-CR22" id="ref-link-section-d22556e416">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH ’99, pp 21–28" href="/article/10.1007/s10055-005-0153-5#ref-CR23" id="ref-link-section-d22556e419">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Ezzat T, Geiger G, Poggio T (2002) Trainable videorealistic speech animation. In: Proceedings of SIGGRAPH’02, pp 388–398" href="/article/10.1007/s10055-005-0153-5#ref-CR24" id="ref-link-section-d22556e422">24</a>] and parameterised systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Parke F (1974) A parametric model for human faces. University of Utah, Salt Lake City" href="/article/10.1007/s10055-005-0153-5#ref-CR25" id="ref-link-section-d22556e426">25</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalmann D (eds) Models and techniques in computer animation, pp 139–156" href="/article/10.1007/s10055-005-0153-5#ref-CR26" id="ref-link-section-d22556e429">26</a>]. Physically based approaches try to model the anatomical structure of the face as well as the underlying dynamics; facial movement is achieved by muscle contraction. Parameterised systems assign weighted vertices of the face mesh to every parameter. During animations, the vertices are displaced according to the parameter value. Example- or performance-based techniques usually re-assemble frames from video footage or track movement of a real person to yield the desired new animation.</p><p>Maybe the most challenging application of facial animation is speech synchronisation. Due to co-articulation, designing a facial expression for every phoneme and then interpolating between the expressions according to the input phonemes does not produce realistic results. Several approaches to lip sync have emerged. Procedural methods try to synthesise lip movement for speech from scratch [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalmann D (eds) Models and techniques in computer animation, pp 139–156" href="/article/10.1007/s10055-005-0153-5#ref-CR26" id="ref-link-section-d22556e435">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Pelachaud C, Badler N, Steedman M (1991) Linguistic issues in facial animation. In: Magnenat-Thalmann N, Thalmann D (eds) Computer animation’91" href="/article/10.1007/s10055-005-0153-5#ref-CR27" id="ref-link-section-d22556e438">27</a>]. Example-based systems re-order video frames of a speaking person so that they fit the new speech [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bregler Ch, Covell M, Slaney M (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH ’97. ACM Press, Palo Alto, pp 353–360" href="/article/10.1007/s10055-005-0153-5#ref-CR22" id="ref-link-section-d22556e441">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH ’99, pp 21–28" href="/article/10.1007/s10055-005-0153-5#ref-CR23" id="ref-link-section-d22556e444">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Ezzat T, Geiger G, Poggio T (2002) Trainable videorealistic speech animation. In: Proceedings of SIGGRAPH’02, pp 388–398" href="/article/10.1007/s10055-005-0153-5#ref-CR24" id="ref-link-section-d22556e447">24</a>] or extract visemes from video and use them to animate 3D head models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Kalberer G, Müller P, Van Gool L (2003) A visual speech generator. In: Proceedings of Videometrics VII. IS&amp;SPIE, pp 173–183" href="/article/10.1007/s10055-005-0153-5#ref-CR28" id="ref-link-section-d22556e451">28</a>].</p><p>In addition, speech is accompanied by non-verbal facial expressions which serve to structure speech and to emphasise important parts of a sentence, thereby facilitating understanding. Eyebrow movement, for instance, can serve to accentuate important words or parts of a sentence, to structure speech or to mark a sentence as question. There are several approaches to incorporate non-verbal speech-related facial expressions into animations of speech. Learning-based systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH ’99, pp 21–28" href="/article/10.1007/s10055-005-0153-5#ref-CR23" id="ref-link-section-d22556e457">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Lee S, Badler J, Badler N (2002) Eyes alive. In: Proceedings of SIGGRAPH’02, pp 637–644" href="/article/10.1007/s10055-005-0153-5#ref-CR29" id="ref-link-section-d22556e460">29</a>] are trained to generate facial animations from speech that include non-verbal speech-related facial expressions. Script-based systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Pearce A, Wyvill B, Wyvill G, Hill D (1986) Speech and expression: a computer solution to face animation. In: Proceedings of Graphics Interface ’86, pp 136–140" href="/article/10.1007/s10055-005-0153-5#ref-CR30" id="ref-link-section-d22556e463">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Ip H, Chan C (1996) Script-based facial gesture and speech animation using a NURBS based face model. Comput Graphics 20(6):881–891" href="/article/10.1007/s10055-005-0153-5#ref-CR31" id="ref-link-section-d22556e466">31</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Kalra P, Mangili A, Magnenat-Thalmann N, Thalmann D (1991) SMILE: a multilayered facial animation system. In: Proceedings of IFIP WG 5.10, Tokyo, pp 189–198" href="/article/10.1007/s10055-005-0153-5#ref-CR32" id="ref-link-section-d22556e469">32</a>] leave synchronisation of non-verbal facial expressions to speech to the user. Rule-based methods generate such expressions from analysis of content, utterance structure and dialog state [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T, Douville B, Prevost S, Stone M (1994) Animated conversation: rule-based generation of facial expression gesture and spoken intonation for multiple conversational agents. In: Proceedings of SIGGRAPH ’94, pp 413–420" href="/article/10.1007/s10055-005-0153-5#ref-CR33" id="ref-link-section-d22556e473">33</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Pelachaud C, Badler N, Steedman M (1996) Generating facial expressions for speech. Cogn Sci 20(1):1–46" href="/article/10.1007/s10055-005-0153-5#ref-CR34" id="ref-link-section-d22556e476">34</a>], from an analysis of the speech signal [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Albrecht I, Haber J, Seidel H-P (2002) Automatic generation of non-verbal facial expressions from speech. In: Proceedings of CGI, pp 283–293" href="/article/10.1007/s10055-005-0153-5#ref-CR35" id="ref-link-section-d22556e479">35</a>], or from intermediate output of a coupled TTS [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Albrecht I, Haber J, Kähler K, Schröder M, Seidel H-P (2002) May I talk to you? :-)—facial animation from text. In: Proceedings of Pacific Graphics, pp 77–86" href="/article/10.1007/s10055-005-0153-5#ref-CR36" id="ref-link-section-d22556e482">36</a>].</p><p>Information is often also relayed through emotional expressions. Ekman [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Ekman P, Keltner D (1997) Universal facial expressions of emotion: an old controversy and new findings. In: segerstrøle U, Molnár P (eds) Nonverbal communication: where nature meets culture. Lawrence Erlbaum Associates Inc., Mahwah, pp 27–46" href="/article/10.1007/s10055-005-0153-5#ref-CR37" id="ref-link-section-d22556e488">37</a>] identified a set of six basic emotional facial expressions that are valid throughout all cultures: joy, anger, fear, disgust, sadness and surprise. Many facial animation systems can display these universal expressions of emotion. However, the human face is capable of displaying many more emotional expressions, but little research has been conducted in this direction so far, mainly due to the limited availability of data on other expressions. The FacEMOTE system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Byun M, Badler N (2002) FacEMOTE: qualitative parametric modifiers for facial animations. In: Proceedings of SCA’02, pp 65–71" href="/article/10.1007/s10055-005-0153-5#ref-CR38" id="ref-link-section-d22556e491">38</a>] relies on the Laban Movement Analysis of body motion which has been transferred to the face. The method modifies an input facial animation stream to change its expressiveness. The four parameter pairs used to steer the process are direct–indirect, light–strong, sustained–quick, and free–bound. A direct mapping from these parameters to emotions is not provided. Ruttkay et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Ruttkay Z, Noot H, ten Hagen P (2003) Emotion disc and emotion squares: tools to explore the facial expression space. Comput Graphics Forum 22(1):49–53" href="/article/10.1007/s10055-005-0153-5#ref-CR39" id="ref-link-section-d22556e494">39</a>] arrange the six basic emotions equidistantly on the border of a disc according to similarity. To every point on the disc a facial expression is associated, which is computed by linear interpolation between the closest basic emotions. Distance from the circle center describes intensity. In the same paper, the authors present a second method to obtain new expressions based on PCA. However, they did not find the significant principal components to be as intuitive as one might expect.</p><p>Tsapatsoulis et al. ([<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e501">1</a>], Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec20">4.2</a>) have also developed a method to interpolate between affect displays to create new ones. They use a mixture of two emotion models, Whissell’s activation-evaluation approach ([<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131" href="/article/10.1007/s10055-005-0153-5#ref-CR40" id="ref-link-section-d22556e507">40</a>, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec7">2.3.2</a>) and Plutchik’s emotion wheel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York" href="/article/10.1007/s10055-005-0153-5#ref-CR41" id="ref-link-section-d22556e513">41</a>].</p><p>Bui et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Bui T, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of CGI’04, pp 284–291" href="/article/10.1007/s10055-005-0153-5#ref-CR42" id="ref-link-section-d22556e519">42</a>] address the problem of combining different channels of facial expressions, i.e., lip sync, conversational displays, emotional expressions, etc.</p><h3 class="c-article__sub-heading" id="Sec5">Emotion representations</h3><p>Modelling of emotional expression needs to start from a suitable representation of the emotional states to be expressed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Emotion categories</h4><p>The most straightforward description of emotions is the use of emotion-denoting words, or category labels. Human languages have proven to be extremely powerful in producing labels for emotional states: lists of emotion-denoting adjectives were compiled that include at least 107 items [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131" href="/article/10.1007/s10055-005-0153-5#ref-CR40" id="ref-link-section-d22556e536">40</a>]. Several approaches exist for reducing these to an essential core set, the most used in the literature being basic emotions, a Darwinian concept [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Cowie R, Cornelius R (2003) Describing the emotional states that are expressed in speech. Speech Commun Spec Issue Speech Emotion 40(1–2):5–32" href="/article/10.1007/s10055-005-0153-5#ref-CR3" id="ref-link-section-d22556e539">3</a>]. Based on the work by Ekman [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Ekman P, Keltner D (1997) Universal facial expressions of emotion: an old controversy and new findings. In: segerstrøle U, Molnár P (eds) Nonverbal communication: where nature meets culture. Lawrence Erlbaum Associates Inc., Mahwah, pp 27–46" href="/article/10.1007/s10055-005-0153-5#ref-CR37" id="ref-link-section-d22556e542">37</a>], basic emotions are usually used for modelling facial expression of emotions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Emotion dimensions</h4><p>Many different approaches reported in the psychological literature have led to the proposal of dimensions underlying emotional concepts (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e553">43</a>] for an overview). Different researchers came to propose two essential dimensions: <i>activation</i> (from active/aroused to passive/relaxed) and <i>evaluation</i> (from negative/bad to positive/good), sometimes complemented by a third dimension: <i>power</i> (from powerful/dominant to weak/submissive). These emotion dimensions are gradual in nature and represent the essential aspects of emotion concepts rather than the fine specifications of individual emotion categories. The names used for these dimensions were selected by the individual researchers <i>interpreting</i> their data, and did not arise from the data itself. This explains the large variation found in the literature regarding the names of the dimensions.</p><p>One concrete proposal for an emotion dimension model is the activation-evaluation space, proposed by Cowie et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Cowie R, Douglas-Cowie E, Savvidou S, McMahon E, Sawey M, Schröder M (2000) ‘FEELTRACE’: an instrument for recording perceived emotion in real time. In: Proceedings of the ISCA workshop on speech and emotion, Northern Ireland, pp 19–24. &#xA;                    http://www.qub.ac.uk/en/isca/proceedings&#xA;                    &#xA;                  , pp 19–24" href="/article/10.1007/s10055-005-0153-5#ref-CR44" id="ref-link-section-d22556e571">44</a>]. In accordance to Plutchik’s emotion wheel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York" href="/article/10.1007/s10055-005-0153-5#ref-CR41" id="ref-link-section-d22556e574">41</a>], they conceived of the space as circular; but they complemented the circle by a disk whose outer bounds represent maximally intense emotions, while its centre (the origin of the two-dimensional space) represents a “neutral”, unemotional state. The further a state is from the centre, the more intense it is, i.e., the radial distance from the centre is a measure of emotion intensity (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig1">1</a>). In accordance with Whissell [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131" href="/article/10.1007/s10055-005-0153-5#ref-CR40" id="ref-link-section-d22556e580">40</a>], emotion categories can be located in that space.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5flb1.eps?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5flb1.eps" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The two-dimensional, disk-shaped activation-evaluation space proposed by Cowie et al</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Requirements for a natural emotionally expressive system</h4><p>Databases of naturally occurring emotions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Douglas-Cowie E, Campbell N, Cowie R, Roach P (2003) Emotional speech: towards a new generation of databases. Speech Commun Spec Issue Speech Emotion 40(1–2):33–60" href="/article/10.1007/s10055-005-0153-5#ref-CR45" id="ref-link-section-d22556e608">45</a>] show that humans usually express low-intensity rather than fullblown emotions, and complex, mixed emotions rather than mere basic emotions downscaled to a to a low intensity. A system intended to simulate this kind of expressivity needs to use an emotion representation capable of representing such states. Emotion dimensions are a suitable representation: they are naturally gradual, and are capable of representing low-intensity as well as high-intensity states. While they do not define the exact properties of an emotional state in the same amount of detail as a category label, they do capture its essential aspects.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Mappings between emotion representations</h4><p>Emotion categories can be <i>located</i> in emotion dimension space via rating tests [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Cowie R, Douglas-Cowie E, Appolloni B, Taylor J, Romano A, Fellenz W (1999) What a neural net needs to know about emotion words. In: Mastorakis N (ed) Computational intelligence and applications. World Scientific &amp; Engineering Society Press, pp 109–114" href="/article/10.1007/s10055-005-0153-5#ref-CR46" id="ref-link-section-d22556e623">46</a>]. The mapping from categories to dimensions is therefore a simple task, as long as the co-ordinates of the emotion category have been determined. The inverse, however, is not possible: as emotion dimensions only capture the most essential aspects of an emotion concept, they provide an underspecified description of an emotional state. For example, the coordinates for anger and disgust may be very close, because the two categories share the same activation/evaluation/power properties. The features distinguishing between the two categories cannot be represented using emotion dimensions, so that the corresponding region in space can only be mapped to “anger-or-disgust” rather than a specific category. One concrete proposal for a mapping from a list of emotion categories to emotion dimensions was brought forward as a working model by the NECA project [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Krenn B, Pirker H, Grice M, Piwek P, van Deemter K, Schröder M, Klesen M, Gstrein E (2002) Generation of multimodal dialogue for net environments. In: Proceedings of Konvens, Saarbrücken.&#xA;                    http://www.ai.univie.ac.at/NECA&#xA;                    &#xA;                  , Saarbrücken" href="/article/10.1007/s10055-005-0153-5#ref-CR47" id="ref-link-section-d22556e626">47</a>] (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Coordinates for a list of emotion categories on the three emotion dimensions activation, evaluation and power, as proposed as a first working model by the NECA project</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0153-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>It should be kept in mind that, given methodological issues [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Cowie R, Cornelius R (2003) Describing the emotional states that are expressed in speech. Speech Commun Spec Issue Speech Emotion 40(1–2):5–32" href="/article/10.1007/s10055-005-0153-5#ref-CR3" id="ref-link-section-d22556e1249">3</a>] as well as the limited empirical basis in existing studies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131" href="/article/10.1007/s10055-005-0153-5#ref-CR40" id="ref-link-section-d22556e1252">40</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Douglas-Cowie E, Campbell N, Cowie R, Roach P (2003) Emotional speech: towards a new generation of databases. Speech Commun Spec Issue Speech Emotion 40(1–2):33–60" href="/article/10.1007/s10055-005-0153-5#ref-CR45" id="ref-link-section-d22556e1255">45</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Cowie R, Douglas-Cowie E, Appolloni B, Taylor J, Romano A, Fellenz W (1999) What a neural net needs to know about emotion words. In: Mastorakis N (ed) Computational intelligence and applications. World Scientific &amp; Engineering Society Press, pp 109–114" href="/article/10.1007/s10055-005-0153-5#ref-CR46" id="ref-link-section-d22556e1258">46</a>], mappings between the currently existing emotion representations are necessarily imperfect.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Building blocks</h2><div class="c-article-section__content" id="Sec10-content"><p>The talking head system used in this paper (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig2">2</a>) consists of two main building blocks, a TTS system and a facial animation system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb2.tif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb2.tif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>System overview. The TTS system performs a text analysis on the input text. It generates the audio signal and passes intermediate results like phonemes etc. to the facial animation module</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The system input is plain text, which is passed to the TTS system. Here the text is transformed into a basic XML skeleton that is enriched continuously as the linguistic analysis of the input proceeds, until all necessary information has been assembled and the actual acoustic speech synthesis takes place. A detailed description of the TTS system is provided in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec11">3.1</a>.</p><p>The final XML structure is not only used by the TTS module, but also by the facial animation system. It extracts the phonemes and their durations for the lip sync, as well as additional linguistic information such as intonation and pauses that serve to generate the non-verbal parts of the animation, e.g., eyebrow raising on accented parts of the speech. The facial animation system will be discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec12">3.2</a>.</p><p>Connecting both components leads to synthetic audio-visual speech. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec15">4</a> will explain how emotional expressivity is added to this base system.</p><h3 class="c-article__sub-heading" id="Sec11">The text-to-speech component</h3><p>The TTS system Mary [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schröder M, Trouvain J (2003) The German text-to-speech synthesis system MARY: a tool for research development and teaching. Int J Speech Technology 6:365–377. &#xA;                    http://mary.dfki.de&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR10" id="ref-link-section-d22556e1317">10</a>] is a TTS server written in Java.<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> It is a very flexible toolkit allowing for easy integration of modules from different origins. An XML-based representation language [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Schröder M, Breuer S (2004) XML representation languages as a way of interconnecting TTS modules. In: Proceedings of ICSLP’04, Jeju" href="/article/10.1007/s10055-005-0153-5#ref-CR48" id="ref-link-section-d22556e1332">48</a>] is used in the system, which makes it easy to access the system’s intermediate processing results, as is required for time-alignment with the visual component. The system uses the following modules.</p><p>Text normalisation consists of an optional input markup parser converting SSML into MaryXML; a tokeniser; a pre-processing component converting numbers, abbreviations, etc. into pronounceable form; a part-of-speech tagger and chunker (local syntactic parser); and an information structure module recognising givenness and contrast based on text structure, optionally using a semantic database.</p><p>Phonemisation is performed using a pronunciation lexicon compiled into a finite state transducer, complemented with letter-to-sound rules.</p><p>Duration prediction is carried out using a set of rules predicting phoneme durations based on intrinsic phoneme properties and the surrounding context.</p><p>Intonation prediction is carried out in two rule-based steps. First, symbolic intonation labels are predicted; second, these symbolic labels are translated into frequency-time targets.</p><p>The synthesis module is instantiated using several synthesis engines, among them MBROLA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Dutoit Th, Pagel V, Pierret N, Bataille F, van der Vrecken O (1996) The MBROLA project: towards a set of high quality speech synthesisers free of use for non commercial purposes. In: Proceedings of the 4th international conference of spoken language processing, Philadelphia, pp 1393–1396" href="/article/10.1007/s10055-005-0153-5#ref-CR18" id="ref-link-section-d22556e1347">18</a>].</p><h3 class="c-article__sub-heading" id="Sec12">The facial animation component</h3><p>The physics-based facial animation system has been developed following human anatomy [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Kähler K, Haber J, Seidel HP (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of Graphics Interface, pp 37–46" href="/article/10.1007/s10055-005-0153-5#ref-CR21" id="ref-link-section-d22556e1358">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Kähler K, Haber J, Seidel H-P (2002) Head shop: generating animated head models with anatomical structure. In: Proceedings of SCA’02, pp 55–64" href="/article/10.1007/s10055-005-0153-5#ref-CR49" id="ref-link-section-d22556e1361">49</a>]. It includes skull, jaw, muscles and skin. Skin and skull consist of triangle meshes. The muscles are modeled as a volume, i.e., they have a geometrical shape that bulges during contraction and is elongated if the muscle is being stretched passively. The three components skull, muscles and skin are connected through a mass-spring network. Thence if a muscle contracts, the skin is moved along appropriately and the bulging of the muscle is propagated to the skin, which deforms accordingly. The model includes simple textured geometry for the eyes, teeth and tongue. In addition to the muscle contraction parameters, parameters for head rotation, jaw rotation, eye and tongue movement are defined. Animations are specified through muscle contraction values and the above additional parameter values, varying over time. They are executed in real-time (40 fps for the simulation, ≈100 fps for rendering, on a Pentium 4 1.7 GHz dual processor PC).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Lip sync</h4><p>Lip sync is implemented following the approach by Cohen and Massaro [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalmann D (eds) Models and techniques in computer animation, pp 139–156" href="/article/10.1007/s10055-005-0153-5#ref-CR26" id="ref-link-section-d22556e1371">26</a>]. It considers co-articulation by assigning a so-called dominance function <i>D</i><sub><i>s</i>,<i>p</i></sub>(<i>t</i>) to every phoneme-facial animation parameter pair (<i>s</i>,<i>p</i>). <i>D</i><sub><i>s</i>,<i>p</i></sub> describes the influence of the phoneme <i>s</i> on the facial parameter <i>p</i> over time. All phonemes <i>s</i> have a target value <i>T</i><sub><i>s</i>,<i>p</i></sub> for every parameter <i>p</i> assigned to them. The target values of one phoneme for all facial animation parameters give the target facial expression of the phoneme, i.e., the corresponding viseme. The trajectory <i>a</i><sub>
                      <i>p</i>
                    </sub>(<i>t</i>) for parameter <i>p</i> over the entire utterance is obtained with the following formula: </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ a_p(t) =\sum_{s=0}^{n-1}\frac{D_{s,p}(t)T_{s,p}}{D_{s,p}(t)},$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <i>n</i> is the number of segments in the utterance. Since we obtain the phonemes and their durations directly from the speech processing system, visible and audible speech are synchronised inherently.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Non-verbal behaviour</h4><p>Apart from speech, several other channels of communication are open to humans, e.g., facial expressions, gestures, body posture, voice quality or touching. Such non-verbal behaviour can be categorised into five groups [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Ekman P, Wallace W (1969) The repertoire of nonverbal behavior: categories origins usage and coding. Semiotica 1:49–98" href="/article/10.1007/s10055-005-0153-5#ref-CR50" id="ref-link-section-d22556e1465">50</a>]: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(1)</span>
                        
                          <p><i>emblems</i>: non-verbal acts that have a well known verbal translation, e.g., nodding for “yes”;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(2)</span>
                        
                          <p><i>illustrators</i>: speech-accompanying movement that illustrates what is being said, e.g., eyebrow raising on accented syllables;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(3)</span>
                        
                          <p><i>affect displays</i>: facial expressions of emotions;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(4)</span>
                        
                          <p><i>regulators</i>: behaviour related to turn-taking during a conversation, e.g., looking towards the speaker at the end of questions to prompt for an answer;</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(5)</span>
                        
                          <p><i>adaptors</i>: content-free behaviour such as touching or rubbing oneself.</p>
                        
                      </li>
                    </ol><p>The animations generated with our system show behaviour pertaining to three of the five categories: illustrators, affect displays and regulators. We decided to leave out the emblems, since they require some kind of semantic analysis, and the adaptors, since they do not convey any information. Synchronisation of these expressions to the speech at phoneme level is rendered possible by the TTS system which provides data on sentence, phrase,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> word, syllable and phoneme boundaries as well as pitch information at phoneme level and information on the type of sentence (e.g., question or informative). In our implementation, we followed the approach given in Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Albrecht I, Haber J, Kähler K, Schröder M, Seidel H-P (2002) May I talk to you? :-)—facial animation from text. In: Proceedings of Pacific Graphics, pp 77–86" href="/article/10.1007/s10055-005-0153-5#ref-CR36" id="ref-link-section-d22556e1543">36</a>].</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Emotional expressivity</h2><div class="c-article-section__content" id="Sec15-content"><p>The expression of emotions by means of a combination of emotion categories and emotion dimensions is explained in detail in the present section.</p><h3 class="c-article__sub-heading" id="Sec16">Emotional text-to-speech</h3><p>The emotional text-to-speech synthesis system used in this work is the one developed by Schröder [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e1562">43</a>]. As it is based on linking emotion dimensions to their acoustic correlates, it integrates well with our approach to visual expression modelling presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec20">4.2</a>.</p><p>The key properties of the emotional speech synthesis system are reported below.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Emotional prosody rules</h4><p>Schröder [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e1577">43</a>] formulated emotional prosody rules on the basis of a literature review and a database analysis. His literature review brought about the following results. An unambiguous agreement exists concerning the link between the activation dimension and the most frequently measured acoustic parameters: activation is positively correlated with mean F0, mean intensity and, in most cases, with speech rate. Additional parameters positively correlated with activation are pitch range, “blaring” timbre, high-frequency energy, late intensity peaks, intensity increase during a “sense unit” and the slope of F0 rises between syllable maxima. Higher activation also corresponds to shorter pauses and shorter inter-pause and inter-breath stretches.</p><p>The evidence for evaluation and power is less stable. There seems to be a tendency that studies which take only a small number of acoustic parameters into account do not find any acoustic correlates of evaluation and/or power.</p><p>The limited evidence regarding the vocal correlates of power indicates that power is basically recognised from the same parameter settings as activation (high tempo, high F0, more high-frequency energy, short or few pauses, large intensity range, steep F0 slope), except that sometimes, high power is correlated with lower F0 instead of higher F0, and power is correlated with vowel duration.</p><p>There is even less evidence regarding the acoustic correlates of evaluation. Positive evaluation seems to correspond to a faster speaking rate, less high-frequency energy, low pitch and large pitch range; a “warm” voice quality; and longer vowel durations and the absence of intensity increase within a “sense unit”.</p><p>In a statistical analysis of the Belfast Naturalistic Emotion Database [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Douglas-Cowie E, Campbell N, Cowie R, Roach P (2003) Emotional speech: towards a new generation of databases. Speech Commun Spec Issue Speech Emotion 40(1–2):33–60" href="/article/10.1007/s10055-005-0153-5#ref-CR45" id="ref-link-section-d22556e1590">45</a>], perceptual ratings of the emotion dimensions activation, evaluation and power were correlated with acoustic measures (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Schröder M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions in view of speech synthesis. In: Proceedings of Eurospeech’01, vol 1, pp 87-90" href="/article/10.1007/s10055-005-0153-5#ref-CR6" id="ref-link-section-d22556e1593">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e1596">43</a>] for details). The study replicated the basic patterns of correlations between emotion dimensions and acoustic variables. It was shown that the acoustic correlates of the activation dimension were highly stable, while correlates of evaluation and power were smaller in number and magnitude and showed a high variability between male and female speakers. In addition, the analysis provided numerical linear regression coefficients which were used as a starting point for the formulation of quantified emotion prosody rules.</p><p>The effects found in the literature and in the database analysis were formulated in a quantified way (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab2">2</a>) and implemented in the MARY TTS system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schröder M, Trouvain J (2003) The German text-to-speech synthesis system MARY: a tool for research development and teaching. Int J Speech Technology 6:365–377. &#xA;                    http://mary.dfki.de&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR10" id="ref-link-section-d22556e1605">10</a>].</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Emotion dimension prosody rules proposed by Schröder</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0153-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab2">2</a>, the columns represent the emotion dimensions, while the rows list all the acoustic parameters for which emotion effects are modelled. The numeric data fields represent the linear coefficients quantifying the effect of the given emotion dimension on the acoustic parameter, i.e., the change from the neutral default value. As an example, the value 0.5% linking <i>activation</i> to <i>rate</i> means that for an activation level of +50, rate increases by +25%, while for an activation level of −30, rate decreases by −15%.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Implementation</h4><p>The Mary system (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec11">3.1</a>) was used as the platform for the implementation of the emotional prosody rules specified in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab2">2</a>. This system was most suitable for the task because of the high degree of flexibility and control over the various processing steps, which arises from the use of the system-internal representation language MaryXML.</p><p>A major design feature in the technical realisation of the emotional speech synthesis system was that the acoustic effects of emotions should be specified in one single module. This module adds appropriate MaryXML annotations to the text which are then realised by the respective modules within the Mary system. As a consequence, all of the parameters are global in the sense that they will be applied to all enclosed text. This approach is considered the most transparent, as the link between emotions and their acoustic realisations is not hidden in various processing components, and the easiest to maintain and adapt, as all rules are contained in one document.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">System evaluation</h4><p>The appropriateness of the generated emotional prosody and voice quality was assessed in a perception test. Due to the multimodal nature of any emotional utterance, this appropriateness was addressed in terms of coherence with other channels expressing the emotion, notably verbal content and the situational context.</p><p>Verbal situation descriptions with known activation and evaluation ratings were used as reference material. For each of the emotional states defined by the situation descriptions, emotional speech prosody settings were calculated, and each of the texts was synthesised with each of the prosodic settings in a factorial design. In a listening test, subjects rated each stimulus according to the question: “How well does the sound of the voice fit with the content of the text?”</p><p>The results confirmed the hypothesis that the prosodic configurations succeed best at conveying the activation dimension. Moreover, the appropriateness of a prosodic configuration for a given emotional state was shown to depend on the <i>degree</i> of similarity between the emotional state intended to be expressed by the prosody and that in the textual situation description. In agreement with previous findings for human speech, the evaluation dimension was found to be more difficult to convey through the prosody. In summary, the speech synthesis system succeeded in expressing the activation dimension (the speaker “arousal”), but not the evaluation dimension. See Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e2113">43</a>] for a full account of the experiment.</p><h3 class="c-article__sub-heading" id="Sec20">Intermediate facial expressions</h3><p>The human face is capable of displaying many more emotional expressions than just those of the six universal emotions joy, anger, fear, disgust, sadness and surprise. However, little visual data is available on expressions of other emotions, and modelling them is hard, since differences between them are often subtle. Hence Tsapatsoulis et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e2125">1</a>] have developed a method to interpolate between affect displays to create new ones. We present their original work before we describe our own model derived from theirs.</p><p>Tsapatsoulis et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e2131">1</a>] modelled emotions using a combination of two emotion models: Plutchik [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York" href="/article/10.1007/s10055-005-0153-5#ref-CR41" id="ref-link-section-d22556e2134">41</a>] ordered 142 emotion words according to their similarity. He found that they can be arranged around a circle, the so-called <i>emotion wheel</i>. Hence the relative position of each emotion can be described by an angle ([<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York" href="/article/10.1007/s10055-005-0153-5#ref-CR41" id="ref-link-section-d22556e2140">41</a>] p. 170). This model does not consider activation or intensity, its goal was to establish similarity. In Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131" href="/article/10.1007/s10055-005-0153-5#ref-CR40" id="ref-link-section-d22556e2143">40</a>], Whissell describes the second model, a rating of emotion words according to their co-ordinates on the activation and evaluation dimensions. Tsapatsoulis et al. use the angles in the emotion wheel as a measure of similarity, while they use Whissell’s activation values to describe emotion intensity.</p><p>Tsapatsoulis et al. use the MPEG-4 facial animation parameters (FAPs) to animate their head model. They identified eight fundamental emotions: acceptance, fear, surprise, sadness, disgust, anger, anticipation and joy. These are the starting points for the interpolation. The facial expression <i>e</i> corresponding to an emotion <i>E</i> is described by the following parameters: the activation value <i>a</i><sub>
                    <i>E</i>
                  </sub> of the emotion <i>E</i>; its angle on the emotion wheel ω<sub>
                    <i>E</i>
                  </sub>; the FAPs involved in forming the expression <i>F</i><sub>
                    <i>e</i>
                  </sub>; for each contributing FAP <i>f</i> ∈<i>F</i><sub>
                    <i>e</i>
                  </sub> the range of variations of its value <i>R</i><sub>
                    <i>e</i>
                  </sub>(<i>f</i>) associated with the expression.</p><p>There are two different ways to generate new expressions: if the new emotion <i>E</i><sub>
                    <i>n</i>
                  </sub> is very similar to the fundamental emotion <i>E</i>, i.e., if their facial expressions differ mainly in strength of muscle contraction, then the new expression <i>e</i><sub>
                    <i>n</i>
                  </sub> can be computed from the expression <i>e</i> in the following way: </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F_{e_n} = F_e$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
<div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$R_{e_n}(f) = \frac{a_{E_n}}{a_E} \cdot R_e(f) \quad \forall \; f \in F_e. $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> If the new emotion <i>E</i><sub>
                    <i>n</i>
                  </sub> does not clearly belong to a fundamental category, its facial expression is computed by interpolation between the shifted expressions of the two emotions <i>E</i><sub>1</sub> and <i>E</i><sub>2</sub> that are closest to <i>E</i><sub>
                    <i>n</i>
                  </sub> on the emotion wheel. For an interval <i>I</i>=[<i>i</i><sub>1</sub>,<i>i</i><sub>2</sub>], let </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \sigma (I) = \left\{ {\begin{array}{*{20}l} {{1,} \hfill} &amp; {{i_{1} \leqslant i_{2} } \hfill} \\ {{ - 1,} \hfill} &amp; {{i_{1} &gt; i_{2} } \hfill} \\ \end{array} } \right. $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p> define the sign σ of <i>I</i>. Let <i>c</i>(<i>I</i>) be the center of interval <i>I</i> and <i>s</i>(<i>I</i>) be its length. Then <i>e</i><sub>
                    <i>n</i>
                  </sub> is determined by </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F_{e_n} = F_{e_1} \cup F_{e_2}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
<div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R'_{e_1}(f) &amp;= \frac{a_{E_n}}{a_{E_1}} \cdot R_{e_1}(f) \\ R'_{e_2}(f) &amp;=\frac{a_{E_n}}{a_{E_2}} \cdot R_{e_2}(f)\\ c(R_{e_n}(f)) &amp;= \left( \frac{\omega_{E_n}-\omega_{E_1}} {\omega_{E_2}-\omega_{E_1}} \cdot c(R'_{e_2}(f)) \; + \; \frac{\omega_{E_2}-\omega_{E_n}} {\omega_{E_2}-\omega_{E_1}} \cdot c(R'_{e_1}(f))\right)\\ s(R_{e_n}(f)) &amp;= \left( \frac{\omega_{E_n}-\omega_{E_1}} {\omega_{E_2}-\omega_{E_1}} \cdot s(R'_{e_2}(f)) \; + \; \frac{\omega_{E_2}-\omega_{E_n}} {\omega_{E_2}-\omega_{E_1}} \cdot s(R'_{e_1}(f))\right)\\ R_{e_n}(f) &amp;= \left[ c(R_{e_n}(f)) - \frac{1}{2}\cdot s(R_{e_n}(f)), \; c(R_{e_n}(f)) + \frac{1} {2}\cdot s(R_{e_n}(f)) \right] \\ &amp; \forall \; f \in F_{e_1} \cap F_{e_2}: \sigma(R_{e_1}(f)) = \sigma(R_{e_2}(f)) \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
<div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R_{e_n}(f) &amp;= \frac{a_{E_n}}{a_{E_1}} \cdot R_{e_1}(f) \; \cap \; \frac{a_{E_n}} {a_{E_2}} \cdot R_{e_2}(f) \\ &amp; \forall \; f \in F_{e_1} \cap F_{e_2}:\sigma(R_{e_1}(f)) \neq \sigma(R_{e_2}(f)) \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
<div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$R_{e_n}(f) =\frac{a_{E_n}} {2 \cdot a_{E_1}} \cdot R_{e_1}(f) \quad \forall \; f \in F_{e_1} \setminus F_{e_2}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
<div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$R_{e_n}(f) = \frac{a_{E_n}} {2\cdot a_{E_2}} \cdot R_{e_2}(f) \quad \forall \; f \in F_{e_2} \setminus F_{e_1} $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p> If now <span class="mathjax-tex">\(R_{e_n}(f) = \emptyset,\)</span> then set <span class="mathjax-tex">\(F_{e_n} := F_{e_n} \setminus \{f\}.\)</span></p><p>In case FAP <i>f</i> is involved in the facial expressions of both generating emotions and its variation intervals for both emotions have the same sign (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ6">6</a>), i.e., it describes movement in the same direction, the variation intervals of the generating expressions <i>e</i><sub>1</sub> and <i>e</i><sub>2</sub> are first shifted, so that the resulting expressions have the same activation as <i>E</i><sub>
                    <i>n</i>
                  </sub>. Then from the centers and lengths of the shifted intervals the interval <span class="mathjax-tex">\(F_{e_n}(f)\)</span> can be computed through linear interpolation between the emotion wheel angles. If the variation intervals of <i>e</i><sub>1</sub> and <i>e</i><sub>2</sub> have different signs (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ7">7</a>), the interval of the new expression is the intersection of the original ones. If <i>f</i> is present only in one generating expression [(<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ8">8</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ9">9</a>)], say, <i>e</i><sub>1</sub>, then its variation interval is averaged with the interval of the neutral face <i>e</i><sub>0</sub>, for which <span class="mathjax-tex">\(a_{E_0} = 0\)</span> and <span class="mathjax-tex">\(R_{e_0}(f) = [0] \quad \forall \; f \in F_{e_0}.\)</span></p><p>We have modified the approach to work with our physics-based model. Instead of combining the data from the Whissell and Plutchik studies, as Tsapatsoulis et al. did, we use the set of emotion words with associated co-ordinates on the three dimensions activation, evaluation and power, as proposed by the NECA project [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Krenn B, Pirker H, Grice M, Piwek P, van Deemter K, Schröder M, Klesen M, Gstrein E (2002) Generation of multimodal dialogue for net environments. In: Proceedings of Konvens, Saarbrücken.&#xA;                    http://www.ai.univie.ac.at/NECA&#xA;                    &#xA;                  , Saarbrücken" href="/article/10.1007/s10055-005-0153-5#ref-CR47" id="ref-link-section-d22556e2538">47</a>] (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab1">1</a>). In this first version of the system, we only use the first two dimensions from this Table (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0153-5#Sec22">6</a>).</p><p>We use Cowie et al.’s disk-shaped activation-evaluation space (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig1">1</a>) as our model of emotion dimensions. It appears natural to describe the states in the activation-evaluation space by means of polar co-ordinates, using angular orientation ω and radial distance from the centre <i>r</i>. Here again, the angle ω describes similarity. In contrast to Tsapatsoulis et al., we consider radial distance from the centre of the activation-evaluation space to be a better indicator of emotional intensity than activation (consider the case of despair, which would have high intensity but low activation), and therefore use this radial distance <i>r</i> rather than the activation level <i>a</i> for normalising the archetypal states’ intensities to the intermediate state’s intensity in our equations.</p><p>As our “basic” emotions, we use the closest correlates to the six Ekmanian emotions (joy, anger, fear, disgust, sadness and surprise) that we can find in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab1">1</a>: joy, anger, fear, hate, sorry-for and surprise (as a state with 100% activation, and 0% on evaluation and power). We are aware that these are crude approximations, which should be taken as illustrating the idea rather than as a final truth.</p><p>Since our animations are based mostly on muscle contractions instead of MPEG-4 FAPs, we had to adapt the approach to also work with muscles. We defined our expressions through single muscle contraction values <i>v</i>. They can be uniformly scaled by a number between 0 and 1 to achieve different intensities of the expressed emotion, but we leave it to the animator to decide how small the scaling value can be so that the resulting expression is still perceived as the same emotion. As a consequence, we have no means of deciding for a given muscle whether to use (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ6">6</a>) or (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ7">7</a>). Hence we identified facial muscles that operate in a roughly antagonistic fashion (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-005-0153-5#Tab3">3</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Facial muscles operating in a roughly antagonistic fashion</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-005-0153-5/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Let <i>M</i><sub>
                    <i>e</i>
                  </sub> be the set of muscles involved in expression <i>e</i> of emotion <i>E</i> and <i>v</i><sub>
                    <i>e</i>
                  </sub>(<i>m</i>) the contraction value of <i>m</i>. For simplicity, the animation parameters of eyes, head and jaw rotation, and tongue are included in <i>M</i><sub>
                    <i>e</i>
                  </sub>. This leads to the following modified algorithm for the case where the new emotion <i>E</i><sub>
                    <i>n</i>
                  </sub> is similar to a fundamental emotion <i>E</i>: </p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$M_{e_n} = M_e$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div>
<div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$v_{e_n}(m) = \frac{r_{E_n}} {r_E} \cdot v_e(m) \quad \forall \; m \in M_e.$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p> Since several muscles can be antagonistic to others, e.g., the <i>orbicularis oris</i> to both the <i>risorius left</i> and the <i>risorius right</i>, we define for every muscle <i>m</i> the set of its antagonists as <i>A</i><sub>−</sub>(<i>m</i>) and the set of muscles that share these antagonists as <i>A</i><sub>+</sub>(<i>m</i>). For <i>m</i> = <i>risorus left</i> for instance, <i>A</i><sub>+</sub>(<i>m</i>) = {<i>risorius </i><i>left</i>,  <i>risorius </i><i>right</i>} and <i>A</i><sub>−</sub>(<i>m</i>) = {<i>orbicularis </i><i>oris</i>}. If the facial expression for <i>E</i><sub>
                    <i>n</i>
                  </sub> is computed from two fundamental expressions <i>E</i><sub>1</sub> and <i>E</i><sub>2</sub>, we get: </p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$M_{e_n} = M_{e_1} \cup M_{e_2} $$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
<div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} v'_{e_1}(m) =&amp;\frac{r_{E_n}}{r_{E_1}} \cdot v_{e_1}(m) \\ v'_{e_2}(m) =&amp; \frac{r_{E_n}} {r_{E_2}} \cdot v_{e_2}(m) \\ v_{e_n}(m) =&amp; \left( \frac{\omega_{E_n}-\omega_{E_1}} {\omega_{E_2}-\omega_{E_1}} \cdot v'_{e_2}(m) \; + \; \frac{\omega_{E_2}-\omega_{E_n}} {\omega_{E_2}-\omega_{E_1}} \cdot v'_{e_1}(m)\right) \\ &amp; \forall \; m \in M_{e_n}: \; A_+(m) \; \cup \; A_-(m) = \emptyset \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
<div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} S_+ =&amp; \sum_{m' \in A_+(m)}\left(\frac{r_{E_n}} {r_{E_1}} \cdot v_{e_1}(m') \;+\; \frac{r_{E_n}} {r_{E_2}} \cdot v_{e_2}(m')\right) \\ S_- =&amp; \sum_{m' \in A_-(m)}\left(\frac{r_{E_n}}{r_{E_1}} \cdot v_{e_1}(m') \;+\; \frac{r_{E_n}} {r_{E_2}} \cdot v_{e_2}(m')\right) \\ v_{e_n}(m)=&amp; \left\{ \begin{array}{l} 0,\quad {\text{if}}\;S_{ + } \leqslant S_{ - } \\ (S_{ + } - S_{ - } ) \cdot \frac{1} {{S_{ + } }} \cdot {\left( {\frac{{r_{{E_{n} }} }} {{r_{{E_{1} }} }} \cdot v_{{e_{1} }} (m) + \frac{{r_{{E_{n} }} }} {{r_{{E_{2} }} }} \cdot v_{{e_{2} }} (m)} \right)}, \\ \;\;\;\;\;\;\;\;{\text{else}} \\ \end{array} \right.\\ &amp;\forall m \in M_{{e_{n} }} :A_{ + } (m) \cup A_{ - } (m) \ne \emptyset. \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>Equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ12">12</a>) is analogue to (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ5">5</a>). The differences in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ13">13</a>) stem from the use of a single value instead of an interval. This obviates the need to compute the center and length of the interval. Instead we can scale and interpolate the contraction values directly. Since they do not describe a direction but a value, no conflict arises. The main difference lies in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ14">14</a>). <i>S</i><sub>+</sub>(<i>m</i>) and <i>S</i><sub>−</sub>(<i>m</i>) are the summed, scaled contraction values of all muscles in the same and different antagonistic class. The overall scaled contraction for all muscles in the same and the antagonistic class of <i>m</i> is <i>S</i><sub>+</sub>(<i>m</i>) − <i>S</i><sub>−</sub>(<i>m</i>). This is distributed to the individual muscles of the set with the stronger scaled overall contraction according to their contribution to that value. If we assign contraction values <span class="mathjax-tex">\(v_{e_1}(m) = 0 \quad \forall m \in M_{e_2} \backslash M_{e_1}\)</span> and <span class="mathjax-tex">\(v_{e_2}(m) = 0 \quad \forall m \in M_{e_1} \backslash M_{e_2},\)</span> this obviates the need for (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ8">8</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ9">9</a>).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig3">3</a>, the new expressions <i>anxiety</i> and <i>panic fear</i> have been generated as a scaled version of <i>fear</i>, following the method in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ10">10</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ11">11</a>). In the examples in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig4">4</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0153-5#Fig5">5</a>, new expressions (center of each row), have been generated from the fundamental expressions to the left and right as described in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ12">12</a>)–(<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-005-0153-5#Equ14">14</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb3.tif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb3.tif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><i>Anxiety</i> and <i>panic</i> belong to the same fundamental class as <i>fear</i>, but differ in intensity. Therefore their facial expressions can be generated from fear by scaling with the ratio of the radii. The angle on the emotion disc is kept fixed for both new expressions, while the radii are varied, thereby yielding new values for activation and evaluation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb4.tif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb4.tif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The emotional expression in the middle has been obtained from those at the left and right using the blending algorithm. The radius <i>r</i> and the angle on the emotion disc ω determine the influence of each generating expression and hence the degree of similarity to the new one. The co-ordinates in emotion space have been obtained from the NECA data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb5.tif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0153-5fmb5.tif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The emotional expression in the middle has been obtained from those at the left and right using the blending algorithm. The radius <i>r</i> and the angle on the emotion disc ω determine the influence of each generating expression and hence the degree of similarity to the new one. The first example is a not too active, but rather negative emotion, while the second one could be pleasant surprise, and the last one unpleasant surprise</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0153-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusions</h2><div class="c-article-section__content" id="Sec21-content"><p>We have presented a flexible approach to generating non-basic, mixed emotional states in the facial expressions of an anatomically based talking head. This has been achieved by modifying the work of Tsapatsoulis et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167" href="/article/10.1007/s10055-005-0153-5#ref-CR1" id="ref-link-section-d22556e3059">1</a>], aimed at an MPEG-4-based face model, to a physics-based facial animation system. In extension of their work, we have used data in a single emotion model, the activation-evaluation space [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Cowie R, Douglas-Cowie E, Savvidou S, McMahon E, Sawey M, Schröder M (2000) ‘FEELTRACE’: an instrument for recording perceived emotion in real time. In: Proceedings of the ISCA workshop on speech and emotion, Northern Ireland, pp 19–24. &#xA;                    http://www.qub.ac.uk/en/isca/proceedings&#xA;                    &#xA;                  , pp 19–24" href="/article/10.1007/s10055-005-0153-5#ref-CR44" id="ref-link-section-d22556e3062">44</a>], for indicating both emotion quality and intensity. As a result, our system is able to generate emotional facial expressions of various intensities, and to show mixed emotions by a gradual blending of facial configurations of basic emotions.</p><p>We have combined the facial animation model with an emotional TTS synthesis system which is also based on emotion dimensions. In combining these two components, we have created photorealistic animations of a talking head capable of expressing a continuum of shades of emotion.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Future work</h2><div class="c-article-section__content" id="Sec22-content"><p>There are several possible directions for future work. The most exciting is the planned extension to 3D emotion space, i.e., to not only consider activation and evaluation, but also power to allow for a more fine-grained model. Since emotions are arranged inside a sphere in this space, we propose to project the individual emotions onto the sphere’s surface and to interpolate between expressions on the surface of the sphere. This would permit interpolation between more than two expressions. The resulting expression is then projected back to the desired distance from the origin.</p><p>A next step should be the evaluation of the system. This could be done in a similar manner as described in Ref. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University &#xA;                    http://www.dfki.de/~schroed&#xA;                    &#xA;                  " href="/article/10.1007/s10055-005-0153-5#ref-CR43" id="ref-link-section-d22556e3078">43</a>].</p><p>Since emotion categories are more intuitive for most people than positions in activation-evaluation-power space, we require co-ordinates for more emotion words to enhance the user-friendliness of the system.</p><p>Another pressing issue is the extension of the system to include several different emotions in a single utterance, allowing for transitions between emotions over time.</p><p>Adapting the frequency and strength of the non-verbal speech-related facial expressions to the current emotion could enhance the realism of the animations, e.g., look downwards more often and in general show movement with less amplitude when sad. As an additional visible effect of emotion the artificial face should be capable of blushing. Frequency and intensity of breathing is also an indicator of the emotion currently felt.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>A publicly accessible web interface can be found at <a href="http://mary.dfki.de.">http://mary.dfki.de.</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>A phrase is a part of a sentence delimited by grammatical pauses.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis base" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Tsapatsoulis N, Raousaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs MPEG-4 facial animation—the standard implementations applications. Wiley, Hillsdale, pp 141–167</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="André E, Dybkyaer L, Minker W, Heisterkamp P (eds) (2004) In: Proceedings of the tutorial and research worksho" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">André E, Dybkyaer L, Minker W, Heisterkamp P (eds) (2004) In: Proceedings of the tutorial and research workshop on affective dialogue systems (ADS04), vol 3068 of lecture notes in artificial intelligence, Kloster Irsee, Germany. Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Cowie, R. Cornelius, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Cowie R, Cornelius R (2003) Describing the emotional states that are expressed in speech. Speech Commun Spec I" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Cowie R, Cornelius R (2003) Describing the emotional states that are expressed in speech. Speech Commun Spec Issue Speech Emotion 40(1–2):5–32</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Describing%20the%20emotional%20states%20that%20are%20expressed%20in%20speech&amp;journal=Speech%20Commun%20Spec%20Issue%20Speech%20Emotion&amp;volume=40&amp;issue=1%E2%80%932&amp;pages=5-32&amp;publication_year=2003&amp;author=Cowie%2CR&amp;author=Cornelius%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Scherer K (2000) Psychological models of emotion. Neuropsychology of emotion. Oxford University Press, Oxford," /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Scherer K (2000) Psychological models of emotion. Neuropsychology of emotion. Oxford University Press, Oxford, pp 137–162</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="The HUMAINE network portal. http://emotion-research.net" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">The HUMAINE network portal. <a href="http://emotion-research.net">http://emotion-research.net</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions " /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Schröder M, Cowie R, Douglas-Cowie E, Westerdijk M, Gielen S (2001) Acoustic correlates of emotion dimensions in view of speech synthesis. In: Proceedings of Eurospeech’01, vol 1, pp 87-90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder M (2004) Dimensional emotion representation as a basis for speech synthesis with non-extreme emotions" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Schröder M (2004) Dimensional emotion representation as a basis for speech synthesis with non-extreme emotions. In: Proceedings of the workshop on affective dialogue systems, Kloster Irsee, Germany, pp 209–220</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Th. Dutoit, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Dutoit Th (1997) An Introduction to text-to-speech synthesis. Kluwer, Dordrecht" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Dutoit Th (1997) An Introduction to text-to-speech synthesis. Kluwer, Dordrecht</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20Introduction%20to%20text-to-speech%20synthesis&amp;publication_year=1997&amp;author=Dutoit%2CTh">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Klabbers, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Klabbers E, Støber K, Veldhuis R, Wagner P, Stefan Breuer S (2001) Speech synthesis development made easy: the" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Klabbers E, Støber K, Veldhuis R, Wagner P, Stefan Breuer S (2001) Speech synthesis development made easy: the Bonn Open Synthesis System. In: Proceedings of Eurospeech 2001, Aalborg, pp 521–524</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Speech%20synthesis%20development%20made&amp;volume=easy&amp;publication_year=2001&amp;author=Klabbers%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Schr, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Schröder M, Trouvain J (2003) The German text-to-speech synthesis system MARY: a tool for research development" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Schröder M, Trouvain J (2003) The German text-to-speech synthesis system MARY: a tool for research development and teaching. Int J Speech Technology 6:365–377. <a href="http://mary.dfki.de">http://mary.dfki.de</a></p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1025708916924" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Int%20J%20Speech%20Technology&amp;volume=6&amp;publication_year=2003&amp;author=Schr%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Banse, K. Scherer, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Banse R, Scherer K (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70(3):614–636" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Banse R, Scherer K (1996) Acoustic profiles in vocal emotion expression. J Pers Soc Psychol 70(3):614–636</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F0022-3514.70.3.614" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8851745" aria-label="View reference 11 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20profiles%20in%20vocal%20emotion%20expression&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=70&amp;issue=3&amp;pages=614-636&amp;publication_year=1996&amp;author=Banse%2CR&amp;author=Scherer%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang L (2001) Prosody as expression of emotion. In: Cavé Ch (ed) Proceedings of ORAGE 2001, Oralité et gestual" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Yang L (2001) Prosody as expression of emotion. In: Cavé Ch (ed) Proceedings of ORAGE 2001, Oralité et gestualité, pp 209–212</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Schr, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Schröder M (2001) Emotional speech synthesis: a review. In: Proceedings of Eurospeech 2001, Aalborg, pp 561–56" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Schröder M (2001) Emotional speech synthesis: a review. In: Proceedings of Eurospeech 2001, Aalborg, pp 561–564. <a href="http://www.dfki.de/~schroed">http://www.dfki.de/~schroed</a></p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Emotional%20speech&amp;volume=synthesis&amp;publication_year=2001&amp;author=Schr%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Allen, S. Hunnicutt, DH. Klatt, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Allen J, Hunnicutt S, Klatt DH (1987) From text to speech: the MITalk system. Cambridge University Press, Camb" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Allen J, Hunnicutt S, Klatt DH (1987) From text to speech: the MITalk system. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20text%20to%20speech%3A%20the%20MITalk%20system&amp;publication_year=1987&amp;author=Allen%2CJ&amp;author=Hunnicutt%2CS&amp;author=Klatt%2CDH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Cahn, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Cahn J (1990) The generation of affect in synthesized speech. J Am Voice I/O Soc 8:1–19" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Cahn J (1990) The generation of affect in synthesized speech. J Am Voice I/O Soc 8:1–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20generation%20of%20affect%20in%20synthesized%20speech&amp;journal=J%20Am%20Voice%20I%2FO%20Soc&amp;volume=8&amp;pages=1-19&amp;publication_year=1990&amp;author=Cahn%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Black AW, Campbell N (1995) Optimising selection of units from speech databases for concatenative synthesis. I" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Black AW, Campbell N (1995) Optimising selection of units from speech databases for concatenative synthesis. In: Proceedings of Eurospeech 1995, Madrid, pp 581–584</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson W, Narayanan S, Whitney R, Das R, Bulut M, LaBore C (2002) Limited domain synthesis of expressive mili" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Johnson W, Narayanan S, Whitney R, Das R, Bulut M, LaBore C (2002) Limited domain synthesis of expressive military speech for animated characters. In: Proceedings of the 7th international conference on spoken language processing, Denver</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Dutoit, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Dutoit Th, Pagel V, Pierret N, Bataille F, van der Vrecken O (1996) The MBROLA project: towards a set of high " /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Dutoit Th, Pagel V, Pierret N, Bataille F, van der Vrecken O (1996) The MBROLA project: towards a set of high quality speech synthesisers free of use for non commercial purposes. In: Proceedings of the 4th international conference of spoken language processing, Philadelphia, pp 1393–1396</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=The%20MBROLA&amp;volume=project&amp;publication_year=1996&amp;author=Dutoit%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder M, Grice M (2003) Expressing vocal effort in concatenative synthesis. In: Proceedings of the 15th int" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Schröder M, Grice M (2003) Expressing vocal effort in concatenative synthesis. In: Proceedings of the 15th international conference of phonetic sciences, Barcelona</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee Y, Terzopoulos D, Waters K (1995) Realistic face modeling for animation. In: Proceedings of SIGGRAPH’95, p" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Lee Y, Terzopoulos D, Waters K (1995) Realistic face modeling for animation. In: Proceedings of SIGGRAPH’95, pp 55–62</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kähler K, Haber J, Seidel HP (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of Gr" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Kähler K, Haber J, Seidel HP (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of Graphics Interface, pp 37–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Bregler, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Bregler Ch, Covell M, Slaney M (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGG" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Bregler Ch, Covell M, Slaney M (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH ’97. ACM Press, Palo Alto, pp 353–360</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Video&amp;volume=rewrite&amp;publication_year=1997&amp;author=Bregler%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH ’99, pp 21–28" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Brand M (1999) Voice puppetry. In: Proceedings of SIGGRAPH ’99, pp 21–28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ezzat T, Geiger G, Poggio T (2002) Trainable videorealistic speech animation. In: Proceedings of SIGGRAPH’02, " /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Ezzat T, Geiger G, Poggio T (2002) Trainable videorealistic speech animation. In: Proceedings of SIGGRAPH’02, pp 388–398</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="F. Parke, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Parke F (1974) A parametric model for human faces. University of Utah, Salt Lake City" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Parke F (1974) A parametric model for human faces. University of Utah, Salt Lake City</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20parametric%20model%20for%20human%20faces&amp;publication_year=1974&amp;author=Parke%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalman" /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Cohen M, Massaro D (1993) Modeling coarticulation in synthetic visual speech. In: Magnenat-Thalmann N, Thalmann D (eds) Models and techniques in computer animation, pp 139–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pelachaud C, Badler N, Steedman M (1991) Linguistic issues in facial animation. In: Magnenat-Thalmann N, Thalm" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Pelachaud C, Badler N, Steedman M (1991) Linguistic issues in facial animation. In: Magnenat-Thalmann N, Thalmann D (eds) Computer animation’91</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kalberer G, Müller P, Van Gool L (2003) A visual speech generator. In: Proceedings of Videometrics VII. IS&amp;SPI" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Kalberer G, Müller P, Van Gool L (2003) A visual speech generator. In: Proceedings of Videometrics VII. IS&amp;SPIE, pp 173–183</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee S, Badler J, Badler N (2002) Eyes alive. In: Proceedings of SIGGRAPH’02, pp 637–644" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Lee S, Badler J, Badler N (2002) Eyes alive. In: Proceedings of SIGGRAPH’02, pp 637–644</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Pearce, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Pearce A, Wyvill B, Wyvill G, Hill D (1986) Speech and expression: a computer solution to face animation. In: " /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Pearce A, Wyvill B, Wyvill G, Hill D (1986) Speech and expression: a computer solution to face animation. In: Proceedings of Graphics Interface ’86, pp 136–140</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Speech%20and&amp;volume=expression&amp;publication_year=1986&amp;author=Pearce%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Ip, C. Chan, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Ip H, Chan C (1996) Script-based facial gesture and speech animation using a NURBS based face model. Comput Gr" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Ip H, Chan C (1996) Script-based facial gesture and speech animation using a NURBS based face model. Comput Graphics 20(6):881–891</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2896%2900058-1" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Script-based%20facial%20gesture%20and%20speech%20animation%20using%20a%20NURBS%20based%20face%20model&amp;journal=Comput%20Graphics&amp;volume=20&amp;issue=6&amp;pages=881-891&amp;publication_year=1996&amp;author=Ip%2CH&amp;author=Chan%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kalra P, Mangili A, Magnenat-Thalmann N, Thalmann D (1991) SMILE: a multilayered facial animation system. In: " /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Kalra P, Mangili A, Magnenat-Thalmann N, Thalmann D (1991) SMILE: a multilayered facial animation system. In: Proceedings of IFIP WG 5.10, Tokyo, pp 189–198</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Cassell, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T, Douville B, Prevost S, Stone M (1994) Animat" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T, Douville B, Prevost S, Stone M (1994) Animated conversation: rule-based generation of facial expression gesture and spoken intonation for multiple conversational agents. In: Proceedings of SIGGRAPH ’94, pp 413–420</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=ANIMATED&amp;volume=CONVERSATION&amp;publication_year=1994&amp;author=Cassell%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Pelachaud, N. Badler, M. Steedman, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Pelachaud C, Badler N, Steedman M (1996) Generating facial expressions for speech. Cogn Sci 20(1):1–46" /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Pelachaud C, Badler N, Steedman M (1996) Generating facial expressions for speech. Cogn Sci 20(1):1–46</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0364-0213%2899%2980001-9" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Generating%20facial%20expressions%20for%20speech&amp;journal=Cogn%20Sci&amp;volume=20&amp;issue=1&amp;pages=1-46&amp;publication_year=1996&amp;author=Pelachaud%2CC&amp;author=Badler%2CN&amp;author=Steedman%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Albrecht I, Haber J, Seidel H-P (2002) Automatic generation of non-verbal facial expressions from speech. In: " /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Albrecht I, Haber J, Seidel H-P (2002) Automatic generation of non-verbal facial expressions from speech. In: Proceedings of CGI, pp 283–293</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Albrecht I, Haber J, Kähler K, Schröder M, Seidel H-P (2002) May I talk to you? :-)—facial animation from text" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">Albrecht I, Haber J, Kähler K, Schröder M, Seidel H-P (2002) May I talk to you? :-)—facial animation from text. In: Proceedings of Pacific Graphics, pp 77–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Keltner D (1997) Universal facial expressions of emotion: an old controversy and new findings. In: se" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Ekman P, Keltner D (1997) Universal facial expressions of emotion: an old controversy and new findings. In: segerstrøle U, Molnár P (eds) Nonverbal communication: where nature meets culture. Lawrence Erlbaum Associates Inc., Mahwah, pp 27–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Byun M, Badler N (2002) FacEMOTE: qualitative parametric modifiers for facial animations. In: Proceedings of S" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">Byun M, Badler N (2002) FacEMOTE: qualitative parametric modifiers for facial animations. In: Proceedings of SCA’02, pp 65–71</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Ruttkay, H. Noot, P. ten Hagen, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ruttkay Z, Noot H, ten Hagen P (2003) Emotion disc and emotion squares: tools to explore the facial expression" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">Ruttkay Z, Noot H, ten Hagen P (2003) Emotion disc and emotion squares: tools to explore the facial expression space. Comput Graphics Forum 22(1):49–53</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2F1467-8659.t01-1-00645" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20disc%20and%20emotion%20squares%3A%20tools%20to%20explore%20the%20facial%20expression%20space&amp;journal=Comput%20Graphics%20Forum&amp;volume=22&amp;issue=1&amp;pages=49-53&amp;publication_year=2003&amp;author=Ruttkay%2CZ&amp;author=Noot%2CH&amp;author=ten%20Hagen%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R" /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Whissell C (1989) The dictionary of affect in language emotion: theory research and experience. In: Plutchik R, Kellerman H (eds) The measurement of emotions, chap 5, vol 4. Academic, San Diego, pp 113–131</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Plutchik, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Plutchik R (1980) Emotions: a psychoevolutionary synthesis. Harper &amp; Row, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotions%3A%20a%20psychoevolutionary%20synthesis&amp;publication_year=1980&amp;author=Plutchik%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bui T, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of CGI" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">Bui T, Heylen D, Nijholt A (2004) Combination of facial movements on a 3D talking head. In: Proceedings of CGI’04, pp 284–291</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Schr, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach t" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Schröder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. PhD Thesis, vol 7 of Phonus, Research Report of the Institute of Phonetics, Saarland University <a href="http://www.dfki.de/~schroed">http://www.dfki.de/~schroed</a></p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Speech%20and%20emotion&amp;volume=research&amp;publication_year=2004&amp;author=Schr%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cowie R, Douglas-Cowie E, Savvidou S, McMahon E, Sawey M, Schröder M (2000) ‘FEELTRACE’: an instrument for rec" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">Cowie R, Douglas-Cowie E, Savvidou S, McMahon E, Sawey M, Schröder M (2000) ‘FEELTRACE’: an instrument for recording perceived emotion in real time. In: Proceedings of the ISCA workshop on speech and emotion, Northern Ireland, pp 19–24. <a href="http://www.qub.ac.uk/en/isca/proceedings">http://www.qub.ac.uk/en/isca/proceedings</a>, pp 19–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Douglas-Cowie, N. Campbell, R. Cowie, P. Roach, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Douglas-Cowie E, Campbell N, Cowie R, Roach P (2003) Emotional speech: towards a new generation of databases. " /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Douglas-Cowie E, Campbell N, Cowie R, Roach P (2003) Emotional speech: towards a new generation of databases. Speech Commun Spec Issue Speech Emotion 40(1–2):33–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20speech%3A%20towards%20a%20new%20generation%20of%20databases&amp;journal=Speech%20Commun%20Spec%20Issue%20Speech%20Emotion&amp;volume=40&amp;issue=1%E2%80%932&amp;pages=33-60&amp;publication_year=2003&amp;author=Douglas-Cowie%2CE&amp;author=Campbell%2CN&amp;author=Cowie%2CR&amp;author=Roach%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cowie R, Douglas-Cowie E, Appolloni B, Taylor J, Romano A, Fellenz W (1999) What a neural net needs to know ab" /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Cowie R, Douglas-Cowie E, Appolloni B, Taylor J, Romano A, Fellenz W (1999) What a neural net needs to know about emotion words. In: Mastorakis N (ed) Computational intelligence and applications. World Scientific &amp; Engineering Society Press, pp 109–114</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krenn B, Pirker H, Grice M, Piwek P, van Deemter K, Schröder M, Klesen M, Gstrein E (2002) Generation of multi" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">Krenn B, Pirker H, Grice M, Piwek P, van Deemter K, Schröder M, Klesen M, Gstrein E (2002) Generation of multimodal dialogue for net environments. In: Proceedings of Konvens, Saarbrücken.<a href="http://www.ai.univie.ac.at/NECA">http://www.ai.univie.ac.at/NECA</a>, Saarbrücken</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder M, Breuer S (2004) XML representation languages as a way of interconnecting TTS modules. In: Proceedi" /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Schröder M, Breuer S (2004) XML representation languages as a way of interconnecting TTS modules. In: Proceedings of ICSLP’04, Jeju</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". K, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Kähler K, Haber J, Seidel H-P (2002) Head shop: generating animated head models with anatomical structure. In:" /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">Kähler K, Haber J, Seidel H-P (2002) Head shop: generating animated head models with anatomical structure. In: Proceedings of SCA’02, pp 55–64</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Head&amp;volume=shop&amp;publication_year=2002&amp;author=K%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, W. Wallace, " /><meta itemprop="datePublished" content="1969" /><meta itemprop="headline" content="Ekman P, Wallace W (1969) The repertoire of nonverbal behavior: categories origins usage and coding. Semiotica" /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">Ekman P, Wallace W (1969) The repertoire of nonverbal behavior: categories origins usage and coding. Semiotica 1:49–98</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20repertoire%20of%20nonverbal%20behavior%3A%20categories%20origins%20usage%20and%20coding&amp;journal=Semiotica&amp;volume=1&amp;pages=49-98&amp;publication_year=1969&amp;author=Ekman%2CP&amp;author=Wallace%2CW">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0153-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>Part of this research is supported by the EC Project HUMAINE (IST-507422).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">MPI Informatik, Saarbrücken, Germany</p><p class="c-article-author-affiliation__authors-list">Irene Albrecht, Jörg Haber &amp; Hans-Peter Seidel</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">DFKI GmbH, Saarbrücken, Germany</p><p class="c-article-author-affiliation__authors-list">Marc Schröder</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Irene-Albrecht"><span class="c-article-authors-search__title u-h3 js-search-name">Irene Albrecht</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Irene+Albrecht&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Irene+Albrecht" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Irene+Albrecht%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Marc-Schr_der"><span class="c-article-authors-search__title u-h3 js-search-name">Marc Schröder</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marc+Schr%C3%B6der&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marc+Schr%C3%B6der" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marc+Schr%C3%B6der%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-J_rg-Haber"><span class="c-article-authors-search__title u-h3 js-search-name">Jörg Haber</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J%C3%B6rg+Haber&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J%C3%B6rg+Haber" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J%C3%B6rg+Haber%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hans_Peter-Seidel"><span class="c-article-authors-search__title u-h3 js-search-name">Hans-Peter Seidel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hans-Peter+Seidel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hans-Peter+Seidel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hans-Peter+Seidel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0153-5/email/correspondent/c1/new">Irene Albrecht</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Mixed%20feelings%3A%20expression%20of%20non-basic%20emotions%20in%20a%20muscle-based%20talking%20head&amp;author=Irene%20Albrecht%20et%20al&amp;contentID=10.1007%2Fs10055-005-0153-5&amp;publication=1359-4338&amp;publicationDate=2005-08-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Albrecht, I., Schröder, M., Haber, J. <i>et al.</i> Mixed feelings: expression of non-basic emotions in a muscle-based talking head.
                    <i>Virtual Reality</i> <b>8, </b>201–212 (2005). https://doi.org/10.1007/s10055-005-0153-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0153-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-08-11">11 August 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-09">September 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0153-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0153-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Continuous emotions</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Emotional speech synthesis</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Facial animation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0153-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=153;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

