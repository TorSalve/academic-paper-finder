<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="AR image generation using view-dependent geometry modification and tex"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="AR image generation using view-dependent geometry modification and texture mapping"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-01-22"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we propose a method for real object virtualization and AR image generation based on view-dependent geometry modification and texture mapping. The proposed method is a hybrid of model- and image-based rendering techniques that uses multiple input images of the real object as well as the object&#8217;s three-dimensional (3D) model obtained by an automatic 3D reconstruction technique. Even with state-of-the-art technology, the reconstructed 3D model&#8217;s accuracy can be insufficient, resulting in such visual artifacts as false object boundaries. The proposed method generates a depth map from a 3D model of a virtualized real object and expands its region in the depth map to remove the false object boundaries. Since such expansion reveals the background pixels in the input images, which is particularly undesirable for AR applications, we preliminarily extract object regions and use them for texture mapping. With our GPU implementation for real-time AR image generation, we experimentally demonstrated that using expanded geometry reduces the number of required input images and maintains visual quality."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-01-22"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="83"/>

    <meta name="prism.endingPage" content="94"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0259-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0259-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0259-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0259-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="AR image generation using view-dependent geometry modification and texture mapping"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2015/06"/>

    <meta name="citation_online_date" content="2015/01/22"/>

    <meta name="citation_firstpage" content="83"/>

    <meta name="citation_lastpage" content="94"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0259-3"/>

    <meta name="DOI" content="10.1007/s10055-015-0259-3"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0259-3"/>

    <meta name="description" content="Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an"/>

    <meta name="dc.creator" content="Yuta Nakashima"/>

    <meta name="dc.creator" content="Yusuke Uno"/>

    <meta name="dc.creator" content="Norihiko Kawai"/>

    <meta name="dc.creator" content="Tomokazu Sato"/>

    <meta name="dc.creator" content="Naokazu Yokoya"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A survey of augmented reality; citation_author=R Azuma; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=355-385; citation_id=CR1"/>

    <meta name="citation_reference" content="Bastian JW, Ward B, Hill R, Hengel AVD, Dick AR (2010) Interactive modelling for AR applications. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 199&#8211;205"/>

    <meta name="citation_reference" content="Buehler C, Bosse M, McMillan L, Gortler S, Cohen M (2001) Unstructured lumigraph rendering. In: Proceedings of ACM SIGGRAPH, pp 425&#8211;432"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Depth synthesis and local warps for plausible image-based navigation; citation_author=G Chaurasia, S Duchene, O Sorkine-Hornun, G Drettakis; citation_volume=32; citation_issue=3; citation_publication_date=2013; citation_pages=30:1-30:12; citation_doi=10.1145/2487228.2487238; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Unstructured light fields; citation_author=A Davis, M Levoy, F Durand; citation_volume=31; citation_issue=2; citation_publication_date=2012; citation_pages=305-314; citation_doi=10.1111/j.1467-8659.2012.03009.x; citation_id=CR5"/>

    <meta name="citation_reference" content="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105&#8211;116"/>

    <meta name="citation_reference" content="Furukawa Y, Curless B, Seitz SM, Szeliski R (2010) Towards internet-scale multi-view stereo. In: Proceedings of 2010 IEEE conference computer vision and pattern recognition, pp 1434&#8211;1441"/>

    <meta name="citation_reference" content="Gortler SJ, Grzeszczuk R, Szeliski R, Cohen MF (1996) The lumigraph. In: Proceedings of ACM SIGGRAPH, pp 43&#8211;54"/>

    <meta name="citation_reference" content="Irani M, Hassner T, Anandan P (2002) What does the scene look like from a scene point? In: Proceedings of 7th European conference computer vision, pp 883&#8211;897"/>

    <meta name="citation_reference" content="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121&#8211;3128"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of 2nd IEEE/ACM international workshop on augmented reality, pp 85&#8211;94"/>

    <meta name="citation_reference" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of 6th IEEE/ACM international symposium on mixed and augmented reality, pp 225&#8211;234"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Fast joint estimation of silhouettes and dense 3D geometry from multiple images; citation_author=K Kolev, T Brox, D Cremers; citation_volume=34; citation_issue=3; citation_publication_date=2012; citation_pages=493-505; citation_doi=10.1109/TPAMI.2011.150; citation_id=CR13"/>

    <meta name="citation_reference" content="Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31&#8211;42"/>

    <meta name="citation_reference" content="Metaio GmbH. Metaio SDK. 
                    http://www.metaio.com/products/sdk/
                    
                  
                "/>

    <meta name="citation_reference" content="NVIDIA Developer Zone. ConjugateGradientUM. 
                    http://docs.nvidia.com/cuda/cuda-samples/#conjugategradient
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Math Softw; citation_title=STRIPACK: Delaunay triangulation and Voronoi diagram on the surface of a sphere; citation_author=R Renka; citation_volume=23; citation_issue=3; citation_publication_date=1997; citation_pages=416-434; citation_doi=10.1145/275323.275329; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Grabcut: interactive foreground extraction using iterated graph cuts; citation_author=C Rother, V Kolmogorov, A Blake; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=309-314; citation_doi=10.1145/1015706.1015720; citation_id=CR18"/>

    <meta name="citation_reference" content="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: Proceedings of 2006 IEEE conference computer vision and pattern recognition, pp 519&#8211;528"/>

    <meta name="citation_reference" content="Wu C (2007) SiftGPU: a GPU implementation of scale invariant feature transform (SIFT). 
                    http://ccwu.me/vsfm/
                    
                  
                "/>

    <meta name="citation_reference" content="Wu C (2011) VisualSFM: a visual structure from motion system. 
                    http://ccwu.me/vsfm/
                    
                  
                "/>

    <meta name="citation_reference" content="Wu C (2013) Towards linear-time incremental structure from motion. In: Proceedings of international conference on 3D vision, pp 127&#8211;134"/>

    <meta name="citation_author" content="Yuta Nakashima"/>

    <meta name="citation_author_email" content="n-yuta@is.naist.jp"/>

    <meta name="citation_author_institution" content="Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Yusuke Uno"/>

    <meta name="citation_author_email" content="yusuke-u@is.naist.jp"/>

    <meta name="citation_author_institution" content="Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Norihiko Kawai"/>

    <meta name="citation_author_email" content="norihi-k@is.naist.jp"/>

    <meta name="citation_author_institution" content="Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Tomokazu Sato"/>

    <meta name="citation_author_email" content="tomoka-s@is.naist.jp"/>

    <meta name="citation_author_institution" content="Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Naokazu Yokoya"/>

    <meta name="citation_author_email" content="yokoya@is.naist.jp"/>

    <meta name="citation_author_institution" content="Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0259-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0259-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="AR image generation using view-dependent geometry modification and texture mapping"/>
        <meta property="og:description" content="Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we propose a method for real object virtualization and AR image generation based on view-dependent geometry modification and texture mapping. The proposed method is a hybrid of model- and image-based rendering techniques that uses multiple input images of the real object as well as the object’s three-dimensional (3D) model obtained by an automatic 3D reconstruction technique. Even with state-of-the-art technology, the reconstructed 3D model’s accuracy can be insufficient, resulting in such visual artifacts as false object boundaries. The proposed method generates a depth map from a 3D model of a virtualized real object and expands its region in the depth map to remove the false object boundaries. Since such expansion reveals the background pixels in the input images, which is particularly undesirable for AR applications, we preliminarily extract object regions and use them for texture mapping. With our GPU implementation for real-time AR image generation, we experimentally demonstrated that using expanded geometry reduces the number of required input images and maintains visual quality."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>AR image generation using view-dependent geometry modification and texture mapping | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0259-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"View-dependent geometry modification, View-dependent texture mapping, Augmented reality, Free-viewpoint image generation","kwrd":["View-dependent_geometry_modification","View-dependent_texture_mapping","Augmented_reality","Free-viewpoint_image_generation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0259-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0259-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=259;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0259-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            AR image generation using view-dependent geometry modification and texture mapping
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0259-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0259-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-01-22" itemprop="datePublished">22 January 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">AR image generation using view-dependent geometry modification and texture mapping</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yuta-Nakashima" data-author-popup="auth-Yuta-Nakashima" data-corresp-id="c1">Yuta Nakashima<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yusuke-Uno" data-author-popup="auth-Yusuke-Uno">Yusuke Uno</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Norihiko-Kawai" data-author-popup="auth-Norihiko-Kawai">Norihiko Kawai</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tomokazu-Sato" data-author-popup="auth-Tomokazu-Sato">Tomokazu Sato</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Naokazu-Yokoya" data-author-popup="auth-Naokazu-Yokoya">Naokazu Yokoya</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">83</span>–<span itemprop="pageEnd">94</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">506 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0259-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we propose a method for real object virtualization and AR image generation based on view-dependent geometry modification and texture mapping. The proposed method is a hybrid of model- and image-based rendering techniques that uses multiple input images of the real object as well as the object’s three-dimensional (3D) model obtained by an automatic 3D reconstruction technique. Even with state-of-the-art technology, the reconstructed 3D model’s accuracy can be insufficient, resulting in such visual artifacts as false object boundaries. The proposed method generates a depth map from a 3D model of a virtualized real object and expands its region in the depth map to remove the false object boundaries. Since such expansion reveals the background pixels in the input images, which is particularly undesirable for AR applications, we preliminarily extract object regions and use them for texture mapping. With our GPU implementation for real-time AR image generation, we experimentally demonstrated that using expanded geometry reduces the number of required input images and maintains visual quality.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>With the recent popularization of augmented reality (AR) technology, many of the applications proposed so far are already available for use by ordinary users. Such applications sometimes require a real object to be virtualized (i.e., building a virtual object from a real one) in order to generate AR images. Particularly for such applications as virtual furniture arrangements and communication tools capable of showing remote real objects on a display, ordinary users of AR applications must build virtualized real objects themselves. These applications require methods for real object virtualization and AR image generation.</p><p>As well as easy use by ordinary users, the requirements of real object virtualization and AR image generation can be summarized as follows:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>high visual quality of AR images based on virtualized real objects</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>sufficiently small data size of virtualized real objects in order to reduce the amount of needed storage and ensure lightweight transmission</p>
                    
                  </li>
                </ol>
              <p>Most AR applications use a model-based technique for virtualizing real objects and generating AR images (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" href="/article/10.1007/s10055-015-0259-3#ref-CR1" id="ref-link-section-d83365e395">1997</a>). Such a technique uses three-dimensional (3D) models of real objects, which can be handcrafted or automatically reconstructed using 3D reconstruction techniques. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a> (top) shows an example with a 3D model that was automatically reconstructed (Jancosek and Pajdla <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e401">2011</a>). Other research efforts have focused on image-based techniques that do not rely on 3D models but on a number of images from which a novel view is synthesized (e.g., Levoy and Hanrahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31–42" href="/article/10.1007/s10055-015-0259-3#ref-CR14" id="ref-link-section-d83365e404">1996</a>; Gortler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gortler SJ, Grzeszczuk R, Szeliski R, Cohen MF (1996) The lumigraph. In: Proceedings of ACM SIGGRAPH, pp 43–54" href="/article/10.1007/s10055-015-0259-3#ref-CR8" id="ref-link-section-d83365e407">1996</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Examples of AR images using (<i>top</i>) model-based technique by Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e424">2011</a>), (<i>middle</i>) model-based technique with VDTM and (<i>bottom</i>) proposed method. Images in right column are close-ups of bottle cap</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Unfortunately, these techniques suffer from some drawbacks. Since handcrafting 3D models requires special skills that are beyond most ordinary users, model-based techniques need to use automatically reconstructed 3D models. However, the accuracy of such 3D models is usually insufficient, which leads to a loss of detail and undesired rough boundaries (false boundaries that are not faithful to the original shape), as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a> (top). For image-based techniques, since the number of images is usually too large, the burden of image acquisition may discourage ordinary users from attempting it. They also require large storage and long transmission time, which may not be appropriate for some AR applications.</p><p>To overcome these drawbacks, image-based techniques have been extended to leverage the geometry of real objects. One of the most well-known techniques is view-dependent texture mapping (VDTM) (Debevec et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e452">1998</a>). This technique applies a texture to each face of a 3D mesh model, where the texture is an image captured from the viewpoint closest to that of the image to be synthesized. Using VDTM, the detailed shapes inside the object region can be regained from simplified or inaccurate 3D models. However, since VDTM applies textures directly to the 3D model, its boundary appears as is in AR images, creating a false object boundary and exposing the background pixels [gray pixels around the bottle cap shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a> (middle)].</p><p>In this paper, we propose a novel method for generating AR images with virtualized real objects. Our proposed method can be considered an image-based technique that leverages geometry, taking a 3D model and multiple images of a real object as its input. To alleviate the problem of false object boundaries, we introduce view-dependent geometry modification (VDGM). After a depth map of the virtualized real object is generated in the rendering pipeline, VDGM expands the object region in it. Since the expanded geometry is likely to expose excessive background regions, which is not desirable in AR image generation, we preliminarily extract the foreground regions (i.e., image regions containing the real object to be virtualized) in the input images and then use the extracted foreground regions for synthesizing AR images (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a>, bottom). The main contributions of this paper can be summarized as follows.</p><ul class="u-list-style-bullet">
                  <li>
                    <p>A novel method for virtualizing real objects and generating AR images has been developed using an image-based technique with 3D models. Unlike existing techniques, the proposed method adaptively modifies the geometry of objects (VDGM) to alleviate artifacts around the object boundaries, which are significant in VDTM-based AR images.</p>
                  </li>
                  <li>
                    <p>The discrete Poisson equation is applied to geometry modification to retain occluding boundaries (i.e., discontinuities in the depth maps) due to partial self-occlusion.</p>
                  </li>
                  <li>
                    <p>The visual quality of AR images obtained by the proposed method is experimentally demonstrated. Furthermore, the method is shown to require fewer input images than existing image-based rendering techniques due to the use of automatically constructed 3D models.</p>
                  </li>
                </ul>
              </div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Various techniques have been proposed for virtualizing real objects and generating novel view images based on virtualized real objects. At one extreme are such techniques as the fully model-based one, in which a 3D model of a real object is handcrafted or automatically reconstructed using 3D reconstruction techniques (Furukawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Furukawa Y, Curless B, Seitz SM, Szeliski R (2010) Towards internet-scale multi-view stereo. In: Proceedings of 2010 IEEE conference computer vision and pattern recognition, pp 1434–1441" href="/article/10.1007/s10055-015-0259-3#ref-CR7" id="ref-link-section-d83365e493">2010</a>; Jancosek and Pajdla <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e496">2011</a>; Kolev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kolev K, Brox T, Cremers D (2012) Fast joint estimation of silhouettes and dense 3D geometry from multiple images. IEEE Trans Pattern Anal Mach Intell 34(3):493–505" href="/article/10.1007/s10055-015-0259-3#ref-CR13" id="ref-link-section-d83365e499">2012</a>). A good comparison of existing algorithms can be found in a previous work (Seitz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: Proceedings of 2006 IEEE conference computer vision and pattern recognition, pp 519–528" href="/article/10.1007/s10055-015-0259-3#ref-CR19" id="ref-link-section-d83365e502">2006</a>) on automatic 3D reconstruction. Novel view images are synthesized by a standard rendering pipeline applied to the 3D model. As mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0259-3#Sec1">1</a>, the problems with these techniques include the difficulty of handcrafting 3D models for ordinary users and the insufficient accuracy of automatic 3D reconstruction, even with state-of-the-art algorithms.</p><p>The techniques on the other extreme are the fully image-based ones (Levoy and Hanrahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31–42" href="/article/10.1007/s10055-015-0259-3#ref-CR14" id="ref-link-section-d83365e511">1996</a>; Gortler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gortler SJ, Grzeszczuk R, Szeliski R, Cohen MF (1996) The lumigraph. In: Proceedings of ACM SIGGRAPH, pp 43–54" href="/article/10.1007/s10055-015-0259-3#ref-CR8" id="ref-link-section-d83365e514">1996</a>). They model rays from a scene based on input images and synthesize a novel view image using the modeled rays. Their visual quality is acceptable if the number of input images is large; however, capturing an enormous number of images with accurate camera poses can be laborious for ordinary users.</p><p>Between the extremes of the model-based and image-based techniques, a variety of other techniques have been proposed. These are basically image-based techniques that leverage the geometry of real objects to improve the visual quality and reduce the number of input images. VDTM, originally proposed by Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e520">1998</a>), handcrafts simplified 3D models of a real scene and applies a texture to each face of the models. The texture is selected from input images that are captured from the viewpoint closest to the novel view image to be synthesized. Bastian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bastian JW, Ward B, Hill R, Hengel AVD, Dick AR (2010) Interactive modelling for AR applications. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 199–205" href="/article/10.1007/s10055-015-0259-3#ref-CR2" id="ref-link-section-d83365e523">2010</a>) interactively built from scratch a relatively accurate 3D model of a real object and used the VDTM technique to color it. Irani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Irani M, Hassner T, Anandan P (2002) What does the scene look like from a scene point? In: Proceedings of 7th European conference computer vision, pp 883–897" href="/article/10.1007/s10055-015-0259-3#ref-CR9" id="ref-link-section-d83365e526">2002</a>), on the other hand, reduced the number of images without explicitly obtaining the geometry of a scene. However, they implicitly used geometric information through color consistency tests.</p><p>More recent techniques leveraged a roughly estimated geometry as a proxy of 3D models. For example, the unstructured light field technique (Davis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" href="/article/10.1007/s10055-015-0259-3#ref-CR5" id="ref-link-section-d83365e532">2012</a>) used several types of proxies, i.e., planes and triangle meshes based on the feature points used in simultaneous localization and mapping (SLAM) techniques (e.g., Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of 6th IEEE/ACM international symposium on mixed and augmented reality, pp 225–234" href="/article/10.1007/s10055-015-0259-3#ref-CR12" id="ref-link-section-d83365e535">2007</a>). Compared to the original light field technique (Levoy and Hanrahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31–42" href="/article/10.1007/s10055-015-0259-3#ref-CR14" id="ref-link-section-d83365e538">1996</a>), this scheme drastically reduced the number of input images required for sufficient visual quality. Chaurasia et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chaurasia G, Duchene S, Sorkine-Hornun O, Drettakis G (2013) Depth synthesis and local warps for plausible image-based navigation. ACM Trans Graph 32(3):30:1–30:12" href="/article/10.1007/s10055-015-0259-3#ref-CR4" id="ref-link-section-d83365e541">2013</a>) also exploited the feature points obtained during a structure-from-motion technique, such as Wu (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wu C (2011) VisualSFM: a visual structure from motion system. &#xA;                    http://ccwu.me/vsfm/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-015-0259-3#ref-CR21" id="ref-link-section-d83365e544">2011</a>). Instead of explicitly representing the geometry, they assigned feature points to superpixels and used them as a proxy of the scene’s geometry.</p><p>Our proposed method, which also lies between the model- and image-based techniques, relies on 3D models of real objects that are automatically reconstructed using, for example, the technique of Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e551">2011</a>). Since 3D models are not sufficiently accurate to synthesize novel view images, we further modify them depending on the viewpoint of the novel view image, which we refer to as VDGM, to remedy the problem of false object boundaries (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a>, middle). Similar ideas were employed by Buehler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Buehler C, Bosse M, McMillan L, Gortler S, Cohen M (2001) Unstructured lumigraph rendering. In: Proceedings of ACM SIGGRAPH, pp 425–432" href="/article/10.1007/s10055-015-0259-3#ref-CR3" id="ref-link-section-d83365e557">2001</a>) and Davis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" href="/article/10.1007/s10055-015-0259-3#ref-CR5" id="ref-link-section-d83365e560">2012</a>), both of whom built triangle meshes given a viewpoint in the synthesis of a novel view. However, as Davis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" href="/article/10.1007/s10055-015-0259-3#ref-CR5" id="ref-link-section-d83365e563">2012</a>) noted, this approach may suffer from temporal discontinuity in a sequence of synthesized images due to the topological changes in the triangle meshes. They also observed visual artifacts around the occluding boundaries because their triangle mesh generation does not take into account the occlusion relationship among objects. In contrast, our proposed method modifies the geometry in the depth map, in which topological changes are never involved. Our VDGM also considers the occlusion relationship based on the original 3D models to avoid the visual artifacts around the occluding boundaries.</p><p>In addition, unlike the techniques in Davis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" href="/article/10.1007/s10055-015-0259-3#ref-CR5" id="ref-link-section-d83365e569">2012</a>) and Chaurasia et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chaurasia G, Duchene S, Sorkine-Hornun O, Drettakis G (2013) Depth synthesis and local warps for plausible image-based navigation. ACM Trans Graph 32(3):30:1–30:12" href="/article/10.1007/s10055-015-0259-3#ref-CR4" id="ref-link-section-d83365e572">2013</a>), our proposed method is tailored for AR image generation, which usually deals with small objects, not an entire scene. This enables us to exploit the extracted foreground regions to smooth the object boundaries.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Overview of proposed method</h2><div class="c-article-section__content" id="Sec3-content"><p>An overview of our proposed method is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig2">2</a>. Given set <span class="mathjax-tex">\(S \in \{I_n|n=1,\dots ,N\}\)</span> of multiple input images of the real object to be virtualized (target, hereinafter), the proposed method generates AR images through offline and online stages.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overview of proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In the offline stage, the proposed method uses a structure-from-motion technique (Wu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wu C (2011) VisualSFM: a visual structure from motion system. &#xA;                    http://ccwu.me/vsfm/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-015-0259-3#ref-CR21" id="ref-link-section-d83365e663">2011</a>) to estimate the pose of the <span class="mathjax-tex">\(n\)</span>th camera <span class="mathjax-tex">\(C_n\)</span> that shot image <span class="mathjax-tex">\(I_n \in S\)</span>. We denote the estimated rotation matrix and translation vector of <span class="mathjax-tex">\(C_n\)</span> in an arbitrary coordinate system by <span class="mathjax-tex">\({\mathbf {R}}_n\)</span> and <span class="mathjax-tex">\({\mathbf {t}}_n\)</span>, respectively. Then, 3D model <span class="mathjax-tex">\(M\)</span> is constructed from <span class="mathjax-tex">\(S\)</span> using a 3D reconstruction technique. We can adopt any suitable technique for automatic 3D reconstruction, such as that of Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e853">2011</a>). The user may modify <span class="mathjax-tex">\(M\)</span> so that only the target is rendered in the AR application. Foreground region <span class="mathjax-tex">\(\varOmega _n\)</span> in <span class="mathjax-tex">\(I_n \in S\)</span> is also extracted, e.g., using the GrabCut algorithm (Rother et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="&#xA;Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts.  ACM Trans Graph 23(3):309–314" href="/article/10.1007/s10055-015-0259-3#ref-CR18" id="ref-link-section-d83365e932">2004</a>). Such semiautomatic region extraction is much easier than manually building 3D models from scratch, which requires special skills. To reduce user effort further, fully automatic extraction of the foreground region is promising because 3D model <span class="mathjax-tex">\(M\)</span> can be used as prior knowledge in the foreground region. In this paper, however, we omit further details on this to focus on the image generation process.</p><p>In the online stage, we generate AR images for user’s camera <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span>. The image from the user’s camera is denoted by <span class="mathjax-tex">\(I_{\mathrm {U}}\)</span> and captures the actual environment onto which the rendered target will be superimposed. The proposed method first estimates in real time the pose of <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span> using such approaches as ARToolkit (Kato and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of 2nd IEEE/ACM international workshop on augmented reality, pp 85–94" href="/article/10.1007/s10055-015-0259-3#ref-CR11" id="ref-link-section-d83365e1032">1999</a>), PTAM (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of 6th IEEE/ACM international symposium on mixed and augmented reality, pp 225–234" href="/article/10.1007/s10055-015-0259-3#ref-CR12" id="ref-link-section-d83365e1035">2007</a>), or Metaio SDK (Metaio GmbH). The camera pose is denoted by <span class="mathjax-tex">\({\mathbf {R}}_{\mathrm {U}}\)</span> and <span class="mathjax-tex">\({\mathbf {t}}_{\mathrm {U}}\)</span>. For simplicity, we assume that <span class="mathjax-tex">\({\mathbf {R}}_n/{\mathbf {t}}_n\)</span> and <span class="mathjax-tex">\({\mathbf {R}}_{\mathrm {U}}/{\mathbf {t}}_{\mathrm {U}}\)</span> are expressed in the same coordinate system, which is called the world coordinate system. The relationships among the coordinate systems used in our proposed method are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig3">3</a>. From the estimated camera pose and the 3D model, we generate a depth map using a standard rendering pipeline. Then, VDGM modifies target region <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> in the depth map as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig4">4</a>. Note that target region <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> represents the region in a synthesized image onto which the 3D model <span class="mathjax-tex">\(M\)</span> is projected given <span class="mathjax-tex">\({\mathbf {R}}_{\mathrm {U}}\)</span> and <span class="mathjax-tex">\({\mathbf {t}}_{\mathrm {U}}\)</span>, and foreground region <span class="mathjax-tex">\(\varOmega _n\)</span> represents a region in the <span class="mathjax-tex">\(n\)</span>th image <span class="mathjax-tex">\(I_n\)</span> in <span class="mathjax-tex">\(S\)</span>. For synthesizing an image of the virtualized target, our VDTM, which is modified to work with depth maps, colors each pixel in the expanded target region using the foreground pixels of the images in <span class="mathjax-tex">\(S\)</span>. The synthesized image is superimposed on <span class="mathjax-tex">\(I_{\mathrm {U}}\)</span> to generate an AR image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Relationships among coordinate systems used in proposed system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Original depth map (<i>left</i>) and expanded one (<i>right</i>). Smaller depth values are presented with <i>lighter colors</i>. <i>Non-black regions</i> in <i>left</i> and <i>right depth maps</i> represent original region <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> and expanded region <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>, respectively,</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>As mentioned above, the proposed method does not use the shape of an automatically reconstructed 3D model as is because it is not completely consistent with the silhouette in the <span class="mathjax-tex">\(n\)</span>th image <span class="mathjax-tex">\(I_n\)</span>. This is because such 3D models contain missing or extra volumes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig1">1</a>, middle). Inaccurate 3D models also result in false object boundaries (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig5">5</a>). VDGM reduces such visual artifacts by (1) expanding and smoothing the target geometry in the depth map and (2) determining the color of each pixel based on the foreground pixels of <span class="mathjax-tex">\(I_n \in S\)</span> to avoid artifacts due to excessive expansion.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Automatically reconstructed 3D mesh model and a close-up view. This illustrates missing and extra volumes. It also suggests that <i>triangles</i> around the object boundary yield false high-frequency components</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The following sections give the details of the online stage: the VDGM- and VDTM-based color assignments. The techniques used in the offline stage are borrowed from the existing ones specified in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0259-3#Sec9">6</a>.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">View-dependent geometry modification</h2><div class="c-article-section__content" id="Sec4-content"><p>Given the estimated pose of camera <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span>, the standard rendering pipeline generates a depth map with model <span class="mathjax-tex">\(M\)</span>, whose depth value at the <span class="mathjax-tex">\(j\)</span>th pixel is denoted by <span class="mathjax-tex">\(d_j\)</span>. Since <span class="mathjax-tex">\(M\)</span> is not sufficiently accurate, the target region <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> in the depth map is chipped (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig4">4</a>, left), which directly leads to the problem of rough boundaries. VDGM reduces them by smoothing the target region and expanding it to ensure that the pixels on the foreground region boundaries form the boundary of the generated image. A straightforward approach to doing this is to apply a low-pass filter, e.g., a box or a Gaussian filter. However, we found that it spoils the occluding boundaries in the depth map that are formed by different surfaces. In addition, the low-pass filter-based approach may introduce undesirable geometry around boundaries between the target and other regions if it is naively applied to the depth map, which can result in the visual artifacts shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig6">6</a>. In this work, we developed a novel formulation of VDGM that uses a discrete Poisson equation instead of a low-pass filter. Appropriate coefficients of the equation preserve the occluding boundaries while smoothing and expanding the target regions. Our discrete Poisson equation-based VDGM can be solved efficiently on GPUs (nVIDIA).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Example AR image with discontinuity inside the original target region (<i>left</i>) and a close-up (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Since <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> represents the region onto which 3D model <span class="mathjax-tex">\(M\)</span> is projected given rotation <span class="mathjax-tex">\({\mathbf {R}}_{\mathrm {U}}\)</span> and translation <span class="mathjax-tex">\({\mathbf {t}}_{\mathrm {U}}\)</span> for user’s camera <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span>, the depth value <span class="mathjax-tex">\(d_j\)</span> at the <span class="mathjax-tex">\(j\)</span>th pixel is defined over <span class="mathjax-tex">\(j \in \varOmega _{\mathrm {U}}\)</span>, as shown in 
Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig7">7</a>. We also denote an expanded region by <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>, obtained by applying the morphological dilation operator, which can be iterated several times. The dilation operator determines how much the target region is expanded. A discrete Poisson equation-based VDGM assigns actual depth values to the expanded region and smoothes the original depth values.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Illustrative example of depth map indicating regions <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> (<i>solid white line</i>) and <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span> (<i>dashed white line</i>). The <i>dashed red line</i> indicates the occluding boundary (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The following are VDGM’s requirements:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>A new depth value must be close to its original value, if any, for correct color assignment.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>A new depth value must also be close to its adjacent depth values for the boundary smoothness of the generated novel viewpoint images to prevent visual artifacts, such as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig6">6</a>.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Requirement (2) can be ignored to preserve the occluding boundaries.</p>
                    
                  </li>
                </ol><p>In this work, we formulate VDGM as a problem to find new depth values that balance the above requirements.</p><p>Letting <span class="mathjax-tex">\(d'_j\)</span> be the new depth value at the <span class="mathjax-tex">\(j\)</span>th pixel in expanded region <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>, we can express the above requirements in the following minimization problem over <span class="mathjax-tex">\(d'_j\)</span> for all pixels <span class="mathjax-tex">\(j \in \varOmega '_{\mathrm {U}}\)</span> based on the discrete Poisson equation:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \min _{\mathbf {d}'} \sum _{j \in \varOmega _{\mathrm {U}}} (d'_j-d_j)^2 + \sum _{(j,i) \in A_{\mathrm {U}}} (d'_j - d'_i)^2, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\({\mathbf {d}'} = \{d'_j|j \in \varOmega '_{\mathrm {U}}\}\)</span>. The first term is calculated over original target region <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}\)</span> for (1), and the second term smoothes the depth values for (2). <span class="mathjax-tex">\(A_{\mathrm {U}}\)</span> is the set of all pairs of adjacent pixels in <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>, excluding <span class="mathjax-tex">\((j,i)\)</span>, such that</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} |d_j - d_i| &gt; \theta _{\mathrm {E}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>to preserve the boundaries in the original depth map, where we assume that adjacent pixels whose depth value differences are greater than the predetermined threshold <span class="mathjax-tex">\(\theta _{\mathrm {E}}\)</span> form an occluding boundary. Depth values <span class="mathjax-tex">\(d'_j\)</span> for <span class="mathjax-tex">\(j \in \varOmega '_{\mathrm {U}}\backslash \varOmega _{\mathrm {U}}\)</span> are determined solely based on the second term for expanding the target region, where operator “<span class="mathjax-tex">\(\backslash\)</span>” stands for the relative complement.</p><p>The minimization problem in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ1">1</a>) is equivalent to a symmetric and positive-definite linear system obtained by setting its partial derivatives with respect to <span class="mathjax-tex">\(d'_j\)</span> to zero for all <span class="mathjax-tex">\(j \in \varOmega '_{\mathrm {U}}\)</span>. Since the system’s coefficient matrix is sparse, we can solve it by a conjugate gradient (CG) method for sparse systems, which works efficiently on GPUs (example implementation is provided at (nVIDIA)). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig4">4</a> (right) shows the depth map after VDGM.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">View-dependent texture mapping-based color assignment</h2><div class="c-article-section__content" id="Sec5-content"><p>In our method, VDGM expands the target regions in the depth maps and smoothes the depth values. This modification removes the actual details in the target’s 3D model as well, which may result in significant visual artifacts in the generated novel view images. As mentioned in Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e2957">1998</a>), color assignment based on the idea of VDTM reduces these visual artifacts by choosing appropriate images in <span class="mathjax-tex">\(S\)</span> for coloring each pixel. Therefore, our proposed method employs VDTM-based color assignment that consists of visibility check, suitable image selection and coloring with mixture weights.</p><h3 class="c-article__sub-heading" id="Sec6">Visibility check</h3><p>For coloring the pixels in expanded region <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>, the proposed method uses images in input image set <span class="mathjax-tex">\(S\)</span>; however, each image only contains a portion of the target. Obviously, the target’s opposite side is invisible in a single image. In addition, the target can be partially occluded by itself, which is called self-occlusion. To avoid using such images for coloring corresponding pixels in the novel view image to be generated, we employ a visibility check based on the depth map of 3D model <span class="mathjax-tex">\(M\)</span> for the <span class="mathjax-tex">\(n\)</span>th camera <span class="mathjax-tex">\(C_n\)</span>. The depth value at the <span class="mathjax-tex">\(i\)</span>th pixel for <span class="mathjax-tex">\(C_n\)</span> is denoted by <span class="mathjax-tex">\(e_{n,i}\)</span>. The depth map for <span class="mathjax-tex">\(C_n\)</span> can be preliminarily generated in the offline stage.</p><p>Given depth value <span class="mathjax-tex">\(d'_j\)</span> and camera parameters <span class="mathjax-tex">\({\mathbf {R}}_{\mathrm {U}}\)</span> and <span class="mathjax-tex">\({\mathbf {t}}_{\mathrm {U}}\)</span> for user’s camera <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span>, we can regain the 3D position of the corresponding point on <span class="mathjax-tex">\(M\)</span> in the world coordinate system, which is denoted by <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>. This point, which is then transformed to the coordinate system of the <span class="mathjax-tex">\(n\)</span>th camera <span class="mathjax-tex">\(C_n\)</span> using its rotation matrix <span class="mathjax-tex">\({\mathbf {R}}_n\)</span> and translation vector <span class="mathjax-tex">\({\mathbf {t}}_n\)</span>, is converted to depth value <span class="mathjax-tex">\(\bar{d}_{n,j}\)</span> for <span class="mathjax-tex">\(C_n\)</span>. If 3D point <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>, which is regained from the depth, is visible in the <span class="mathjax-tex">\(n\)</span>th image <span class="mathjax-tex">\(I_n\)</span> and projected to the <span class="mathjax-tex">\(i\)</span>th pixel of <span class="mathjax-tex">\(I_n\)</span>, <span class="mathjax-tex">\(\bar{d}_{n,j}\)</span> has the same value as <span class="mathjax-tex">\(e_{n,i}\)</span>. Otherwise, since point <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> is occluded by different surfaces, <span class="mathjax-tex">\(\bar{d}_{n,j}\)</span> is larger than <span class="mathjax-tex">\(e_{n,i}\)</span>. Therefore, if</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} |\bar{d}_{n,j} - e_{n,i}| &lt; \theta _{\mathrm {VC}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>is satisfied with threshold <span class="mathjax-tex">\(\theta _{\mathrm {VC}}\)</span>, we set visibility label <span class="mathjax-tex">\(v_{n,j}\)</span> to 1, and 0 otherwise, where <span class="mathjax-tex">\(v_{n,j} = 1\)</span> means that <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> is visible in <span class="mathjax-tex">\(I_n\)</span>.</p><h3 class="c-article__sub-heading" id="Sec7">Suitable image selection</h3><p>As mentioned in Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e4044">1998</a>), the target details that are lost during 3D modeling and VDGM can be reproduced by the VDTM technique. Devebec et al.’s original method applies a selected image as the texture to each face of the 3D model; however, our proposed method represents the target’s modified geometry as a depth map. We thus introduce a per-pixel VDTM, which selects a suitable image for each pixel in expanded region <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span>. Due to GPU implementation, this process is sufficiently fast for real-time novel view image generation.</p><p>According to Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e4078">1998</a>), we first calculate similarity <span class="mathjax-tex">\(s_{j,n}\)</span> for regained 3D point <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> involving user’s camera <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span> and camera <span class="mathjax-tex">\(C_n\)</span> for the <span class="mathjax-tex">\(n\)</span>th image defined by</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s_{n,j} = \frac{({\mathbf {t}'}_{\mathrm {U}}-{\mathbf {p}}_j)^{\top } ({\mathbf {t}'}_{n}-{\mathbf {p}}_j)}{\Vert {\mathbf {t}'}_{\mathrm {U}}-{\mathbf {p}}_j\Vert \; \Vert {\mathbf {t}'}_{n}-{\mathbf {p}}_j\Vert } + 1, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\({\mathbf {t}'}_{\mathrm {U}}\)</span> and <span class="mathjax-tex">\({\mathbf {t}'}_{n}\)</span> are camera positions defined by <span class="mathjax-tex">\({\mathbf {t}'}_{\mathrm {U}} = -{\mathbf {R}}_{\mathrm {U}} {\mathbf {t}}_{\mathrm {U}}\)</span> and <span class="mathjax-tex">\({\mathbf {t}'}_{n} = -{\mathbf {R}}_{n} {\mathbf {t}}_{n}\)</span>, respectively. <span class="mathjax-tex">\(\top\)</span> represents the transposition and <span class="mathjax-tex">\(\Vert \cdot \Vert\)</span> the Euclidean norm. We add one to make the similarity positive. This similarity corresponds to the angle formed by <span class="mathjax-tex">\(({\mathbf {t}'}_{\mathrm {U}}-{\mathbf {p}}_j)\)</span> and <span class="mathjax-tex">\(({\mathbf {t}'}_{n}-{\mathbf {p}}_j)\)</span>, and a large value means a small difference in the view directions of these cameras. The suitable image for a pixel in <span class="mathjax-tex">\(\varOmega _{\mathrm {U}}'\)</span> is selected based on the following criteria: (1) The 3D point on 3D model <span class="mathjax-tex">\(M\)</span> that corresponds to the pixel is visible in it. (2) The cosine similarity given by Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ4">4</a>) is the largest among all input images that satisfies the criterion (1). Using visibility label <span class="mathjax-tex">\(v_{n,j}\)</span>, these criteria can be encoded in</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \hat{I}_j = I_{\hat{n}_j} \quad\hbox { where }\quad \hat{n}_j = {\mathop {\hbox {argmax}}\limits _n v_{n,j}} s_{n,j}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                <h3 class="c-article__sub-heading" id="Sec8">Coloring using mixture weights</h3><p>Suitable image selection finds the best image based on a similarity measure; however, coloring solely based on this image causes significant visual artifacts due to the transition of the selected images (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig8">8</a>). One way to reduce such visual artifacts is to smoothly mix the colors from multiple images. The proposed method determines the <span class="mathjax-tex">\(j\)</span>th pixel’s color using images selected based on the precomputed spherical Delaunay triangulation (Renka <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Renka R (1997) STRIPACK: Delaunay triangulation and Voronoi diagram on the surface of a sphere. ACM Trans Math Softw 23(3):416–434" href="/article/10.1007/s10055-015-0259-3#ref-CR17" id="ref-link-section-d83365e5039">1997</a>) of camera positions <span class="mathjax-tex">\({\mathbf {t}'}_n\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig9">9</a>). Given regained 3D position <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>, we can identify the triangle intersecting the ray from <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span> to <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>. The <span class="mathjax-tex">\(j\)</span>th pixel is colored by a weighted average of the corresponding pixels’ colors in the images that form the triangle. Since exhaustively finding the intersecting triangle for each pixel is computationally expensive, we use a suitable image identified by <span class="mathjax-tex">\(\hat{n}_j\)</span> to reduce the possible triangles.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Visual artifacts due to texture change in a single color region, exposing significant color discontinuities within an object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Example results of spherical Delaunay triangulation with 3D model <span class="mathjax-tex">\(M\)</span>. <i>Each blue pyramid</i> represents the camera frustum determined by <span class="mathjax-tex">\(C_n\)</span>, and <i>each black line</i> is a <i>triangle edge</i> obtained from spherical Delaunay triangulation (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>After spherical Delaunay triangulation, which can be done in the offline stage, we store the association between each camera position and the triangles whose vertices include that camera position so that we can rapidly identify the triangles that may intersect the line specified by <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> and <span class="mathjax-tex">\({\mathbf {t}'}_{\mathrm {U}}\)</span>.</p><p>In the online stage, given suitable image index <span class="mathjax-tex">\(\hat{n}_j\)</span> selected in the previous section, the proposed method enumerates possible intersecting triangles using the stored association. Let <span class="mathjax-tex">\(\tilde{\mathbf {p}}_j\)</span> be the intersecting point on the plane determined by one of the enumerated triangles, whose vertices (i.e., camera positions) are denoted by <span class="mathjax-tex">\({\mathbf {t}'}_1\)</span>, <span class="mathjax-tex">\({\mathbf {t}'}_2\)</span>, and <span class="mathjax-tex">\({\mathbf {t}'}_3\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig10">10</a>). The barycentric coordinates of <span class="mathjax-tex">\(\tilde{\mathbf {p}}_j\)</span> with respect to <span class="mathjax-tex">\({\mathbf {t}'}_1\)</span>, <span class="mathjax-tex">\({\mathbf {t}'}_2\)</span>, and <span class="mathjax-tex">\({\mathbf {t}'}_3\)</span> are</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \tilde{\mathbf {p}}_j = \alpha {\mathbf {t}'}_1 + \beta {\mathbf {t}'}_2 + \gamma {\mathbf {t}'}_3. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>We can identify the triangle that intersects the ray from <span class="mathjax-tex">\({\mathbf {t}'}_{\mathrm {U}}\)</span> to <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> by finding one that satisfies both of the following conditions:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\left\{ \begin{array}{l}\alpha, \beta, \gamma \ge 0 \\ \alpha + \beta + \gamma = 1. \end{array}\right.$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Color <span class="mathjax-tex">\({\mathbf {c}}_j\)</span> of the <span class="mathjax-tex">\(j\)</span>th pixel of the novel view image is determined by</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {c}}_j = \alpha {\mathbf {c}}_1 + \beta {\mathbf {c}}_2 + \gamma {\mathbf {c}}_3, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <span class="mathjax-tex">\({\mathbf {c}}_1\)</span>, <span class="mathjax-tex">\({\mathbf {c}}_2\)</span>, and <span class="mathjax-tex">\({\mathbf {c}}_3\)</span> are the pixel colors corresponding to <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> in images associated with <span class="mathjax-tex">\({\mathbf {t}'}_1\)</span>, <span class="mathjax-tex">\({\mathbf {t}'}_2\)</span>, and <span class="mathjax-tex">\({\mathbf {t}'}_3\)</span>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Our coloring process finds an intersecting triangle represented by pale red. <i>Each black dot</i> represents the regained position, i.e., <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>. <i>Each black circle</i> is the camera position determined by <span class="mathjax-tex">\(C_n\)</span>, and the <i>blue circle</i> is camera position determined by <span class="mathjax-tex">\(C_{\mathrm {U}}\)</span> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Basically, this coloring process works well; however, it can suffer from the following two problems:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>It can fail to find the intersecting triangles because (1) the target on the floor cannot be shot from a position below the floor or (2) the camera identified by index <span class="mathjax-tex">\(\hat{n}_j\)</span> is not included in the actual intersecting triangle, which can happen if Delaunay triangulation gives obtuse triangles or <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> is invisible in the image that gives the smallest similarity value, <span class="mathjax-tex">\(s_{n,j}\)</span>. This failure to find the triangles is indicated by the negative values of at least one of <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>, and <span class="mathjax-tex">\(\gamma\)</span>, i.e., </p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \alpha &lt; 0,\;\; \beta &lt; 0, \;\;\text{ or }\;\; \gamma &lt; 0. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
                        
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>The 3D point, <span class="mathjax-tex">\({\mathbf {p}}_j\)</span>, can be invisible in the three images that correspond to the vertices of the intersecting triangle due to occlusion, which is indicated by </p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} v_{n,j} = 0 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p> for indices <span class="mathjax-tex">\(n\)</span> that correspond to those three images.</p>
                      
                    </li>
                  </ol><p>If Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ9">9</a>) or (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ10">10</a>) holds true, let <span class="mathjax-tex">\({\mathbf {c}'}_j\)</span> be the color of the pixel corresponding to <span class="mathjax-tex">\({\mathbf {p}}_j\)</span> in <span class="mathjax-tex">\(\hat{I}'_j\)</span>, and the <span class="mathjax-tex">\(j\)</span>th pixel’s color is given by <span class="mathjax-tex">\({\mathbf {c}}_j = {\mathbf {c}'}_j\)</span>. Otherwise, we use Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ8">8</a>).</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Implementation</h2><div class="c-article-section__content" id="Sec9-content"><p>In the offline stage, we used VisualSFM (Wu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Wu C (2013) Towards linear-time incremental structure from motion. In: Proceedings of international conference on 3D vision, pp 127–134" href="/article/10.1007/s10055-015-0259-3#ref-CR22" id="ref-link-section-d83365e6923">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wu C (2011) VisualSFM: a visual structure from motion system. &#xA;                    http://ccwu.me/vsfm/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-015-0259-3#ref-CR21" id="ref-link-section-d83365e6926">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wu C (2007) SiftGPU: a GPU implementation of scale invariant feature transform (SIFT). &#xA;                    http://ccwu.me/vsfm/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-015-0259-3#ref-CR20" id="ref-link-section-d83365e6929">2007</a>) for estimating the camera poses of the images in <span class="mathjax-tex">\(S\)</span> and CMPMVS (Jancosek and Pajdla <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e6951">2011</a>) for 3D model reconstruction. For the spherical Delaunay triangulation, we used an implementation (Renka <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Renka R (1997) STRIPACK: Delaunay triangulation and Voronoi diagram on the surface of a sphere. ACM Trans Math Softw 23(3):416–434" href="/article/10.1007/s10055-015-0259-3#ref-CR17" id="ref-link-section-d83365e6955">1997</a>). Foreground extraction was manually done using Adobe Photoshop CS5/CC.</p><p>Our implementation of the online stage was built on OpenGL and Windows 7 OS. For real-time and stable camera position estimation, we used Metaio SDK (Metaio GmbH). VDGM and most parts of the VDTM-based color assignments were implemented on CUDA to achieve real-time AR image generation. More specifically, we implemented the process related to Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ8">8</a>) on CPU because it uses all of the input images, which cannot usually be stored in the current consumer-grade GPU’s memory. Our experiments used a PC with an Intel Core i7-2600 CPU at 3.4 GHz, 16 GB RAM, and NVIDIA GeForce GTX 550 Ti with 1 GB GPU memory.</p><p>Since parameter <span class="mathjax-tex">\(\theta _{\mathrm {E}}\)</span> for finding the occluding boundaries depends on the scaling and the shape of targets, it may be determined adaptively. However, note that this parameter mainly depends on proportional scaling, which means we can scale the depth values so that they fall within a certain range and apply Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ2">2</a>) with constant <span class="mathjax-tex">\(\theta _{\mathrm {E}}\)</span>. By doing this, the parameter is immune to scaling. In this work, we empirically set constant value <span class="mathjax-tex">\(\theta _{\mathrm {E}} = 0.005\)</span> regardless of the target. Parameter <span class="mathjax-tex">\(\theta _{\mathrm {VC}}\)</span> also proportionally depends on the scaling. For simplicity, our implementation evaluates Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ3">3</a>) in the camera coordinate systems, not in the depths obtained from the standard rendering pipeline. This parameter is also immune to scaling because we can normalize the target size. However, since the targets in our datasets are almost all the same size, we did not normalize them, and parameter <span class="mathjax-tex">\(\theta _{\mathrm {VC}}\)</span> was set to <span class="mathjax-tex">\(0.3\)</span>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Properties of datasets used in our experiments</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0259-3/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Results and discussion</h2><div class="c-article-section__content" id="Sec10-content"><p>This section demonstrates the visual quality of the AR images generated from four datasets using our proposed method. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig11">11</a> shows the reconstructed 3D models and some input images in the datasets, and Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0259-3#Tab1">1</a> summarizes the dataset specifications. The Bottle and Book dataset borrowed from Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e7262">2011</a>) is challenging because it has only 24 images, which are captured from above the targets, and because the bottle is translucent. The Stuffed Penguin dataset is relatively easy because the automatically reconstructed 3D model was faithful to the target’s shape. The 3D model for the Ceramic Cat shows significant missing volumes in the back of its head (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig11">11</a>). This was probably caused by the specular reflection on its surface. The Japanese Tower is another challenging dataset because it suffers from many occluding boundaries.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Datasets used in our experiments. From <i>top to bottom</i> Bottle and Book borrowed from Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e7281">2011</a>), Stuffed Penguin, Ceramic Cat and Japanese Tower. <i>Left-most column</i> shows reconstructed 3D models with their estimated camera positions of input images. Second and third columns show reconstructed 3D models. The other columns show example input images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>As mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0259-3#Sec2">2</a>, many methods have been proposed for image-based rendering, including (Davis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" href="/article/10.1007/s10055-015-0259-3#ref-CR5" id="ref-link-section-d83365e7302">2012</a>) and (Chaurasia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chaurasia G, Duchene S, Sorkine-Hornun O, Drettakis G (2013) Depth synthesis and local warps for plausible image-based navigation. ACM Trans Graph 32(3):30:1–30:12" href="/article/10.1007/s10055-015-0259-3#ref-CR4" id="ref-link-section-d83365e7305">2013</a>); however, most of these were designed for generating images of entire scenes, not for AR image generation, which involves small objects, and thus the visual quality greatly depends on object boundaries. Therefore, we compared two baselines that are applicable to AR image generation: model-based and VDTM-based methods. The model-based method uses only automatically reconstructed 3D models with colors that were preliminarily assigned to all of the vertices by Jancosek and Pajdla (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128" href="/article/10.1007/s10055-015-0259-3#ref-CR10" id="ref-link-section-d83365e7308">2011</a>). Such a model-based method, including one that uses handcrafted 3D models, is frequently used in AR applications (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" href="/article/10.1007/s10055-015-0259-3#ref-CR1" id="ref-link-section-d83365e7311">1997</a>). Using this method, we can show the reconstructed 3D models themselves. The VDTM-based method applies VDTM to each pixel in the depth map. This is a slightly modified version of an original method (Debevec et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116" href="/article/10.1007/s10055-015-0259-3#ref-CR6" id="ref-link-section-d83365e7315">1998</a>) that applies texture to each face of 3D mesh models. Our modified version gave almost the same visual quality as the original one. In addition, we did not use the foreground extraction results in this method to emphasize its effectiveness.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig12">12</a>a–d shows example AR images generated by the proposed method and by the baselines. Compared with the VDTM-based method, our results demonstrate that the proposed method reduced the visual artifacts due to missing and excessive volumes as well as the false high-frequency components in the target boundaries in the closer views (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig13">13</a>). For a viewpoint that is not very close to that of <span class="mathjax-tex">\(C_n\)</span>, the proposed method produced blurry boundaries around the bottle cap and the middle of the bottle in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig13">13</a> (top left). This is because it uses three input images to color a single pixel in a novel view image of the target. Each pixel of the generated image is a weighted sum of the pixels in these three images, and in general, the target boundaries are not consistent with each other due to inaccurate 3D models and camera poses.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>AR images: <b>a</b> Bottle and Book, <b>b</b> Stuffed Penguin, <b>c</b> Ceramic Cat and <b>d</b> Japanese Tower</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The reduction of the missing volume is significant for the Ceramic Cat dataset. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig11">11</a>, the reconstructed 3D model for the Ceramic Cat has a large missing volume on its back (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig13">13</a>, middle right). VDGM fills this missing part of the target, although not completely (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig13">13</a>, middle left). For the Japanese Tower dataset, the missing volume around the tower’s top roof in the left image of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig13">13</a> (bottom) was also partly filled. The spire on the top of the roof, however, cannot be regained because it is elongated and is almost completely missing in the reconstructed 3D model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Close views of targets by proposed method (<i>left</i>) and model-based method with VDTM (<i>right</i>). From <i>top to bottom</i> Bottle and Book shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig12">12</a>a, Ceramic Cat shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig12">12</a>b and Japanese Tower shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig12">12</a>d</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec11">Limitations</h3><p>One of the limitations of the proposed method is that it may produce artifacts due to using VDGM and VDTM, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig14">14</a> (left). To expand the target region, VDGM calculates the depth values in <span class="mathjax-tex">\(\varOmega '_{\mathrm {U}}\)</span> using Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0259-3#Equ1">1</a>), which forces them to be smooth. Therefore, the depth values are not faithful to the true geometry of the target, which means that pixels around target boundaries can be projected to background regions in <span class="mathjax-tex">\(I_n\)</span>. This generates visual artifacts in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig14">14</a> (left).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Visual artifacts in our proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Another limitation that produced artifacts in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig14">14</a> (right) is due to a small number of input images. As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0259-3#Sec8">5.3</a>, the coloring strategy of the proposed method uses only one suitable image when it cannot find an intersecting triangle. In this case, the proposed method cannot reduce the artifacts due to selected image transition (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig8">8</a>). Combined with the first limitation, the proposed method can produce the false shape in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig14">14</a> (right).</p><h3 class="c-article__sub-heading" id="Sec12">Timing results</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig15">15</a>a–d shows the timing results. Instead of showing the averaged time of each process, we plotted the number of pixels versus the elapsed time per frame for each dataset with a single sequence because it greatly depends on the target size. The overall timing linearly increased due to VDGM- and VDTM-based color assignment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig15">15</a>a). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0259-3#Fig15">15</a>d suggests that VDTM-based color assignment depends on the number of input images. The dominant process was VDTM-based color assignment, followed by VDGM. The time of the depth map generation was nearly constant.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0259-3/MediaObjects/10055_2015_259_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Elapsed time per frame required for <b>a</b> rendering a complete frame, <b>b</b> depth map generation, <b>c</b> VDGM and <b>d</b> VDTM-based color assignment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0259-3/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Conclusion</h2><div class="c-article-section__content" id="Sec13-content"><p>In this paper, we proposed an image-based rendering method for AR image generation that modifies the geometry (i.e., a rough 3D model) of a target object for a given viewpoint by solving the discrete Poisson equation. Such a view-dependent approach is beneficial because it does not require the construction of a sufficiently accurate 3D model of the target object. Our results confirm that our proposed method successfully reduced the visual artifacts due to the 3D models inaccuracy. For a small number of input images and target object regions that consist of fewer than about 20,000 pixels, it worked in real time with our GPU implementation, and the elapsed time for the AR image generation increased linearly with respect to the target object size. Future work will remedy the limitations that are mostly due to the false depth values produced by VDGM. Another interesting direction is to integrate foreground extraction into our method.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" /><p class="c-article-references__text" id="ref-CR1">Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Presence&amp;volume=6&amp;issue=4&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bastian JW, Ward B, Hill R, Hengel AVD, Dick AR (2010) Interactive modelling for AR applications. In: Proceedi" /><p class="c-article-references__text" id="ref-CR2">Bastian JW, Ward B, Hill R, Hengel AVD, Dick AR (2010) Interactive modelling for AR applications. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 199–205</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buehler C, Bosse M, McMillan L, Gortler S, Cohen M (2001) Unstructured lumigraph rendering. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR3">Buehler C, Bosse M, McMillan L, Gortler S, Cohen M (2001) Unstructured lumigraph rendering. In: Proceedings of ACM SIGGRAPH, pp 425–432</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Chaurasia, S. Duchene, O. Sorkine-Hornun, G. Drettakis, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chaurasia G, Duchene S, Sorkine-Hornun O, Drettakis G (2013) Depth synthesis and local warps for plausible ima" /><p class="c-article-references__text" id="ref-CR4">Chaurasia G, Duchene S, Sorkine-Hornun O, Drettakis G (2013) Depth synthesis and local warps for plausible image-based navigation. ACM Trans Graph 32(3):30:1–30:12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2487228.2487238" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Depth%20synthesis%20and%20local%20warps%20for%20plausible%20image-based%20navigation&amp;journal=ACM%20Trans%20Graph&amp;volume=32&amp;issue=3&amp;pages=30%3A1-30%3A12&amp;publication_year=2013&amp;author=Chaurasia%2CG&amp;author=Duchene%2CS&amp;author=Sorkine-Hornun%2CO&amp;author=Drettakis%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Davis, M. Levoy, F. Durand, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314" /><p class="c-article-references__text" id="ref-CR5">Davis A, Levoy M, Durand F (2012) Unstructured light fields. Comput Graph Forum 31(2):305–314</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2012.03009.x" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unstructured%20light%20fields&amp;journal=Comput%20Graph%20Forum&amp;volume=31&amp;issue=2&amp;pages=305-314&amp;publication_year=2012&amp;author=Davis%2CA&amp;author=Levoy%2CM&amp;author=Durand%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-map" /><p class="c-article-references__text" id="ref-CR6">Debevec P, Yu Y, Borshukov G (1998) Efficient view-dependent image-based rendering with projective texture-mapping. In: Proceedings of rendering techniques, pp 105–116</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Furukawa Y, Curless B, Seitz SM, Szeliski R (2010) Towards internet-scale multi-view stereo. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR7">Furukawa Y, Curless B, Seitz SM, Szeliski R (2010) Towards internet-scale multi-view stereo. In: Proceedings of 2010 IEEE conference computer vision and pattern recognition, pp 1434–1441</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gortler SJ, Grzeszczuk R, Szeliski R, Cohen MF (1996) The lumigraph. In: Proceedings of ACM SIGGRAPH, pp 43–54" /><p class="c-article-references__text" id="ref-CR8">Gortler SJ, Grzeszczuk R, Szeliski R, Cohen MF (1996) The lumigraph. In: Proceedings of ACM SIGGRAPH, pp 43–54</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Irani M, Hassner T, Anandan P (2002) What does the scene look like from a scene point? In: Proceedings of 7th " /><p class="c-article-references__text" id="ref-CR9">Irani M, Hassner T, Anandan P (2002) What does the scene look like from a scene point? In: Proceedings of 7th European conference computer vision, pp 883–897</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR10">Jancosek M, Pajdla T (2011) Multi-view reconstruction preserving weakly-supported surfaces. In: Proceedings of 2011 IEEE conference computer vision and pattern recognition, pp 3121–3128</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferen" /><p class="c-article-references__text" id="ref-CR11">Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of 2nd IEEE/ACM international workshop on augmented reality, pp 85–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of 6th IEEE/AC" /><p class="c-article-references__text" id="ref-CR12">Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of 6th IEEE/ACM international symposium on mixed and augmented reality, pp 225–234</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Kolev, T. Brox, D. Cremers, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Kolev K, Brox T, Cremers D (2012) Fast joint estimation of silhouettes and dense 3D geometry from multiple ima" /><p class="c-article-references__text" id="ref-CR13">Kolev K, Brox T, Cremers D (2012) Fast joint estimation of silhouettes and dense 3D geometry from multiple images. IEEE Trans Pattern Anal Mach Intell 34(3):493–505</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2011.150" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20joint%20estimation%20of%20silhouettes%20and%20dense%203D%20geometry%20from%20multiple%20images&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=34&amp;issue=3&amp;pages=493-505&amp;publication_year=2012&amp;author=Kolev%2CK&amp;author=Brox%2CT&amp;author=Cremers%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31–42" /><p class="c-article-references__text" id="ref-CR14">Levoy M, Hanrahan P (1996) Light field rendering. In: Proceedings of ACM SIGGRAPH, pp 31–42</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Metaio GmbH. Metaio SDK. http://www.metaio.com/products/sdk/&#xA;                " /><p class="c-article-references__text" id="ref-CR15">Metaio GmbH. Metaio SDK. <a href="http://www.metaio.com/products/sdk/">http://www.metaio.com/products/sdk/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="NVIDIA Developer Zone. ConjugateGradientUM. http://docs.nvidia.com/cuda/cuda-samples/#conjugategradient&#xA;      " /><p class="c-article-references__text" id="ref-CR16">NVIDIA Developer Zone. ConjugateGradientUM. <a href="http://docs.nvidia.com/cuda/cuda-samples/#conjugategradient">http://docs.nvidia.com/cuda/cuda-samples/#conjugategradient</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Renka, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Renka R (1997) STRIPACK: Delaunay triangulation and Voronoi diagram on the surface of a sphere. ACM Trans Math" /><p class="c-article-references__text" id="ref-CR17">Renka R (1997) STRIPACK: Delaunay triangulation and Voronoi diagram on the surface of a sphere. ACM Trans Math Softw 23(3):416–434</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0903.65111" aria-label="View reference 17 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1672176" aria-label="View reference 17 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F275323.275329" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=STRIPACK%3A%20Delaunay%20triangulation%20and%20Voronoi%20diagram%20on%20the%20surface%20of%20a%20sphere&amp;journal=ACM%20Trans%20Math%20Softw&amp;volume=23&amp;issue=3&amp;pages=416-434&amp;publication_year=1997&amp;author=Renka%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Rother, V. Kolmogorov, A. Blake, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="&#xA;Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. " /><p class="c-article-references__text" id="ref-CR18">
Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts.  ACM Trans Graph 23(3):309–314</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1015706.1015720" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grabcut%3A%20interactive%20foreground%20extraction%20using%20iterated%20graph%20cuts&amp;journal=ACM%20Trans%20Graph&amp;volume=23&amp;issue=3&amp;pages=309-314&amp;publication_year=2004&amp;author=Rother%2CC&amp;author=Kolmogorov%2CV&amp;author=Blake%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stere" /><p class="c-article-references__text" id="ref-CR19">Seitz SM, Curless B, Diebel J, Scharstein D, Szeliski R (2006) A comparison and evaluation of multi-view stereo reconstruction algorithms. In: Proceedings of 2006 IEEE conference computer vision and pattern recognition, pp 519–528</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu C (2007) SiftGPU: a GPU implementation of scale invariant feature transform (SIFT). http://ccwu.me/vsfm/&#xA;  " /><p class="c-article-references__text" id="ref-CR20">Wu C (2007) SiftGPU: a GPU implementation of scale invariant feature transform (SIFT). <a href="http://ccwu.me/vsfm/">http://ccwu.me/vsfm/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu C (2011) VisualSFM: a visual structure from motion system. http://ccwu.me/vsfm/&#xA;                " /><p class="c-article-references__text" id="ref-CR21">Wu C (2011) VisualSFM: a visual structure from motion system. <a href="http://ccwu.me/vsfm/">http://ccwu.me/vsfm/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu C (2013) Towards linear-time incremental structure from motion. In: Proceedings of international conference" /><p class="c-article-references__text" id="ref-CR22">Wu C (2013) Towards linear-time incremental structure from motion. In: Proceedings of international conference on 3D vision, pp 127–134</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0259-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work is partly supported by the Japan Society for the Promotion of Science KAKENHI No. 23240024.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma, Nara, 630-0192, Japan</p><p class="c-article-author-affiliation__authors-list">Yuta Nakashima, Yusuke Uno, Norihiko Kawai, Tomokazu Sato &amp; Naokazu Yokoya</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Yuta-Nakashima"><span class="c-article-authors-search__title u-h3 js-search-name">Yuta Nakashima</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yuta+Nakashima&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yuta+Nakashima" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yuta+Nakashima%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Yusuke-Uno"><span class="c-article-authors-search__title u-h3 js-search-name">Yusuke Uno</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yusuke+Uno&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yusuke+Uno" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yusuke+Uno%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Norihiko-Kawai"><span class="c-article-authors-search__title u-h3 js-search-name">Norihiko Kawai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Norihiko+Kawai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Norihiko+Kawai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Norihiko+Kawai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tomokazu-Sato"><span class="c-article-authors-search__title u-h3 js-search-name">Tomokazu Sato</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tomokazu+Sato&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tomokazu+Sato" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tomokazu+Sato%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Naokazu-Yokoya"><span class="c-article-authors-search__title u-h3 js-search-name">Naokazu Yokoya</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Naokazu+Yokoya&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Naokazu+Yokoya" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Naokazu+Yokoya%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0259-3/email/correspondent/c1/new">Yuta Nakashima</a>.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec14-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdivB2n8smo6vkcBGEgofijd7H"><div mi24-video-player="true" video-id="B2n8smo6vkcBGEgofijd7H" player-id="8PcXmCm9nWqE6posBEkd1h" config-type="vmpro" flash-path="//e.video-cdn.net/v2/" api-url="//d.video-cdn.net/play"></div><script src="//e.video-cdn.net/v2/embed.js"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>Supplementary material 1 (mp4 80974 KB)</p></div></div>
                </div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=AR%20image%20generation%20using%20view-dependent%20geometry%20modification%20and%20texture%20mapping&amp;author=Yuta%20Nakashima%20et%20al&amp;contentID=10.1007%2Fs10055-015-0259-3&amp;publication=1359-4338&amp;publicationDate=2015-01-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0259-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0259-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Nakashima, Y., Uno, Y., Kawai, N. <i>et al.</i> AR image generation using view-dependent geometry modification and texture mapping.
                    <i>Virtual Reality</i> <b>19, </b>83–94 (2015). https://doi.org/10.1007/s10055-015-0259-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0259-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-04-22">22 April 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-01-06">06 January 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-01-22">22 January 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-06">June 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0259-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0259-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">View-dependent geometry modification</span></li><li class="c-article-subject-list__subject"><span itemprop="about">View-dependent texture mapping</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Free-viewpoint image generation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0259-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=259;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

