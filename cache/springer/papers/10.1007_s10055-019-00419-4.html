<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="EVA: EVAluating at-home rehabilitation exercises using augmented reali"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among the emerging social needs, rehabilitation..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-12-17"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among the emerging social needs, rehabilitation services are the most required. However, they are scarce and expensive what considerably limits access to them. In this paper, we propose EVA, an augmented reality platform to engage and supervise rehabilitation sessions at home using low-cost sensors. It also stores the user&#8217;s statistics and allows therapists to tailor the exercise programs according to their performance. This system has been evaluated in both qualitative and quantitative ways obtaining very promising results."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-12-17"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="15"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00419-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00419-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00419-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00419-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors"/>

    <meta name="citation_online_date" content="2019/12/17"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="15"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00419-4"/>

    <meta name="DOI" content="10.1007/s10055-019-00419-4"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00419-4"/>

    <meta name="description" content="Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among "/>

    <meta name="dc.creator" content="Felix Escalona"/>

    <meta name="dc.creator" content="Ester Martinez-Martin"/>

    <meta name="dc.creator" content="Edmanuel Cruz"/>

    <meta name="dc.creator" content="Miguel Cazorla"/>

    <meta name="dc.creator" content="Francisco Gomez-Donoso"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Phys Ther Rev; citation_title=Augmented reality applications in rehabilitation to improve physical outcomes; citation_author=H Al-Issa, H Regenbrecht, L Hale; citation_volume=17; citation_issue=1; citation_publication_date=2013; citation_pages=16-28; citation_doi=10.1179/1743288X11Y.0000000051; citation_id=CR1"/>

    <meta name="citation_reference" content="Aung YM, Al-Jumaily A, Anam K (2014) A novel upper limb rehabilitation system with self-driven virtual arm illusion. In: 36th Annual international conference of the IEEE engineering in medicine and biology society, pp 3614&#8211;3617, Chicago, IL, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Mechatron Autom; citation_title=Augmented reality-based rehabio system for shoulder rehabilitation; citation_author=YM Aung, A Al-Jumaily; citation_volume=4; citation_issue=1; citation_publication_date=2014; citation_pages=52-62; citation_doi=10.1504/IJMA.2014.059774; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=J Usability Stud; citation_title=Determining what individual SUS scores mean: adding an adjective rating scale; citation_author=A Bangor, P Kortum, J Miller; citation_volume=4; citation_issue=3; citation_publication_date=2009; citation_pages=114-123; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=J Phys Ther Sci; citation_title=Visual biofeedback balance training using wii fit after stroke: a randomized controlled trial; citation_author=L Barcala, LAC Grecco, F Colella, PRG Lucareli, ASI Salgado, CS Oliveira; citation_volume=25; citation_issue=8; citation_publication_date=2013; citation_pages=1027-1032; citation_doi=10.1589/jpts.25.1027; citation_id=CR5"/>

    <meta name="citation_reference" content="Berndt DJ, Clifford J (1994) Using dynamic time warping to find patterns in time series. In: KDD workshop, vol 10, pp 359&#8211;370. Seattle, WA"/>

    <meta name="citation_reference" content="British National Health Security (NHS) (2018) 
https://www.nhs.uk/Tools/Documents/NHS_ExercisesForOlderPeople.pdf


"/>

    <meta name="citation_reference" content="citation_journal_title=Usability Eval Ind; citation_title=SUS-a quick and dirty usability scale; citation_author=J Brooke; citation_volume=189; citation_issue=194; citation_publication_date=1996; citation_pages=4-7; citation_id=CR8"/>

    <meta name="citation_reference" content="Cao Zhe, Simon Tomas, Wei Shih-En, Sheikh Yaser (2017) Realtime multi-person 2d pose estimation using part affinity fields. In: Computer Vision and Pattern Regognition (CVPR)
"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Pharos-physical assistant robot system; citation_author=A Costa, E Martinez-Martin, M Cazorla, V Julian; citation_volume=18; citation_publication_date=2018; citation_pages=2633; citation_doi=10.3390/s18082633; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Disabil Rehabil: Assist Technol; citation_title=An augmented reality system for upper-limb post-stroke motor rehabilitation: a feasibility study; citation_author=GA Assis, AGD Correa, MBR Martins, WG Pedrozo, RDD Lopes; citation_volume=11; citation_issue=16; citation_publication_date=2014; citation_pages=521-526; citation_id=CR11"/>

    <meta name="citation_reference" content="Desai K, Bahirat K, Ramalingam S, Prabhakaran B, Annaswamy T, Makris UE (2016) Augmented reality-based exergames for rehabilitation. In: 7th International conference on multimedia systems. Klagenfurt, Austria"/>

    <meta name="citation_reference" content="citation_journal_title=Physiotherapy; citation_title=Motor learning, retention and transfer after virtual-reality-based training in parkinson&#8217;s disease-effect of motor and cognitive demands of games: a longitudinal, controlled clinical study; citation_author=FA Santos Mendesa, JE Pompeua, AM Loboa, KG Silva, OT Paula, AP Zomignani, MEP Piemonte; citation_volume=98; citation_publication_date=2012; citation_pages=217-223; citation_doi=10.1016/j.physio.2012.06.001; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Physiotherapy; citation_title=Use of nintendo wii fit in the rehabilitation of outpatients following total knee replacement: a preliminary randomised controlled trial; citation_author=V Fung, A Ho, J Shaffer, E Chung, M Gomez; citation_volume=98; citation_publication_date=2012; citation_pages=183-188; citation_doi=10.1016/j.physio.2012.04.001; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Hum-Mach Syst; citation_title=Human activity recognition process using 3-D posture data; citation_author=S Gaglio, GL Re, M Morana; citation_volume=45; citation_issue=5; citation_publication_date=2015; citation_pages=586-597; citation_doi=10.1109/THMS.2014.2377111; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Phys Rehabil Med; citation_title=Augmented reality system for muscle activity biofeedback; citation_author=M Gazzoni, GL Cerone; citation_volume=61; citation_publication_date=2018; citation_pages=e483-e484; citation_doi=10.1016/j.rehab.2018.05.1129; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit Lett; citation_title=A robotic platform for customized and interactive rehabilitation of persons with disabilities; citation_author=F Gomez-Donoso, S Orts-Escolano, A Garcia-Garcia, J Garcia-Rodriguez, JA Castro-Vargas, S Ovidiu-Oprea, M Cazorla; citation_volume=99; citation_publication_date=2017; citation_pages=105-113; citation_doi=10.1016/j.patrec.2017.05.027; citation_id=CR17"/>

    <meta name="citation_reference" content="Hocoma. Lokomat. 
https://esa.un.org/unpd/wpp/

, 2018"/>

    <meta name="citation_reference" content="Indra (2019) Toyra. 
https://www.tecnologiasaccesibles.com/es/proyectos/toyra


"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Syst Rehabil Eng; citation_title=Virtual reality-enhanced stroke rehabilitation; citation_author=D Jack, R Boian, AS Merians, M Tremaine, GC Burdea, SV Adamovich, M Recce, H Poizner; citation_volume=9; citation_issue=3; citation_publication_date=2001; citation_pages=308-318; citation_doi=10.1109/7333.948460; citation_id=CR20"/>

    <meta name="citation_reference" content="Kanazawa A, Black MJ, Jacobs DW, Malik J (2018) End-to-end recovery of human shape and pose. In: Computer vision and pattern recognition (CVPR)"/>

    <meta name="citation_reference" content="Levy-Tzedek S, Berman S, Stiefel Y, Sharlin E, Young J, Rea D (2017) Robotic mirror game for movement rehabilitation. In: 2017 International conference on virtual rehabilitation (ICVR), pp 1&#8211;2. 
https://doi.org/10.1109/ICVR.2017.8007494


"/>

    <meta name="citation_reference" content="citation_title=Microsoft COCO: Common Objects in Context; citation_inbook_title=Computer Vision &#8211; ECCV 2014; citation_publication_date=2014; citation_pages=740-755; citation_id=CR23; citation_author=Tsung-Yi Lin; citation_author=Michael Maire; citation_author=Serge Belongie; citation_author=James Hays; citation_author=Pietro Perona; citation_author=Deva Ramanan; citation_author=Piotr Doll&#225;r; citation_author=C. Lawrence Zitnick; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="citation_title=Personal Robot Assistants for Elderly Care: An Overview; citation_inbook_title=Intelligent Systems Reference Library; citation_publication_date=2017; citation_pages=77-91; citation_id=CR24; citation_author=Ester Martinez-Martin; citation_author=Angel P. del Pobil; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="citation_journal_title=Disabil Rehabil: Assist Technol; citation_title=Virtual reality rehabilitation of balance: assessment of the usability of the nintendo Wii&#174;fit plus; citation_author=D Meldrum, A Glennon, S Herdman, D Murray, R McConn-Walsh; citation_volume=7; citation_issue=3; citation_publication_date=2012; citation_pages=205-210; citation_doi=10.3109/17483107.2011.616922; citation_id=CR25"/>

    <meta name="citation_reference" content="Monge J, Postolache O (2018) Augmented reality and smart sensors for physical rehabilitation. In: 10th International conference and exposition on electrical and power engineering (EPE2018), pp 1010&#8211;1014"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot; citation_title=ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras; citation_author=R Mur-Artal, JD Tardos; citation_volume=33; citation_publication_date=2017; citation_pages=1255-1262; citation_doi=10.1109/TRO.2017.2705103; citation_id=CR27"/>

    <meta name="citation_reference" content="Nintendo (2008) Wii fit. 
https://www.nintendo.es/Juegos/Wii/Wii-Fit-283894.html


"/>

    <meta name="citation_reference" content="PTC Inc (2019) Vuforia. 
https://developer.vuforia.com/


"/>

    <meta name="citation_reference" content="Redmon J, Farhadi A (2018) Yolov3: an incremental improvement. ArXiv e-prints"/>

    <meta name="citation_reference" content="citation_journal_title=Intell Data Anal; citation_title=Toward accurate dynamic time warping in linear time and space; citation_author=S Salvador, P Chan; citation_volume=11; citation_issue=5; citation_publication_date=2007; citation_pages=561-580; citation_doi=10.3233/IDA-2007-11508; citation_id=CR31"/>

    <meta name="citation_reference" content="SilverFit (2019) Silverfit 3d. 
https://silverfit.com/


"/>

    <meta name="citation_reference" content="Sousa M, Vieira J, Medeiros D, Arsenio A, Jorge J (2016) Sleevear: augmented reality for rehabilitation using realtime feedback. In: Proceedings of the 21st international conference on intelligent user interfaces, pp 175&#8211;185, Sonoma, California, USA. 
https://doi.org/10.1145/2856767.2856773


"/>

    <meta name="citation_reference" content="citation_journal_title=BioMed Research International; citation_title=Robotic Upper Limb Rehabilitation after Acute Stroke by NeReBot: Evaluation of Treatment Costs; citation_author=Masiero Stefano, Poli Patrizia, Armani Mario, Gregorio Ferlini, Roberto Rizzello, Giulio Rosati; citation_volume=2014; citation_publication_date=2014; citation_pages=1-5; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=J Ambient Intell Smart Environ; citation_title=A personalized exercise trainer for the elderly; citation_author=D Steffen, G Bleser, M Weber, D Stricker, L Fradet, F Marin; citation_volume=5; citation_publication_date=2013; citation_pages=547-562; citation_doi=10.3233/AIS-130234; citation_id=CR35"/>

    <meta name="citation_reference" content="Tyromotion (2018) Amadeo. 
https://tyromotion.com/en/produkte/amadeo/


"/>

    <meta name="citation_reference" content="World Health Organization (2010) Community-based rehabilitation: CBR guidelines. Technical report, Geneva"/>

    <meta name="citation_reference" content="World Health Organization (2015) Who global disability action plan 2014&#8211;2021: better health for all people with disability. Technical report"/>

    <meta name="citation_reference" content="Yang Shichao, Maturana Daniel, Scherer Sebastian (May 2016) Real-time 3d scene layout from a single image using convolutional neural networks. In: 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 2183&#8211;2189. 
https://doi.org/10.1109/ICRA.2016.7487368


"/>

    <meta name="citation_author" content="Felix Escalona"/>

    <meta name="citation_author_institution" content="University Institute for Computer Research, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Ester Martinez-Martin"/>

    <meta name="citation_author_institution" content="University Institute for Computer Research, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Edmanuel Cruz"/>

    <meta name="citation_author_institution" content="University Institute for Computer Research, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Miguel Cazorla"/>

    <meta name="citation_author_institution" content="University Institute for Computer Research, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Francisco Gomez-Donoso"/>

    <meta name="citation_author_email" content="fgomez@dccia.ua.es"/>

    <meta name="citation_author_institution" content="University Institute for Computer Research, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00419-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00419-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors"/>
        <meta property="og:description" content="Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among the emerging social needs, rehabilitation services are the most required. However, they are scarce and expensive what considerably limits access to them. In this paper, we propose EVA, an augmented reality platform to engage and supervise rehabilitation sessions at home using low-cost sensors. It also stores the user’s statistics and allows therapists to tailor the exercise programs according to their performance. This system has been evaluated in both qualitative and quantitative ways obtaining very promising results."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00419-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Rehabilitation exercises, Deep learning, Augmented reality, Human–computer interaction, 3D visualization, Low-cost sensors","kwrd":["Rehabilitation_exercises","Deep_learning","Augmented_reality","Human–computer_interaction","3D_visualization","Low-cost_sensors"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00419-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00419-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=419;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00419-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00419-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00419-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-12-17" itemprop="datePublished">17 December 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Felix-Escalona" data-author-popup="auth-Felix-Escalona">Felix Escalona</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="grid.5268.9, 0000 0001 2168 1800, University Institute for Computer Research, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ester-Martinez_Martin" data-author-popup="auth-Ester-Martinez_Martin">Ester Martinez-Martin</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="grid.5268.9, 0000 0001 2168 1800, University Institute for Computer Research, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Edmanuel-Cruz" data-author-popup="auth-Edmanuel-Cruz">Edmanuel Cruz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="grid.5268.9, 0000 0001 2168 1800, University Institute for Computer Research, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Miguel-Cazorla" data-author-popup="auth-Miguel-Cazorla">Miguel Cazorla</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="grid.5268.9, 0000 0001 2168 1800, University Institute for Computer Research, Universidad de Alicante, Alicante, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Francisco-Gomez_Donoso" data-author-popup="auth-Francisco-Gomez_Donoso" data-corresp-id="c1">Francisco Gomez-Donoso<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-7830-2661"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-7830-2661</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="grid.5268.9, 0000 0001 2168 1800, University Institute for Computer Research, Universidad de Alicante, Alicante, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">164 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00419-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among the emerging social needs, rehabilitation services are the most required. However, they are scarce and expensive what considerably limits access to them. In this paper, we propose EVA, an augmented reality platform to engage and supervise rehabilitation sessions at home using low-cost sensors. It also stores the user’s statistics and allows therapists to tailor the exercise programs according to their performance. This system has been evaluated in both qualitative and quantitative ways obtaining very promising results.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>According to the World Health Organization (WHO), about 15% of the world’s population suffers some form of disability. In addition, this rate is continuously increasing as a result of the society aging and the growth in the prevalence of chronic diseases such as cancer or mental health disorders. This fact has led to a social concern about their health care, especially those people with significant difficulties in functioning.</p><p>In this regard, one of the most noteworthy shortcomings is the rehabilitation services since they play a main role in the person’s autonomy reinforcement, the decrease in their vulnerability and the improvement in their physical condition. Furthermore, a proper recovery prevents an early retirement from work, a considerable decrease in accumulated wealth and/or a reduction in social functions. However, the deficiencies in rehabilitation services, their cost and their long duration demand the development of technology supporting this process at home.</p><p>In this work, we propose an augmented reality system to perform and evaluate rehabilitation exercises at home. This system is aimed at two kind of people: patients who require rehabilitation at home after an injury and the elderly people. Therefore, our purpose is to help them to recover from their affections and, consequently, to improve their quality of life. Our proposal consists of spawning a personal trainer on the patient’s home by taking advantage of the augmented reality methods. The user is able to watch the personal trainer and carry out the exercises by imitating him in real time, just like if they were in an actual gym. Upon the end of an exercise, the system automatically grades the patient’s performance taking into account the similarity between the trainer’s movements and the patient’s ones. The exercise sessions are recorded such that the patient and the therapist could review them anytime in order to improve their performance and know the patient’s health status at any time. In addition, low-cost sensors like regular color cameras are used, making the system easily affordable.</p><p>We conceived the approach as a cloud-based service. The heavy computation part of the system is carried out in remote servers that are maintained by the entity that offers the service, namely the government, hospitals, clinics or retirement homes for instance. The final user only needs a low-end terminal like an embedded device or a tablet/smartphone. Despite the computation power requirements are high in the server side, a single machine could render service to several clients.</p><p>Summarizing, the main contributions of this work are as follows:</p><ul class="u-list-style-bullet"><li><p>A low-cost AR rehabilitation app which successfully integrates different deep learning methods</p></li><li><p>Our system enables a remote rehabilitation, making therapy accessible to users who would otherwise not have access. This feature is desirable as stated by the WHO (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Al-Issa H, Regenbrecht H, Hale L (2013) Augmented reality applications in rehabilitation to improve physical outcomes. Phys Ther Rev 17(1):16–28. &#xA;https://doi.org/10.1179/1743288X11Y.0000000051&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR1" id="ref-link-section-d95949e338">2015</a>)</p></li><li><p>In contrast with other solutions, ours is non-intrusive. It relies only on vision algorithms</p></li><li><p>A system usability scale study that validates the benefits of the app</p></li><li><p>Our system integrates a <i>reference mat</i>, who shows how to do any required movement. This is crucial to guide the patient when at-home rehabilitation takes place and the therapist is not present</p></li></ul><p>The rest of the paper is organized as follows: First, the state of the art in the field is presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec2">2</a>. Next, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec3">3</a> describes EVA workflow. Then, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec7">4</a> explains the augmented reality application as well as the user interface. The considered rehabilitation exercises are detailed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec9">5</a>. The procedures for testing the proposed approach and their results are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec10">6</a>. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec14">7</a> includes the discussion, conclusions and limitations of the work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The increase in disabled people together with a growing demand for rehabilitation worldwide requires new ways to reduce the cost of the rehabilitation process while maintaining its quality. In this context, assistive technologies play an important role in functioning and increasing independence and participation (World Health Organization <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Aung YM, Al-Jumaily A, Anam K (2014) A novel upper limb rehabilitation system with self-driven virtual arm illusion. In: 36th Annual international conference of the IEEE engineering in medicine and biology society, pp 3614–3617, Chicago, IL, USA" href="/article/10.1007/s10055-019-00419-4#ref-CR2" id="ref-link-section-d95949e381">2010</a>). However, the literature has mainly focused on the physical therapy.</p><p>In this line, Robotics has been a very active research area, going from companion robots for therapy to social assistive robots (Martinez-Martin and del Pobil <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Aung YM, Al-Jumaily A (2014) Augmented reality-based rehabio system for shoulder rehabilitation. Int J Mechatron Autom 4(1):52–62" href="/article/10.1007/s10055-019-00419-4#ref-CR3" id="ref-link-section-d95949e387">2017</a>). So, rehabilitation robots are designed to assist people recovery in two different scenarios. On the one hand, robot systems can be used as a support tool in the rehabilitation process. This is the case of the Lokomat (Hocoma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bangor A, Kortum P, Miller J (2009) Determining what individual SUS scores mean: adding an adjective rating scale. J Usability Stud 4(3):114–123" href="/article/10.1007/s10055-019-00419-4#ref-CR4" id="ref-link-section-d95949e390">2018</a>), an exoskeletal robot for physiological gait rehabilitation; NeReBot (NEuro REhabilitation roBOT) (Stefano et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Barcala L, Grecco LAC, Colella F, Lucareli PRG, Salgado ASI, Oliveira CS (2013) Visual biofeedback balance training using wii fit after stroke: a randomized controlled trial. J Phys Ther Sci 25(8):1027–1032. &#xA;https://doi.org/10.1589/jpts.25.1027&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR5" id="ref-link-section-d95949e393">2014</a>), a cable-suspended device for upper-limb rehabilitation of post-stroke patients; or, AMADEO (Tyromotion <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Berndt DJ, Clifford J (1994) Using dynamic time warping to find patterns in time series. In: KDD workshop, vol 10, pp 359–370. Seattle, WA" href="/article/10.1007/s10055-019-00419-4#ref-CR6" id="ref-link-section-d95949e396">2018</a>), a neurological rehabilitation device designed for the rehabilitation of the hand, fingers and thumb.</p><p>On the other hand, social autonomous robots could supervise the rehabilitation at home, a treatment appropriate for both people who cannot travel easily and those who require less care. From this starting point, for instance, Gomez-Donoso et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="British National Health Security (NHS) (2018) &#xA;https://www.nhs.uk/Tools/Documents/NHS_ExercisesForOlderPeople.pdf&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR7" id="ref-link-section-d95949e402">2017</a>) developed a multisensor system for rehabilitation and interaction with persons with motor and cognitive disabilities. More recently, Costa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Brooke J (1996) SUS-a quick and dirty usability scale. Usability Eval Ind 189(194):4–7" href="/article/10.1007/s10055-019-00419-4#ref-CR8" id="ref-link-section-d95949e405">2018</a>) proposed PHAROS, an interactive robot system that recommends and monitors physical exercises at home designed for staying active, an important part of rehabilitation for chronic diseases.</p><p>Nonetheless, robotic solutions are far from being affordable, particularly in some low- and middle- income countries. In this regard, a virtual environment may be a reasonable substitute as shown in Levy-Tzedek et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Cao Zhe, Simon Tomas, Wei Shih-En, Sheikh Yaser (2017) Realtime multi-person 2d pose estimation using part affinity fields. In: Computer Vision and Pattern Regognition (CVPR)&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR9" id="ref-link-section-d95949e411">2017</a>). Actually, Virtual Reality (VR) has been used for evaluating and treating a number of pathologies. We have to note that the term VR nowadays implies the use of immersive technologies (like</p><p>So, Jack et al. presented a virtual reality system for rehabilitating hand function in stroke patients (Jack et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Costa A, Martinez-Martin E, Cazorla M, Julian V (2018) Pharos-physical assistant robot system. Sensors 18:2633. &#xA;https://doi.org/10.3390/s18082633&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR10" id="ref-link-section-d95949e418">2001</a>). Four rehabilitation routines for hand recovery could be carried out: range, speed, fractionation or strength. For that, two different input devices must be worn: a Cyber-Glove and a Rutgers Master II-ND (RMII) force feedback glove.</p><p>Steffen et al. proposed a home-based platform for physical activity supervision and motivation (Steffen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="de Assis GA, Correa AGD, Martins MBR, Pedrozo WG, Lopes RDD (2014) An augmented reality system for upper-limb post-stroke motor rehabilitation: a feasibility study. Disabil Rehabil: Assist Technol 11(16):521–526" href="/article/10.1007/s10055-019-00419-4#ref-CR11" id="ref-link-section-d95949e424">2013</a>). In particular, its system PAMAP (Physical Activity Monitoring for Aging People) uses the television as an interface for two applications: to set the exercises to be done (defined by a healthcare professional) and to provide feedback to the person. It is noteworthy that a group of sensors capturing person’s motion is required to be able to provide any information about their performance. Given that those sensors must be worn in strategic body positions, the person’s performance could not be properly measured. In addition, a training phase is required to adjust the exercise performance evaluation to the person’s physical limitations.</p><p>A more complex system is proposed by Toyra (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Desai K, Bahirat K, Ramalingam S, Prabhakaran B, Annaswamy T, Makris UE (2016) Augmented reality-based exergames for rehabilitation. In: 7th International conference on multimedia systems. Klagenfurt, Austria" href="/article/10.1007/s10055-019-00419-4#ref-CR12" id="ref-link-section-d95949e430">2019</a>), a rehabilitation platform integrating healthcare information technology, virtual reality and motion capture to develop tailored interactive therapy exercises for upper limb recovery. As previously, several sensors should be worn to properly measure the patient’s motion.</p><p>Virtual games can also be used in this context. Among them, Wii Fit (Nintendo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="dos Santos Mendesa FA, Pompeua JE, Loboa AM, da Silva KG, de Paula OT, Zomignani AP, Piemonte MEP (2012) Motor learning, retention and transfer after virtual-reality-based training in parkinson’s disease-effect of motor and cognitive demands of games: a longitudinal, controlled clinical study. Physiotherapy 98:217–223" href="/article/10.1007/s10055-019-00419-4#ref-CR13" id="ref-link-section-d95949e436">2008</a>) is highlighted since it has been applied in several scenarios such as Parkinson’s disease (dos Santos Mendesa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Fung V, Ho A, Shaffer J, Chung E, Gomez M (2012) Use of nintendo wii fit in the rehabilitation of outpatients following total knee replacement: a preliminary randomised controlled trial. Physiotherapy 98:183–188" href="/article/10.1007/s10055-019-00419-4#ref-CR14" id="ref-link-section-d95949e439">2012</a>), knee replacement (Fung et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gaglio S, Re GL, Morana M (2015) Human activity recognition process using 3-D posture data. IEEE Trans Hum-Mach Syst 45(5):586–597. &#xA;https://doi.org/10.1109/THMS.2014.2377111&#xA;&#xA; ISSN 2168-2291" href="/article/10.1007/s10055-019-00419-4#ref-CR15" id="ref-link-section-d95949e442">2012</a>), stroke rehabilitation (Barcala et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Gazzoni M, Cerone GL (2018) Augmented reality system for muscle activity biofeedback. Ann Phys Rehabil Med 61:e483–e484" href="/article/10.1007/s10055-019-00419-4#ref-CR16" id="ref-link-section-d95949e445">2013</a>) or balance recovery (Meldrum et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gomez-Donoso F, Orts-Escolano S, Garcia-Garcia A, Garcia-Rodriguez J, Castro-Vargas JA, Ovidiu-Oprea S, Cazorla M (2017) A robotic platform for customized and interactive rehabilitation of persons with disabilities. Pattern Recognit Lett 99:105–113. &#xA;https://doi.org/10.1016/j.patrec.2017.05.027&#xA;&#xA; ISSN 0167-8655" href="/article/10.1007/s10055-019-00419-4#ref-CR17" id="ref-link-section-d95949e448">2012</a>).</p><p>Another technology widely studied in the rehabilitation field is Augmented Reality (AR). In fact, AR provides the user with a better sense of presence and reality judgements of the environment as the interaction elements are real (Al-Issa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Hocoma. Lokomat. &#xA;https://esa.un.org/unpd/wpp/&#xA;&#xA;, 2018" href="/article/10.1007/s10055-019-00419-4#ref-CR18" id="ref-link-section-d95949e454">2013</a>). This is the underlying idea of NeuroR (de Assis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Indra (2019) Toyra. &#xA;https://www.tecnologiasaccesibles.com/es/proyectos/toyra&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR19" id="ref-link-section-d95949e457">2014</a>), an AR system for motor rehabilitation of chronic stroke patients that replaces the paralyzed arm in a virtual avatar. However, this kind of systems rely on privative technologies, such as the devices Apple Iphone or Apple Ipad that come with the Apple ARKit (Augmented Reality Kit), specialized hardware like Simblee, bio-signal sensors such as electrocardiography (ECG) or electromyography (EMG) or positional sensors such as inertial measurament unit (IMU). This is the case of the AR systems for upper-limb rehabilitation presented by Aung et al., i.e., ARIS (Augmented Reality based Illusion System) (Aung et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE Trans Neural Syst Rehabil Eng 9(3):308–318. &#xA;https://doi.org/10.1109/7333.948460&#xA;&#xA; ISSN 1534-4320" href="/article/10.1007/s10055-019-00419-4#ref-CR20" id="ref-link-section-d95949e460">2014</a>) and RehaBio (Aung and Al-Jumaily <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Kanazawa A, Black MJ, Jacobs DW, Malik J (2018) End-to-end recovery of human shape and pose. In: Computer vision and pattern recognition (CVPR)" href="/article/10.1007/s10055-019-00419-4#ref-CR21" id="ref-link-section-d95949e463">2014</a>). These systems combine a visual illusory environment with EMG signal (electromyography signal) to monitor the user’s performance. Similarly, Gazzoni and Cerone (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Levy-Tzedek S, Berman S, Stiefel Y, Sharlin E, Young J, Rea D (2017) Robotic mirror game for movement rehabilitation. In: 2017 International conference on virtual rehabilitation (ICVR), pp 1–2. &#xA;https://doi.org/10.1109/ICVR.2017.8007494&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR22" id="ref-link-section-d95949e466">2018</a>) used AR together with surface EMG (sEMG) detection/acquisition systems for physical rehabilitation. Monge and Postolache (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Lin T-Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick CL (2014) Microsoft coco: common objects in context. In: Fleet D, Pajdla T, Schiele B, Tuytelaars T (eds) European conference on computer vision (ECCV), pp 740–755, Cham. Springer. ISBN 978-3-319-10602-1" href="/article/10.1007/s10055-019-00419-4#ref-CR23" id="ref-link-section-d95949e470">2018</a>) combined AR serious games with a wearable sensor network based on <i>Simblee</i>, an Arduino-based programmable board with a wearable design, for physical rehabilitation of lower limb. Although the experimental results are very promising, the system requires an IPhone, what is not affordable for all the people.</p><p>Alternatively, SleeveAR (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Martinez-Martin E, del Pobil AP (2017) Personal robot assistants for elderly care: an overview. In: Costa A, Julian V, Novais P (eds), Personal assistants: emerging computational technologies, pp 77–91. Springer, Berlin. &#xA;https://doi.org/10.1007/978-3-319-62530-0_5&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR24" id="ref-link-section-d95949e479">2016</a>) is an AR system to perform rehabilitation exercises at home to complement in-clinic physical therapy of an injured arm. For that, three different stages take place: <i>the Recording Stage</i>, involves the demonstration of the exercise being recorded by the therapist; <i>the Movement Guidance Stage</i>, focuses on guiding the patient to recreate the prescribed exercise as previously recorded; and, <i>the Performance Review Stage</i>, provides the patient with an overview of their performance, by comparing with the original prescribed exercise. Note that this is a stationary system that requires a special covering floor and a custom sleeve, what considerably restricts its use. On its behalf, Desai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Meldrum D, Glennon A, Herdman S, Murray D, McConn-Walsh R (2012) Virtual reality rehabilitation of balance: assessment of the usability of the nintendo Wii®fit plus. Disabil Rehabil: Assist Technol 7(3):205–210. &#xA;https://doi.org/10.3109/17483107.2011.616922&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR25" id="ref-link-section-d95949e491">2016</a>) used low-cost RGB-D cameras (i.e., Microsoft Kinect) to place the user into an AR scene for exercising by using Mirror therapy. Their pilot experiments result in a positive reinforcement. However, the set of exercises is very reduced (only 4 exercises) what considerably restricts its use. In a similar way, SilverFit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Monge J, Postolache O (2018) Augmented reality and smart sensors for physical rehabilitation. In: 10th International conference and exposition on electrical and power engineering (EPE2018), pp 1010–1014" href="/article/10.1007/s10055-019-00419-4#ref-CR26" id="ref-link-section-d95949e495">2019</a>) employs a 3D camera to register the user’s movements and links them to a game. At the end, the game progress and the final results are displayed on the screen. It could be said that this system is the most similar to our approach since visual data is used to analyze the user’s performance that can be recorded and reviewed afterward. Despite the user progress and score is shown, there is no way to know the errors made except with a session with the therapist.</p><p>Note that all the proposed approaches have three main handicaps. The first one is the used of worn devices, what may considerably affect the patient’s evaluation apart from the inconvenience caused. The second one is their cost what makes them unreachable for most of the disabled people. Thus, with the aim of overcoming these issues, we present a low-cost augmented reality system to monitor and evaluate physical rehabilitation at home. Finally, they do not include any <i>reference person</i> showing them how to do a specific exercise or a precise pose.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Proposal</h2><div class="c-article-section__content" id="Sec3-content"><p>In this work we propose an augmented reality system to perform and evaluate at-home rehabilitation exercises. This system is aimed at two kinds of people: patients requiring at-home rehabilitation after an injury, and elderly citizens. Thus, our purpose is aiding them to recover the mobility of the affected limbs and, consequently, to improve their quality of life.</p><p>With that aim, our system comprehends several rehabilitation exercises that are displayed in a television, computer screen or projector. These exercises are performed by a virtual personal trainer that appears in the patient’s room through augmented reality. So, the display acts as the mirror in a gym: the user is watching himself aside the virtual personal trainer. Then, the user must choose an exercise and mimic the virtual trainer movements. After each exercise, a score is given so that the patient has immediate feedback about their performance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="343" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Flowchart of EVA system. Note that the cloud-based parts are shown in a cloud shape</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In rough lines, the system’s workflow can be summarized as follows (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig1">1</a>): when an exercise is running, the system first takes an image of the scene using a regular camera. Then, it looks for <i>“the trainer’s mat”</i>. This is an object that is used to estimate the floor plane and to set a common 3D reference frame. Once the floor plane is detected, a person detector is then used to extract the person’s position within the image. The area of interest corresponding to the person is sent to the human 3D pose estimator that returns the 3D pose of the person. After that, the virtual trainer and the user interface are rendered over the image to be properly displayed on the screen. When the exercise is completed, the stored 3D human poses and the virtual trainer’s poses are used to measure the user’s performance. This measure is displayed to the user together with their feedback, and is also stored for further analysis.</p><p>It is worth noting that the virtual trainer performing the exercises has been recorded beforehand and the corresponding 3D poses were also estimated offline. The estimation of the trainer’s poses is computed following the same method we used for the user’s ones. In this way, the system is only rendering the trainer’s poses like a video.</p><p>As above mentioned, each exercise execution is stored allowing the user or the therapists to replay their exercises to spot mistakes and further improve their performance. By taking advantage of augmented reality, the user can show the replays in the room’s floor or even in their desk. Furthermore, our system EVA features a web service that accepts an image stream and renders on it the replay of an exercise session. The web server follows the client-server methodology to provide remote access to the capabilities of the system. This option could be used to build a smartphone and/or tablet application to show the replays anywhere and anytime by connecting with the system via WiFi or even 4G. Note that the scores are also stored with the purpose to provide user’s statistics to both users and therapists.</p><p>Additionally, EVA has an easy-to-use, friendly user interface based on virtual touch buttons leveraging augmented reality.</p><h3 class="c-article__sub-heading" id="Sec4">Common 3D coordinate frame estimation</h3><p>As above mentioned, the common 3D coordinate frame is estimated by detecting <i>“the trainer’s mat”</i>. For that, a well-known chessboard pattern has been used (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig2">2</a>). It must be placed on the floor away from the user. In this way, the common 3D coordinate frame allows EVA to accurately detect the floor plane and properly render the avatar of the virtual personal trainer in the scene. As a result, the user senses the virtual trainer next to them in the room. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig2_HTML.jpg" alt="figure2" loading="lazy" width="685" height="237" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A chessboard pattern is used as <i>“the trainer’s mat”</i>. It is a well-known pattern that allows the system to estimate a common 3D coordinate frame. The leftmost image depicts the pattern with the 3D axis superimposed. The rightmost image depicts the personal trainer (skeleton in green) in augmented reality in front of the patient who is imitating them. The personal trainer is rendered in situ thanks to the pattern’s detection (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Although some state-of-the-art approaches are able to detect the floor plane without placing any pattern within a scene (e.g., Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Mur-Artal R, Tardos JD (2017) ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Trans Robot 33:1255–1262. &#xA;https://doi.org/10.1109/TRO.2017.2705103&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR27" id="ref-link-section-d95949e569">2016</a>; Mur-Artal and Tardos <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Nintendo (2008) Wii fit. &#xA;https://www.nintendo.es/Juegos/Wii/Wii-Fit-283894.html&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR28" id="ref-link-section-d95949e572">2017</a>), their lack of accuracy, the high complexity of their calibration procedure or the need of non-static cameras make them unsuitable for the problem at hand. In addition, the presence of a physical element in the room prevents the user to inadvertently trespass the trainer <i>personal</i> space, what would negatively affect the perception of the augmented reality.</p><p>With the aim to detect the floor plane and to set a common coordinate frame, EVA firstly looks for the intersections between the pattern squares in the image. As a result, a set of 2D points on the image plane is obtained. Then, as the pattern is known, the corresponding 3D points can be easily stated. The points lay in a plane so the Z-value is 0. Then, the length of each square side is also known. Therefore, we can easily compute a set of corresponding 3D points for each 2D point detected in the color image. Finally, we solve the Perspective-n-Point problem. As a result, a transformation matrix is obtained. This matrix transforms a point from the 3D world coordinate frame to the 2D coordinate frame of the image.</p><p>Finally, it is worth noting that the intrinsic camera parameters are required to solve the Perspective-n-Point problem and, as a consequence, the camera must be calibrated. This calibration process is carried out offline.</p><p>As the personal trainer poses are stated in the 3D space, they can be easily translated into the image coordinate frame from the transformation matrix computed in this stage. This helps to achieve the illusion of the personal trainer being actually aside the user in their own living room. Furthermore, the trainer is scaled according to the user’s size. This virtual scaled view is a useful feature since it adjusts the taken video recordings to the chessboard pattern size. So, for instance, the therapist could reproduce any exercise session by means of a reduced chessboard pattern on its desk.</p><h3 class="c-article__sub-heading" id="Sec5">Human 3D pose estimation from monocular frames</h3><p>With the aim to compare the user’s and the trainer’s movements in a robust way, a 3D pose is mandatory. A solution could be the estimation of the 2D human poses since low processing load techniques can be found in the literature (Cao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="PTC Inc (2019) Vuforia. &#xA;https://developer.vuforia.com/&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR29" id="ref-link-section-d95949e596">2017</a>). However, the 2D poses may cause ambiguities and singularities between two poses. This fact could lead to a bad similarity estimation. This is the case depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig3">3</a>, where it is impossible to state whether the user is rising his arm forward or backward.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig3_HTML.jpg" alt="figure3" loading="lazy" width="685" height="353" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The 2D poses cannot be used for a robust pose comparison between the user and the trainer because it may eventually fall into a singularity or ambiguous poses such as the depicted in this figure. According to the 2D pose, it is impossible to state whether the patient is rising his arm forward or backward</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Due to the required accuracy in human pose comparison, a 3D pose estimation approach has been used. Our 3D human pose estimation system is based on the Human Mesh Recovery (HMR) approach (Kanazawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Redmon J, Farhadi A (2018) Yolov3: an incremental improvement. ArXiv e-prints" href="/article/10.1007/s10055-019-00419-4#ref-CR30" id="ref-link-section-d95949e612">2018</a>). This method consists of an end-to-end framework for reconstructing a full 3D mesh of a human body from a monocular <i>RGB</i> image. So, a deep learning-based encoder is able to predict the camera pose, the person’s shape and the person’s pose for each taken image. These predictions are used to render a model which is then validated by a discriminator. The discriminator is able to state if the predictions correspond to a real person or not. Given that this approach does not use a 2D intermediate representation being able to make final predictions in one forward step, it is very fast. Note that EVA only works on the inferred 3D human pose, discarding the camera pose and the person’s shape. This 3D human pose is expressed as a list of 3D points in camera coordinates, corresponding to 19 joints of the human body.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig4_HTML.jpg" alt="figure4" loading="lazy" width="685" height="358" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The 3D human pose estimation system is able to accurately estimate the human pose even in difficult cases such as persons in wheelchair or scenarios with high self-occlusion level caused by the exercises</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This approach for 3D human pose estimation is able to accurately estimate the human pose even under different scenarios as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig4">4</a>. For instance, the system works well for persons sitting in wheelchairs. It can also deal with high levels of self-occlusion. These cases play a main role when working with physical injured people and elderly.</p><p>Nonetheless, this approach has an important issue to be pointed out. The best human pose estimations are obtained when there is only one person in the image. However, our system captures images that includes the trainer’s mat and the room elements apart from the patient. For that reason, a person detection stage is needed. To carry out this process, we leveraged YOLOv3 (Redmon and Farhadi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Salvador S, Chan P (2007) Toward accurate dynamic time warping in linear time and space. Intell Data Anal 11(5):561–580" href="/article/10.1007/s10055-019-00419-4#ref-CR31" id="ref-link-section-d95949e634">2018</a>). It is a well-known method for object detection and recognition, providing a decent accuracy while keeping the computation cost at bay. This region convolutional neural network architecture is able to detect the position of the objects in the image plane, estimate the label of those objects as well as their corresponding confidence score. This architecture achieved a 0.51 mAP (measured over the intersection over union) on the test set of the COCO MS dataset (Lin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="SilverFit (2019) Silverfit 3d. &#xA;https://silverfit.com/&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR32" id="ref-link-section-d95949e637">2014</a>). From this pretrained model, a wide range of objects including persons can be detected. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig5">5</a> shows the performance of this architecture for person detection.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig5_HTML.jpg" alt="figure5" loading="lazy" width="685" height="126" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The person detector YOLOv3 (Redmon and Farhadi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Salvador S, Chan P (2007) Toward accurate dynamic time warping in linear time and space. Intell Data Anal 11(5):561–580" href="/article/10.1007/s10055-019-00419-4#ref-CR31" id="ref-link-section-d95949e649">2018</a>) is able to accurately estimate the area of interest enclosing the patient</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>So, the complete human 3D pose estimation pipeline can be described as follows: first, an image is grabbed from the camera. Then, this image feeds the YOLOv3 detector, which provides the area of interest of several objects. From them, the patient is identified as the area of interest with the best confidence score for person. This area of interest is cropped from the original image and forwarded to the HMR network. As a result, the 3D pose of the patient is obtained. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig6">6</a> shows the performance of the HMR network feeding it on the whole image and on the person area detected by YOLOv3. As it can be observed, the predictions are more precise when the network is fed with just the person area. It is worth mentioning that we use the models released by the original authors from both HMR and YOLOv3 methods.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig6_HTML.jpg" alt="figure6" loading="lazy" width="685" height="295" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Performance of the Human Mesh Recovery network feeding it the whole image (center) and the person area of interest detected by YOLOv3 (rightmost)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6">Scoring the user’s performance</h3><p>With the purpose to evaluate the user’s performance during rehabilitation exercises, we propose to compare the user’s joints trajectory with the trainer’s ones by using a dynamic time warping (DTW) approach (Berndt and Clifford <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Sousa M, Vieira J, Medeiros D, Arsenio A, Jorge J (2016) Sleevear: augmented reality for rehabilitation using realtime feedback. In: Proceedings of the 21st international conference on intelligent user interfaces, pp 175–185, Sonoma, California, USA. &#xA;https://doi.org/10.1145/2856767.2856773&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR33" id="ref-link-section-d95949e676">1994</a>). This state-of-the-art method is aimed to find patterns in time series. So, it finds a warping path to align the elements of two time sequences such that the distance between them is minimized. This distance between two elements must be defined, being the euclidean distance the most common. We can see the dynamic time warping problem as a minimization of the cumulative distance over the whole possible paths of two time series elements.</p><p>This method provides a score quantifying the degree of adjustment of two times series when stretching or compressing their elements along the time. Note that this score stays in the range [0, 1] when comparing different series. For that, the distance measurement is often modified to be relative to a baseline distance.</p><p>Given that the time dimension is not considered, this method is appropriate to compare two 3D human poses separated by a short period of time as it is the case (i.e., the user may take time to imitate the trainer’s movements due to their disability).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig7_HTML.jpg" alt="figure7" loading="lazy" width="685" height="734" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Joint positions considered by the system as returned by the human 3D pose estimation system and used to compare the human poses of the user and the virtual trainer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Assuming that different patients use different cameras at different places, it is necessary to define a common reference frame to avoid the influence of these factors in the comparison of the joint positions. We have to transform from the camera frame, defined by 3 camera axis (<i>c</i>) and the origin of coordinates (<span class="mathjax-tex">\(Q_{0}\)</span>) in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00419-4#Equ1">1</a>, to a new invariant reference frame. This reference frame is defined using the user’s body in a preset position. The reference point (<span class="mathjax-tex">\(P_{0}\)</span>) of this frame is the neck (joint 1 in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig7">7</a>). Then, we build two perpendicular vectors (<span class="mathjax-tex">\(v_{0}\)</span> and <span class="mathjax-tex">\(v_{1}\)</span>) from shoulder center to the head (joint 0) and to the right shoulder (joint <b>2</b>) respectively. The last axis vector (<span class="mathjax-tex">\(v_{2}\)</span>) is calculated applying the cross product of the previous vectors, as shown in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00419-4#Equ2">2</a>.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;{\text {Camera}}\,{\text {frame}}: (c_{0},c_{1},c_{2},Q_{0})\\&amp;c_{0} = (0,0,1)\\&amp;c_{1} = (0,1,0)\\&amp;c_{2} = (1,0,0)\\&amp;Q_{0} = (0,0,0) \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;{\text {Body}}\,{\text {frame}}: (v_{0},v_{1},v_{2},P_{0})\\&amp;v_{0} = (j0.x-j1.x, j0.y-j1.y, j0.z-j1.z)\\&amp;v_{1} = (j2.x-j1.x, j2.y-j1.y, j2.z-j1.z)\\&amp;v_{2} = v_{0} \times v_{1}\\&amp;P_{0} = (j1.x, j1.y, j1.z) \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Once we have defined the reference frame centered in the body, we have to estimate the transformation matrix from the camera’s frame to this reference frame. The transformation matrix is estimated from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00419-4#Equ3">3</a> that calculates the coefficients necessary to put the new basis vectors <span class="mathjax-tex">\(v_{0}\)</span>, <span class="mathjax-tex">\(v_{1}\)</span> and <span class="mathjax-tex">\(v_{2}\)</span> as a linear combination of the camera axis <span class="mathjax-tex">\(c_{0}\)</span>, <span class="mathjax-tex">\(c_{1}\)</span> and <span class="mathjax-tex">\(c_{2}\)</span> and the new origin of coordinates in terms of these vectors and the camera’s origin of coordinates.</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;v_{0} = a_{11}c_{0} + a_{12}c_{1} + a_{13}c_{2}\\&amp;v_{1} = a_{21}c_{0} + a_{22}c_{1} + a_{23}c_{2}\\&amp;v_{2} = a_{31}c_{0} + a_{32}c_{1} + a_{33}c_{2}\\&amp;P_{0} = a_{41}c_{0} + a_{42}c_{1} + a_{43}c_{2} + Q_{0}\\&amp;M= \begin{bmatrix} a_{11}&amp;a_{12}&amp;a_{13}&amp;0\\ a_{21}&amp;a_{22}&amp;a_{23}&amp;0\\ a_{31}&amp;a_{32}&amp;a_{33}&amp;0\\ a_{41}&amp;a_{42}&amp;a_{43}&amp;1\\ \end{bmatrix} \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>With the transformation matrix between the camera and the body reference frames, we can simply transform the points multiplying this matrix with the column vector of homogeneous coordinates of each point. Suppose that <i>a</i> and <i>b</i> are the homogeneous representation of the same point on the camera and the body reference frames respectively, so they satisfy Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00419-4#Equ4">4</a>.</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;a = M^{\mathrm{T}}b \\&amp;b = (M^{\mathrm{T}})^{-1}a \\ \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Then, we transform the whole points into the body reference frame and calculate the distance between two sequences of movements for every pair of joints using FastDTW, an accurate and efficient DTW implementation presented in Salvador and Chan (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stefano M, Patrizia P, Mario A, Ferlini G, Rizzello R, Rosati G (2014) Robotic upper limb rehabilitation after acute stroke by NeReBot: evaluation of treatment costs. BioMed Res Int. &#xA;https://doi.org/10.1155/2014/265634&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR34" id="ref-link-section-d95949e1407">2007</a>). So, the final distance is the average of the accumulated distance of the joints.</p><p>Due to the fact that the final distance is estimated for every joint individually, we can report to the user which are the joints they are performing worse. In addition, the DTW is computed every 120 frames with an offset of 60 frames. In this way, we can even know which moment of the rehabilitation session has been performed accurately and which has not.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig8">8</a> depicts a representation of joints in body coordinates for two different bodies. The left image shows the body joints without coordinate normalization, while the right image illustrates both body joints after coordinate normalization.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="315" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Representation of joints in body coordinates for two different bodies</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Augmented reality application</h2><div class="c-article-section__content" id="Sec7-content"><p>The browsing within the menus of the developed application is made through body motion. The user must lay the hand on a button to trigger the corresponding action. For that, the Human 3D pose estimation pipeline described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec5">3.2</a> has been used. So, the estimated 3D position of the right hand is translated into the image plane. If this point lays on a button, this button is selected (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig9">9</a>, where the user is selecting the <i>Replay</i> button in the EVA’s main menu). Note that the user must lay the hand on a button for 5 s to trigger the corresponding action. This prevents from inadvertently activating a button. In addition, the user interface distinguishing the two actions (i.e., selection and trigger) such that an arrow marks the selected button, while a progress circle in the bottom left shows the time left to trigger the corresponding action.</p><p>Additionally, a keyboard could be also used for browsing the menus. This feature is useful when the user does not want to perform a rehabilitation session but only replay it on their desktop, or review their statistics. In this way, it is not mandatory to set up the big <i>“trainer’s mat”</i> nor to use the body for browsing the application.</p><p>It is also worth noting that the user interface was created considering accessibility and usability. It features big and descriptive buttons with high contrast colors so that the use of the application is intuitive and straightforward.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig9_HTML.jpg" alt="figure9" loading="lazy" width="685" height="387" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Screenshot of the main menu of the developed application. The user can select the different options from the menus with its own body by placing the hand behind the buttons during 5 s</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">User interface</h3><p>The first button of the main menu is <i>“Start”</i>. This option triggers a rehabilitation exercise session as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig10">10</a>. So, after pushing the button, a list of exercises are shown where the user should choose the ones to be performed in the same way as before. Once an exercise session is selected, a virtual personal trainer is rendered over the <i>“trainer’s mat”</i>. The feed of the camera is being displayed so that the user is watching the trainer besides him in an augmented reality fashion. The user must imitate the movements of the virtual trainer upon the completion of the session. A plot in the left corner of the display is continuously showing the user’s update score. Finally, a summarized score representing the patient’s performance over the whole exercise is shown. In addition, a timeline of the session is also displayed so that the user can review which part of the session they did better and which one needs more work on.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig10_HTML.jpg" alt="figure10" loading="lazy" width="685" height="387" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>A sample of a patient performing a rehabilitation session. The virtual trainer is being rendered using augmented reality methods</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The next button of the main menu is <i>“Replay”</i>. As above mentioned, this option allows both patient and therapist to replay an exercise session. During the replay, the app shows the positions where the patient stood rendered in an augmented reality video-like way. The replay can be paused such that the therapist or the patient could rotate or zoom in/out a certain pose. This can be done by moving the <i>“trainer’s mat”</i>. The avatar is customizable so that it can render a person or a skeleton-like character. A score histogram is also displayed in the bottom left corner. This histogram shows the DTW score for each instant of the rehabilitation session. Finally, a progress bar in the top of the window shows how much of the replay is left. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig11">11</a> depicts a still of a replay being displayed. Note that, in this case, the therapist is using a reduced size <i>“trainer’s mat”</i> allowing the app to render the augmented reality avatar right on their desk. The replay feature allows an easy analysis of the rehabilitation sessions with the goal of evaluating the patients’ performance and modifying the rehabilitation program if needed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig11_HTML.jpg" alt="figure11" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>The image depicts a replay being displayed. Note that, in this case, the therapist is using a reduced size “trainer’s mat” that allowed the application to render the augmented reality avatar right on his desk</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The last entry of the main menu is <i>“Statistics”</i>. This menu shows a new window with the history of the past sessions performed by the user. This window displays a summary of the scores and plots the patient’s adherence and performance. These metrics could be used for the patient and the therapist for review and evaluation purposes.</p><p>Finally, a very early version of a web service was implemented. This web service allows EVA to be offered as an external service. Therefore, patients can benefit from EVA wherever through low computation powered machines such as tablets, embedded computers and smartphones. In this way, the taken images from the user’s terminal are sent to the remote server to be processed. So, the user can get their performance score in real-time independent of the computing power.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Rehabilitation exercises</h2><div class="c-article-section__content" id="Sec9-content"><p>An important step in any rehabilitation treatment is the home exercise program. This program consists in the patient’s performance of prescribed physical exercises at home. In this sense, there is a wide variety of exercises depending on the body part to be recovered and/or their goal. In particular, a public exercise program suggested by the British National Health Security (NHS) to improve elderly’s fitness and well-being has been considered in this paper (British National Health Security (NHS) <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Steffen D, Bleser G, Weber M, Stricker D, Fradet L, Marin F (2013) A personalized exercise trainer for the elderly. J Ambient Intell Smart Environ 5:547–562" href="/article/10.1007/s10055-019-00419-4#ref-CR35" id="ref-link-section-d95949e1524">2018</a>). This guide is composed of twenty-three exercises divided into four groups: flexibility, strength, balance and sitting. However, the therapist is who will decide the exercise program to be followed by each patient. So, the virtual trainer reproduces the tailored set of exercises while analyzes the patient’s evolution in their performance.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Experimentation and results</h2><div class="c-article-section__content" id="Sec10-content"><p>With the aim to validate EVA, two kinds of experiments were carried out. On the one hand, each subsystem has been individually evaluated in a qualitative and quantitative way. On the other hand, the whole system was qualitatively analyzed.</p><p>Firstly, the evaluation of the human 3D pose estimation and the dynamic time warping scoring subsystems was performed by using the Kinect Activity Recognition Dataset (KARD) (Gaglio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Tyromotion (2018) Amadeo. &#xA;https://tyromotion.com/en/produkte/amadeo/&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR36" id="ref-link-section-d95949e1539">2015</a>). This dataset contains 18 different short-time activities, summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00419-4#Tab1">1</a>, which overall match the features of some rehabilitation exercises. It involved 10 individuals repeating 3 times each activity. The dataset provides the video of each person performing an activity and the position of the body joints in both 2D and 3D coordinates as captured by a Kinect device. In addition, it provides the corresponding depth maps (discarded in our experiments).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Activities included in the KARD (Gaglio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Tyromotion (2018) Amadeo. &#xA;https://tyromotion.com/en/produkte/amadeo/&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR36" id="ref-link-section-d95949e1551">2015</a>)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00419-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For these experiments, the following hardware was used: an Intel Core i5-3570 with 8 GiB of Kingston HyperX 1600 MHz and CL10 DDR3 RAM on an Asus P8H77-M PRO motherboard (Intel H77 chipset). The system also included an Nvidia GTX1080Ti. The framework of choice was Keras 1.2.0 with Tensor Flow 1.8 as the backbone, running on Ubuntu 16.04. CUDA 9.0 and cuDNN v7.1 were used to accelerate the computations.</p><h3 class="c-article__sub-heading" id="Sec11">Human 3D pose estimation experiments</h3><p>The accuracy of the human 3D pose estimation was evaluated through the KARD dataset. Thus, it was required to divide the activity videos in still frames. As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec5">3.2</a>, this process involves two different stages. Firstly, each video frame is forwarded to the YOLO architecture which provides the area of interest of the patient within the scene. Then, this area is cropped and used to perform the 3D pose estimation with the HMR approach. As a result, the 3D position of the patient body joints is estimated. Then, the 3D points are projected to the image plane. Some results randomly chosen are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig12">12</a>. Note that the images were resized to <span class="mathjax-tex">\(224\times 224\)</span> px since it is the architecture’s input size.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig12_HTML.jpg" alt="figure12" loading="lazy" width="685" height="430" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>The top row depicts random samples extracted from the KARD and the bottom row shows the estimated human 3D poses</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig13">13</a> illustrates the amount of samples per distance error threshold. Note that the amount of joints are expressed in percentages. As it can be seen, <span class="mathjax-tex">\(92\%\)</span> of the joints yielded an error below 20 px, and <span class="mathjax-tex">\(80\%\)</span> below 12 px. The mean error averaged across the entire dataset is 9.58 px for an image of <span class="mathjax-tex">\(224 \times 224\)</span> px resolution. As the results show, this approach is accurate enough to be used for a successful human pose detection from a single RGB frame.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig13_HTML.png?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig13_HTML.png" alt="figure13" loading="lazy" width="685" height="351" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>The accumulated percentage of samples per error (px on a <span class="mathjax-tex">\(224\times 224\)</span> image) threshold between the 2D points provided by KARD and those estimated by our human 3D pose estimation approach</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In addition, we also benchmarked the human 3D pose estimation system in the 3D space using the KARD. In this case, we also use YOLO to detect the person in the scene and then forward only the area of interest to the pose estimator. As the 3D pose returned by the system yields no scale, the tridimensional positions of the joints are arbitrary located in the space, yet depicting the correct pose. To enable a fair comparison, we followed the method described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00419-4#Sec6">3.3</a>. Thus, we express all the poses in a coordinate frame local to the body pose. Then, as the scale is also different, we computed a scale factor using the norm of the vector that goes from the neck to the right shoulder. This process is applied to both ground truth and estimated poses. As a result, both poses are in the same coordinate frame and yield the same scale. Finally, we measure the mean euclidean distance between each joint averaged across all the frames.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig14_HTML.png?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig14_HTML.png" alt="figure14" loading="lazy" width="685" height="365" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>The accumulated percentage of samples per error (mm) threshold between the 3D points provided by KARD and those estimated by our human 3D pose estimation approach</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig14">14</a>, the 70.66% of the samples are under the 150 mm threshold. Note that this threshold is the same as that used by the authors of the HMR system to report the percentage of correct keypoints. In their proposal, they achieved a <span class="mathjax-tex">\(72.9\%\)</span> on the MMPI-INF-3DHP dataset. In addition, we have obtained a mean error of 129.52 mm.</p><p>These experiments demonstrate the accuracy of the system. Regarding the 2D pose estimation, it can be concluded that it is accurate enough to provide a good representation on the screen, what is important to encourage both users and therapists to use the application. Nonetheless, the 3D poses computed by the system are approximated. Therefore, the obtained evaluation is a reference, but not a medical diagnosis.</p><h3 class="c-article__sub-heading" id="Sec12">Dynamic time warping scoring experiments</h3><p>The KARD has been also used to evaluate the effectiveness of the scoring system. As above mentioned, this dataset contains 18 activities and each activity is performed by 10 different subjects. We compare them to each other. The effectiveness of the DTW is determined in the following way: the lower the value obtained, the closer or similar the activity will be. Moreover, the different score will be then compared against another activity. The obtained results are given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00419-4#Tab2">2</a> where each value indicates the similarity between the activity in the row with respect to that in the column. So, the value in bold represents the closest similarity, while a higher value indicates a higher difference between two activities. As it can be observed, the system is clearly confused between the activity pairs 7–5, 11–8 and 16–5. This is due to two main factors: (1) an overlap of movements (7–5, 16–5), and (2) the similarity in the activity movements (an object is released in activity 8 while an object is grabbed in activity 11). Some samples of this experiment are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig15">15</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Confusion matrix for the DTW Scoring experiments with the KARD</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00419-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig15_HTML.jpg" alt="figure15" loading="lazy" width="685" height="363" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>The first row shows some sample activities from a certain class and the second row depicts different samples from different classes that causes confusion in the system. As the images depict, the confusion is expectable as the classes are very similar despite being labeled as different</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">Integrated system experiments</h3><p>The integrated system was qualitatively tested in a controlled environment. Ten different individuals were involved. The experiment consisted in using EVA to perform one exercise per individual and to provide feedback.</p><p>For the evaluation of the feedback, the System Usability Scale (SUS) questionnaire proposed by Brooke was used (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="World Health Organization (2010) Community-based rehabilitation: CBR guidelines. Technical report, Geneva" href="/article/10.1007/s10055-019-00419-4#ref-CR37" id="ref-link-section-d95949e1823">1996</a>). This questionnaire is composed of 10 items such that the odd items were written positively, while the even ones were written negatively. So, the SUS questions are as follows:</p><ul class="u-list-style-bullet"><li><p>I think that I would like to use this system frequently.</p></li><li><p>I found the system unnecessarily complex.</p></li><li><p>I thought the system was easy to use.</p></li><li><p>I think that I would need the support of a technical person to be able to use this system.</p></li><li><p>I found the various functions in this system were well integrated.</p></li><li><p>I thought there was too much inconsistency in this system.</p></li><li><p>I would imagine that most people would learn to use this system very quickly.</p></li><li><p>I found the system very cumbersome to use.</p></li><li><p>I felt very confident using the system.</p></li><li><p>I needed to learn a lot of things before I could get going with this system.</p></li></ul><p>To evaluate the results of the SUS test, we used the adjectives method proposed by Bangor et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="World Health Organization (2015) Who global disability action plan 2014–2021: better health for all people with disability. Technical report" href="/article/10.1007/s10055-019-00419-4#ref-CR38" id="ref-link-section-d95949e1858">2009</a>). This method is based on the association of the SUS score with an adjective scale (i.e., worst, poor, ok, good, excellent, best) as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig16">16</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig16_HTML.png?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig16_HTML.png" alt="figure16" loading="lazy" width="685" height="100" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Adjectives scale method to measure SUS</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00419-4#Fig17">17</a>, the total score obtained for each user was positive, obtaining an average of 78.0 (good according to the adjective scale method). This result is significant providing a level of confidence in the use of the system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig17_HTML.png?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00419-4/MediaObjects/10055_2019_419_Fig17_HTML.png" alt="figure17" loading="lazy" width="685" height="221" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Score per user obtained by SUS evaluation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00419-4/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In addition, some conclusions could be extracted from this study. Participants felt comfortable using the system. Many users commented that the application effectively encouraged them to complete the rehabilitation session and thought that EVA would help improve adherence to these home rehabilitation sessions. Some of the users were reluctant with the presence of the trainer’s carpet at first, but then got used to it.</p><p>A therapist from ADACEA (A Spanish foundation for the acquired brain damaged) supervised the experiment. He praised EVA by highlighting its potential to improve the adherence to the long-term rehabilitation programs, which he admitted to be one of the weakest points in the patient’s rehabilitation. He also found interesting the idea of replaying the exercising sessions in his office or home with the use of augmented reality. Finally, the therapist felt indifferent about the scoring system. He admitted to be an interesting feature to encourage the patients, but he hesitated about the effectiveness to measure the actual patient’s performance. He also remarked that a professional review is mandatory for the sake of ameliorating the patient’s health.</p><p>Regarding the computation time, EVA is currently able to run at about 22 fps, which is enough to provide real-time output. More details about the individual computation time of each piece of the pipeline could be seen in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00419-4#Tab3">3</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Runtime for each piece of the proposed pipeline</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00419-4/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusions and future work</h2><div class="c-article-section__content" id="Sec14-content"><p>In the face of the relentless demand for rehabilitation services and the lack of resources to access them, we propose EVA, an augmented reality application for evaluating rehabilitation programs at home. This low-cost application only requires a regular camera to capture and evaluate the 3D patient’s pose. So, EVA stores the rehabilitation sessions such that patients and therapists could review them and adjust the exercises accordingly. EVA was qualitative and quantitative evaluated to show their suitability. For that, two different types of experiments were carried out. Firstly, each integrating module was individually tested and then a complete evaluation was performed.</p><p>Despite the promising results, the experiments brought to light an EVA limitation in terms of human pose estimation. The human pose estimation sometimes fails when people has an amputated limb since it assumes that the images depict a canonical person and it tries to estimate the position of all their joints. This fact will be pointed out in the future since a considerable amount of EVA’s users may present this handicap.</p><p>Apart from that, we plan to further explore the web service potential and the idea of a smartphone application. It would not be only able to show replays, but also to provide statistics to the therapists and patients and act as a client as well. We also plan to extend the amount of exercises and to add more customization features to our proposal. For instance, more trainer avatars or new user interface themes to help the visually impaired will be added. The possibility to add wearable sensors like smartbands will be also explored in order to gather more information about the user’s health status (e.g., ECG or heart rate). In this way, the therapist could have more information to properly adapt the rehabilitation process. Finally, a native VR/AR environment like Vuforia (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Yang Shichao, Maturana Daniel, Scherer Sebastian (May 2016) Real-time 3d scene layout from a single image using convolutional neural networks. In: 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 2183–2189. &#xA;https://doi.org/10.1109/ICRA.2016.7487368&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00419-4#ref-CR39" id="ref-link-section-d95949e1959">2019</a>) would be used to leverage and enhance the visualization part.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Al-Issa, H. Regenbrecht, L. Hale, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Al-Issa H, Regenbrecht H, Hale L (2013) Augmented reality applications in rehabilitation to improve physical o" /><p class="c-article-references__text" id="ref-CR1">Al-Issa H, Regenbrecht H, Hale L (2013) Augmented reality applications in rehabilitation to improve physical outcomes. Phys Ther Rev 17(1):16–28. <a href="https://doi.org/10.1179/1743288X11Y.0000000051">https://doi.org/10.1179/1743288X11Y.0000000051</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1179%2F1743288X11Y.0000000051" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20applications%20in%20rehabilitation%20to%20improve%20physical%20outcomes&amp;journal=Phys%20Ther%20Rev&amp;doi=10.1179%2F1743288X11Y.0000000051&amp;volume=17&amp;issue=1&amp;pages=16-28&amp;publication_year=2013&amp;author=Al-Issa%2CH&amp;author=Regenbrecht%2CH&amp;author=Hale%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aung YM, Al-Jumaily A, Anam K (2014) A novel upper limb rehabilitation system with self-driven virtual arm ill" /><p class="c-article-references__text" id="ref-CR2">Aung YM, Al-Jumaily A, Anam K (2014) A novel upper limb rehabilitation system with self-driven virtual arm illusion. In: 36th Annual international conference of the IEEE engineering in medicine and biology society, pp 3614–3617, Chicago, IL, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YM. Aung, A. Al-Jumaily, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Aung YM, Al-Jumaily A (2014) Augmented reality-based rehabio system for shoulder rehabilitation. Int J Mechatr" /><p class="c-article-references__text" id="ref-CR3">Aung YM, Al-Jumaily A (2014) Augmented reality-based rehabio system for shoulder rehabilitation. Int J Mechatron Autom 4(1):52–62</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1504%2FIJMA.2014.059774" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality-based%20rehabio%20system%20for%20shoulder%20rehabilitation&amp;journal=Int%20J%20Mechatron%20Autom&amp;volume=4&amp;issue=1&amp;pages=52-62&amp;publication_year=2014&amp;author=Aung%2CYM&amp;author=Al-Jumaily%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Bangor, P. Kortum, J. Miller, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Bangor A, Kortum P, Miller J (2009) Determining what individual SUS scores mean: adding an adjective rating sc" /><p class="c-article-references__text" id="ref-CR4">Bangor A, Kortum P, Miller J (2009) Determining what individual SUS scores mean: adding an adjective rating scale. J Usability Stud 4(3):114–123</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Determining%20what%20individual%20SUS%20scores%20mean%3A%20adding%20an%20adjective%20rating%20scale&amp;journal=J%20Usability%20Stud&amp;volume=4&amp;issue=3&amp;pages=114-123&amp;publication_year=2009&amp;author=Bangor%2CA&amp;author=Kortum%2CP&amp;author=Miller%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Barcala, LAC. Grecco, F. Colella, PRG. Lucareli, ASI. Salgado, CS. Oliveira, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Barcala L, Grecco LAC, Colella F, Lucareli PRG, Salgado ASI, Oliveira CS (2013) Visual biofeedback balance tra" /><p class="c-article-references__text" id="ref-CR5">Barcala L, Grecco LAC, Colella F, Lucareli PRG, Salgado ASI, Oliveira CS (2013) Visual biofeedback balance training using wii fit after stroke: a randomized controlled trial. J Phys Ther Sci 25(8):1027–1032. <a href="https://doi.org/10.1589/jpts.25.1027">https://doi.org/10.1589/jpts.25.1027</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1589%2Fjpts.25.1027" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20biofeedback%20balance%20training%20using%20wii%20fit%20after%20stroke%3A%20a%20randomized%20controlled%20trial&amp;journal=J%20Phys%20Ther%20Sci&amp;doi=10.1589%2Fjpts.25.1027&amp;volume=25&amp;issue=8&amp;pages=1027-1032&amp;publication_year=2013&amp;author=Barcala%2CL&amp;author=Grecco%2CLAC&amp;author=Colella%2CF&amp;author=Lucareli%2CPRG&amp;author=Salgado%2CASI&amp;author=Oliveira%2CCS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berndt DJ, Clifford J (1994) Using dynamic time warping to find patterns in time series. In: KDD workshop, vol" /><p class="c-article-references__text" id="ref-CR6">Berndt DJ, Clifford J (1994) Using dynamic time warping to find patterns in time series. In: KDD workshop, vol 10, pp 359–370. Seattle, WA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="British National Health Security (NHS) (2018) https://www.nhs.uk/Tools/Documents/NHS_ExercisesForOlderPeople.p" /><p class="c-article-references__text" id="ref-CR7">British National Health Security (NHS) (2018) <a href="https://www.nhs.uk/Tools/Documents/NHS_ExercisesForOlderPeople.pdf">https://www.nhs.uk/Tools/Documents/NHS_ExercisesForOlderPeople.pdf</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Brooke, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Brooke J (1996) SUS-a quick and dirty usability scale. Usability Eval Ind 189(194):4–7" /><p class="c-article-references__text" id="ref-CR8">Brooke J (1996) SUS-a quick and dirty usability scale. Usability Eval Ind 189(194):4–7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SUS-a%20quick%20and%20dirty%20usability%20scale&amp;journal=Usability%20Eval%20Ind&amp;volume=189&amp;issue=194&amp;pages=4-7&amp;publication_year=1996&amp;author=Brooke%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cao Zhe, Simon Tomas, Wei Shih-En, Sheikh Yaser (2017) Realtime multi-person 2d pose estimation using part aff" /><p class="c-article-references__text" id="ref-CR9">Cao Zhe, Simon Tomas, Wei Shih-En, Sheikh Yaser (2017) Realtime multi-person 2d pose estimation using part affinity fields. In: <i>Computer Vision and Pattern Regognition (CVPR)</i>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Costa, E. Martinez-Martin, M. Cazorla, V. Julian, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Costa A, Martinez-Martin E, Cazorla M, Julian V (2018) Pharos-physical assistant robot system. Sensors 18:2633" /><p class="c-article-references__text" id="ref-CR10">Costa A, Martinez-Martin E, Cazorla M, Julian V (2018) Pharos-physical assistant robot system. Sensors 18:2633. <a href="https://doi.org/10.3390/s18082633">https://doi.org/10.3390/s18082633</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs18082633" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pharos-physical%20assistant%20robot%20system&amp;journal=Sensors&amp;doi=10.3390%2Fs18082633&amp;volume=18&amp;publication_year=2018&amp;author=Costa%2CA&amp;author=Martinez-Martin%2CE&amp;author=Cazorla%2CM&amp;author=Julian%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GA. Assis, AGD. Correa, MBR. Martins, WG. Pedrozo, RDD. Lopes, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="de Assis GA, Correa AGD, Martins MBR, Pedrozo WG, Lopes RDD (2014) An augmented reality system for upper-limb " /><p class="c-article-references__text" id="ref-CR11">de Assis GA, Correa AGD, Martins MBR, Pedrozo WG, Lopes RDD (2014) An augmented reality system for upper-limb post-stroke motor rehabilitation: a feasibility study. Disabil Rehabil: Assist Technol 11(16):521–526</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20augmented%20reality%20system%20for%20upper-limb%20post-stroke%20motor%20rehabilitation%3A%20a%20feasibility%20study&amp;journal=Disabil%20Rehabil%3A%20Assist%20Technol&amp;volume=11&amp;issue=16&amp;pages=521-526&amp;publication_year=2014&amp;author=Assis%2CGA&amp;author=Correa%2CAGD&amp;author=Martins%2CMBR&amp;author=Pedrozo%2CWG&amp;author=Lopes%2CRDD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Desai K, Bahirat K, Ramalingam S, Prabhakaran B, Annaswamy T, Makris UE (2016) Augmented reality-based exergam" /><p class="c-article-references__text" id="ref-CR12">Desai K, Bahirat K, Ramalingam S, Prabhakaran B, Annaswamy T, Makris UE (2016) Augmented reality-based exergames for rehabilitation. In: 7th International conference on multimedia systems. Klagenfurt, Austria</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FA. Santos Mendesa, JE. Pompeua, AM. Loboa, KG. Silva, OT. Paula, AP. Zomignani, MEP. Piemonte, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="dos Santos Mendesa FA, Pompeua JE, Loboa AM, da Silva KG, de Paula OT, Zomignani AP, Piemonte MEP (2012) Motor" /><p class="c-article-references__text" id="ref-CR13">dos Santos Mendesa FA, Pompeua JE, Loboa AM, da Silva KG, de Paula OT, Zomignani AP, Piemonte MEP (2012) Motor learning, retention and transfer after virtual-reality-based training in parkinson’s disease-effect of motor and cognitive demands of games: a longitudinal, controlled clinical study. Physiotherapy 98:217–223</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.physio.2012.06.001" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Motor%20learning%2C%20retention%20and%20transfer%20after%20virtual-reality-based%20training%20in%20parkinson%E2%80%99s%20disease-effect%20of%20motor%20and%20cognitive%20demands%20of%20games%3A%20a%20longitudinal%2C%20controlled%20clinical%20study&amp;journal=Physiotherapy&amp;volume=98&amp;pages=217-223&amp;publication_year=2012&amp;author=Santos%20Mendesa%2CFA&amp;author=Pompeua%2CJE&amp;author=Loboa%2CAM&amp;author=Silva%2CKG&amp;author=Paula%2COT&amp;author=Zomignani%2CAP&amp;author=Piemonte%2CMEP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Fung, A. Ho, J. Shaffer, E. Chung, M. Gomez, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Fung V, Ho A, Shaffer J, Chung E, Gomez M (2012) Use of nintendo wii fit in the rehabilitation of outpatients " /><p class="c-article-references__text" id="ref-CR14">Fung V, Ho A, Shaffer J, Chung E, Gomez M (2012) Use of nintendo wii fit in the rehabilitation of outpatients following total knee replacement: a preliminary randomised controlled trial. Physiotherapy 98:183–188</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.physio.2012.04.001" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Use%20of%20nintendo%20wii%20fit%20in%20the%20rehabilitation%20of%20outpatients%20following%20total%20knee%20replacement%3A%20a%20preliminary%20randomised%20controlled%20trial&amp;journal=Physiotherapy&amp;volume=98&amp;pages=183-188&amp;publication_year=2012&amp;author=Fung%2CV&amp;author=Ho%2CA&amp;author=Shaffer%2CJ&amp;author=Chung%2CE&amp;author=Gomez%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Gaglio, GL. Re, M. Morana, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Gaglio S, Re GL, Morana M (2015) Human activity recognition process using 3-D posture data. IEEE Trans Hum-Mac" /><p class="c-article-references__text" id="ref-CR15">Gaglio S, Re GL, Morana M (2015) Human activity recognition process using 3-D posture data. IEEE Trans Hum-Mach Syst 45(5):586–597. <a href="https://doi.org/10.1109/THMS.2014.2377111">https://doi.org/10.1109/THMS.2014.2377111</a> ISSN 2168-2291</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTHMS.2014.2377111" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20activity%20recognition%20process%20using%203-D%20posture%20data&amp;journal=IEEE%20Trans%20Hum-Mach%20Syst&amp;doi=10.1109%2FTHMS.2014.2377111&amp;volume=45&amp;issue=5&amp;pages=586-597&amp;publication_year=2015&amp;author=Gaglio%2CS&amp;author=Re%2CGL&amp;author=Morana%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Gazzoni, GL. Cerone, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Gazzoni M, Cerone GL (2018) Augmented reality system for muscle activity biofeedback. Ann Phys Rehabil Med 61:" /><p class="c-article-references__text" id="ref-CR16">Gazzoni M, Cerone GL (2018) Augmented reality system for muscle activity biofeedback. Ann Phys Rehabil Med 61:e483–e484</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.rehab.2018.05.1129" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20system%20for%20muscle%20activity%20biofeedback&amp;journal=Ann%20Phys%20Rehabil%20Med&amp;volume=61&amp;pages=e483-e484&amp;publication_year=2018&amp;author=Gazzoni%2CM&amp;author=Cerone%2CGL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Gomez-Donoso, S. Orts-Escolano, A. Garcia-Garcia, J. Garcia-Rodriguez, JA. Castro-Vargas, S. Ovidiu-Oprea, M. Cazorla, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Gomez-Donoso F, Orts-Escolano S, Garcia-Garcia A, Garcia-Rodriguez J, Castro-Vargas JA, Ovidiu-Oprea S, Cazorl" /><p class="c-article-references__text" id="ref-CR17">Gomez-Donoso F, Orts-Escolano S, Garcia-Garcia A, Garcia-Rodriguez J, Castro-Vargas JA, Ovidiu-Oprea S, Cazorla M (2017) A robotic platform for customized and interactive rehabilitation of persons with disabilities. Pattern Recognit Lett 99:105–113. <a href="https://doi.org/10.1016/j.patrec.2017.05.027">https://doi.org/10.1016/j.patrec.2017.05.027</a> ISSN 0167-8655</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patrec.2017.05.027" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robotic%20platform%20for%20customized%20and%20interactive%20rehabilitation%20of%20persons%20with%20disabilities&amp;journal=Pattern%20Recognit%20Lett&amp;doi=10.1016%2Fj.patrec.2017.05.027&amp;volume=99&amp;pages=105-113&amp;publication_year=2017&amp;author=Gomez-Donoso%2CF&amp;author=Orts-Escolano%2CS&amp;author=Garcia-Garcia%2CA&amp;author=Garcia-Rodriguez%2CJ&amp;author=Castro-Vargas%2CJA&amp;author=Ovidiu-Oprea%2CS&amp;author=Cazorla%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hocoma. Lokomat. https://esa.un.org/unpd/wpp/, 2018" /><p class="c-article-references__text" id="ref-CR18">Hocoma. Lokomat. <a href="https://esa.un.org/unpd/wpp/">https://esa.un.org/unpd/wpp/</a>, 2018</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Indra (2019) Toyra. https://www.tecnologiasaccesibles.com/es/proyectos/toyra&#xA;" /><p class="c-article-references__text" id="ref-CR19">Indra (2019) Toyra. <a href="https://www.tecnologiasaccesibles.com/es/proyectos/toyra">https://www.tecnologiasaccesibles.com/es/proyectos/toyra</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Jack, R. Boian, AS. Merians, M. Tremaine, GC. Burdea, SV. Adamovich, M. Recce, H. Poizner, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-en" /><p class="c-article-references__text" id="ref-CR20">Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE Trans Neural Syst Rehabil Eng 9(3):308–318. <a href="https://doi.org/10.1109/7333.948460">https://doi.org/10.1109/7333.948460</a> ISSN 1534-4320</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F7333.948460" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality-enhanced%20stroke%20rehabilitation&amp;journal=IEEE%20Trans%20Neural%20Syst%20Rehabil%20Eng&amp;doi=10.1109%2F7333.948460&amp;volume=9&amp;issue=3&amp;pages=308-318&amp;publication_year=2001&amp;author=Jack%2CD&amp;author=Boian%2CR&amp;author=Merians%2CAS&amp;author=Tremaine%2CM&amp;author=Burdea%2CGC&amp;author=Adamovich%2CSV&amp;author=Recce%2CM&amp;author=Poizner%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kanazawa A, Black MJ, Jacobs DW, Malik J (2018) End-to-end recovery of human shape and pose. In: Computer visi" /><p class="c-article-references__text" id="ref-CR21">Kanazawa A, Black MJ, Jacobs DW, Malik J (2018) End-to-end recovery of human shape and pose. In: Computer vision and pattern recognition (CVPR)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levy-Tzedek S, Berman S, Stiefel Y, Sharlin E, Young J, Rea D (2017) Robotic mirror game for movement rehabili" /><p class="c-article-references__text" id="ref-CR22">Levy-Tzedek S, Berman S, Stiefel Y, Sharlin E, Young J, Rea D (2017) Robotic mirror game for movement rehabilitation. In: 2017 International conference on virtual rehabilitation (ICVR), pp 1–2. <a href="https://doi.org/10.1109/ICVR.2017.8007494">https://doi.org/10.1109/ICVR.2017.8007494</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Tsung-Yi. Lin, Michael. Maire, Serge. Belongie, James. Hays, Pietro. Perona, Deva. Ramanan, Piotr. Dollár, C. Lawrence. Zitnick, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Lin T-Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick CL (2014) Microsoft coco: common " /><p class="c-article-references__text" id="ref-CR23">Lin T-Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick CL (2014) Microsoft coco: common objects in context. In: Fleet D, Pajdla T, Schiele B, Tuytelaars T (eds) European conference on computer vision (ECCV), pp 740–755, Cham. Springer. ISBN 978-3-319-10602-1</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20Vision%20%E2%80%93%20ECCV%202014&amp;pages=740-755&amp;publication_year=2014&amp;author=Lin%2CTsung-Yi&amp;author=Maire%2CMichael&amp;author=Belongie%2CSerge&amp;author=Hays%2CJames&amp;author=Perona%2CPietro&amp;author=Ramanan%2CDeva&amp;author=Doll%C3%A1r%2CPiotr&amp;author=Zitnick%2CC.%20Lawrence">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Ester. Martinez-Martin, Angel P.. del Pobil, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Martinez-Martin E, del Pobil AP (2017) Personal robot assistants for elderly care: an overview. In: Costa A, J" /><p class="c-article-references__text" id="ref-CR24">Martinez-Martin E, del Pobil AP (2017) Personal robot assistants for elderly care: an overview. In: Costa A, Julian V, Novais P (eds), Personal assistants: emerging computational technologies, pp 77–91. Springer, Berlin. <a href="https://doi.org/10.1007/978-3-319-62530-0_5">https://doi.org/10.1007/978-3-319-62530-0_5</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intelligent%20Systems%20Reference%20Library&amp;pages=77-91&amp;publication_year=2017&amp;author=Martinez-Martin%2CEster&amp;author=del%20Pobil%2CAngel%20P.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Meldrum, A. Glennon, S. Herdman, D. Murray, R. McConn-Walsh, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Meldrum D, Glennon A, Herdman S, Murray D, McConn-Walsh R (2012) Virtual reality rehabilitation of balance: as" /><p class="c-article-references__text" id="ref-CR25">Meldrum D, Glennon A, Herdman S, Murray D, McConn-Walsh R (2012) Virtual reality rehabilitation of balance: assessment of the usability of the nintendo Wii®fit plus. Disabil Rehabil: Assist Technol 7(3):205–210. <a href="https://doi.org/10.3109/17483107.2011.616922">https://doi.org/10.3109/17483107.2011.616922</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3109%2F17483107.2011.616922" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20rehabilitation%20of%20balance%3A%20assessment%20of%20the%20usability%20of%20the%20nintendo%20Wii%C2%AEfit%20plus&amp;journal=Disabil%20Rehabil%3A%20Assist%20Technol&amp;doi=10.3109%2F17483107.2011.616922&amp;volume=7&amp;issue=3&amp;pages=205-210&amp;publication_year=2012&amp;author=Meldrum%2CD&amp;author=Glennon%2CA&amp;author=Herdman%2CS&amp;author=Murray%2CD&amp;author=McConn-Walsh%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Monge J, Postolache O (2018) Augmented reality and smart sensors for physical rehabilitation. In: 10th Interna" /><p class="c-article-references__text" id="ref-CR26">Monge J, Postolache O (2018) Augmented reality and smart sensors for physical rehabilitation. In: 10th International conference and exposition on electrical and power engineering (EPE2018), pp 1010–1014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Mur-Artal, JD. Tardos, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Mur-Artal R, Tardos JD (2017) ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. I" /><p class="c-article-references__text" id="ref-CR27">Mur-Artal R, Tardos JD (2017) ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Trans Robot 33:1255–1262. <a href="https://doi.org/10.1109/TRO.2017.2705103">https://doi.org/10.1109/TRO.2017.2705103</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTRO.2017.2705103" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ORB-SLAM2%3A%20an%20open-source%20SLAM%20system%20for%20monocular%2C%20stereo%20and%20RGB-D%20cameras&amp;journal=IEEE%20Trans%20Robot&amp;doi=10.1109%2FTRO.2017.2705103&amp;volume=33&amp;pages=1255-1262&amp;publication_year=2017&amp;author=Mur-Artal%2CR&amp;author=Tardos%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nintendo (2008) Wii fit. https://www.nintendo.es/Juegos/Wii/Wii-Fit-283894.html&#xA;" /><p class="c-article-references__text" id="ref-CR28">Nintendo (2008) Wii fit. <a href="https://www.nintendo.es/Juegos/Wii/Wii-Fit-283894.html">https://www.nintendo.es/Juegos/Wii/Wii-Fit-283894.html</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="PTC Inc (2019) Vuforia. https://developer.vuforia.com/&#xA;" /><p class="c-article-references__text" id="ref-CR29">PTC Inc (2019) Vuforia. <a href="https://developer.vuforia.com/">https://developer.vuforia.com/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Redmon J, Farhadi A (2018) Yolov3: an incremental improvement. ArXiv e-prints" /><p class="c-article-references__text" id="ref-CR30">Redmon J, Farhadi A (2018) Yolov3: an incremental improvement. ArXiv e-prints</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Salvador, P. Chan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Salvador S, Chan P (2007) Toward accurate dynamic time warping in linear time and space. Intell Data Anal 11(5" /><p class="c-article-references__text" id="ref-CR31">Salvador S, Chan P (2007) Toward accurate dynamic time warping in linear time and space. Intell Data Anal 11(5):561–580</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3233%2FIDA-2007-11508" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20accurate%20dynamic%20time%20warping%20in%20linear%20time%20and%20space&amp;journal=Intell%20Data%20Anal&amp;volume=11&amp;issue=5&amp;pages=561-580&amp;publication_year=2007&amp;author=Salvador%2CS&amp;author=Chan%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SilverFit (2019) Silverfit 3d. https://silverfit.com/&#xA;" /><p class="c-article-references__text" id="ref-CR32">SilverFit (2019) Silverfit 3d. <a href="https://silverfit.com/">https://silverfit.com/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sousa M, Vieira J, Medeiros D, Arsenio A, Jorge J (2016) Sleevear: augmented reality for rehabilitation using " /><p class="c-article-references__text" id="ref-CR33">Sousa M, Vieira J, Medeiros D, Arsenio A, Jorge J (2016) Sleevear: augmented reality for rehabilitation using realtime feedback. In: Proceedings of the 21st international conference on intelligent user interfaces, pp 175–185, Sonoma, California, USA. <a href="https://doi.org/10.1145/2856767.2856773">https://doi.org/10.1145/2856767.2856773</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Masiero. Stefano, Poli. Patrizia, Armani. Mario, Gregorio. Ferlini, Roberto. Rizzello, Giulio. Rosati, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Stefano M, Patrizia P, Mario A, Ferlini G, Rizzello R, Rosati G (2014) Robotic upper limb rehabilitation after" /><p class="c-article-references__text" id="ref-CR34">Stefano M, Patrizia P, Mario A, Ferlini G, Rizzello R, Rosati G (2014) Robotic upper limb rehabilitation after acute stroke by NeReBot: evaluation of treatment costs. BioMed Res Int. <a href="https://doi.org/10.1155/2014/265634">https://doi.org/10.1155/2014/265634</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robotic%20Upper%20Limb%20Rehabilitation%20after%20Acute%20Stroke%20by%20NeReBot%3A%20Evaluation%20of%20Treatment%20Costs&amp;journal=BioMed%20Research%20International&amp;volume=2014&amp;pages=1-5&amp;publication_year=2014&amp;author=Stefano%2CMasiero&amp;author=Patrizia%2CPoli&amp;author=Mario%2CArmani&amp;author=Ferlini%2CGregorio&amp;author=Rizzello%2CRoberto&amp;author=Rosati%2CGiulio">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Steffen, G. Bleser, M. Weber, D. Stricker, L. Fradet, F. Marin, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Steffen D, Bleser G, Weber M, Stricker D, Fradet L, Marin F (2013) A personalized exercise trainer for the eld" /><p class="c-article-references__text" id="ref-CR35">Steffen D, Bleser G, Weber M, Stricker D, Fradet L, Marin F (2013) A personalized exercise trainer for the elderly. J Ambient Intell Smart Environ 5:547–562</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3233%2FAIS-130234" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20personalized%20exercise%20trainer%20for%20the%20elderly&amp;journal=J%20Ambient%20Intell%20Smart%20Environ&amp;volume=5&amp;pages=547-562&amp;publication_year=2013&amp;author=Steffen%2CD&amp;author=Bleser%2CG&amp;author=Weber%2CM&amp;author=Stricker%2CD&amp;author=Fradet%2CL&amp;author=Marin%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tyromotion (2018) Amadeo. https://tyromotion.com/en/produkte/amadeo/&#xA;" /><p class="c-article-references__text" id="ref-CR36">Tyromotion (2018) Amadeo. <a href="https://tyromotion.com/en/produkte/amadeo/">https://tyromotion.com/en/produkte/amadeo/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="World Health Organization (2010) Community-based rehabilitation: CBR guidelines. Technical report, Geneva" /><p class="c-article-references__text" id="ref-CR37">World Health Organization (2010) Community-based rehabilitation: CBR guidelines. Technical report, Geneva</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="World Health Organization (2015) Who global disability action plan 2014–2021: better health for all people wit" /><p class="c-article-references__text" id="ref-CR38">World Health Organization (2015) Who global disability action plan 2014–2021: better health for all people with disability. Technical report</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang Shichao, Maturana Daniel, Scherer Sebastian (May 2016) Real-time 3d scene layout from a single image usin" /><p class="c-article-references__text" id="ref-CR39">Yang Shichao, Maturana Daniel, Scherer Sebastian (May 2016) Real-time 3d scene layout from a single image using convolutional neural networks. In: <i>2016 IEEE International Conference on Robotics and Automation (ICRA)</i>, pages 2183–2189. <a href="https://doi.org/10.1109/ICRA.2016.7487368">https://doi.org/10.1109/ICRA.2016.7487368</a>
</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00419-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work has been supported by the Spanish Government TIN2016-76515R Grant, supported with Feder funds. Edmanuel Cruz is funded by a Panamenian grant for Ph.D. studies IFARHU and SENACYT 270-2016-207. This work has also been supported by a Spanish grant for PhD studies ACIF/2017/243 and FPU16/00887. Thanks also to Nvidia for the generous donation of a Titan Xp and a Quadro P6000.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">University Institute for Computer Research, Universidad de Alicante, Alicante, Spain</p><p class="c-article-author-affiliation__authors-list">Felix Escalona, Ester Martinez-Martin, Edmanuel Cruz, Miguel Cazorla &amp; Francisco Gomez-Donoso</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Felix-Escalona"><span class="c-article-authors-search__title u-h3 js-search-name">Felix Escalona</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Felix+Escalona&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Felix+Escalona" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Felix+Escalona%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ester-Martinez_Martin"><span class="c-article-authors-search__title u-h3 js-search-name">Ester Martinez-Martin</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ester+Martinez-Martin&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ester+Martinez-Martin" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ester+Martinez-Martin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Edmanuel-Cruz"><span class="c-article-authors-search__title u-h3 js-search-name">Edmanuel Cruz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Edmanuel+Cruz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Edmanuel+Cruz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Edmanuel+Cruz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Miguel-Cazorla"><span class="c-article-authors-search__title u-h3 js-search-name">Miguel Cazorla</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Miguel+Cazorla&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Miguel+Cazorla" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Miguel+Cazorla%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Francisco-Gomez_Donoso"><span class="c-article-authors-search__title u-h3 js-search-name">Francisco Gomez-Donoso</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Francisco+Gomez-Donoso&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Francisco+Gomez-Donoso" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Francisco+Gomez-Donoso%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00419-4/email/correspondent/c1/new">Francisco Gomez-Donoso</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=EVA%3A%20EVAluating%20at-home%20rehabilitation%20exercises%20using%20augmented%20reality%20and%20low-cost%20sensors&amp;author=Felix%20Escalona%20et%20al&amp;contentID=10.1007%2Fs10055-019-00419-4&amp;publication=1359-4338&amp;publicationDate=2019-12-17&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00419-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00419-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Escalona, F., Martinez-Martin, E., Cruz, E. <i>et al.</i> EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00419-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00419-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-01-29">29 January 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-06">06 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-17">17 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00419-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00419-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Rehabilitation exercises</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Deep learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D visualization</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Low-cost sensors</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00419-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=419;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

