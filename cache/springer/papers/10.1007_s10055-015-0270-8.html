<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="On the validity of virtual reality-based auditory experiments: a case "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity compared to real-world experiments. To this..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-08-15"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity compared to real-world experiments. To this end, an experiment was carried out which compares responses given in a real-world environment to responses given in a VR environment. In the experiment, thirty participants rated the overall listening experience of music excerpts while sitting in a cinema and a listening booth being in a real-world environment and in a VR environment. In addition, the VR system that was used to carry out the sessions in the VR environment is presented in detail. Results indicate that there are only minor statistically significant differences between the two environments when the overall listening experience is rated. Furthermore, in the real-world environment, the ratings given in the listening booth were slightly higher than in the cinema."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-08-15"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="181"/>

    <meta name="prism.endingPage" content="200"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0270-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0270-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0270-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0270-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2015/11"/>

    <meta name="citation_online_date" content="2015/08/15"/>

    <meta name="citation_firstpage" content="181"/>

    <meta name="citation_lastpage" content="200"/>

    <meta name="citation_article_type" content="SI: SPATIAL SOUND"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0270-8"/>

    <meta name="DOI" content="10.1007/s10055-015-0270-8"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0270-8"/>

    <meta name="description" content="In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity comp"/>

    <meta name="dc.creator" content="Michael Schoeffler"/>

    <meta name="dc.creator" content="Jan Lukas Gernert"/>

    <meta name="dc.creator" content="Maximilian Neumayer"/>

    <meta name="dc.creator" content="Susanne Westphal"/>

    <meta name="dc.creator" content="J&#252;rgen Herre"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_title=Categorial data analysis; citation_publication_date=2002; citation_isbn=0-471-36093-7; citation_id=CR1; citation_author=A Agresti; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="Astheimer P (1993) What you see is what you hear-acoustics applied in virtual worlds. In: Proceedings of the IEEE 1993 symposium on research frontiers in virtual reality, pp 100&#8211;107. ISBN: 0-8186-4910-0"/>

    <meta name="citation_reference" content="Bella F (2004) Driving simulation in virtual reality for work zone design on highway: a validation study. In: The second SIIV international congress, Florence, Italy"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Acoustic control by wave field synthesis; citation_author=AJ Berkhout, D Vries, P Vogel; citation_volume=93; citation_issue=5; citation_publication_date=1993; citation_pages=2764-2778; citation_doi=10.1121/1.405852; citation_id=CR4"/>

    <meta name="citation_reference" content="Blauert J (1997) Spacial hearing. The psychophysics of human sound localization. The MIT Press, Cambridge. ISBN: 978-262-02413-6"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=A layer model of sound quality; citation_author=J Blauert, U Jekosch; citation_volume=60; citation_issue=1/2; citation_publication_date=2012; citation_pages=4-12; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Transfer of learning in virtual environments: a new challenge?; citation_author=C Bossard, G Kermarrec, C Buche, J Tisseau; citation_volume=12; citation_issue=3; citation_publication_date=2008; citation_pages=151-161; citation_doi=10.1007/s10055-008-0093-y; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Virtual reality: how much immersion is enough?; citation_author=DA Bowman, RP McMahan; citation_volume=40; citation_issue=7; citation_publication_date=2007; citation_pages=36-43; citation_doi=10.1109/MC.2007.257; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Disp Technol; citation_title=Head-worn displays: a review; citation_author=O Cakmakci, J Rolland; citation_volume=2; citation_issue=3; citation_publication_date=2006; citation_pages=199-216; citation_doi=10.1109/JDT.2006.879846; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_title=Statistical power analysis for the behavioral sciences; citation_publication_date=1998; citation_isbn=978-080-580283-2; citation_id=CR10; citation_author=J Cohen; citation_publisher=Lawrence Erlbaum"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The CAVE: audio visual experience automatic virtual environment; citation_author=C Cruz-Neira, DJ Sandin, TA DeFanti, RV Kenyon, JC Hart; citation_volume=35; citation_issue=6; citation_publication_date=1992; citation_pages=64-72; citation_doi=10.1145/129888.129892; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Futur Gener Comput Syst; citation_title=The StarCAVE, a third-generation CAVE and virtual reality optiportal; citation_author=TA DeFanti, G Dawe, DJ Sandin, JP Schulze, P Otto, J Girado, F Kuester, L Smarr, R Rao; citation_volume=25; citation_issue=2; citation_publication_date=2009; citation_pages=169-178; citation_doi=10.1016/j.future.2008.07.015; citation_id=CR12"/>

    <meta name="citation_reference" content="European Broadcasting Union (2011) Practical guidelines for production and implementation in accordance with EBU R 128 (version 2.0). European Broadcasting Union, Geneva"/>

    <meta name="citation_reference" content="Frechaud V (2013) Gui3D, v. 1.11"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Gen; citation_title=Effect size estimates: current use, calculations, and interpretation; citation_author=CO Fritz, PE Morris, JJ Richler; citation_volume=141; citation_issue=1; citation_publication_date=2012; citation_pages=2-18; citation_doi=10.1037/a0024338; citation_id=CR15"/>

    <meta name="citation_reference" content="Garcia G (2002) Optimal filter partition for efficient convolution with short input/output delay. In: Proceedings of the AES 113th convention"/>

    <meta name="citation_reference" content="Gardner WG (1994) Efficient convolution without input/output delay. J. Audio Eng Soc 127&#8211;136 (preprint 3897)
                        "/>

    <meta name="citation_reference" content="citation_title=Binaural and spatial hearing in real and virtual environments; citation_publication_date=2014; citation_isbn=978-131-778026-7; citation_id=CR18; citation_author=R Gilkey; citation_author=T Anderson; citation_publisher=Taylor &amp; Francis"/>

    <meta name="citation_reference" content="Gorzel M, Corrigan D, Kearney G, Squires J, Boland F (2012) Distance perception in virtual audio-visual environment. In: 25th UK conference of the audio engineering society: spatial audio in today&#8217;s 3D world, York, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Br J Surg; citation_title=Systematic review of randomized controlled trials on the effectiveness of virtual reality training for laparoscopic surgery; citation_author=K Gurusamy, R Aggarwal, L Palanivelu, BR Davidson; citation_volume=95; citation_issue=9; citation_publication_date=2008; citation_pages=1088-1097; citation_doi=10.1002/bjs.6344; citation_id=CR20"/>

    <meta name="citation_reference" content="Hess W, Weish&#228;upl J (2014) Replication of human head movements in 3 dimensions by a mechanical joint. In: Proceedings of the international conference on spatial audio (ICSA)"/>

    <meta name="citation_reference" content="citation_journal_title=Ergonomics; citation_title=Transfer of training from virtual reality; citation_author=JJ Kozak, PA Hancock, EJ Arthur, ST Chrysler; citation_volume=36; citation_issue=7; citation_publication_date=1993; citation_pages=777-784; citation_doi=10.1080/00140139308967941; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_title=A true spatial sound system for CAVE-like displays using four loudspeakers; citation_inbook_title=Virtual reality; citation_publication_date=2007; citation_pages=270-279; citation_id=CR23; citation_author=T Kuhlen; citation_author=I Assenmacher; citation_author=T Lentz; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Larsson P, V&#228;stfj&#228;ll D, Kleiner M (2004) Perception of self-motion and presence in auditory virtual environments. In: Proceedings of the presence, pp 252&#8211;258"/>

    <meta name="citation_reference" content="citation_title=Essentials of digital signal processing; citation_publication_date=2014; citation_isbn=978-110-705932-0; citation_id=CR25; citation_author=BP Lathi; citation_author=RA Green; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="Le Callet P, M&#246;ller S, Perkis A (2012) Qualinet white paper on definitions of quality of experience (version 1.1). Qualinet, Dagstuhl"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Minimum BRIR grid resolution for dynamic binaural synthesis; citation_author=A Lindau, H Maempel, S Weinzierl; citation_volume=123; citation_issue=5; citation_publication_date=2008; citation_pages=3498-3498; citation_doi=10.1121/1.2934364; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Res Methods Instrum Comput; citation_title=Immersive virtual environment technology as a basic research tool in psychology; citation_author=JM Loomis, JJ Blascovich, AC Beall; citation_volume=31; citation_issue=4; citation_publication_date=1999; citation_pages=557-564; citation_doi=10.3758/BF03200735; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=Perceived loudness and visually-determined auditory distance; citation_author=DH Mershon, DH Desaulniers, SA Kiefer, TL Amerson, JT Mills; citation_volume=10; citation_issue=5; citation_publication_date=1981; citation_pages=531-543; citation_doi=10.1068/p100531; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Transfer-function measurement with sweeps; citation_author=S M&#252;ller, P Massarani; citation_volume=49; citation_issue=6; citation_publication_date=2001; citation_pages=443-471; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_title=Auditory virtual environments; citation_inbook_title=Communication acoustics; citation_publication_date=2005; citation_pages=277-297; citation_id=CR31; citation_author=P Novo; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Brain Res; citation_title=Spatial processing in human auditory cortex: the effects of 3D, ITD, and ILD stimulation techniques; citation_author=KJ Palom&#228;ki, H Tiitinen, V M&#228;kinen, JC May, P Alku; citation_volume=24; citation_issue=3; citation_publication_date=2005; citation_pages=364-379; citation_doi=10.1016/j.cogbrainres.2005.02.013; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Personal Individ Differ; citation_title=Music preference correlates of jungian types; citation_author=JL Pearson, SJ Dollinger; citation_volume=36; citation_issue=5; citation_publication_date=2004; citation_pages=1005-1008; citation_doi=10.1016/S0191-8869(03)00168-5; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=J Res Music Educ; citation_title=A paradigm for research on music listening; citation_author=WF Prince; citation_volume=20; citation_issue=4; citation_publication_date=1972; citation_pages=445-455; citation_doi=10.2307/3343802; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Instr Sci; citation_title=Immersive training systems: virtual reality and education and training; citation_author=J Psotka; citation_volume=23; citation_issue=5&#8211;6; citation_publication_date=1995; citation_pages=405-431; citation_doi=10.1007/BF00896880; citation_id=CR35"/>

    <meta name="citation_reference" content="Pysiewicz A (2014) On the validity of web-based auditory perception experiments. Master&#8217;s thesis, TU Berlin"/>

    <meta name="citation_reference" content="citation_journal_title=Ergonomics; citation_title=Training in virtual environments: transfer to real world tasks and equivalence to real task training; citation_author=FD Rose, EA Attree, BM Brooks, DM Parslow, PR Penn, N Ambihaipahan; citation_volume=43; citation_issue=4; citation_publication_date=2000; citation_pages=494-511; citation_doi=10.1080/001401300184378; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Relationships between experienced listener ratings of multichannel audio quality and naive listener preferences; citation_author=F Rumsey, S Zielinski, R Kassier, S Bech; citation_volume=117; citation_issue=6; citation_publication_date=2005; citation_pages=3832-3840; citation_doi=10.1121/1.1904305; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=Nat Rev Neurosci; citation_title=From presence to consciousness through virtual reality; citation_author=MV Sanchez-Vives, M Slater; citation_volume=6; citation_issue=6; citation_publication_date=2005; citation_pages=332-339; citation_doi=10.1038/nrn1651; citation_id=CR39"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Localization of sound from single and paired sources; citation_author=TT Sandel, DC Teas, WE Feddersen, LA Jeffress; citation_volume=27; citation_publication_date=1955; citation_pages=842-852; citation_doi=10.1121/1.1908052; citation_id=CR40"/>

    <meta name="citation_reference" content="Schoeffler M, Herre J (2013) About the impact of audio quality on overall listening experience. In: Proceedings of the sound and music computing conference, Stockholm, Sweden, pp 48&#8211;53"/>

    <meta name="citation_reference" content="Schoeffler M, Herre J (2014a) Towards a listener model for predicting the overall listening experience. In: Proceedings of the audiomostly, Aalborg, Denmark"/>

    <meta name="citation_reference" content="Schoeffler M, Herre J (2014b) About the different types of listeners for rating the overall listening experience. In: Proceedings of the sound and music computing conference, Athens, Greece"/>

    <meta name="citation_reference" content="Schoeffler M, Hess W (2012) A comparison of highly configurable CPU- and GPU-based convolution engines. In: Audio engineering society convention no. 133, San Francisco, CA, USA"/>

    <meta name="citation_reference" content="Schoeffler M, Edler B, Herre J (2013a) How much does audio quality influence ratings of overall listening experience? In: Proceedings of the 10th international symposium on computer music multidisciplinary research (CMMR), pp 678&#8211;693, Marseille, France"/>

    <meta name="citation_reference" content="Schoeffler M, St&#246;ter F-R, Bayerlein H, Edler B, Herre J (2013b) An experiment about estimating the number of instruments in polyphonic music: a comparison between internet and laboratory results. In: Proceedings of the 14th international society for music information retrieval conference, Curitiba, Brazil"/>

    <meta name="citation_reference" content="Schoeffler M, Adami A, Herre J (2014a) The influence of up- and down-mixes on the overall listening experience. In: Proceedings of the AES 137th convention, Los Angeles, CA, USA (preprint 9140)
                        "/>

    <meta name="citation_reference" content="Schoeffler M, Conrad S, Herre J (2014b) The influence of the single/multi-channel-system on the overall listening experience. In: Proceedings of the AES 55th conference on spatial audio, Helsinki, Finland"/>

    <meta name="citation_reference" content="Schoeffler M, St&#246;ter F, Edler B, Herre J (2015) Towards the next generation of web-based experiments: a case study assessing basic audio quality following the itu-r recommendation bs.1534 (MUSHRA). In: 1st web audio conference, Paris, France"/>

    <meta name="citation_reference" content="Schr&#246;der D, Wefers F, Pelzer S, Rausch D, Vorl&#228;nder M, Kuhlen T (2010) Virtual reality system at RWTH Aachen University. In: Proceedings of the international symposium on room acoustics, Sydney, NSW, Australia. Australian Acoustical Society, NSW Division"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav Soc Netw; citation_title=Research on presence in virtual reality: a survey; citation_author=MJ Schuemie, P Straaten, M Krijn, CA Mast; citation_volume=4; citation_issue=2; citation_publication_date=2001; citation_pages=183-201; citation_doi=10.1089/109493101300117884; citation_id=CR51"/>

    <meta name="citation_reference" content="Seeber BU, Fastl H (2004) On auditory-visual interaction in real and virtual environments. In: Proceedings of the 18th international congress on acoustics, pp 2293&#8211;2296, Kyoto, Japan"/>

    <meta name="citation_reference" content="Silzle A, Strauss H, Novo P (2004) IKA-SIM: a system to generate auditory virtual environments. In: Audio engineering society convention no. 116 (preprint 6016)
                        "/>

    <meta name="citation_reference" content="Stanney K (1995) Realizing the full potential of virtual reality: human factors issues that could stand in the way. In: Proceedings the virtual reality annual international symposium &#8217;95. IEEE Computer Society Press, Los Alamitos, pp 28&#8211;34"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Human factors issues in virtual environments: a review of the literature; citation_author=KM Stanney, RR Mourant, RS Kennedy, AROT Literature; citation_volume=7; citation_publication_date=1998; citation_pages=327-351; citation_doi=10.1162/105474698565767; citation_id=CR55"/>

    <meta name="citation_reference" content="citation_journal_title=J Commun; citation_title=Defining virtual reality: dimensions determining telepresence; citation_author=J Steuer; citation_volume=42; citation_issue=4; citation_publication_date=1992; citation_pages=73-93; citation_doi=10.1111/j.1460-2466.1992.tb00812.x; citation_id=CR56"/>

    <meta name="citation_reference" content="Stockham Jr TG (1966) High-speed convolution and correlation. In: Proceedings of the spring joint computer conference, April 26&#8211;28, pp 229&#8211;233. ACM, New York, NY, USA"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav Soc Netw; citation_title=Experimental studies of virtual reality-delivered compared to conventional exercise programs for rehabilitation; citation_author=H Sveistrup, J McComas, M Thornton, S Marshall, H Finestone, A McCormick, K Babulic, A Mayhew; citation_volume=6; citation_issue=3; citation_publication_date=2003; citation_pages=245-249; citation_doi=10.1089/109493103322011524; citation_id=CR58"/>

    <meta name="citation_reference" content="The OGRE Team (2013) OGRE game engine, v. 1.9.0"/>

    <meta name="citation_reference" content="Torger A, Farina A (2001) Real-time partitioned convolution for ambiophonics surround sound. In: IEEE workshop on applications of signal processing to audio and acoustics, pp 195&#8211;198"/>

    <meta name="citation_reference" content="V&#228;ljam&#228;e A, Larsson P, V&#228;stfj&#228;ll D, Kleiner M (2004) Auditory presence, individualized head-related transfer functions, and illusory ego-motion in virtual environments. In: Proceedings of the 7th annual workshop on presence"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Experiments in immersive virtual reality for scientific visualization; citation_author=A Dam, DH Laidlaw, RM Simpson; citation_volume=26; citation_issue=4; citation_publication_date=2002; citation_pages=535-555; citation_doi=10.1016/S0097-8493(02)00113-9; citation_id=CR62"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=The subjective sense of presence, emotion recognition, and experienced emotions in auditory virtual environments; citation_author=D V&#228;stfj&#228;ll; citation_volume=6; citation_issue=2; citation_publication_date=2003; citation_pages=181-188; citation_doi=10.1089/109493103321640374; citation_id=CR63"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Ergon; citation_title=Using virtual reality technology for aircraft visual inspection training: presence and comparison studies; citation_author=J Vora, S Nair, AK Gramopadhye, AT Duchowski, BJ Melloy, B Kanki; citation_volume=33; citation_issue=6; citation_publication_date=2002; citation_pages=559-570; citation_doi=10.1016/S0003-6870(02)00039-X; citation_id=CR64"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Res Methods Instrum Comput; citation_title=The world wide web as a medium for psychoacoustical demonstrations and experiments: experience and results; citation_author=N Welch, JH Krantz; citation_volume=28; citation_issue=2; citation_publication_date=1996; citation_pages=192-196; citation_doi=10.3758/BF03204764; citation_id=CR65"/>

    <meta name="citation_reference" content="Werner S, Siegel A (2011) Effects of binaural auralization via headphones on the perception of acoustic scenes. In: Proceedings of the 3rd international symposium on auditory and audiological research (ISAAR), Nyborg, Denmark"/>

    <meta name="citation_reference" content="Werner S, Liebetrau J, Sporer T (2012) Audio&#8211;visual discrepancy and the influence on vertical sound source localization. In: Proceedings of the 4th international workshop on quality of multimedia experience (QoMEX), pp 133&#8211;139, Melbourne, Australia"/>

    <meta name="citation_reference" content="citation_journal_title=Biom Bull; citation_title=Individual comparisons by ranking methods; citation_author=F Wilcoxon; citation_volume=1; citation_issue=6; citation_publication_date=1945; citation_pages=80-83; citation_doi=10.2307/3001968; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Virtual spaces and real world places: transfer of route knowledge; citation_author=BG Witmer, JH Bailey, BW Knerr, KC Parsons; citation_volume=45; citation_issue=4; citation_publication_date=1996; citation_pages=413-428; citation_doi=10.1006/ijhc.1996.0060; citation_id=CR69"/>

    <meta name="citation_author" content="Michael Schoeffler"/>

    <meta name="citation_author_email" content="michael.schoeffler@audiolabs-erlangen.de"/>

    <meta name="citation_author_institution" content="International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universit&#228;t Erlangen-N&#252;rnberg (FAU), Erlangen, Germany"/>

    <meta name="citation_author" content="Jan Lukas Gernert"/>

    <meta name="citation_author_institution" content="International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universit&#228;t Erlangen-N&#252;rnberg (FAU), Erlangen, Germany"/>

    <meta name="citation_author" content="Maximilian Neumayer"/>

    <meta name="citation_author_institution" content="International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universit&#228;t Erlangen-N&#252;rnberg (FAU), Erlangen, Germany"/>

    <meta name="citation_author" content="Susanne Westphal"/>

    <meta name="citation_author_institution" content="International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universit&#228;t Erlangen-N&#252;rnberg (FAU), Erlangen, Germany"/>

    <meta name="citation_author" content="J&#252;rgen Herre"/>

    <meta name="citation_author_institution" content="International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universit&#228;t Erlangen-N&#252;rnberg (FAU), Erlangen, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0270-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0270-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience"/>
        <meta property="og:description" content="In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity compared to real-world experiments. To this end, an experiment was carried out which compares responses given in a real-world environment to responses given in a VR environment. In the experiment, thirty participants rated the overall listening experience of music excerpts while sitting in a cinema and a listening booth being in a real-world environment and in a VR environment. In addition, the VR system that was used to carry out the sessions in the VR environment is presented in detail. Results indicate that there are only minor statistically significant differences between the two environments when the overall listening experience is rated. Furthermore, in the real-world environment, the ratings given in the listening booth were slightly higher than in the cinema."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0270-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality-based experiments, Overall listening experience, Convolution engine, Oculus Rift","kwrd":["Virtual_reality-based_experiments","Overall_listening_experience","Convolution_engine","Oculus_Rift"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0270-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0270-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=270;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0270-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0270-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0270-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: SPATIAL SOUND</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-08-15" itemprop="datePublished">15 August 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michael-Schoeffler" data-author-popup="auth-Michael-Schoeffler" data-corresp-id="c1">Michael Schoeffler<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)" /><meta itemprop="address" content="grid.5330.5, 0000000121073311, International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jan_Lukas-Gernert" data-author-popup="auth-Jan_Lukas-Gernert">Jan Lukas Gernert</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)" /><meta itemprop="address" content="grid.5330.5, 0000000121073311, International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Maximilian-Neumayer" data-author-popup="auth-Maximilian-Neumayer">Maximilian Neumayer</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)" /><meta itemprop="address" content="grid.5330.5, 0000000121073311, International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Susanne-Westphal" data-author-popup="auth-Susanne-Westphal">Susanne Westphal</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)" /><meta itemprop="address" content="grid.5330.5, 0000000121073311, International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J_rgen-Herre" data-author-popup="auth-J_rgen-Herre">Jürgen Herre</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)" /><meta itemprop="address" content="grid.5330.5, 0000000121073311, International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">181</span>–<span itemprop="pageEnd">200</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">711 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0270-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity compared to real-world experiments. To this end, an experiment was carried out which compares responses given in a real-world environment to responses given in a VR environment. In the experiment, thirty participants rated the overall listening experience of music excerpts while sitting in a cinema and a listening booth being in a real-world environment and in a VR environment. In addition, the VR system that was used to carry out the sessions in the VR environment is presented in detail. Results indicate that there are only minor statistically significant differences between the two environments when the overall listening experience is rated. Furthermore, in the real-world environment, the ratings given in the listening booth were slightly higher than in the cinema.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The ways auditory experiments are conducted are evolving. Until a few years ago, the majority of experiments have mainly been conducted in laboratory environments. The advantage of laboratory experiments is that confounding variables can be controlled to a certain extend and experimenters can monitor the participants.</p><p>Nowadays, conducting so-called Web-based experiments (or Internet experiments) is becoming more popular (Welch and Krantz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Welch N, Krantz JH (1996) The world wide web as a medium for psychoacoustical demonstrations and experiments: experience and results. Behav Res Methods Instrum Comput 28(2):192–196" href="/article/10.1007/s10055-015-0270-8#ref-CR65" id="ref-link-section-d38762e374">1996</a>; Schoeffler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Schoeffler M, Stöter F, Edler B, Herre J (2015) Towards the next generation of web-based experiments: a case study assessing basic audio quality following the itu-r recommendation bs.1534 (MUSHRA). In: 1st web audio conference, Paris, France" href="/article/10.1007/s10055-015-0270-8#ref-CR49" id="ref-link-section-d38762e377">2015</a>; Pysiewicz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Pysiewicz A (2014) On the validity of web-based auditory perception experiments. Master’s thesis, TU Berlin" href="/article/10.1007/s10055-015-0270-8#ref-CR36" id="ref-link-section-d38762e380">2014</a>). The validity of Web-based experiments has been evaluated by various researchers who came to the conclusion that this type of experiment results in very similar outcomes compared to real-world experiments<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> (Pysiewicz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Pysiewicz A (2014) On the validity of web-based auditory perception experiments. Master’s thesis, TU Berlin" href="/article/10.1007/s10055-015-0270-8#ref-CR36" id="ref-link-section-d38762e395">2014</a>; Schoeffler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013b" title="Schoeffler M, Stöter F-R, Bayerlein H, Edler B, Herre J (2013b) An experiment about estimating the number of instruments in polyphonic music: a comparison between internet and laboratory results. In: Proceedings of the 14th international society for music information retrieval conference, Curitiba, Brazil" href="/article/10.1007/s10055-015-0270-8#ref-CR46" id="ref-link-section-d38762e399">2013b</a>). Compared to laboratory experiments, Web-based experiments support a simplified recruitment process since the experiment can be accessed by any Web-enabled devices from all over the world. One of the main disadvantages of Web-based experiments is that they lack full control over the experiment procedure. Moreover, Web-based experiments are not suited for all types of experiments related to spatial audio, especially, e.g., if they require specific room acoustics. VR experiments have the potential to overcome this issue by providing an authentic visual and auditory representation of a specific room.</p><p>The term VR describes the simulation of an environment that creates the sensation of being present in places in a real or in an imagined world (Steuer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Steuer J (1992) Defining virtual reality: dimensions determining telepresence. J Commun 42(4):73–93" href="/article/10.1007/s10055-015-0270-8#ref-CR56" id="ref-link-section-d38762e405">1992</a>). A simulated physical presence becomes only plausible for humans when several requirements are met. Stanney (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stanney K (1995) Realizing the full potential of virtual reality: human factors issues that could stand in the way. In: Proceedings the virtual reality annual international symposium ’95. IEEE Computer Society Press, Los Alamitos, pp 28–34" href="/article/10.1007/s10055-015-0270-8#ref-CR54" id="ref-link-section-d38762e408">1995</a>) described issues related to these requirements which must be solved to reach the full potential of VR systems, including human performance, user characteristics, visual and auditory perception, absence of cybersickness, and social impacts (see also Stanney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Stanney KM, Mourant RR, Kennedy RS, Literature AROT (1998) Human factors issues in virtual environments: a review of the literature. Presence 7:327–351" href="/article/10.1007/s10055-015-0270-8#ref-CR55" id="ref-link-section-d38762e411">1998</a>). Some of these issues were investigated by experimental studies in order to measure their influence on the perceived level of presence (van Dam et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization. Comput Graph 26(4):535–555" href="/article/10.1007/s10055-015-0270-8#ref-CR62" id="ref-link-section-d38762e414">2002</a>; Sanchez-Vives and Slater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sanchez-Vives MV, Slater M (2005) From presence to consciousness through virtual reality. Nat Rev Neurosci 6(6):332–339" href="/article/10.1007/s10055-015-0270-8#ref-CR39" id="ref-link-section-d38762e417">2005</a>; Schuemie et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Schuemie MJ, van der Straaten P, Krijn M, van der Mast CA (2001) Research on presence in virtual reality: a survey. Cyberpsychol Behav Soc Netw 4(2):183–201" href="/article/10.1007/s10055-015-0270-8#ref-CR51" id="ref-link-section-d38762e421">2001</a>). One could argue that an authentic visual stimulus might not be important when conducting auditory experiments, but many studies have shown that the visual stimulus has a significant influence when participants give responses to auditory stimuli (Seeber and Fastl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Seeber BU, Fastl H (2004) On auditory-visual interaction in real and virtual environments. In: Proceedings of the 18th international congress on acoustics, pp 2293–2296, Kyoto, Japan" href="/article/10.1007/s10055-015-0270-8#ref-CR52" id="ref-link-section-d38762e424">2004</a>; Werner and Siegel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Werner S, Siegel A (2011) Effects of binaural auralization via headphones on the perception of acoustic scenes. In: Proceedings of the 3rd international symposium on auditory and audiological research (ISAAR), Nyborg, Denmark" href="/article/10.1007/s10055-015-0270-8#ref-CR66" id="ref-link-section-d38762e427">2011</a>; Werner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Werner S, Liebetrau J, Sporer T (2012) Audio–visual discrepancy and the influence on vertical sound source localization. In: Proceedings of the 4th international workshop on quality of multimedia experience (QoMEX), pp 133–139, Melbourne, Australia" href="/article/10.1007/s10055-015-0270-8#ref-CR67" id="ref-link-section-d38762e430">2012</a>; Gorzel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gorzel M, Corrigan D, Kearney G, Squires J, Boland F (2012) Distance perception in virtual audio-visual environment. In: 25th UK conference of the audio engineering society: spatial audio in today’s 3D world, York, UK" href="/article/10.1007/s10055-015-0270-8#ref-CR19" id="ref-link-section-d38762e433">2012</a>).</p><p>Increasing computational power and capabilities of VR devices, e.g., higher display resolution of head-mounted displays, allow rendering virtual environments more accurately in real time, resulting in more authentic visual and auditory representations. An authentic auditory representation of an environment can be achieved by binaural synthesis. When using binaural synthesis, audio algorithms process two signals, where each signal corresponds to a single ear of a listener. For example, room impulse responses of a room (which contain information about the acoustical characteristics of a room) can be measured by a dummy head having one microphone in each ear. A set of such room impulse responses is called binaural room impulse responses (BRIRs). When convolving a (single-channel) auditory stimulus with a BRIR, the resulting audio signal sounds like an accurate reproduction of the stimulus played back in the measured room at the exact position where the BRIRs were recorded.</p><p>In the future, Web-based experiments might be combined with VR experiments having both advantages: theoretical access to millions of participants and the possibility to create any type of environment, provided that participants have a compatible VR device at home. In order to draw scientifically correct conclusions from such experiments, both types, Web-based and VR experiments, must be valid compared to real-world experiments. Therefore, results of VR experiments must be compared to the results of real-world experiments, to find out under which circumstances they become valid.</p><p>This work investigates the validity of an auditory experiment conducted in a real-world environment and a VR environment. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec7">3</a>, a system is presented which was developed to carry out the VR sessions of the auditory experiment. The system consists of a modified game engine which allows a virtual scene to be rendered on a head-mounted display, an apparatus to measure BRIRs for any number of head positions, and a real-time convolution engine for convolving the BRIRs with auditory stimuli. A pair of headphones is used to reproduce the convolved auditory stimuli.</p><p>In the experiment, participants rated their perceived overall listening experience (OLE) while listening to short music excerpts in different types of rooms. OLE is a term used for describing the sensation, perception, and cognition that is active when someone listens to sound events. A rating of the OLE reflects the resulting enjoyment of listening to this sound event (Schoeffler and Herre <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Schoeffler M, Herre J (2013) About the impact of audio quality on overall listening experience. In: Proceedings of the sound and music computing conference, Stockholm, Sweden, pp 48–53" href="/article/10.1007/s10055-015-0270-8#ref-CR41" id="ref-link-section-d38762e452">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014a" title="Schoeffler M, Herre J (2014a) Towards a listener model for predicting the overall listening experience. In: Proceedings of the audiomostly, Aalborg, Denmark" href="/article/10.1007/s10055-015-0270-8#ref-CR42" id="ref-link-section-d38762e455">2014a</a>). Thus, when listeners rate the OLE, they are asked to take into account every factor that influences their enjoyment while listening to something. Such factors of influence might include song, lyrics, audio quality, listener’s mood, listening room, and reproduction system. Since so many factors might have an influence, the OLE has been shown to be a very holistic attribute, where each person has different preferences (Schoeffler and Herre <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Schoeffler M, Herre J (2014b) About the different types of listeners for rating the overall listening experience. In: Proceedings of the sound and music computing conference, Athens, Greece" href="/article/10.1007/s10055-015-0270-8#ref-CR43" id="ref-link-section-d38762e458">2014b</a>). As the term OLE is only used in the context of listening experience, it is a subattribute of the more general term quality of experience (QoE) which describes the degree of enjoyment or satisfaction of humans while using a system. Le Callet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Le Callet P, Möller S, Perkis A (2012) Qualinet white paper on definitions of quality of experience (version 1.1). Qualinet, Dagstuhl" href="/article/10.1007/s10055-015-0270-8#ref-CR26" id="ref-link-section-d38762e461">2012</a>) defined QoE as follows:</p>
                <blockquote class="c-blockquote"><div class="c-blockquote__body">
                  <p>Quality of Experience (QoE) is the degree of delight or annoyance of the user of an application or service. It results from the fulfillment of his or her expectations with respect to the utility and/or enjoyment of the application or service in the light of the user’s personality and current state.</p>
                </div></blockquote>
              <p>The participants of the presented experiment evaluated the OLE of short music excerpts in a cinema and in a small listening booth. The short music excerpts were mixed in 5.1 surround sound. In order to contribute to the validation of VR experiments, the participants were present in a real cinema and listening booth, and also in a virtual representation of the two rooms. Two main hypotheses are stated that were answered by the experiment:</p>
                <h3 class="c-article__sub-heading">
                  <b>Hypothesis I</b>
                </h3>
                <p>The OLE ratings given in the VR environment do not differ from those given in the real-world experiment.</p>
              
                <h3 class="c-article__sub-heading">
                  <b>Hypothesis II</b>
                </h3>
                <p>The OLE ratings that were given while being present in the (real-world) cinema do not differ from those given in the (real-world) listening booth.</p>
              <p>Besides adding questions regarding those hypotheses to the experiment, we also added some questions that are related to the acoustical characteristics of the room and reproduction system used. For example, we asked the participants how far away they perceived a sound source or what amount of reverberation they estimated. The goal of asking these questions is not to find out whether acoustical characteristics are perceived differently in a VR experiment and in a real-world experiment. In our opinion, answering such a research question would require a dedicated and more comprehensive experiment. Instead, we want to give some indications as to which might be the basis for hypotheses of experiments conducted in the near future.</p><p>The detailed methodology of the experiment is described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec14">4.1</a>. By comparing the responses given in the real-world environment and given in the VR environment, we give some indication to what degree the VR environment was valid compared to the real-world environment (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec19">4.2</a>). The results are subsequently discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec20">4.3</a>.</p><p>In the next section, we give a short introduction into three approaches used to create a VR environment. Furthermore, state-of-the-art VR systems are presented focusing on the audio rendering. Next, studies are reviewed that compared the results of a VR experiment to results of a real-world experiment. Lastly, we summarize the current literature that is related to our second hypothesis and might give some hints as to how much the room influences the OLE.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Virtual reality technologies</h3><p>VR environments can be created by various approaches. The most common ones are cave automatic virtual environments (CAVEs) and head-mounted displays (HMDs).</p><p>In a CAVE, images of a scene are projected on multiple walls (and sometimes also on the floor and ceiling) of a room (Cruz-Neira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72" href="/article/10.1007/s10055-015-0270-8#ref-CR11" id="ref-link-section-d38762e534">1992</a>). The images of the scene are updated according to the user’s head position and viewed by stereoscopic glasses. One major advantage of a CAVE is that the field of view is typically very wide, allowing the user to walk around within the virtual scene without decreasing the state of immersion. A disadvantage of CAVEs is that a lot of equipment (projectors, head-tracker, glasses, etc.) and space are needed to create a VR environment. Both a wide field of view and the need for no additional resources are offered by binocular HMDs which are devices, worn on the head, having one or two integrated displays.</p><p>Binocular HMDs which have only one display typically show two views. The views are separated, whereby each eye is focusing on a different view, which allows monocular HMDs to offer the same stereoscopy effect as binocular HMDs with two displays. A comprehensive review of HMDs is given by Cakmakci and Rolland (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Cakmakci O, Rolland J (2006) Head-worn displays: a review. Journal of Disp Technol 2(3):199–216" href="/article/10.1007/s10055-015-0270-8#ref-CR9" id="ref-link-section-d38762e540">2006</a>). Since the release of the Oculus Rift, a low-cost HMD currently offering low-latency headtracking, and a 75-Hz display with a resolution of 960 × 1080 pixels per eye (Development Kit 2), VR devices have received a lot of attention. The release followed announcements of big electronics companies, like Samsung and Sony, to release their own VR HMDs (Samsung’s Gear VR and Sony’s Morpheus). In our study, a Oculus Rift is utilized in our system to track the user head position and to render the graphical representation of our virtual scenes.</p><h3 class="c-article__sub-heading" id="Sec4">Audio in virtual reality</h3><p>Although the visual stimulus is probably the main point of interest of publications related to VR, there are a few publications describing the audio processing in more detail or propose new approaches for enhancing the listening experience of users (Astheimer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Astheimer P (1993) What you see is what you hear-acoustics applied in virtual worlds. In: Proceedings of the IEEE 1993 symposium on research frontiers in virtual reality, pp 100–107. ISBN: 0-8186-4910-0" href="/article/10.1007/s10055-015-0270-8#ref-CR2" id="ref-link-section-d38762e551">1993</a>).</p><p>The CAVE at RWTH Aachen University uses an audio rendering system based on binaural synthesis that uses loudspeakers for reproduction instead of headphones (Kuhlen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kuhlen T, Assenmacher I, Lentz T (2007) A true spatial sound system for CAVE-like displays using four loudspeakers. In: Shumaker R (ed) Virtual reality, vol 4563. Springer, Berlin, pp 270–279. ISBN: 978-354-073334-8" href="/article/10.1007/s10055-015-0270-8#ref-CR23" id="ref-link-section-d38762e557">2007</a>; Schröder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Schröder D, Wefers F, Pelzer S, Rausch D, Vorländer M, Kuhlen T (2010) Virtual reality system at RWTH Aachen University. In: Proceedings of the international symposium on room acoustics, Sydney, NSW, Australia. Australian Acoustical Society, NSW Division" href="/article/10.1007/s10055-015-0270-8#ref-CR50" id="ref-link-section-d38762e560">2010</a>). When using a pair of loudspeakers for reproduction of binaural signals, it is intended that the audio signal of the left speaker is emitted only to the left ear and the audio signal of the right speaker is emitted only to the right ear. When reproducing binaural signals by loudspeakers, there is cross talk between the loudspeakers, meaning that, e.g., audio signals from the left loudspeaker arrive also at the right ear. To overcome this problem, they use dynamic cross talk cancelation which suppresses the cross talk between the loudspeakers.</p><p>In order to process an auditory representation of the virtual scene in real time, their system uses a <i>fast convolution</i> which is a technique that also our VR system uses. The term fast convolution describes techniques that use a fast Fourier transform (FFT) to convolve audio signals with BRIRs in the frequency domain (Stockham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1966" title="Stockham Jr TG (1966) High-speed convolution and correlation. In: Proceedings of the spring joint computer conference, April 26–28, pp 229–233. ACM, New York, NY, USA" href="/article/10.1007/s10055-015-0270-8#ref-CR57" id="ref-link-section-d38762e569">1966</a>; Torger and Farina <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Torger A, Farina A (2001) Real-time partitioned convolution for ambiophonics surround sound. In: IEEE workshop on applications of signal processing to audio and acoustics, pp 195–198" href="/article/10.1007/s10055-015-0270-8#ref-CR60" id="ref-link-section-d38762e572">2001</a>). Thereby, the convolution is performed by a multiplication of the discrete Fourier spectra. Moreover, the convolution is processed block-wise, where each block contains a number of samples that correspond to the block length of the sound interface. Thus, the input-to-output latency is dependent on the block length. For example, with a sample rate of 48,000 Hz, a block length of 4096 samples leads to a delay of <span class="mathjax-tex">\(\frac{4096}{48,\!000\,\text {Hz}} = 0.085\overline{3}\,\text {s}\)</span>. A uniformly partitioned convolution splits BRIRs into multiple parts, where each part typically has the same number of samples as the block length. Besides the uniformly partitioned convolution, nonuniformly partitioned convolution also exists, where BRIRs are split into multiple parts with different numbers of samples (Gardner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Gardner WG (1994) Efficient convolution without input/output delay. J. Audio Eng Soc 127–136 (preprint 3897)&#xA;                        " href="/article/10.1007/s10055-015-0270-8#ref-CR17" id="ref-link-section-d38762e632">1994</a>). By using parts with a number of samples higher than the block length, the computational effort is reduced compared to a uniformly partitioned convolution (Garcia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Garcia G (2002) Optimal filter partition for efficient convolution with short input/output delay. In: Proceedings of the AES 113th convention" href="/article/10.1007/s10055-015-0270-8#ref-CR16" id="ref-link-section-d38762e636">2002</a>). The system of Schröder et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Schröder D, Wefers F, Pelzer S, Rausch D, Vorländer M, Kuhlen T (2010) Virtual reality system at RWTH Aachen University. In: Proceedings of the international symposium on room acoustics, Sydney, NSW, Australia. Australian Acoustical Society, NSW Division" href="/article/10.1007/s10055-015-0270-8#ref-CR50" id="ref-link-section-d38762e639">2010</a>) utilizes a nonuniformly partitioned convolution since their use-cases require simultaneously rendering a high number of virtual sound sources. Since our use-case requires only a limited number of virtual sound sources to be rendered, a uniformly partitioned convolution is used in our system.</p><p> DeFanti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="DeFanti TA, Dawe G, Sandin DJ, Schulze JP, Otto P, Girado J, Kuester F, Smarr L, Rao R (2009) The StarCAVE, a third-generation CAVE and virtual reality optiportal. Futur Gener Comput Syst 25(2):169–178" href="/article/10.1007/s10055-015-0270-8#ref-CR12" id="ref-link-section-d38762e645">2009</a>) argue that binaural approaches, especially headphone-based and head-tracked systems, are very useful for single-user scenarios, but are not well suited for multiple simultaneous users who may also want to converse with each other. The audio rendering of their system (StarCAVE) is achieved using surround speakers and wave field synthesis (WFS). The basic idea behind the WFS is based on the Huygens–Fresnel principle, which states that any wavefront can be assembled by a superposition of elementary spherical waves (Berkhout et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93(5):2764–2778" href="/article/10.1007/s10055-015-0270-8#ref-CR4" id="ref-link-section-d38762e648">1993</a>). In practice, this is achieved by a large array of independently controlled loudspeakers that is used to create the same pressure wave of a virtual sound source as an actual sound source located somewhere inside the sound field.</p><p>More work on the connection between audio and VR has been done in the field of auditory virtual environments (AVEs). AVEs are defined as the auditory components of virtual environments which aim at creating situations in which humans have perceptions that do not correspond to their physical environment but to the virtual one (Novo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Novo P (2005) Auditory virtual environments. In: Blauert J (ed) Communication acoustics. Springer, Berlin, pp. 277–297" href="/article/10.1007/s10055-015-0270-8#ref-CR31" id="ref-link-section-d38762e655">2005</a>). A comprehensive overview of AVEs is given by  Gilkey and Anderson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gilkey R, Anderson T (2014) Binaural and spatial hearing in real and virtual environments. Taylor &amp; Francis, London. ISBN: 978-131-778026-7" href="/article/10.1007/s10055-015-0270-8#ref-CR18" id="ref-link-section-d38762e658">2014</a>). Silzle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Silzle A, Strauss H, Novo P (2004) IKA-SIM: a system to generate auditory virtual environments. In: Audio engineering society convention no. 116 (preprint 6016)&#xA;                        " href="/article/10.1007/s10055-015-0270-8#ref-CR53" id="ref-link-section-d38762e661">2004</a>) addressed the basic concepts of AVEs and described a comprehensive system with the purpose to generate AVEs. How a strong sense of presence is achieved by AVEs has been studied by Västfjäll (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Västfjäll D (2003) The subjective sense of presence, emotion recognition, and experienced emotions in auditory virtual environments. Cyberpsychol Behav 6(2):181–188" href="/article/10.1007/s10055-015-0270-8#ref-CR63" id="ref-link-section-d38762e664">2003</a>) and Larsson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Larsson P, Västfjäll D, Kleiner M (2004) Perception of self-motion and presence in auditory virtual environments. In: Proceedings of the presence, pp 252–258" href="/article/10.1007/s10055-015-0270-8#ref-CR24" id="ref-link-section-d38762e667">2004</a>).</p><h3 class="c-article__sub-heading" id="Sec5">Real-world experiments versus virtual reality experiments</h3><p>Nowadays, VR systems are used in various fields, e.g., phobia therapy, military training, entertainment, and scientific experimental research (Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in psychology. Behav Res Methods Instrum Comput 31(4):557–564" href="/article/10.1007/s10055-015-0270-8#ref-CR28" id="ref-link-section-d38762e678">1999</a>; Sanchez-Vives and Slater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sanchez-Vives MV, Slater M (2005) From presence to consciousness through virtual reality. Nat Rev Neurosci 6(6):332–339" href="/article/10.1007/s10055-015-0270-8#ref-CR39" id="ref-link-section-d38762e681">2005</a>; Bowman and McMahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" href="/article/10.1007/s10055-015-0270-8#ref-CR8" id="ref-link-section-d38762e684">2007</a>). In some of these works, VR experiments were carried out and their results were compared to a real-world experiment.</p><p> Bella (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bella F (2004) Driving simulation in virtual reality for work zone design on highway: a validation study. In: The second SIIV international congress, Florence, Italy" href="/article/10.1007/s10055-015-0270-8#ref-CR3" id="ref-link-section-d38762e690">2004</a>) conducted a social experiment to investigate the speed of vehicles while driving in work zone areas and compared the results of a VR experiment with a real-world scenario. The VR system he built was a driving simulator, aiming to be as similar as possible to an actual car. User interfaces (pedals, steering wheels, and gear lever) were installed on a real vehicle. The scenario of the construction site was projected onto three big screens: one in the center in front of the vehicle and two lateral ones angled at 60° with respect to the plane of the central screen. The whole setup was connected to a sound system to reproduce the sounds of the engine. The visuals and audio were rendered according to the traveling conditions of the vehicle, depending on the actions of the driver on the pedals and the steering wheel. The vehicles’ speeds in the real-world work zone were measured by a laser speedometer and compared to speed measurements obtained by the VR experiment. Bella’s results show that samples from the VR experiment and the real-world scenario belong to the same population, i.e., no significant differences existed between the two environments in his scenario.</p><p>
Gurusamy et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Gurusamy K, Aggarwal R, Palanivelu L, Davidson BR (2008) Systematic review of randomized controlled trials on the effectiveness of virtual reality training for laparoscopic surgery. Br J Surg 95(9):1088–1097" href="/article/10.1007/s10055-015-0270-8#ref-CR20" id="ref-link-section-d38762e696">2008</a>) investigated the effectiveness of VR trainings for laparoscopic surgery compared to other training methods including conventional (real-world) training. To this end, they compiled clinical trials that address laparoscopic surgery and analyzed the results. They came to the conclusion that VR trainings are helpful, especially for young surgeons at the beginning of their laparoscopic training, e.g., VR training reduced the operating time, error, and unnecessary movements during laparoscopic cholecystectomy (removal of the gallbladder). Moreover, there is convincing evidence that VR training is a useful supplement to conventional training in laparoscopic cholecystectomy for surgical residents with limited laparoscopic experience.</p><p> Vora et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vora J, Nair S, Gramopadhye AK, Duchowski AT, Melloy BJ, Kanki B (2002) Using virtual reality technology for aircraft visual inspection training: presence and comparison studies. Appl Ergon 33(6):559–570" href="/article/10.1007/s10055-015-0270-8#ref-CR64" id="ref-link-section-d38762e702">2002</a>) measured the degree of immersion and presence felt by subjects when conducting an aircraft visual inspection training in a VR simulator compared to the conventional PC-based training application. Although PC-based training is not performed in the real-world, the work of Vora et al. shows how VR can be utilized to substitute for conventional training methods. Their VR system was based on a HMD with six-degree-of-freedom headtracking and used to create a virtual cargo bay environment. The results of their study show that the VR system scored well in most aspects of presence and was favored over the PC-based training.</p><p>More work on training has been done by Kozak et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Kozak JJ, Hancock PA, Arthur EJ, Chrysler ST (1993) Transfer of training from virtual reality. Ergonomics 36(7):777–784" href="/article/10.1007/s10055-015-0270-8#ref-CR22" id="ref-link-section-d38762e709">1993</a>), Witmer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Witmer BG, Bailey JH, Knerr BW, Parsons KC (1996) Virtual spaces and real world places: transfer of route knowledge. Int J Hum Comput Stud 45(4):413–428" href="/article/10.1007/s10055-015-0270-8#ref-CR69" id="ref-link-section-d38762e712">1996</a>), Sveistrup et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sveistrup H, McComas J, Thornton M, Marshall S, Finestone H, McCormick A, Babulic K, Mayhew A (2003) Experimental studies of virtual reality-delivered compared to conventional exercise programs for rehabilitation. Cyberpsychol Behav Soc Netw 6(3):245–249" href="/article/10.1007/s10055-015-0270-8#ref-CR58" id="ref-link-section-d38762e715">2003</a>), Rose et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Rose FD, Attree EA, Brooks BM, Parslow DM, Penn PR, Ambihaipahan N (2000) Training in virtual environments: transfer to real world tasks and equivalence to real task training. Ergonomics 43(4):494–511" href="/article/10.1007/s10055-015-0270-8#ref-CR37" id="ref-link-section-d38762e718">2000</a>), Bossard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bossard C, Kermarrec G, Buche C, Tisseau J (2008) Transfer of learning in virtual environments: a new challenge? Virtual Real 12(3):151–161" href="/article/10.1007/s10055-015-0270-8#ref-CR7" id="ref-link-section-d38762e721">2008</a>), and Psotka (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Psotka J (1995) Immersive training systems: virtual reality and education and training. Instr Sci 23(5–6):405–431" href="/article/10.1007/s10055-015-0270-8#ref-CR35" id="ref-link-section-d38762e725">1995</a>).</p><p>As one can see, a lot of work has been published to quantify the effect of VR in comparison with real-world tasks. To our knowledge, none of this work has mainly focused on auditory experiments. This paper is a contribution toward quantifying the effect of VR environments on auditory experiments.</p><h3 class="c-article__sub-heading" id="Sec6">Listening room and overall listening experience</h3><p>The OLE (or a related attribute) is included in many models that aim to describe the process from starting with the sensation of an auditory stimulus and ending with a qualitative rating of the perceived listening experience. Prince (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1972" title="Prince WF (1972) A paradigm for research on music listening. J Res Music Educ 20(4):445–455" href="/article/10.1007/s10055-015-0270-8#ref-CR34" id="ref-link-section-d38762e739">1972</a>) published a paradigm for describing this process. He presented his paradigm as a dependency graph showing which factors might be involved in a response to music and how they might be connected. His conclusion is that a wide range of factors (e.g., personality, maturity, musical ability, expectation, and even muscle movement) might be involved in the process of formulating a response and should be considered for research. Another model that was published by Blauert and Jekosch (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Blauert J, Jekosch U (2012) A layer model of sound quality. J Audio Eng Soc 60(1/2):4–12" href="/article/10.1007/s10055-015-0270-8#ref-CR6" id="ref-link-section-d38762e742">2012</a>) structures the formation process of <i>sound-quality</i> judgments. Their model is divided into four different layers: auditive quality, aural-scene quality, acoustic quality, and aural-communication quality. If one assigns the attribute OLE to their model, he or she would very likely assign it to the aural-communication quality layer. The aural-communication quality layer is described as the most abstract layer, and its assigned attributes cannot be easily described by physical measures or features of an audio signal. Schoeffler and Herre (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014a" title="Schoeffler M, Herre J (2014a) Towards a listener model for predicting the overall listening experience. In: Proceedings of the audiomostly, Aalborg, Denmark" href="/article/10.1007/s10055-015-0270-8#ref-CR42" id="ref-link-section-d38762e748">2014a</a>) proposed a model that allows various other models to be integrated and that can be used to predict OLE ratings. Their model includes the sensation, perception, and cognition of an auditory stimulus and the personality, ability, and state of a listener.</p><p>The OLE, being a very holistic attribute, is considered to be influenced by many factors, but only a few studies have been conducted to investigate the influence of these factors on the OLE. For example, degradations in audio quality (like distortions in frequency domain or bandwidth degradation) have a strong effect on the OLE (Schoeffler and Herre <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Schoeffler M, Herre J (2013) About the impact of audio quality on overall listening experience. In: Proceedings of the sound and music computing conference, Stockholm, Sweden, pp 48–53" href="/article/10.1007/s10055-015-0270-8#ref-CR41" id="ref-link-section-d38762e754">2013</a>; Schoeffler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013a" title="Schoeffler M, Edler B, Herre J (2013a) How much does audio quality influence ratings of overall listening experience? In: Proceedings of the 10th international symposium on computer music multidisciplinary research (CMMR), pp 678–693, Marseille, France" href="/article/10.1007/s10055-015-0270-8#ref-CR45" id="ref-link-section-d38762e757">2013a</a>; Rumsey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rumsey F, Zielinski S, Kassier R, Bech S (2005) Relationships between experienced listener ratings of multichannel audio quality and naive listener preferences. J Acoust Soc Am 117(6):3832–3840" href="/article/10.1007/s10055-015-0270-8#ref-CR38" id="ref-link-section-d38762e760">2005</a>). Furthermore, the individual listener, including his or her personality, has been shown to have a significant influence on the OLE (Pearson and Dollinger <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Pearson JL, Dollinger SJ (2004) Music preference correlates of jungian types. Personal Individ Differ 36(5):1005–1008" href="/article/10.1007/s10055-015-0270-8#ref-CR33" id="ref-link-section-d38762e763">2004</a>; Schoeffler and Herre <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Schoeffler M, Herre J (2014b) About the different types of listeners for rating the overall listening experience. In: Proceedings of the sound and music computing conference, Athens, Greece" href="/article/10.1007/s10055-015-0270-8#ref-CR43" id="ref-link-section-d38762e766">2014b</a>).</p><p>Related to spatial audio, the influence on up- and down-mix algorithms has been investigated by Schoeffler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014a" title="Schoeffler M, Adami A, Herre J (2014a) The influence of up- and down-mixes on the overall listening experience. In: Proceedings of the AES 137th convention, Los Angeles, CA, USA (preprint 9140)&#xA;                        " href="/article/10.1007/s10055-015-0270-8#ref-CR47" id="ref-link-section-d38762e772">2014a</a>). An up- or down-mix algorithm is needed when an audio source material has fewer, or more, respectively, channels than the reproduction system. For example, a down-mix algorithm is needed when 5.1 surround material is played back by a stereo reproduction system. If no down-mix algorithm is applied, information contained in the center and surround loudspeakers would be discarded. In their study, participants rated the overall listening experience while listening to up- and down-mixed music. The results of the study indicated that the down- or up-mix algorithm has only a minor influence on OLE ratings. There was one exception, a low-quality up-mix and a low-quality down-mix that were considered to be lower anchors in the study. In another study, Rumsey et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rumsey F, Zielinski S, Kassier R, Bech S (2005) Relationships between experienced listener ratings of multichannel audio quality and naive listener preferences. J Acoust Soc Am 117(6):3832–3840" href="/article/10.1007/s10055-015-0270-8#ref-CR38" id="ref-link-section-d38762e775">2005</a>) showed that surround sound is very important for preference ratings given by naïve listeners. Furthermore, they proposed a regression model fitted with their experiment results that predicts preference ratings of naïve listeners based on the timbral quality and spatial quality ratings of expert listeners. The findings of Rumsey et al. were confirmed by an experiment of Schoeffler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Schoeffler M, Conrad S, Herre J (2014b) The influence of the single/multi-channel-system on the overall listening experience. In: Proceedings of the AES 55th conference on spatial audio, Helsinki, Finland" href="/article/10.1007/s10055-015-0270-8#ref-CR48" id="ref-link-section-d38762e778">2014b</a>) where the influence of single-/multi-channel systems (mono, stereo, and 5.1 surround sound) used for reproduction was subject to investigation. In their study, participants rated the overall listening experience while listening to music reproduced by different reproduction systems. The mono, stereo, and 5.1 surround sound system had significant influences on the OLE ratings. In particular, the mono system had the weakest effect, and the 5.1 surround sound system had the strongest effect. In the same study, the effect of the listening room was investigated. The study consisted of two main experiment sessions. In the first experiment session, listeners were asked to rate the OLE while sitting in a professional listening room and listening to a short music excerpt reproduced by mono, stereo, and 5.1 surround sound. Two and a half months later, the same experiment was conducted, but this time listeners sat in a common office room (second experiment session). In both sessions, participants sat inside a black-colored 360° masking curtain made of deco-molton that was installed to veil the loudspeakers and the appearance of the room. By using a masking curtain, the experimenters controlled the influence of the visual stimulus on the OLE with the expectation that the participants would focus on the acoustic characteristics of the room. The office room had a reverberation time that would have been rated by acousticians very low compared to the professional listening room. However, comparing the results of both sessions, no significant differences were found. Although the experiment presented in this paper focuses on investigating the effect of VR environments on experiments, the experiment also continues the study of Schoeffler et al. by investigating the differences between two rooms without veiling the rooms’ visual appearance (Hypothesis II).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">System for VR experiments</h2><div class="c-article-section__content" id="Sec7-content"><h3 class="c-article__sub-heading" id="Sec8">Overview</h3><p>Our VR system consists of two main components: the VR application and the convolution engine (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig1">1</a>). The main purpose of the VR application is to process the user’s input and to render the visual representation of the virtual scene on the display of the HMD. Thus, the VR application contains the whole business logic, e.g., showing the instructions to the participants, controlling the workflow of the experiment, and fetching sensor data from the HMD. The second main component is the convolution engine which retrieves head-tracking data from the VR application and renders the auditory stimuli. The system we present does not contain any major new algorithms that we want to propose to the VR community. The components use state-of-the-art algorithms or common practices interacting with each other in order to create an authentic VR environment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Software architecture of the VR system depicted as an UML component diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec9">VR application</h3><p>The VR application must render virtual scenes in which a participant of an experiment is sitting inside a room and taking part in an auditory experiment. In order to authentically render appearance of a room, we created models of the two rooms we used in the experiment. The first room was a medium-sized cinema, and the second one was a small listening booth, both located at the venue of the Fraunhofer IIS in Erlangen, Germany. More details about the rooms are given in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec17">4.1.3</a>. Both models were modeled true-to-scale, since we were using blueprints of the rooms. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig2">2</a> depicts wire-frame views of the two models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Wire-frame view of the cinema model (<i>upper</i> image) and listening booth model (<i>lower</i> image)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>For rendering the models, we used a game engine called OGRE (The OGRE Team <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="The OGRE Team (2013) OGRE game engine, v. 1.9.0" href="/article/10.1007/s10055-015-0270-8#ref-CR59" id="ref-link-section-d38762e857">2013</a>). The main reason for using OGRE was that it is completely open-source. When we started to implement our VR system, no graphics engine had official support for the Oculus Rift. Therefore, we relied on modifying parts of the source code of a graphics engine in order to render a virtual scene into two views (of a HMD). In addition, OGRE has also an active community, which made it the optimal choice for us at that point in time.</p><p>Besides rendering the room appearance, the VR application allows participants to read instructions and to listen to auditory stimuli and rate them. In typical real-world experiments, a graphical user interface (GUI) (or sometimes sheets of papers) is used where participants can read instructions and rate stimuli. To have the same experience in VR, we programmed a GUI framework [based on Gui3D (Frechaud <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Frechaud V (2013) Gui3D, v. 1.11" href="/article/10.1007/s10055-015-0270-8#ref-CR14" id="ref-link-section-d38762e863">2013</a>)] that allowed us to create a virtual screen within the virtual environment. On this screen, we could arrange GUI elements (buttons, textfields, audio players, etc.) into an interface similar to the one used in the real-world experiment. For our experiment, we placed such a virtual screen at a monitor model of the VR listening booth. In the VR cinema, we placed the virtual screen at the cinema screen.</p><p>One could design the same GUI for a VR experiment as would be used in a real-world experiment. However, due to technical limitations of the Oculus Rift (and other VR HMDs), having the same look and feel can lead to unexpected results. Nowadays, the resolution of a typical monitor is about 1920 × 1080 pixels. Assuming the GUI of the real-world experiment is shown in full-screen, all pixels are used for presenting the GUI. The display of the current version of the Oculus Rift has a resolution of 1920 × 1080 pixels. As the display is split into two views (for each eye one view), the maximum resolution for rendering the virtual scene is limited to 960 × 1080 pixels. Moreover, only a small area of the provided resolution is used for rendering the virtual screen that shows the GUI. In the VR listening booth, the actual resolution used for rendering the virtual screen is about 550 × 325 pixels, considering that the participant is sitting in front of the monitor and looking toward the monitor. Sitting in the VR cinema, about 500 × 295 pixels are used for rendering the virtual screen. In addition to having only a very limited number of pixels to render a GUI in the VR environment, the so-called screen-door effect (or fixed-pattern noise) degrades the appearance of the GUI. The screen-door effect is a visual artifact in which the fine lines separating the display’s pixels become visible in the rendered image, especially on white backgrounds. Such an effect is very present on HMD since users’ eyes are very close to the display. Moreover, HMDs have in most cases a low resolution, which enhances this effect. When a GUI of 1920 × 1080 pixels is scaled to 550 × 325 or 500 × 295 pixels and the screen-door effect is present, the GUI significantly loses the intended look. To overcome this problem, one could design a GUI having only very simple control elements that use a lot of space. We tested such an approach in a usability test, where very basic GUIs were shown on a display in a real-world environment. The participants reported that it felt very “unnatural” to use such a GUI, e.g., where a button has the width and the height of about 10 % of the total display resolution. Therefore, we decided to design two different look and feels for the experiment, where great care was taken to present the same information by the two different GUIs. The differences are depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig3">3</a>, where screenshots of the two different GUIs are shown.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Screenshots taken from the real-world experiment software (<i>upper</i> image) and the VR experiment software (<i>lower</i> image)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In our system, the mouse is used to give responses by clicking on control elements shown by a screen of a virtual display. Another feature of our VR application is that mouse movements of the user in the real-world are also shown in the VR environment. The reason for integrating this feature is that many participants of a pilot experiment reported that the visual and auditory representation was very satisfying, but it just felt unnatural to move the mouse and not to have any visual feedback in the VR environment. In order to implement visual feedback, we used the <i>x</i>- and <i>y</i>-coordinates of the mouse cursor which could be retrieved from the operating system. The values of the coordinates were converted to a mm-scale of the real-world. For example, if the value of the <i>x</i>-coordinate was about the width of the screen, the virtual mouse was moved to the right end of the area where the virtual mouse was located. This basic approach has some limitations, so, e.g., if someone lifted or moved the mouse very far away, the location of the visual mouse would not correspond to the location of the real-world mouse. A more advanced approach would require tracking the coordinates of the real-world by a camera or sensor located inside the mouse, but results of another usability test indicated that the used approach would be sufficient for the final experiment.</p><h3 class="c-article__sub-heading" id="Sec10">Audio rendering</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">BRIR measurements</h4><p>In general, BRIRs are measured by placing a dummy head (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig4">4</a>) with two in-ear microphones at the listening position and playing back a stimulus (excitation signal) from a loudspeaker. The stimulus is recorded by the two microphones resulting in two signals from which the BRIRs are extracted. There are several approaches to do this, where each approach has its advantages and disadvantages. Nowadays, it is common practice to use a variant of the logarithmic-sweep method, since these kinds of methods provide excellent signal-to-noise ratios. The BRIRs that are used for rendering the auditory stimulus in the VR rooms were measured with a logarithmic-sweep method based on the approach proposed by Müller and Massarani (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Müller S, Massarani P (2001) Transfer-function measurement with sweeps. J Audio Eng Soc 49(6):443–471" href="/article/10.1007/s10055-015-0270-8#ref-CR30" id="ref-link-section-d38762e925">2001</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The dummy head used for the BRIRs measurement. On the <i>left</i>-hand side, the torso and a part of the four stepper motors can be seen. The <i>right</i>-hand side shows the dummy head while conducting a measurement. The lap mouse pad in front of the dummy head was also used in the real-world sessions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>For the VR environment, we measured BRIRs with a dummy head in both rooms at the same location where the virtual avatar of the participant would sit. In order to achieve satisfying immersion, we had to measure BRIRs at different head rotations. The reason for this is that the localization of sound sources (or spatial hearing in general) is strongly influenced by perceived differences between the ears. For example, the localization of sound sources is, among other factors, dependent on the so-called interaural time difference (ITD) and interaural level difference (ILD) (Palomäki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Palomäki KJ, Tiitinen H, Mäkinen V, May JC, Alku P (2005) Spatial processing in human auditory cortex: the effects of 3D, ITD, and ILD stimulation techniques. Cogn Brain Res 24(3):364–379" href="/article/10.1007/s10055-015-0270-8#ref-CR32" id="ref-link-section-d38762e956">2005</a>; Sandel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1955" title="Sandel TT, Teas DC, Feddersen WE, Jeffress LA (1955) Localization of sound from single and paired sources. J Acoust Soc Am 27:842–852" href="/article/10.1007/s10055-015-0270-8#ref-CR40" id="ref-link-section-d38762e959">1955</a>). The ITD is the difference in arrival time between two ears of two sounds. If a sound arrives earlier at the right ear, we instinctively assume that the sound is coming from the right. The ILD, sometimes also called interaural intensity difference (IID), is similar to the ITD, but describes the differences in loudness between the two ears. If a sound source is located at the front of the listener and the listener moves his or her head, the ITD and ILD change.</p><p>Since participants usually move their heads while sitting in an auditory experiment with loudspeakers, head movement must be supported by a VR system. Moreover, sound sources should be perceived as similar as possible to the way they are perceived in the real-world while moving. Therefore, we measured BRIRs at different head positions with a custom-made dummy head [designed by Hess and Weishäupl (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Hess W, Weishäupl J (2014) Replication of human head movements in 3 dimensions by a mechanical joint. In: Proceedings of the international conference on spatial audio (ICSA)" href="/article/10.1007/s10055-015-0270-8#ref-CR21" id="ref-link-section-d38762e965">2014</a>)] that supports neck and head movements in the three rotational degrees of freedom. The dummy head has a mechanical compression spring which allows a range of motion of ±30° for pitch and roll. Four stepper motors are used to tilt a base board holding the dummy head in which two microphones (DPA 4061) in the ear canals are incorporated for binaural measurements. This construction is placed on a rotatory actuator, a combination of stepper motor and turntable. For rotation in the horizontal plane, a full turn would be possible, but it is limited by software to ±90°. The dummy head consists of many parts, including motors and microphones, that influence the measured impulse responses. Great care was therefore taken to obtain impulse responses as authentic as possible (e.g., by powering off the motors during the sweep playback).</p><p>The dummy head was programmed to automatically move its head according to a list of configured head rotations. Moreover, the head was connected to the sound system of the room to be measured, allowing automatically triggering measurement at each position. The dummy head was configured to measure BRIRs with a length of 32,768 samples using a sample rate of 48,000 Hz. The resulting length of each impulse response was <span class="mathjax-tex">\(\frac{32,\!768}{48,\!000\,\text {Hz}} = 0.683\,\text {s}\)</span>. The range and resolution of head rotations measured were different for yaw, pitch, and roll. The dummy head yawed from −40° to 40° with a step size of 1°. Pitch movement ranged from −6° to 6°, and roll movement ranged from −3° to 3°. Both movements used a step size of 3°. The reason for introducing a wider range in yaw than in pitch and roll was that changes in yaw have a stronger influence on human spatial perception (Blauert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Blauert J (1997) Spacial hearing. The psychophysics of human sound localization. The MIT Press, Cambridge. ISBN: 978-262-02413-6" href="/article/10.1007/s10055-015-0270-8#ref-CR5" id="ref-link-section-d38762e1029">1997</a>), especially if a surround sound setup without height loudspeakers is used. The same applies to the step sizes used. The step size in yaw was chosen to be smallest due to its importance for the spatial perception. The maximum step sizes of 3° were chosen as they lead to adequate results and are a very good compromise regarding the time needed for measuring all BRIRs (Lindau et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lindau A, Maempel H, Weinzierl S (2008) Minimum BRIR grid resolution for dynamic binaural synthesis. J Acoust Soc Am 123(5):3498–3498" href="/article/10.1007/s10055-015-0270-8#ref-CR27" id="ref-link-section-d38762e1032">2008</a>). We recorded the 5.1 surround sound system that was installed in each room, resulting in six input channels.</p><p>The user experience would have been significantly improved if individual/person-specific BRIRs had been used (Väljamäe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Väljamäe A, Larsson P, Västfjäll D, Kleiner M (2004) Auditory presence, individualized head-related transfer functions, and illusory ego-motion in virtual environments. In: Proceedings of the 7th annual workshop on presence" href="/article/10.1007/s10055-015-0270-8#ref-CR61" id="ref-link-section-d38762e1039">2004</a>). When individual BRIRs are measured, the person itself, or an accurate replica of his or her ears, head, and torso, must be used during the measurement. Due to the high number of participants and the high number of sessions, using individual BRIRs was considered to be too time-consuming. Moreover, in total, 1215 different head rotations and six different channels were measured, which took about 10 h for one room. Therefore, if individual BRIRs had been measured, each person would have had to spend the same amount of time plus additional time for orienting the person’s head, since each measurement had to be accurate to much &lt;1°.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Convolution engine</h4><p>We implemented a convolution engine that allowed us to convolve any number of input signals and BRIRs (Schoeffler and Hess <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Schoeffler M, Hess W (2012) A comparison of highly configurable CPU- and GPU-based convolution engines. In: Audio engineering society convention no. 133, San Francisco, CA, USA" href="/article/10.1007/s10055-015-0270-8#ref-CR44" id="ref-link-section-d38762e1050">2012</a>). As a 5.1 surround sound format was used in the experiment, we configured the convolution engine to convolve six input channels with six BRIRs (filters) for each ear. The convolution engine used is based on a fast convolution, so multiplication in the frequency domain is applied [a detailed introduction into convolutions in frequency domain is given by Lathi and Green (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Lathi BP, Green RA (2014) Essentials of digital signal processing. Cambridge University Press, Cambridge. ISBN: 978-110-705932-0" href="/article/10.1007/s10055-015-0270-8#ref-CR25" id="ref-link-section-d38762e1053">2014</a>)]. The following equations show how our convolution engine works in detail.</p><p>The convolution engine is parameterized by the following attributes:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                                       <span class="mathjax-tex">\({\mathrm{s}} \in \{\text {left, right} \}\)</span> is the side of the headphone;</p>
                      </li>
                      <li>
                        <p>
                                       <span class="mathjax-tex">\({\mathrm{p}} \in \{ \text {left, right, center},\, \ldots \}\)</span> is the loudspeaker channel;</p>
                      </li>
                      <li>
                        <p>block number <i>n</i>;</p>
                      </li>
                      <li>
                        <p>
                                       <span class="mathjax-tex">\({\mathrm{r}}_n\)</span> is the position and rotation (in all three axes) of the listener’s head at block number <i>n</i>.</p>
                      </li>
                    </ul>
                           <p>The output signal <span class="mathjax-tex">\(y^{\mathrm{s}}\)</span> is dependent on the input signal <span class="mathjax-tex">\(x^{\mathrm{p}}\)</span> of indefinite length and the impulse response <span class="mathjax-tex">\(h^{{\mathrm{p,r}}_n{\mathrm{,s}}}\)</span> of finite length <span class="mathjax-tex">\(L_H\)</span>. Since the signals are block-wise processed, the input signal <span class="mathjax-tex">\(x^{\mathrm{p}}\)</span> is divided into blocks, where each block consists of a number of samples equal to the block length of the audio interface.</p><p>The input signal consists of blocks <span class="mathjax-tex">\(x_i^{\mathrm{p}}\)</span> with block length <span class="mathjax-tex">\(L_I\)</span>
                              </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$x^{\mathrm{p}} = \left[ x_0^{\mathrm{p}}, x_1^{\mathrm{p}}, x_2^{\mathrm{p}}, \ldots \right] ,$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>i</i> is the block index. Next, the filter <span class="mathjax-tex">\(h^{{\mathrm{p,r}}_n{\mathrm{,s}}}\)</span> is divided into <i>N</i> sub-filters <span class="mathjax-tex">\(h^{{\mathrm{p,r}}_n{\mathrm{,s}}}_j\)</span> (<span class="mathjax-tex">\(j \in \{0, 1, \ldots , N-1\}\)</span>) with length <span class="mathjax-tex">\(L_I\)</span>
                              </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$h^{\mathrm{p,r}_n{\mathrm{{,s}}}} = \left[h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_0,h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_1,h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_2, \ldots ,h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_{N-1}\right] .$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                           <p>
                              <span class="mathjax-tex">\(L_I\)</span> does not need to be a factor of <span class="mathjax-tex">\(L_H\)</span> since in case of <span class="mathjax-tex">\(N \cdot L_I &gt; L_H\)</span> zero-padding is applied</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_{N-1}[u] = \left\{ \begin{array}{ll} h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}[w + u] &amp;{}\quad {\text{if}}\, w + u &lt; L_H \\ 0 &amp;{}\quad {\text{otherwise}} \end{array}\right. , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <span class="mathjax-tex">\(w = (N-1)\cdot L_I\)</span> and <span class="mathjax-tex">\(u \in \{ 0, 1, \ldots , L_I -1\}\)</span>.</p><p>To prevent aliasing due to circular convolution, <span class="mathjax-tex">\(h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_j\)</span> is zero-padded to length <span class="mathjax-tex">\(2L_I\)</span>
                              </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$h^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_j = \left[ h^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_j, 0, 0, \ldots \right] .$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                           <p>The signal block <span class="mathjax-tex">\(x^{\prime{\mathrm{p}}}_i\)</span> is expanded to length <span class="mathjax-tex">\(2L_I\)</span>
                              </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} x^{\prime {\mathrm{p}}}_i = \left\{ \begin{array}{ll} \left[ 0, 0, \ldots , 0, x^{\mathrm{{p}}}_i\right] &amp;{}\quad {\text{if}}\, i = 0 \\ \left[ x^{\mathrm{p}}_{i-1}, x^{\mathrm{p}}_i\right] &amp;{}\quad {\text{otherwise}} \end{array}\right. , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <span class="mathjax-tex">\(i \in \{0, 1, 2, \ldots \}\)</span>.</p><p>Next, the filter and the input signal are transformed into the frequency domain</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$H^{{\mathrm{p,r}}_n{\mathrm{,s}}}_j = \hbox {FFT} \left\{ h^{\prime {\mathrm{p,r}}_n{\mathrm{,s}}}_j\right\} \quad {\text{and}}\quad X^{\mathrm{p}}_i = {\mathrm{FFT}}\left\{ x^{\prime {\mathrm{p}}}_i \right\} .$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                           <p>Since the filtering is linear, successive blocks can be filtered with the corresponding subfilter one at a time and the output blocks are fitted together to form the overall signal</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$y_n^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}} = \hbox {IFFT}\left\{ \sum _{k = 0}^{N-1} X^{\mathrm{{p}}}_{n-k} \cdot H^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_k \right\}.$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>
                              <span class="mathjax-tex">\(X^{\mathrm{{p}}}_{n-k}\)</span> is set to zero if <span class="mathjax-tex">\(n-k &lt; 0\)</span>.</p><p>
                              <span class="mathjax-tex">\(y_n^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}}\)</span> has length <span class="mathjax-tex">\(2L_I\)</span>. While the first half is the overlapping part of the convolution and is discarded, the second half contains the required output block of one channel <span class="mathjax-tex">\({\mathrm{{p}}}\)</span> at one position <span class="mathjax-tex">\({\mathrm{{r}}}_n\)</span>
                              </p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$y^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_n = \left[ y_n^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}}[L_I], y_n^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}}[L_I+1], \ldots , y_n^{\prime {\mathrm{{p,r}}}_n{\mathrm{{,s}}}}[2L_I-1]\right] .$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                           <p>Due to the fact that there are measurements of impulse responses for only a finite number of rotations r, the closest r to the actual rotation is chosen. If the value of the current r, <span class="mathjax-tex">\({\mathrm{{r}}}_n\)</span>, is different from the previous <span class="mathjax-tex">\({\mathrm{{r}}}_{n-1}\)</span>, the output signal <span class="mathjax-tex">\(y_n^{\mathrm{{s}}}\)</span> is computed by a squared cosine cross-fading of <span class="mathjax-tex">\(y^{{\mathrm{{p,r}}}_{n-1}{\mathrm{{,s}}}}_n\)</span> and <span class="mathjax-tex">\(y^{{\mathrm{{p,r}}}_{n}{\mathrm{{,s}}}}_{n}\)</span>
                              </p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$y^{{\mathrm{{p,r}}}_{n-1}{\mathrm{{, s}}}}_{n{\mathrm{{, weighted}}}}[u]= y^{{\mathrm{{p,r}}}_{n-1}{\mathrm{{,s}}}}_n[u]\cos ^2\left( \frac{\pi \cdot u}{2 L_I}\right)$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
                              <div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$y^{{\mathrm{{p,r}}}_{n}{\mathrm{{, s}}}}_{n{\mathrm{{, weighted}}}}[u]= y^{{\mathrm{{p,r}}}_{n}{\mathrm{{,s}}}}_{n}[u]\left( 1-\cos ^2\left( \frac{\pi \cdot u}{2 L_I}\right) \right)$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>where <span class="mathjax-tex">\(u\in \{0,1,\ldots , L_I-1\}.\)</span>
                           </p><p>Finally, the output signal is calculated</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} y^{\mathrm{{s}}}_n = \left\{ \begin{array}{ll} \frac{1}{P} \sum _{\mathrm{{p}}} y^{{\mathrm{{p,r}}}_n{\mathrm{{,s}}}}_n &amp;{}\quad {\text{if}}\, {\mathrm{{r}}}_{n-1} = {\mathrm{{r}}}_n \\ \frac{1}{P} \sum _{\mathrm{{p}}} y^{{\mathrm{{p,r}}}_{n-1}{\mathrm{{, s}}}}_{n{\mathrm{{, weighted}}}} + y^{{\mathrm{{p,r}}}_n{\mathrm{{, s}}}}_{n{\mathrm{{, weighted}}}} &amp;{}\quad {\text{otherwise}} \end{array}\right. , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <i>P</i> is the number of loudspeaker channels.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Experiment</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Method</h3><p>The purpose of the experiment was to test Hypothesis I (“The OLE ratings given in the VR environment do not differ from those given in the real-world experiment”) and Hypothesis II (“The OLE ratings that were given while being present in the (real-world) cinema do not differ from those given in the (real-world) listening booth.”). To this end, participants joined five sessions, where they rated the OLE of music excerpts. In the first session (basic item session), they rated the OLE of music stimuli in a “neutral room” with headphones. These ratings were expected to reflect how much a participant likes a specific stimulus without being influenced by the room or the environment. In the other four sessions, participants sat in the cinema or listening booth either in the VR or in the real-world environment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Participants</h4><p>Thirty participants (24 males, 6 females) volunteered to participate in the experiment. Twenty participants reported an age between 20 and 29 years, nine reported being between 30 and 39 years old, and one participant reported being between 40 and 59 years old. Twenty participants identified themselves as professionals in audio (audio researchers, audio engineers, Tonmeisters, etc.). Twenty-seven participants were familiar with listening tests and indicated that they had volunteered for at least one listening test before. Twelve participants reported that they regularly play computer games for more than 1 h a week.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Stimuli</h4><p>Two sets of stimuli were used in the experiment. The first set was used for giving some indication of how much the responses of listening-room- and reproduction-system-dependent attributes differ between the VR and real-world environments. The first set contained the following stimuli: <dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>
                          <i>pink noise</i>
                        :</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>The pink noise signal (peak = −10.4 dB, crest factor = 12.3 dB) had a duration of 10 s and was rendered in mono. It was thus played back by the center channel of the surround systems.</p>
                        </dd><dt class="c-abbreviation_list__term"><dfn>
                          <i>castanets</i>
                        :</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>The castanets signal was recorded dry and had a duration of about 7 s. The castanets recording was mixed in stereo. It was thus played back by the left and the right channels.</p>
                        </dd><dt class="c-abbreviation_list__term"><dfn>
                          <i>drums</i>
                        :</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>The drums recording was a 7-s-long beat where mainly the bass drum, toms, and cymbals were played. The drums recording was also mixed in stereo.</p>
                        </dd></dl>
                           </p><p>The second set of stimuli was used to rate the OLE and contained fifteen music excerpts of songs of various genres (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0270-8#Tab1">1</a>). The songs were obtained from the “Mercedes-Benz Signature Sound” DVD and the “BR Klangdimensionen” DVD. The excerpts had a duration of about ten seconds, and mainly covered the most recognizable part of the song (e.g., the refrain). The stimuli of the second set were originally mixed in 5.1 surround sound. In the first session, the basic item session, the participants used headphones, so stereo down-mixes of the stimuli were played back.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Selected music excerpts of the experiment</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0270-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In order to have all stimuli within the same range of loudness, an EBU-R128 loudness normalization was applied (European Broadcasting Union <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="European Broadcasting Union (2011) Practical guidelines for production and implementation in accordance with EBU R 128 (version 2.0). European Broadcasting Union, Geneva" href="/article/10.1007/s10055-015-0270-8#ref-CR13" id="ref-link-section-d38762e4681">2011</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Materials and apparatus</h4><h5 class="c-article__sub-heading" id="Sec22">Software infrastructure</h5><p>The undertaking of the experiment required a complex software infrastructure since many components were involved and sessions had to be taken by participants in parallel. The main reason for parallel sessions was that in total, 150 sessions (30 participants × 5 sessions) had to be supervised and the rooms could only be booked for a limited period of time. The software infrastructure mainly consisted of the VR application, which was already described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec7">3</a>, and a Web application, which was used for the basic item sessions and the two real-world sessions. An overview of the complete software infrastructure as a UML component diagram is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Software architecture of all systems used in the experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>The Web application is mainly a framework that was developed at the International Audio Laboratories Erlangen for the purpose of easily and time-efficiently creating GUIs for experiments by using Web technologies. The Web application was deployed in two variants: The first variant was used in the basic item session, and the second variant was used in the two real-world sessions. The difference between the two variants was that the basic item session had a totally different procedure than the two real-world sessions.</p><p>All three applications accessed the same data source containing the music excerpts that were used as stimuli in the experiment. The Web application of the basic item sessions retrieved the stereo down-mixes as they were used during the first session. The other two applications fetched the original 5.1 surround sound mixes. The responses of the participants and all other data needed were stored into and retrieved from a MySQL database. Furthermore, the database component executed consistency checks, e.g., checking that the same session was not performed twice by a participant and confirming that participants took the sessions in the intended order.</p><h5 class="c-article__sub-heading" id="Sec120">Professional listening room</h5><p>The participants sat in a professional listening room during the basic item session and the VR sessions.</p><p>In the basic item session, participants sat at a desk, upon which a 24<span class="mathjax-tex">\(^{\prime \prime }\)</span> widescreen LCD monitor, mouse, and keyboard were placed. The monitor displayed the experiment software. The participants used Beyerdynamics DT 770 PRO headphones connected to a LAKE People Phone AMP G109 amplifier.</p><p>The audio equipment used for the VR environment sessions is described in the next paragraph.</p><h5 class="c-article__sub-heading" id="Sec121">Virtual environment</h5><p>The VR sessions were conducted on a high-end PC on which the VR application was running. A PreSonus Audio Box 44VSL sound interface was connected to the PC. Participants used electrostatic headphones (Stax SR-507) driven by a Stax SRM 600 amplifier which was connected to the PreSonus sound interface. The measured BRIRs were post-processed to compensate for the headphone’s transfer function by inverse filtering.</p><p>As mentioned before, the Oculus Rift HMD was used to render the images of the virtual scene and to track head positions of the participants. The display of the Oculus Rift had a resolution of 960 × 1080 pixels per eye and could be set to a refresh rate of 60, 72, and 75 Hz. For the virtual sessions, we set the refresh rate to 75 Hz. The orientational head-tracking is based on data from a gyroscope, accelerometer, and magnetometer and has an update rate of 1000 Hz.</p><p>The Oculus Rift also supports positional tracking which was not fully enabled in our VR application, since we measured only BRIRs with different orientational positions. The VR application allowed only translational movements of ±10 cm in normal direction of the median, frontal, and horizontal planes. If a participant moved his or her head more than ±10 cm in any of these axes, the virtual avatar of the participant stopped moving its head. In previous usability tests, stopping the movement was well recognized as a feedback that translational movements are not fully supported, and the limit of 10 cm turned out to be a good trade-off.</p><p>The stereoscopic effect of the Oculus Rift can be optimized by configuring the interpupillary distance (IPD) of a person. Since it would have been too time-consuming to measure the individual IPD of each participant, we defined and used only two profiles (male and female). The male profile had an IPD of 64.5 mm, and the female profile had an IPD of 62.5 mm.</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig7">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig8">8</a> depict the VR listening booth and the VR cinema. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig6">6</a> comprises two screenshots that show how the virtual scenes of the cinema and listening booth were rendered on the display of the Oculus Rift.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Screenshots of the virtual scenes as they were displayed on the Oculus Rift. The <i>upper</i> screenshot shows the VR cinema, and the <i>lower</i> screenshot shows the VR listening booth. Each screenshot shows two views of the scene: the <i>left</i> one rendered for the <i>left</i> eye, and the <i>right</i> one rendered for the <i>right</i> eye</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div> 
                                 <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The <i>upper</i> image shows the real-world cinema and the <i>lower</i> image shows the VR cinema</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div> 
                                 <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The <i>left</i> image shows a picture of the real-world listening booth. The <i>right</i> image shows the corresponding picture from the VR listening booth</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <h5 class="c-article__sub-heading" id="Sec140">Listening booth</h5><p>The listening booth had room measurements of 1.81 × 2.44 × 2.07 m and provided enough space for one participant. A desk was placed inside the listening booth on which a monitor, keyboard, and mouse were placed. The monitor was used to display the experiment software. Inside the listening booth, a 5.1 surround sound system was installed with five Genelec 8030 APM speakers and one Genelec 7050B subwoofer. The speakers and the subwoofer were controlled by a SPL Surround Monitor Controller (Model 2489).</p><p>The reverberation times of the listening booth are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig9">9</a>. A picture of the listening booth is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig8">8</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Reverberation time (RT 60) of the listening booth and the cinema</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <h5 class="c-article__sub-heading" id="Sec163">Cinema</h5><p>The cinema had room measurements of 12.3 × 8.75 × 5.6 m (height measured at stage) and a capacity of 70 seats (seven rows with ten seats each). The cinema had multiple sound systems installed. We used the 5.1 surround sound system which is based on the Alcons CRMS (Cinema Ribbon Monitor) integrating a three-way main channel (left/center/right) and a two-way surround sound system. Since the system must provide an acoustical sweet spot wide enough for an entire audience, six loudspeakers are used for each surround sound channel. The aspect ratio of the cinema screen was set to 16:9, and room lighting was rather darkened. As in the VR cinema, every participant sat at the fifth seat in the fourth row. Since participants had to use a physical mouse during the experiment, a mouse pad was provided that could be laid on the lap.</p><p>The reverberation times of the cinema are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig9">9</a>. A picture of the cinema is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig7">7</a>.</p><h5 class="c-article__sub-heading" id="Sec170">Loudness equalization</h5><p>Since the loudness level of music being played back might have a significant impact on the listening experience, the loudness was equalized between the real-world environment and the VR environment. A pink noise stimulus (peak = −0.7 dB, crest factor = 12.8 dB) was used for all loudness measurements. The stimulus was recorded with a dummy head (Cortex Manikin MK1) which had one microphone installed in each ear.</p><p>In the first session, the participants listened to stereo stimuli with headphones. The loudness of the playback system was calibrated to 80 dBA SPL for each microphone.</p><p>The loudness of the virtual environment and the real-world environment was measured separately for each channel. The left and surround left channels were measured by the microphone installed in the left ear of the dummy head. The other channels (right, center, LFE, and surround right) were measured by the right ear of the dummy head. The loudness of each channel was calibrated to 70 dBA SPL, except the LFE channel which was calibrated to 50 dBA SPL. The main reason for calibrating the LFE channel to a lower loudness was that the LFE channel plays back only the lower frequency range of a signal. High volume in such a low frequency range leads to audible artifacts when reproduced by headphones.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Procedure</h4><p>The participants had to accomplish five sessions in total. An overview of the procedure is depicted as a UML activity diagram in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig10">10</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>The procedure of the experiment depicted as a UML activity diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The first session, the basic item session, took place in the professional listening room. Participants sat in front of a computer that ran the experiment software. All instructions were presented by the experiment software and participants responded only by using the experiment software. In the basic item session, headphones were used for listening to the stimuli. At the beginning of the session, participants filled out a questionnaire. They were asked their gender, their age group, whether they are professionals in audio (e.g., sound engineers, audio researchers), and about their familiarization with listening tests. In addition, we asked them whether they play computer games more than 1 h a week and whether they listen via headphones for more than 1 h a week.</p><p>The reason for asking the latter two questions was that the whole procedure of the VR sessions probably slightly resembles computer games. Therefore, participants who often play computer games might find it easier to interact with the VR environment. The last question about the headphones was asked because one major difference between the real-world and the VR environments is that participants wore headphones in the VR environment. In case headphones would have decreased the degree of immersion, we wanted to check whether this is especially true for participants that are not used to wearing headphones.</p><p>After answering all questions, participants read the instructions about rating the OLE of music excerpts subsequently presented. The instructions stated that participants are asked to rate each music excerpt according to how much they enjoy listening to it. In addition, it was emphasized that we were interested in participants’ personal opinion and that they should take everything into account in their rating that they would take into account in a real-world (nonexperiment) scenario. The reason for giving the participants this additional instruction is that the majority of listening tests conducted at our institute are about assessing audio quality. In order to avoid that a participant accidentally rates the audio quality of the items, we added this additional instruction.</p><p>After reading the instructions, participants were asked to rate the OLE of 14 music excerpts in a multi-stimulus comparison, which means that all music excerpts were presented on the same screen. As mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec16">4.1.2</a>, the stereo down-mixes of the stimuli were played back in this session. The order in which the music excerpts were presented was randomized. The question that was asked to the participants was “How much do you enjoy listening to each item?”. Participants responded by using a five-star Likert scale which was labeled with “not at all,” “not a lot,” “average,” “much,” and “very much.” Each music excerpt could be played back as often as desired. In order to rate a music excerpt, participants had to completely play back the music excerpt at least once. In the remainder of the paper, the ratings retrieved from the first session are called <i>basic item ratings</i>.</p><p>Next, the participants filled out another questionnaire where feedback could be given to the experimenters. At the end of the first session, each participant had to spend a few minutes trying out a demo application of the Oculus Rift. The purpose of playing around with the Oculus Rift was to familiarize the participants to better avoid cybersickness during the VR sessions. Furthermore, we tested if each participant could clearly see using either the male or female IPD profile.</p><p>The basic item session was followed by another four sessions: one session in the real-world cinema, one in the real-world listening booth, one in the VR cinema, and one in the VR listening booth. Between each session, participants had to take a break of at least a few hours. If possible, the next session was taken 1 day after the previous session. The reason for having these long breaks was that participants might get annoyed or bored when they listen to the same song several times over a short period of time. The four sessions had almost the same procedure. One major difference between the real-world sessions and the VR sessions was that the GUI of the experiment software had a different look and feel, which was already mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec9">3.2</a>. The real-world cinema and real-world listening booth sessions took place in the respective rooms. The VR sessions took place in the professional listening room which was also used for the first session.</p><p>In all four sessions, the experimenter was not in the room, so all instructions were given by the experiment software and participants gave all responses using the experiment software. The experimenters were only present at the beginning of the VR sessions to help the participants setting up the Oculus Rift and to make sure that they wore the headphones with the correct orientation.</p><p>Each session was divided into two parts. In the first part, questions were asked that were related to room acoustics. In the second part, participants rated the OLE of music excerpts.</p><p>At the beginning of each of the four sessions, instructions were shown to the participants. The instructions gave some information that the following questions are related to the room acoustics and the perception of sound. The first question asked was how loud they perceive the presented stimulus (pink noise). As with the other stimuli, participants were allowed to play back the stimulus as often as desired. To report the loudness, participants used a Likert scale with the values “very quiet,” “quiet,” “normal,” “loud,” and “very loud.” Next, the stimulus “castanets” was presented, and participants were asked how much reverb the room had. The question about reverb was answered on a Likert scale with the values “none,” “a little,” “average,” “much,” and “very much.” Subsequently, the participants had to indicate how far away they perceive a stimulus (pink noise). They reported the location by a Likert scale with the values “very near,” “near,” “average,” “far,” and “very far.” Then, two questions followed where participants were asked how much they like the bass and the treble of a stimulus (drums). The participants gave both ratings on a Likert scale with the values “not at all,” “not al lot,” “average,” “much,” and “very much.” When the last question was answered, the second part of the session started.</p><p>Again, instructions were shown to the participants which were very similar to the instructions shown in the basic item session. They were instructed to rate the OLE of short music excerpts, and it was again emphasized that we were interested in participants’ personal opinion. Next, the fourteen music excerpts were presented in a single-stimulus comparison, i.e., each music excerpt was separately rated. The question we asked to the participants was “How much do you enjoy listening to this item?”. Participants gave their responses on the same five-star Likert scale used in the first session. In order to rate a music excerpt, participants had to completely play back the music excerpt at least once. A screenshot of the GUI as it was used in the real-world sessions and in the VR sessions is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig3">3</a>. In case the session was a VR session, participants were asked whether they felt dizzy while doing the session. This question could be answered with “no,” “a bit,” or “yes.”</p><h3 class="c-article__sub-heading" id="Sec19">Results</h3><p>The most important dependent variable, item rating, is strictly speaking an ordinal variable. Therefore, the hypotheses are mainly verified by nonparametric statistics. Nevertheless, due to predominant use of parametric statistics and since the item ratings can also be interpreted as an interval or ratio variable (number of stars), parametric statistics might additionally be used to confirm the results of the nonparametric statistics. Moreover, since significance levels do not provide enough information about the practical or theoretical importance of an effect, effect sizes are also reported (Fritz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Fritz CO, Morris PE, Richler JJ (2012) Effect size estimates: current use, calculations, and interpretation. J Exp Psychol Gen 141(1):2–18" href="/article/10.1007/s10055-015-0270-8#ref-CR15" id="ref-link-section-d38762e5034">2012</a>). Throughout the paper, Pearson’s <i>r</i> is used as an effect size (Fritz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Fritz CO, Morris PE, Richler JJ (2012) Effect size estimates: current use, calculations, and interpretation. J Exp Psychol Gen 141(1):2–18" href="/article/10.1007/s10055-015-0270-8#ref-CR15" id="ref-link-section-d38762e5040">2012</a>) when nonparametric statistics (e.g., Wilcoxon signed-rank test) were applied. The values of <i>r</i> can vary from −1 to 1: −1 indicating a perfect negative relation, 1 indicating a perfect positive relation, and 0 indicating no relation between two variables. The strength of an effect is interpreted according to Cohen’s guidelines (Cohen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Cohen J (1988) Statistical power analysis for the behavioral sciences. Lawrence Erlbaum, Hillsdale. ISBN: 978-080-580283-2" href="/article/10.1007/s10055-015-0270-8#ref-CR10" id="ref-link-section-d38762e5046">1988</a>), with <span class="mathjax-tex">\(r=0.1\)</span> recognized as a weak effect, <span class="mathjax-tex">\(r=0.3\)</span> as a moderate effect, and <span class="mathjax-tex">\(r=0.5\)</span> as a strong effect. In the case that parametric statistics (e.g., paired <i>t</i> test) were applied, Cohen’s <i>d</i> is used as an effect size metric. The meaning of the effect size value is interpreted according to the standard interpretation: <span class="mathjax-tex">\(d=0.2\)</span> is a small effect, <span class="mathjax-tex">\(d=0.5\)</span> is a moderate effect, and <span class="mathjax-tex">\(d=0.8\)</span> is a strong effect.</p><p>Participants needed on average 6.7 min (SD = 2.2<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>) for the basic item session. A single real-world session took on average 6.9 min (SD = 2.2) and a single VR session on average 7.4 min (SD = 1.1). All item ratings retrieved from the participants are visualized by a frequency plot shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig11">11</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Relative frequencies of the basic item ratings retrieved from the first session and the item ratings retrieved from the other four sessions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        
                  <h3 class="c-article__sub-heading">
                    <b>Hypothesis I</b>
                  </h3>
                  <p> In order to test the first hypothesis (“The OLE ratings given in the VR environment do not differ from those given in the real-world experiment”), differences between the item ratings retrieved from the real-world sessions and from the VR sessions are investigated in more detail. The mean absolute difference is 0.51 stars (SD = 0.62) between item ratings given in the VR sessions and item ratings given in the real-world sessions. A Wilcoxon signed-rank test is applied to test whether the differences between the ratings are statistically significant<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> (Wilcoxon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1945" title="Wilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83" href="/article/10.1007/s10055-015-0270-8#ref-CR68" id="ref-link-section-d38762e5283">1945</a>). The Wilcoxon signed-rank test indicated that the ratings given in the VR environment were statistically significantly lower than ratings given in the real-world environment (<i>Z</i> = −3.623, <i>p</i> = <span class="mathjax-tex">\(&lt;\)</span>.001). However, the effect size of the differences is very low <span class="mathjax-tex">\(r = \frac{Z}{\sqrt{N}} = -0.088\)</span>. A paired <i>t</i> test results in the same outcome since the ratings were significantly different [<i>t</i>(839) = −3.6, <i>p</i> = <span class="mathjax-tex">\(&lt;\)</span>.001]. Additionally, the weak effect is confirmed by Cohen’s <i>d</i> = −0.10. To confirm the weak negative effect of the VR environment, a cumulative link model (CLM) was calculated. A CLM is a modification of a regression model for ordinal dependent variables (Agresti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Agresti A (2002) Categorial data analysis, 2nd edn. Wiley, New York. ISBN: 0-471-36093-7" href="/article/10.1007/s10055-015-0270-8#ref-CR1" id="ref-link-section-d38762e5385">2002</a>). The calculated CLM predicts item ratings based on the corresponding basic item ratings, the room and the environment. Additionally, the CLM checked for an interaction effect between room and environment. As one can see in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0270-8#Tab2">2</a>, the VR environment has a very weak and nonsignificant effect on the item ratings. Furthermore, a negative interaction effect is detected for the VR environment in combination with the listening booth. This interaction effect is not significant for the chosen significance level <span class="mathjax-tex">\(\alpha\)</span>. In conclusion, according to our results, Hypothesis I must be rejected: OLE ratings obtained from the VR sessions are significantly lower than the ratings obtained from the real-world sessions.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Logit cumulative link model of the item ratings</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0270-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           
                
                  <h3 class="c-article__sub-heading">
                    <b>Hypothesis II</b>
                  </h3>
                  <p> For testing the second hypothesis (“The OLE ratings that were given while being present in the (real-world) cinema do not differ from those given in the (real-world) listening booth”), the differences between the ratings given in different rooms are investigated. Overall, the mean of the absolute difference between the item ratings given in the cinema and given in the listening booth was 0.53 stars (SD = 0.62). A Wilcoxon signed-rank test (<i>Z</i> = 1.937, <i>p</i> = .054) indicated that the difference between ratings given in the listening booth and in the cinema is not significant. The effect size of the differences is very low <span class="mathjax-tex">\(r = 0.067\)</span> (slightly positive effect for the listening booth). When analyzing the ratings of both rooms by a paired <i>t</i> test, the difference turns out to be just statistically significant [<span class="mathjax-tex">\(t(419)=1.98\)</span>, <span class="mathjax-tex">\(p=.048\)</span>). In accordance with the effect size <i>r</i>, Cohen’s <i>d</i> also indicates a very weak effect of the listening room (<span class="mathjax-tex">\(d=0.078\)</span>). The cumulative link model (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0270-8#Tab2">2</a>) confirms both results by showing a slightly positive effect of the listening booth. Moreover, in the cumulative link model, the effect size of the listening booth is not statistically significantly different from zero. In conclusion, based on the results of the Wilcoxon signed-rank test and the cumulative link model, Hypothesis II is accepted. Based on the results of the paired <i>t</i> test, Hypothesis II is rejected, but the effect size is very weak; so, practical importance of the differences is doubtful.</p>
                <p>As mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0270-8#Sec1">1</a>, we added some questions related to the room acoustics and the spatial perception of the participants. The relative frequency plots of the answers to these questions are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig12">12</a>. For each of these additional questions, a Wilcoxon signed-rank test was applied on the ratings given in the VR environment and on the ratings given in the real-world environment. Since the responses to these additional questions were not the main subject of interest, the independent variable room is not evaluated in detail. The answers to the question “How loud is this item?” indicated that the VR environment was statistically not significantly rated more quiet than the real-world environment (<span class="mathjax-tex">\(Z = -1.988, p=.064, r=-0.181\)</span>). According to the answers to question “How much reverb has this room?”, participants indicated that they perceived more reverb in the VR environment than in the real-world environment (<span class="mathjax-tex">\(Z = 0.482\)</span>, <span class="mathjax-tex">\(p=.649\)</span>, <span class="mathjax-tex">\(r=0.044\)</span>). The stimuli of question “How far away do you perceive the sound?” were perceived further away in the VR environment than in the real-world environment (<span class="mathjax-tex">\(Z = 1.028\)</span>, <span class="mathjax-tex">\(p=.345\)</span>, <span class="mathjax-tex">\(r=0.094\)</span>). The answers to the question “How much do you like the bass of this item?” indicated that the bass was less liked in the VR environment than in the real-world environment (<span class="mathjax-tex">\(Z = -2.986\)</span>, <span class="mathjax-tex">\(p=.003\)</span>, <span class="mathjax-tex">\(r=-0.273\)</span>). The treble was also slightly less liked in the VR environment than in the real-world environment (<span class="mathjax-tex">\(Z = -0.674\)</span>, <span class="mathjax-tex">\(p=.600\)</span>, <span class="mathjax-tex">\(r=-0.061\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0270-8/MediaObjects/10055_2015_270_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Relative frequencies of the responses to the questions about room acoustics and spatial perception</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0270-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>After each VR session, the participants were asked whether they felt dizzy while taking part in the session. The participants answered 34 times with “no” (56.67 %), 19 times with “a bit” (31.67 %), and 7 times with “yes” (11.67 %).</p><h3 class="c-article__sub-heading" id="Sec20">Discussion</h3><p>The experiment revealed some differences between the real-world environment and the VR environment. First of all, participants needed more time in the VR sessions (M = 7.4 min) than in the real-world sessions (M 6.9 min). Many participants reported that, especially at the beginning of the VR sessions, they spent a few moments to just look around and to examine the VR rooms. Spending some time just for examining the VR environment might not have a strong effect on the results of an experiment. The perfect outcome for the VR sessions would have been if the participants had not behaved significantly differently from the real-world sessions, meaning that they would have needed approximately the same amount of time in both environments. This might have been achieved by additional training sessions in the same rooms that were later used in the VR sessions. Such additional training sessions would have allowed participants to familiarize themselves with the virtual rooms. These were not included because then additional training sessions should have been included for the real-world environment, too, in order to treat the two environments equally. Moreover, adding more training sessions would have been too time-consuming for the participants.</p><p>The first hypothesis had to be rejected as the item ratings retrieved from the VR environment turned out to be significantly lower than the ratings retrieved from the real-world environment. However, the effect size of the VR environment was found to be very weak. In particular, ratings given in the VR listening booth were lower than ratings given in the other sessions. Regarding this issue, participants reported that the instructions and control elements in the VR listening booth were harder to read than in the VR cinema, which might have had an influence on their overall enjoyment. One reason for the limited readability is that the VR listening booth was brighter and more colorful than the VR cinema, having the consequence that the “screen-door effect” was more apparent. The VR cinema was a bit darkened, so the black lines separating the display’s pixels were less perceivable. Furthermore, displaying a more colorful and brighter room, the limited resolution of the Oculus Rift becomes more perceivable. Another reason was that the virtual monitor of the listening booth scene was much closer to the participants than the stage of the VR cinema. Participants described the greater distance to the VR cinema stage as much more relaxing compared to the shorter distance to the monitor in the VR listening booth. Based on the weak effect sizes, we conclude that our results are consistent with those of Bella’s experiment (Bella <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bella F (2004) Driving simulation in virtual reality for work zone design on highway: a validation study. In: The second SIIV international congress, Florence, Italy" href="/article/10.1007/s10055-015-0270-8#ref-CR3" id="ref-link-section-d38762e6503">2004</a>), where the VR environment and the real-world environment also had a weak effect on the experiment’s results. In summary, although the first hypothesis was rejected, we conclude that VR environments are suitable for experiments related to OLE since we found only minor differences between the VR and the real-world environments.</p><p>Based on results of a Wilcoxon signed-rank test and a cumulative link model, the second hypothesis had to be accepted since ratings given in the real-world listening booth were nonsignificantly higher than the ratings given in the real-world cinema. One reason for the slightly favored listening booth might be that the sound emitted from the surround channels was much better perceived in this room. In the cinema, the surround left and surround right channels are displayed by a large loudspeaker array where some loudspeakers were located at the front left and front right of the participant. Thus, the surround channels could be perceived as additional front channels. Moreover, the majority of the music excerpts were mixed as foreground–background mixes, meaning that mainly ambient signal-parts were emitted by the surround loudspeakers. In addition, the loudness of the background was low compared to the foreground. Altogether, the music excerpts sounded more like stereo mixes in the cinema. As shown by a previous experiment, surround sound strongly enhances the OLE, which might be the reason for the lower ratings (Schoeffler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Schoeffler M, Conrad S, Herre J (2014b) The influence of the single/multi-channel-system on the overall listening experience. In: Proceedings of the AES 55th conference on spatial audio, Helsinki, Finland" href="/article/10.1007/s10055-015-0270-8#ref-CR48" id="ref-link-section-d38762e6509">2014b</a>). However, music excerpts auditioned in the listening booth were not rated much higher than music excerpts listened to in the cinema. That surround sound is only slightly higher rated than stereo when rated in two different sessions has already been demonstrated by Schoeffler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014a" title="Schoeffler M, Adami A, Herre J (2014a) The influence of up- and down-mixes on the overall listening experience. In: Proceedings of the AES 137th convention, Los Angeles, CA, USA (preprint 9140)&#xA;                        " href="/article/10.1007/s10055-015-0270-8#ref-CR47" id="ref-link-section-d38762e6512">2014a</a>).</p><p>The additional questions we asked to the participants were answered quite differently depending on the room and environment (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0270-8#Fig12">12</a>). As already mentioned a few times before, analysis of these answers must be interpreted with great care since this part of the experiment is rather informal and these questions were added with the purpose of giving some indications for future experiments.</p><p>The loudness was equalized to the same level for all rooms and environments. The fact that participants would equally rate the apparent loudness was therefore more or less expected. However, in the real-world listening booth, participants rated the loudness as “loud” more often than in the other rooms. Unfortunately, no feedback from the participants addressed this issue, so we can only guess what the reasons could be. Loudness is perceived subjectively and dependent on many factors. For example, differences in perceived loudness between the real-world cinema and real-world listening booth can be explained by the findings of Mershon et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Mershon DH, Desaulniers DH, Kiefer SA Jr, Amerson TL, Mills JT (1981) Perceived loudness and visually-determined auditory distance. Perception 10(5):531–543" href="/article/10.1007/s10055-015-0270-8#ref-CR29" id="ref-link-section-d38762e6525">1981</a>), who conducted an experiment to investigate the relationship between distance and perceived loudness. They found out that by increasing the distance but keeping the same loudness level, the loudness is perceived louder. In the listening booth, the speakers were located much closer to the participants than in the real-world cinema, so the loudness in the listening booth should be perceived to be quieter according to the findings of Mershon et al. However, our results are in contrast to those findings. Moreover, no other studies exist to our knowledge that investigated the loudness-to-distance relationship related to our scenario. Therefore, the differences in loudness ratings cannot be conclusively clarified. Another issue is that in the VR environment, there is almost no difference between the answers, indicating that the loudness-to-distance relationship is not present in VR environments. In order to test this hypothesis, another VR experiment focusing on the effect of distance on the loudness must be performed.</p><p>The questions about the amount of reverb and the distance of the sound source were answered quite differently. Therefore, no conclusions can be drawn based on the answers to these two questions.</p><p>Participants differently answered the question about how much the bass was liked when listening to the <i>drums</i> stimulus. In particular, the bass was rated significantly higher in the real-world environment. The reason for this might be that the stimulus was reproduced by headphones in the VR environment and by loudspeakers in the real-world environment. Loudspeakers have the advantage of reproducing the bass of a stimulus more powerful than headphones. When participants rated the treble, there was no significant effect of the environment on the ratings. In contrast to reproducing the bass, headphones do not have such limitations with the treble.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusion</h2><div class="c-article-section__content" id="Sec21-content"><p>A VR system was developed that allows creation of virtual scenes of experiments. The system renders the auditory stimuli by utilizing a set of BRIRs which is selected according to the user’s head position. An experiment was implemented using the system, where ratings given in a VR environment were compared to ratings given in a real-world environment. In the experiment, participants rated the OLE of music excerpts while being in a cinema and a listening booth. For each room, the participants rated the music excerpts while being present in either the real-world or the VR. Comparison of the results indicates that the ratings associated with the VR are slightly lower than the ratings retrieved from the real-world. In the real-world environment, music excerpts were rated slightly higher in the listening booth than in the cinema. In order to contribute to the validity of VR auditory experiments in general, future experiments must investigate other dependent perceptional variables such as sound quality, reverberation, loudness, and distance.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>In this paper, the term <i>real-world experiment</i> is used to describe experiments that were conducted under laboratory conditions in the real-world (and not in the VR). Sometimes this term is also used as a synonym for the term <i>field experiment</i>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>M = mean, SD = standard deviation, <i>N</i> = number of samples.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>The significance level <span class="mathjax-tex">\(\alpha\)</span> is set to 0.05 in this paper.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Agresti, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Agresti A (2002) Categorial data analysis, 2nd edn. Wiley, New York. ISBN: 0-471-36093-7" /><p class="c-article-references__text" id="ref-CR1">Agresti A (2002) Categorial data analysis, 2nd edn. Wiley, New York. ISBN: 0-471-36093-7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Categorial%20data%20analysis&amp;publication_year=2002&amp;author=Agresti%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Astheimer P (1993) What you see is what you hear-acoustics applied in virtual worlds. In: Proceedings of the I" /><p class="c-article-references__text" id="ref-CR2">Astheimer P (1993) What you see is what you hear-acoustics applied in virtual worlds. In: Proceedings of the IEEE 1993 symposium on research frontiers in virtual reality, pp 100–107. ISBN: 0-8186-4910-0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bella F (2004) Driving simulation in virtual reality for work zone design on highway: a validation study. In: " /><p class="c-article-references__text" id="ref-CR3">Bella F (2004) Driving simulation in virtual reality for work zone design on highway: a validation study. In: The second SIIV international congress, Florence, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Berkhout, D. Vries, P. Vogel, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93(5):2764–2" /><p class="c-article-references__text" id="ref-CR4">Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93(5):2764–2778</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.405852" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20control%20by%20wave%20field%20synthesis&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=93&amp;issue=5&amp;pages=2764-2778&amp;publication_year=1993&amp;author=Berkhout%2CAJ&amp;author=Vries%2CD&amp;author=Vogel%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blauert J (1997) Spacial hearing. The psychophysics of human sound localization. The MIT Press, Cambridge. ISB" /><p class="c-article-references__text" id="ref-CR5">Blauert J (1997) Spacial hearing. The psychophysics of human sound localization. The MIT Press, Cambridge. ISBN: 978-262-02413-6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Blauert, U. Jekosch, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Blauert J, Jekosch U (2012) A layer model of sound quality. J Audio Eng Soc 60(1/2):4–12" /><p class="c-article-references__text" id="ref-CR6">Blauert J, Jekosch U (2012) A layer model of sound quality. J Audio Eng Soc 60(1/2):4–12</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20layer%20model%20of%20sound%20quality&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=60&amp;issue=1%2F2&amp;pages=4-12&amp;publication_year=2012&amp;author=Blauert%2CJ&amp;author=Jekosch%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Bossard, G. Kermarrec, C. Buche, J. Tisseau, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bossard C, Kermarrec G, Buche C, Tisseau J (2008) Transfer of learning in virtual environments: a new challeng" /><p class="c-article-references__text" id="ref-CR7">Bossard C, Kermarrec G, Buche C, Tisseau J (2008) Transfer of learning in virtual environments: a new challenge? Virtual Real 12(3):151–161</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-008-0093-y" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20of%20learning%20in%20virtual%20environments%3A%20a%20new%20challenge%3F&amp;journal=Virtual%20Real&amp;volume=12&amp;issue=3&amp;pages=151-161&amp;publication_year=2008&amp;author=Bossard%2CC&amp;author=Kermarrec%2CG&amp;author=Buche%2CC&amp;author=Tisseau%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, RP. McMahan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" /><p class="c-article-references__text" id="ref-CR8">Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2007.257" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20how%20much%20immersion%20is%20enough%3F&amp;journal=Computer&amp;volume=40&amp;issue=7&amp;pages=36-43&amp;publication_year=2007&amp;author=Bowman%2CDA&amp;author=McMahan%2CRP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Cakmakci, J. Rolland, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Cakmakci O, Rolland J (2006) Head-worn displays: a review. Journal of Disp Technol 2(3):199–216" /><p class="c-article-references__text" id="ref-CR9">Cakmakci O, Rolland J (2006) Head-worn displays: a review. Journal of Disp Technol 2(3):199–216</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJDT.2006.879846" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Head-worn%20displays%3A%20a%20review&amp;journal=Journal%20of%20Disp%20Technol&amp;volume=2&amp;issue=3&amp;pages=199-216&amp;publication_year=2006&amp;author=Cakmakci%2CO&amp;author=Rolland%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Cohen, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Cohen J (1988) Statistical power analysis for the behavioral sciences. Lawrence Erlbaum, Hillsdale. ISBN: 978-" /><p class="c-article-references__text" id="ref-CR10">Cohen J (1988) Statistical power analysis for the behavioral sciences. Lawrence Erlbaum, Hillsdale. ISBN: 978-080-580283-2</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20power%20analysis%20for%20the%20behavioral%20sciences&amp;publication_year=1998&amp;author=Cohen%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cruz-Neira, DJ. Sandin, TA. DeFanti, RV. Kenyon, JC. Hart, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic vir" /><p class="c-article-references__text" id="ref-CR11">Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F129888.129892" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20CAVE%3A%20audio%20visual%20experience%20automatic%20virtual%20environment&amp;journal=Commun%20ACM&amp;volume=35&amp;issue=6&amp;pages=64-72&amp;publication_year=1992&amp;author=Cruz-Neira%2CC&amp;author=Sandin%2CDJ&amp;author=DeFanti%2CTA&amp;author=Kenyon%2CRV&amp;author=Hart%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TA. DeFanti, G. Dawe, DJ. Sandin, JP. Schulze, P. Otto, J. Girado, F. Kuester, L. Smarr, R. Rao, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="DeFanti TA, Dawe G, Sandin DJ, Schulze JP, Otto P, Girado J, Kuester F, Smarr L, Rao R (2009) The StarCAVE, a " /><p class="c-article-references__text" id="ref-CR12">DeFanti TA, Dawe G, Sandin DJ, Schulze JP, Otto P, Girado J, Kuester F, Smarr L, Rao R (2009) The StarCAVE, a third-generation CAVE and virtual reality optiportal. Futur Gener Comput Syst 25(2):169–178</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.future.2008.07.015" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20StarCAVE%2C%20a%20third-generation%20CAVE%20and%20virtual%20reality%20optiportal&amp;journal=Futur%20Gener%20Comput%20Syst&amp;volume=25&amp;issue=2&amp;pages=169-178&amp;publication_year=2009&amp;author=DeFanti%2CTA&amp;author=Dawe%2CG&amp;author=Sandin%2CDJ&amp;author=Schulze%2CJP&amp;author=Otto%2CP&amp;author=Girado%2CJ&amp;author=Kuester%2CF&amp;author=Smarr%2CL&amp;author=Rao%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="European Broadcasting Union (2011) Practical guidelines for production and implementation in accordance with E" /><p class="c-article-references__text" id="ref-CR13">European Broadcasting Union (2011) Practical guidelines for production and implementation in accordance with EBU R 128 (version 2.0). European Broadcasting Union, Geneva</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Frechaud V (2013) Gui3D, v. 1.11" /><p class="c-article-references__text" id="ref-CR14">Frechaud V (2013) Gui3D, v. 1.11</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CO. Fritz, PE. Morris, JJ. Richler, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Fritz CO, Morris PE, Richler JJ (2012) Effect size estimates: current use, calculations, and interpretation. J" /><p class="c-article-references__text" id="ref-CR15">Fritz CO, Morris PE, Richler JJ (2012) Effect size estimates: current use, calculations, and interpretation. J Exp Psychol Gen 141(1):2–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fa0024338" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effect%20size%20estimates%3A%20current%20use%2C%20calculations%2C%20and%20interpretation&amp;journal=J%20Exp%20Psychol%20Gen&amp;volume=141&amp;issue=1&amp;pages=2-18&amp;publication_year=2012&amp;author=Fritz%2CCO&amp;author=Morris%2CPE&amp;author=Richler%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Garcia G (2002) Optimal filter partition for efficient convolution with short input/output delay. In: Proceedi" /><p class="c-article-references__text" id="ref-CR16">Garcia G (2002) Optimal filter partition for efficient convolution with short input/output delay. In: Proceedings of the AES 113th convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gardner WG (1994) Efficient convolution without input/output delay. J. Audio Eng Soc 127–136 (preprint 3897)&#xA; " /><p class="c-article-references__text" id="ref-CR17">Gardner WG (1994) Efficient convolution without input/output delay. J. Audio Eng Soc 127–136 <b>(preprint 3897)</b>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Gilkey, T. Anderson, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Gilkey R, Anderson T (2014) Binaural and spatial hearing in real and virtual environments. Taylor &amp; Francis, L" /><p class="c-article-references__text" id="ref-CR18">Gilkey R, Anderson T (2014) Binaural and spatial hearing in real and virtual environments. Taylor &amp; Francis, London. ISBN: 978-131-778026-7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Binaural%20and%20spatial%20hearing%20in%20real%20and%20virtual%20environments&amp;publication_year=2014&amp;author=Gilkey%2CR&amp;author=Anderson%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gorzel M, Corrigan D, Kearney G, Squires J, Boland F (2012) Distance perception in virtual audio-visual enviro" /><p class="c-article-references__text" id="ref-CR19">Gorzel M, Corrigan D, Kearney G, Squires J, Boland F (2012) Distance perception in virtual audio-visual environment. In: 25th UK conference of the audio engineering society: spatial audio in today’s 3D world, York, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Gurusamy, R. Aggarwal, L. Palanivelu, BR. Davidson, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Gurusamy K, Aggarwal R, Palanivelu L, Davidson BR (2008) Systematic review of randomized controlled trials on " /><p class="c-article-references__text" id="ref-CR20">Gurusamy K, Aggarwal R, Palanivelu L, Davidson BR (2008) Systematic review of randomized controlled trials on the effectiveness of virtual reality training for laparoscopic surgery. Br J Surg 95(9):1088–1097</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fbjs.6344" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Systematic%20review%20of%20randomized%20controlled%20trials%20on%20the%20effectiveness%20of%20virtual%20reality%20training%20for%20laparoscopic%20surgery&amp;journal=Br%20J%20Surg&amp;volume=95&amp;issue=9&amp;pages=1088-1097&amp;publication_year=2008&amp;author=Gurusamy%2CK&amp;author=Aggarwal%2CR&amp;author=Palanivelu%2CL&amp;author=Davidson%2CBR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hess W, Weishäupl J (2014) Replication of human head movements in 3 dimensions by a mechanical joint. In: Proc" /><p class="c-article-references__text" id="ref-CR21">Hess W, Weishäupl J (2014) Replication of human head movements in 3 dimensions by a mechanical joint. In: Proceedings of the international conference on spatial audio (ICSA)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Kozak, PA. Hancock, EJ. Arthur, ST. Chrysler, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Kozak JJ, Hancock PA, Arthur EJ, Chrysler ST (1993) Transfer of training from virtual reality. Ergonomics 36(7" /><p class="c-article-references__text" id="ref-CR22">Kozak JJ, Hancock PA, Arthur EJ, Chrysler ST (1993) Transfer of training from virtual reality. Ergonomics 36(7):777–784</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F00140139308967941" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20of%20training%20from%20virtual%20reality&amp;journal=Ergonomics&amp;volume=36&amp;issue=7&amp;pages=777-784&amp;publication_year=1993&amp;author=Kozak%2CJJ&amp;author=Hancock%2CPA&amp;author=Arthur%2CEJ&amp;author=Chrysler%2CST">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Kuhlen, I. Assenmacher, T. Lentz, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Kuhlen T, Assenmacher I, Lentz T (2007) A true spatial sound system for CAVE-like displays using four loudspea" /><p class="c-article-references__text" id="ref-CR23">Kuhlen T, Assenmacher I, Lentz T (2007) A true spatial sound system for CAVE-like displays using four loudspeakers. In: Shumaker R (ed) Virtual reality, vol 4563. Springer, Berlin, pp 270–279. ISBN: 978-354-073334-8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality&amp;pages=270-279&amp;publication_year=2007&amp;author=Kuhlen%2CT&amp;author=Assenmacher%2CI&amp;author=Lentz%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Larsson P, Västfjäll D, Kleiner M (2004) Perception of self-motion and presence in auditory virtual environmen" /><p class="c-article-references__text" id="ref-CR24">Larsson P, Västfjäll D, Kleiner M (2004) Perception of self-motion and presence in auditory virtual environments. In: Proceedings of the presence, pp 252–258</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="BP. Lathi, RA. Green, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Lathi BP, Green RA (2014) Essentials of digital signal processing. Cambridge University Press, Cambridge. ISBN" /><p class="c-article-references__text" id="ref-CR25">Lathi BP, Green RA (2014) Essentials of digital signal processing. Cambridge University Press, Cambridge. ISBN: 978-110-705932-0</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Essentials%20of%20digital%20signal%20processing&amp;publication_year=2014&amp;author=Lathi%2CBP&amp;author=Green%2CRA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Le Callet P, Möller S, Perkis A (2012) Qualinet white paper on definitions of quality of experience (version 1" /><p class="c-article-references__text" id="ref-CR26">Le Callet P, Möller S, Perkis A (2012) Qualinet white paper on definitions of quality of experience (version 1.1). Qualinet, Dagstuhl</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Lindau, H. Maempel, S. Weinzierl, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Lindau A, Maempel H, Weinzierl S (2008) Minimum BRIR grid resolution for dynamic binaural synthesis. J Acoust " /><p class="c-article-references__text" id="ref-CR27">Lindau A, Maempel H, Weinzierl S (2008) Minimum BRIR grid resolution for dynamic binaural synthesis. J Acoust Soc Am 123(5):3498–3498</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.2934364" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Minimum%20BRIR%20grid%20resolution%20for%20dynamic%20binaural%20synthesis&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=123&amp;issue=5&amp;pages=3498-3498&amp;publication_year=2008&amp;author=Lindau%2CA&amp;author=Maempel%2CH&amp;author=Weinzierl%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Loomis, JJ. Blascovich, AC. Beall, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in" /><p class="c-article-references__text" id="ref-CR28">Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in psychology. Behav Res Methods Instrum Comput 31(4):557–564</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03200735" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20virtual%20environment%20technology%20as%20a%20basic%20research%20tool%20in%20psychology&amp;journal=Behav%20Res%20Methods%20Instrum%20Comput&amp;volume=31&amp;issue=4&amp;pages=557-564&amp;publication_year=1999&amp;author=Loomis%2CJM&amp;author=Blascovich%2CJJ&amp;author=Beall%2CAC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DH. Mershon, DH. Desaulniers, SA. Kiefer, TL. Amerson, JT. Mills, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Mershon DH, Desaulniers DH, Kiefer SA Jr, Amerson TL, Mills JT (1981) Perceived loudness and visually-determin" /><p class="c-article-references__text" id="ref-CR29">Mershon DH, Desaulniers DH, Kiefer SA Jr, Amerson TL, Mills JT (1981) Perceived loudness and visually-determined auditory distance. Perception 10(5):531–543</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp100531" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceived%20loudness%20and%20visually-determined%20auditory%20distance&amp;journal=Perception&amp;volume=10&amp;issue=5&amp;pages=531-543&amp;publication_year=1981&amp;author=Mershon%2CDH&amp;author=Desaulniers%2CDH&amp;author=Kiefer%2CSA&amp;author=Amerson%2CTL&amp;author=Mills%2CJT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Müller, P. Massarani, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Müller S, Massarani P (2001) Transfer-function measurement with sweeps. J Audio Eng Soc 49(6):443–471" /><p class="c-article-references__text" id="ref-CR30">Müller S, Massarani P (2001) Transfer-function measurement with sweeps. J Audio Eng Soc 49(6):443–471</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer-function%20measurement%20with%20sweeps&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=49&amp;issue=6&amp;pages=443-471&amp;publication_year=2001&amp;author=M%C3%BCller%2CS&amp;author=Massarani%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Novo, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Novo P (2005) Auditory virtual environments. In: Blauert J (ed) Communication acoustics. Springer, Berlin, pp." /><p class="c-article-references__text" id="ref-CR31">Novo P (2005) Auditory virtual environments. In: Blauert J (ed) Communication acoustics. Springer, Berlin, pp. 277–297</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Communication%20acoustics&amp;pages=277-297&amp;publication_year=2005&amp;author=Novo%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KJ. Palomäki, H. Tiitinen, V. Mäkinen, JC. May, P. Alku, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Palomäki KJ, Tiitinen H, Mäkinen V, May JC, Alku P (2005) Spatial processing in human auditory cortex: the eff" /><p class="c-article-references__text" id="ref-CR32">Palomäki KJ, Tiitinen H, Mäkinen V, May JC, Alku P (2005) Spatial processing in human auditory cortex: the effects of 3D, ITD, and ILD stimulation techniques. Cogn Brain Res 24(3):364–379</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cogbrainres.2005.02.013" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20processing%20in%20human%20auditory%20cortex%3A%20the%20effects%20of%203D%2C%20ITD%2C%20and%20ILD%20stimulation%20techniques&amp;journal=Cogn%20Brain%20Res&amp;volume=24&amp;issue=3&amp;pages=364-379&amp;publication_year=2005&amp;author=Palom%C3%A4ki%2CKJ&amp;author=Tiitinen%2CH&amp;author=M%C3%A4kinen%2CV&amp;author=May%2CJC&amp;author=Alku%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JL. Pearson, SJ. Dollinger, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Pearson JL, Dollinger SJ (2004) Music preference correlates of jungian types. Personal Individ Differ 36(5):10" /><p class="c-article-references__text" id="ref-CR33">Pearson JL, Dollinger SJ (2004) Music preference correlates of jungian types. Personal Individ Differ 36(5):1005–1008</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0191-8869%2803%2900168-5" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Music%20preference%20correlates%20of%20jungian%20types&amp;journal=Personal%20Individ%20Differ&amp;volume=36&amp;issue=5&amp;pages=1005-1008&amp;publication_year=2004&amp;author=Pearson%2CJL&amp;author=Dollinger%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WF. Prince, " /><meta itemprop="datePublished" content="1972" /><meta itemprop="headline" content="Prince WF (1972) A paradigm for research on music listening. J Res Music Educ 20(4):445–455" /><p class="c-article-references__text" id="ref-CR34">Prince WF (1972) A paradigm for research on music listening. J Res Music Educ 20(4):445–455</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F3343802" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20paradigm%20for%20research%20on%20music%20listening&amp;journal=J%20Res%20Music%20Educ&amp;volume=20&amp;issue=4&amp;pages=445-455&amp;publication_year=1972&amp;author=Prince%2CWF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Psotka, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Psotka J (1995) Immersive training systems: virtual reality and education and training. Instr Sci 23(5–6):405–" /><p class="c-article-references__text" id="ref-CR35">Psotka J (1995) Immersive training systems: virtual reality and education and training. Instr Sci 23(5–6):405–431</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00896880" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20training%20systems%3A%20virtual%20reality%20and%20education%20and%20training&amp;journal=Instr%20Sci&amp;volume=23&amp;issue=5%E2%80%936&amp;pages=405-431&amp;publication_year=1995&amp;author=Psotka%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pysiewicz A (2014) On the validity of web-based auditory perception experiments. Master’s thesis, TU Berlin" /><p class="c-article-references__text" id="ref-CR36">Pysiewicz A (2014) On the validity of web-based auditory perception experiments. Master’s thesis, TU Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FD. Rose, EA. Attree, BM. Brooks, DM. Parslow, PR. Penn, N. Ambihaipahan, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Rose FD, Attree EA, Brooks BM, Parslow DM, Penn PR, Ambihaipahan N (2000) Training in virtual environments: tr" /><p class="c-article-references__text" id="ref-CR37">Rose FD, Attree EA, Brooks BM, Parslow DM, Penn PR, Ambihaipahan N (2000) Training in virtual environments: transfer to real world tasks and equivalence to real task training. Ergonomics 43(4):494–511</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F001401300184378" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Training%20in%20virtual%20environments%3A%20transfer%20to%20real%20world%20tasks%20and%20equivalence%20to%20real%20task%20training&amp;journal=Ergonomics&amp;volume=43&amp;issue=4&amp;pages=494-511&amp;publication_year=2000&amp;author=Rose%2CFD&amp;author=Attree%2CEA&amp;author=Brooks%2CBM&amp;author=Parslow%2CDM&amp;author=Penn%2CPR&amp;author=Ambihaipahan%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Rumsey, S. Zielinski, R. Kassier, S. Bech, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Rumsey F, Zielinski S, Kassier R, Bech S (2005) Relationships between experienced listener ratings of multicha" /><p class="c-article-references__text" id="ref-CR38">Rumsey F, Zielinski S, Kassier R, Bech S (2005) Relationships between experienced listener ratings of multichannel audio quality and naive listener preferences. J Acoust Soc Am 117(6):3832–3840</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1904305" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Relationships%20between%20experienced%20listener%20ratings%20of%20multichannel%20audio%20quality%20and%20naive%20listener%20preferences&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=117&amp;issue=6&amp;pages=3832-3840&amp;publication_year=2005&amp;author=Rumsey%2CF&amp;author=Zielinski%2CS&amp;author=Kassier%2CR&amp;author=Bech%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MV. Sanchez-Vives, M. Slater, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Sanchez-Vives MV, Slater M (2005) From presence to consciousness through virtual reality. Nat Rev Neurosci 6(6" /><p class="c-article-references__text" id="ref-CR39">Sanchez-Vives MV, Slater M (2005) From presence to consciousness through virtual reality. Nat Rev Neurosci 6(6):332–339</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnrn1651" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20presence%20to%20consciousness%20through%20virtual%20reality&amp;journal=Nat%20Rev%20Neurosci&amp;volume=6&amp;issue=6&amp;pages=332-339&amp;publication_year=2005&amp;author=Sanchez-Vives%2CMV&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TT. Sandel, DC. Teas, WE. Feddersen, LA. Jeffress, " /><meta itemprop="datePublished" content="1955" /><meta itemprop="headline" content="Sandel TT, Teas DC, Feddersen WE, Jeffress LA (1955) Localization of sound from single and paired sources. J A" /><p class="c-article-references__text" id="ref-CR40">Sandel TT, Teas DC, Feddersen WE, Jeffress LA (1955) Localization of sound from single and paired sources. J Acoust Soc Am 27:842–852</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1908052" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Localization%20of%20sound%20from%20single%20and%20paired%20sources&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=27&amp;pages=842-852&amp;publication_year=1955&amp;author=Sandel%2CTT&amp;author=Teas%2CDC&amp;author=Feddersen%2CWE&amp;author=Jeffress%2CLA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Herre J (2013) About the impact of audio quality on overall listening experience. In: Proceeding" /><p class="c-article-references__text" id="ref-CR41">Schoeffler M, Herre J (2013) About the impact of audio quality on overall listening experience. In: Proceedings of the sound and music computing conference, Stockholm, Sweden, pp 48–53</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Herre J (2014a) Towards a listener model for predicting the overall listening experience. In: Pr" /><p class="c-article-references__text" id="ref-CR42">Schoeffler M, Herre J (2014a) Towards a listener model for predicting the overall listening experience. In: Proceedings of the audiomostly, Aalborg, Denmark</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Herre J (2014b) About the different types of listeners for rating the overall listening experien" /><p class="c-article-references__text" id="ref-CR43">Schoeffler M, Herre J (2014b) About the different types of listeners for rating the overall listening experience. In: Proceedings of the sound and music computing conference, Athens, Greece</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Hess W (2012) A comparison of highly configurable CPU- and GPU-based convolution engines. In: Au" /><p class="c-article-references__text" id="ref-CR44">Schoeffler M, Hess W (2012) A comparison of highly configurable CPU- and GPU-based convolution engines. In: Audio engineering society convention no. 133, San Francisco, CA, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Edler B, Herre J (2013a) How much does audio quality influence ratings of overall listening expe" /><p class="c-article-references__text" id="ref-CR45">Schoeffler M, Edler B, Herre J (2013a) How much does audio quality influence ratings of overall listening experience? In: Proceedings of the 10th international symposium on computer music multidisciplinary research (CMMR), pp 678–693, Marseille, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Stöter F-R, Bayerlein H, Edler B, Herre J (2013b) An experiment about estimating the number of i" /><p class="c-article-references__text" id="ref-CR46">Schoeffler M, Stöter F-R, Bayerlein H, Edler B, Herre J (2013b) An experiment about estimating the number of instruments in polyphonic music: a comparison between internet and laboratory results. In: Proceedings of the 14th international society for music information retrieval conference, Curitiba, Brazil</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Adami A, Herre J (2014a) The influence of up- and down-mixes on the overall listening experience" /><p class="c-article-references__text" id="ref-CR47">Schoeffler M, Adami A, Herre J (2014a) The influence of up- and down-mixes on the overall listening experience. In: Proceedings of the AES 137th convention, Los Angeles, CA, USA <b>(preprint 9140)</b>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Conrad S, Herre J (2014b) The influence of the single/multi-channel-system on the overall listen" /><p class="c-article-references__text" id="ref-CR48">Schoeffler M, Conrad S, Herre J (2014b) The influence of the single/multi-channel-system on the overall listening experience. In: Proceedings of the AES 55th conference on spatial audio, Helsinki, Finland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schoeffler M, Stöter F, Edler B, Herre J (2015) Towards the next generation of web-based experiments: a case s" /><p class="c-article-references__text" id="ref-CR49">Schoeffler M, Stöter F, Edler B, Herre J (2015) Towards the next generation of web-based experiments: a case study assessing basic audio quality following the itu-r recommendation bs.1534 (MUSHRA). In: 1st web audio conference, Paris, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder D, Wefers F, Pelzer S, Rausch D, Vorländer M, Kuhlen T (2010) Virtual reality system at RWTH Aachen U" /><p class="c-article-references__text" id="ref-CR50">Schröder D, Wefers F, Pelzer S, Rausch D, Vorländer M, Kuhlen T (2010) Virtual reality system at RWTH Aachen University. In: Proceedings of the international symposium on room acoustics, Sydney, NSW, Australia. Australian Acoustical Society, NSW Division</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Schuemie, P. Straaten, M. Krijn, CA. Mast, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Schuemie MJ, van der Straaten P, Krijn M, van der Mast CA (2001) Research on presence in virtual reality: a su" /><p class="c-article-references__text" id="ref-CR51">Schuemie MJ, van der Straaten P, Krijn M, van der Mast CA (2001) Research on presence in virtual reality: a survey. Cyberpsychol Behav Soc Netw 4(2):183–201</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2F109493101300117884" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Research%20on%20presence%20in%20virtual%20reality%3A%20a%20survey&amp;journal=Cyberpsychol%20Behav%20Soc%20Netw&amp;volume=4&amp;issue=2&amp;pages=183-201&amp;publication_year=2001&amp;author=Schuemie%2CMJ&amp;author=Straaten%2CP&amp;author=Krijn%2CM&amp;author=Mast%2CCA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seeber BU, Fastl H (2004) On auditory-visual interaction in real and virtual environments. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR52">Seeber BU, Fastl H (2004) On auditory-visual interaction in real and virtual environments. In: Proceedings of the 18th international congress on acoustics, pp 2293–2296, Kyoto, Japan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Silzle A, Strauss H, Novo P (2004) IKA-SIM: a system to generate auditory virtual environments. In: Audio engi" /><p class="c-article-references__text" id="ref-CR53">Silzle A, Strauss H, Novo P (2004) IKA-SIM: a system to generate auditory virtual environments. In: Audio engineering society convention no. 116 <b>(preprint 6016)</b>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stanney K (1995) Realizing the full potential of virtual reality: human factors issues that could stand in the" /><p class="c-article-references__text" id="ref-CR54">Stanney K (1995) Realizing the full potential of virtual reality: human factors issues that could stand in the way. In: Proceedings the virtual reality annual international symposium ’95. IEEE Computer Society Press, Los Alamitos, pp 28–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Stanney, RR. Mourant, RS. Kennedy, AROT. Literature, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Stanney KM, Mourant RR, Kennedy RS, Literature AROT (1998) Human factors issues in virtual environments: a rev" /><p class="c-article-references__text" id="ref-CR55">Stanney KM, Mourant RR, Kennedy RS, Literature AROT (1998) Human factors issues in virtual environments: a review of the literature. Presence 7:327–351</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565767" aria-label="View reference 55">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factors%20issues%20in%20virtual%20environments%3A%20a%20review%20of%20the%20literature&amp;journal=Presence&amp;volume=7&amp;pages=327-351&amp;publication_year=1998&amp;author=Stanney%2CKM&amp;author=Mourant%2CRR&amp;author=Kennedy%2CRS&amp;author=Literature%2CAROT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Steuer, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Steuer J (1992) Defining virtual reality: dimensions determining telepresence. J Commun 42(4):73–93" /><p class="c-article-references__text" id="ref-CR56">Steuer J (1992) Defining virtual reality: dimensions determining telepresence. J Commun 42(4):73–93</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1460-2466.1992.tb00812.x" aria-label="View reference 56">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Defining%20virtual%20reality%3A%20dimensions%20determining%20telepresence&amp;journal=J%20Commun&amp;volume=42&amp;issue=4&amp;pages=73-93&amp;publication_year=1992&amp;author=Steuer%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stockham Jr TG (1966) High-speed convolution and correlation. In: Proceedings of the spring joint computer con" /><p class="c-article-references__text" id="ref-CR57">Stockham Jr TG (1966) High-speed convolution and correlation. In: Proceedings of the spring joint computer conference, April 26–28, pp 229–233. ACM, New York, NY, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Sveistrup, J. McComas, M. Thornton, S. Marshall, H. Finestone, A. McCormick, K. Babulic, A. Mayhew, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Sveistrup H, McComas J, Thornton M, Marshall S, Finestone H, McCormick A, Babulic K, Mayhew A (2003) Experimen" /><p class="c-article-references__text" id="ref-CR58">Sveistrup H, McComas J, Thornton M, Marshall S, Finestone H, McCormick A, Babulic K, Mayhew A (2003) Experimental studies of virtual reality-delivered compared to conventional exercise programs for rehabilitation. Cyberpsychol Behav Soc Netw 6(3):245–249</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2F109493103322011524" aria-label="View reference 58">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Experimental%20studies%20of%20virtual%20reality-delivered%20compared%20to%20conventional%20exercise%20programs%20for%20rehabilitation&amp;journal=Cyberpsychol%20Behav%20Soc%20Netw&amp;volume=6&amp;issue=3&amp;pages=245-249&amp;publication_year=2003&amp;author=Sveistrup%2CH&amp;author=McComas%2CJ&amp;author=Thornton%2CM&amp;author=Marshall%2CS&amp;author=Finestone%2CH&amp;author=McCormick%2CA&amp;author=Babulic%2CK&amp;author=Mayhew%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="The OGRE Team (2013) OGRE game engine, v. 1.9.0" /><p class="c-article-references__text" id="ref-CR59">The OGRE Team (2013) OGRE game engine, v. 1.9.0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Torger A, Farina A (2001) Real-time partitioned convolution for ambiophonics surround sound. In: IEEE workshop" /><p class="c-article-references__text" id="ref-CR60">Torger A, Farina A (2001) Real-time partitioned convolution for ambiophonics surround sound. In: IEEE workshop on applications of signal processing to audio and acoustics, pp 195–198</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Väljamäe A, Larsson P, Västfjäll D, Kleiner M (2004) Auditory presence, individualized head-related transfer f" /><p class="c-article-references__text" id="ref-CR61">Väljamäe A, Larsson P, Västfjäll D, Kleiner M (2004) Auditory presence, individualized head-related transfer functions, and illusory ego-motion in virtual environments. In: Proceedings of the 7th annual workshop on presence</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Dam, DH. Laidlaw, RM. Simpson, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization" /><p class="c-article-references__text" id="ref-CR62">van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization. Comput Graph 26(4):535–555</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2802%2900113-9" aria-label="View reference 62">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Experiments%20in%20immersive%20virtual%20reality%20for%20scientific%20visualization&amp;journal=Comput%20Graph&amp;volume=26&amp;issue=4&amp;pages=535-555&amp;publication_year=2002&amp;author=Dam%2CA&amp;author=Laidlaw%2CDH&amp;author=Simpson%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Västfjäll, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Västfjäll D (2003) The subjective sense of presence, emotion recognition, and experienced emotions in auditory" /><p class="c-article-references__text" id="ref-CR63">Västfjäll D (2003) The subjective sense of presence, emotion recognition, and experienced emotions in auditory virtual environments. Cyberpsychol Behav 6(2):181–188</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2F109493103321640374" aria-label="View reference 63">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 63 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20subjective%20sense%20of%20presence%2C%20emotion%20recognition%2C%20and%20experienced%20emotions%20in%20auditory%20virtual%20environments&amp;journal=Cyberpsychol%20Behav&amp;volume=6&amp;issue=2&amp;pages=181-188&amp;publication_year=2003&amp;author=V%C3%A4stfj%C3%A4ll%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Vora, S. Nair, AK. Gramopadhye, AT. Duchowski, BJ. Melloy, B. Kanki, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Vora J, Nair S, Gramopadhye AK, Duchowski AT, Melloy BJ, Kanki B (2002) Using virtual reality technology for a" /><p class="c-article-references__text" id="ref-CR64">Vora J, Nair S, Gramopadhye AK, Duchowski AT, Melloy BJ, Kanki B (2002) Using virtual reality technology for aircraft visual inspection training: presence and comparison studies. Appl Ergon 33(6):559–570</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0003-6870%2802%2900039-X" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20virtual%20reality%20technology%20for%20aircraft%20visual%20inspection%20training%3A%20presence%20and%20comparison%20studies&amp;journal=Appl%20Ergon&amp;volume=33&amp;issue=6&amp;pages=559-570&amp;publication_year=2002&amp;author=Vora%2CJ&amp;author=Nair%2CS&amp;author=Gramopadhye%2CAK&amp;author=Duchowski%2CAT&amp;author=Melloy%2CBJ&amp;author=Kanki%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Welch, JH. Krantz, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Welch N, Krantz JH (1996) The world wide web as a medium for psychoacoustical demonstrations and experiments: " /><p class="c-article-references__text" id="ref-CR65">Welch N, Krantz JH (1996) The world wide web as a medium for psychoacoustical demonstrations and experiments: experience and results. Behav Res Methods Instrum Comput 28(2):192–196</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03204764" aria-label="View reference 65">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 65 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20world%20wide%20web%20as%20a%20medium%20for%20psychoacoustical%20demonstrations%20and%20experiments%3A%20experience%20and%20results&amp;journal=Behav%20Res%20Methods%20Instrum%20Comput&amp;volume=28&amp;issue=2&amp;pages=192-196&amp;publication_year=1996&amp;author=Welch%2CN&amp;author=Krantz%2CJH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Werner S, Siegel A (2011) Effects of binaural auralization via headphones on the perception of acoustic scenes" /><p class="c-article-references__text" id="ref-CR66">Werner S, Siegel A (2011) Effects of binaural auralization via headphones on the perception of acoustic scenes. In: Proceedings of the 3rd international symposium on auditory and audiological research (ISAAR), Nyborg, Denmark</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Werner S, Liebetrau J, Sporer T (2012) Audio–visual discrepancy and the influence on vertical sound source loc" /><p class="c-article-references__text" id="ref-CR67">Werner S, Liebetrau J, Sporer T (2012) Audio–visual discrepancy and the influence on vertical sound source localization. In: Proceedings of the 4th international workshop on quality of multimedia experience (QoMEX), pp 133–139, Melbourne, Australia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Wilcoxon, " /><meta itemprop="datePublished" content="1945" /><meta itemprop="headline" content="Wilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83" /><p class="c-article-references__text" id="ref-CR68">Wilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F3001968" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20comparisons%20by%20ranking%20methods&amp;journal=Biom%20Bull&amp;volume=1&amp;issue=6&amp;pages=80-83&amp;publication_year=1945&amp;author=Wilcoxon%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BG. Witmer, JH. Bailey, BW. Knerr, KC. Parsons, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Witmer BG, Bailey JH, Knerr BW, Parsons KC (1996) Virtual spaces and real world places: transfer of route know" /><p class="c-article-references__text" id="ref-CR69">Witmer BG, Bailey JH, Knerr BW, Parsons KC (1996) Virtual spaces and real world places: transfer of route knowledge. Int J Hum Comput Stud 45(4):413–428</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.1996.0060" aria-label="View reference 69">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20spaces%20and%20real%20world%20places%3A%20transfer%20of%20route%20knowledge&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=45&amp;issue=4&amp;pages=413-428&amp;publication_year=1996&amp;author=Witmer%2CBG&amp;author=Bailey%2CJH&amp;author=Knerr%2CBW&amp;author=Parsons%2CKC">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0270-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to thank Alexander Adami for taking pictures of the experiment apparatus and Marlene Röß for representing a participant in the pictures.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">International Audio Laboratories Erlangen, A Joint Institution of Fraunhofer IIS and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Am Wolfsmantel 33, 91058, Erlangen, Germany</p><p class="c-article-author-affiliation__authors-list">Michael Schoeffler, Jan Lukas Gernert, Maximilian Neumayer, Susanne Westphal &amp; Jürgen Herre</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Michael-Schoeffler"><span class="c-article-authors-search__title u-h3 js-search-name">Michael Schoeffler</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Michael+Schoeffler&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michael+Schoeffler" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael+Schoeffler%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jan_Lukas-Gernert"><span class="c-article-authors-search__title u-h3 js-search-name">Jan Lukas Gernert</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jan Lukas+Gernert&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jan Lukas+Gernert" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jan Lukas+Gernert%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Maximilian-Neumayer"><span class="c-article-authors-search__title u-h3 js-search-name">Maximilian Neumayer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Maximilian+Neumayer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Maximilian+Neumayer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Maximilian+Neumayer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Susanne-Westphal"><span class="c-article-authors-search__title u-h3 js-search-name">Susanne Westphal</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Susanne+Westphal&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Susanne+Westphal" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Susanne+Westphal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-J_rgen-Herre"><span class="c-article-authors-search__title u-h3 js-search-name">Jürgen Herre</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;J%C3%BCrgen+Herre&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J%C3%BCrgen+Herre" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J%C3%BCrgen+Herre%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0270-8/email/correspondent/c1/new">Michael Schoeffler</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=On%20the%20validity%20of%20virtual%20reality-based%20auditory%20experiments%3A%20a%20case%20study%20about%20ratings%20of%20the%20overall%20listening%20experience&amp;author=Michael%20Schoeffler%20et%20al&amp;contentID=10.1007%2Fs10055-015-0270-8&amp;publication=1359-4338&amp;publicationDate=2015-08-15&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0270-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0270-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Schoeffler, M., Gernert, J.L., Neumayer, M. <i>et al.</i> On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience.
                    <i>Virtual Reality</i> <b>19, </b>181–200 (2015). https://doi.org/10.1007/s10055-015-0270-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0270-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-02-06">06 February 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-04">04 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-15">15 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11">November 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0270-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0270-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality-based experiments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Overall listening experience</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Convolution engine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Oculus Rift</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0270-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=270;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

