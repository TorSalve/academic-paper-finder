<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Shadow removal of projected imagery by occluder shape measurement in a"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents a shadow removal technique for a multiple overlapping projection system. In particular, this paper deals with situations where cameras cannot be placed between the occluder and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system"/>

    <meta name="dc.source" content="Virtual Reality 2014 18:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-08-22"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents a shadow removal technique for a multiple overlapping projection system. In particular, this paper deals with situations where cameras cannot be placed between the occluder and projection surface. We apply a synthetic aperture capturing technique to estimate the appearance of the projection surface, and a visual hull reconstruction technique to measure the shape of the occluder. Once the shape is acquired, shadow regions on the surface can be estimated. The proposed shadow removal technique allows users to balance between the following two criteria: the likelihood of new shadow emergence and the spatial resolution of the projected results. Through a real projection experiment, we evaluate the proposed shadow removal technique"/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-08-22"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="245"/>

    <meta name="prism.endingPage" content="254"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0250-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0250-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0250-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0250-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2014/11"/>

    <meta name="citation_online_date" content="2014/08/22"/>

    <meta name="citation_firstpage" content="245"/>

    <meta name="citation_lastpage" content="254"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0250-4"/>

    <meta name="DOI" content="10.1007/s10055-014-0250-4"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0250-4"/>

    <meta name="description" content="This paper presents a shadow removal technique for a multiple overlapping projection system. In particular, this paper deals with situations where cameras "/>

    <meta name="dc.creator" content="Daisuke Iwai"/>

    <meta name="dc.creator" content="Momoyo Nagase"/>

    <meta name="dc.creator" content="Kosuke Sato"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Abdel-Aziz YI, Karara HM (1971) Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry. In: Proceedings of the symposium on close-range photogrammetry (American Society of Photogrammetry), pp 1&#8211;18
"/>

    <meta name="citation_reference" content="Audet S, Cooperstock JR (2007) Shadow removal in front projection environments using object tracking. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented teality, pp 207&#8211;216"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Multi-focal projection: a multi-projector technique for increasing focal depth; citation_author=O Bimber, A Emmerling; citation_volume=12; citation_issue=4; citation_publication_date=2006; citation_pages=658-667; citation_doi=10.1109/TVCG.2006.75; citation_id=CR3"/>

    <meta name="citation_reference" content="Bimber O, Wetzstein G, Emmerling A, Nitschke C (2005) Enabling view-dependent stereoscopic projection in real environments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 14&#8211;23"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=The visual computing of projector&#8211;camera systems; citation_author=O Bimber, D Iwai, G Wetzstein, A Grundh&#246;fer; citation_volume=27; citation_issue=8; citation_publication_date=2008; citation_pages=2219-2254; citation_doi=10.1111/j.1467-8659.2008.01175.x; citation_id=CR5"/>

    <meta name="citation_reference" content="Cham TJ, Rehg JM, Sukthankar R, Sukthankar G (2003) Shadow elimination and occluder light suppression for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 513&#8211;520"/>

    <meta name="citation_reference" content="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of IEEE visualization, pp 175&#8211;182"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Camera-based detection and removal of shadows from interactive multiprojector displays; citation_author=C Jaynes, S Webb, RM Steele; citation_volume=10; citation_issue=3; citation_publication_date=2004; citation_pages=290-301; citation_doi=10.1109/TVCG.2004.1272728; citation_id=CR8"/>

    <meta name="citation_reference" content="Jones BR, Sodhi R, Campbell RH, Garnett G, Bailey BP (2010) Build your world and play in it: interacting with surface particles on complex objects. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 165&#8211;174"/>

    <meta name="citation_reference" content="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: Proceedings of IEEE workshop on visual computer vision on GPU"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=The visual hull concept for silhouette-based image understanding; citation_author=A Laurentini; citation_volume=16; citation_issue=2; citation_publication_date=1994; citation_pages=150-162; citation_doi=10.1109/34.273735; citation_id=CR11"/>

    <meta name="citation_reference" content="Low KL, Welch G, Lastra A, Fuchs H (2001) Life-sized projector-based dioramas. In: Proceedings of ACM symposium on virtual reality software and technology, pp 93&#8211;101"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Dynamic defocus and occlusion compensation of projected imagery by model-based optimal projector selection in multi-projection environment; citation_author=M Nagase, D Iwai, K Sato; citation_volume=15; citation_issue=2; citation_publication_date=2011; citation_pages=119-132; citation_doi=10.1007/s10055-010-0168-4; citation_id=CR13"/>

    <meta name="citation_reference" content="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of eurographics workshop on rendering, pp 89&#8211;102"/>

    <meta name="citation_reference" content="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision, pp 657&#8211;661"/>

    <meta name="citation_reference" content="Sugaya Y, Miyagawa I, Koike H (2010) Contrasting shadow for occluder light suppression from one-shot image. In: Proceedings of international workshop on projector&#8211;camera systems, pp 13&#8211;18"/>

    <meta name="citation_reference" content="Sukthankar R, Cham TJ, Sukthankar G (2001) Dynamic shadow elimination for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 151&#8211;157"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Shadow elimination and blinding light suppression for interactive projected displays; citation_author=J Summet, M Flagg, TJ Cham, JM Rehg, R Sukthankar; citation_volume=13; citation_issue=3; citation_publication_date=2007; citation_pages=508-517; citation_doi=10.1109/TVCG.2007.1007; citation_id=CR18"/>

    <meta name="citation_reference" content="Vaish V, Levoy M, Szeliski R, Zitnick CL, Kang SB (2006) Reconstructing occluded surfaces using synthetic apertures: stereo, focus and robust measures. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 2331&#8211;2338"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Image Process; citation_title=Perceptual image quality assessment: from error visibility to structural similarity; citation_author=Z Wang, AC Bovik, HR Sheikh, EP Simoncelli; citation_volume=13; citation_issue=4; citation_publication_date=2004; citation_pages=600-612; citation_doi=10.1109/TIP.2003.819861; citation_id=CR20"/>

    <meta name="citation_reference" content="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia, pp 161&#8211;168"/>

    <meta name="citation_author" content="Daisuke Iwai"/>

    <meta name="citation_author_email" content="daisuke.iwai@sys.es.osaka-u.ac.jp"/>

    <meta name="citation_author_institution" content="Graduate School of Engineering Science, Osaka University, Toyonaka, Japan"/>

    <meta name="citation_author" content="Momoyo Nagase"/>

    <meta name="citation_author_institution" content="Graduate School of Engineering Science, Osaka University, Toyonaka, Japan"/>

    <meta name="citation_author" content="Kosuke Sato"/>

    <meta name="citation_author_institution" content="Graduate School of Engineering Science, Osaka University, Toyonaka, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0250-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0250-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system"/>
        <meta property="og:description" content="This paper presents a shadow removal technique for a multiple overlapping projection system. In particular, this paper deals with situations where cameras cannot be placed between the occluder and projection surface. We apply a synthetic aperture capturing technique to estimate the appearance of the projection surface, and a visual hull reconstruction technique to measure the shape of the occluder. Once the shape is acquired, shadow regions on the surface can be estimated. The proposed shadow removal technique allows users to balance between the following two criteria: the likelihood of new shadow emergence and the spatial resolution of the projected results. Through a real projection experiment, we evaluate the proposed shadow removal technique"/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0250-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Shadow removal, Multiple overlapping projection, Synthetic aperture capturing, Visual hull","kwrd":["Shadow_removal","Multiple_overlapping_projection","Synthetic_aperture_capturing","Visual_hull"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0250-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0250-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=250;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0250-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0250-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0250-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-08-22" itemprop="datePublished">22 August 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daisuke-Iwai" data-author-popup="auth-Daisuke-Iwai" data-corresp-id="c1">Daisuke Iwai<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Graduate School of Engineering Science, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Momoyo-Nagase" data-author-popup="auth-Momoyo-Nagase">Momoyo Nagase</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Graduate School of Engineering Science, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kosuke-Sato" data-author-popup="auth-Kosuke-Sato">Kosuke Sato</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Graduate School of Engineering Science, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">245</span>–<span itemprop="pageEnd">254</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">318 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0250-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents a shadow removal technique for a multiple overlapping projection system. In particular, this paper deals with situations where cameras cannot be placed between the occluder and projection surface. We apply a synthetic aperture capturing technique to estimate the appearance of the projection surface, and a visual hull reconstruction technique to measure the shape of the occluder. Once the shape is acquired, shadow regions on the surface can be estimated. The proposed shadow removal technique allows users to balance between the following two criteria: the likelihood of new shadow emergence and the spatial resolution of the projected results. Through a real projection experiment, we evaluate the proposed shadow removal technique</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Projection-based mixed/augmented reality (MR/AR) visually augments physical objects by controlling the appearance of their surfaces with projected imagery from distributed multiple projectors (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of eurographics workshop on rendering, pp 89–102" href="/article/10.1007/s10055-014-0250-4#ref-CR14" id="ref-link-section-d36541e310">2001</a>). This approach has several advantages such as wide field-of-view imagery, natural auto-stereoscopic vision and eye accommodation, and high spatial and geometric fidelity. Consequently, many researchers have employed it in their interactive systems where users can directly manipulate the appearance of physical objects by touching surfaces with tracked handheld tools (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented teality, pp 207–216" href="/article/10.1007/s10055-014-0250-4#ref-CR2" id="ref-link-section-d36541e313">2001</a>; Jones et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Jones BR, Sodhi R, Campbell RH, Garnett G, Bailey BP (2010) Build your world and play in it: interacting with surface particles on complex objects. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 165–174" href="/article/10.1007/s10055-014-0250-4#ref-CR9" id="ref-link-section-d36541e316">2010</a>; Low et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Low KL, Welch G, Lastra A, Fuchs H (2001) Life-sized projector-based dioramas. In: Proceedings of ACM symposium on virtual reality software and technology, pp 93–101" href="/article/10.1007/s10055-014-0250-4#ref-CR12" id="ref-link-section-d36541e319">2001</a>). In such systems, the user’s body (e.g., hands) or the tools can easily and inadvertently block the projected light, and consequently, cast shadows on the surfaces. These shadows must be removed to guarantee natural and smooth interaction, as they potentially occlude important projected information and detract from the users’ visually immersive experience.</p><p>One of the most promising shadow removal solutions is to employ multiple overlapping projectors. When the shape and position of the occluder are known, its shadow from a projector onto a surface can be estimated and thus removed by projecting a compensation image from another unoccluded projector. However, in general, such geometric information about the occluder is not known a priori and thus needs to be measured online. The Kinect sensor<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> achieves this by measuring the shape of a scene on the basis of real-time dot pattern projection. However, we cannot estimate shadows from multiple projectors unless the whole occluder shape is measured. To solve this issue, we need to simultaneously use multiple Kinect sensors placed at different locations, but in this case, the projected patterns interfere with each other and the measurement becomes inaccurate. In addition, when the desired appearance for a shadow area is divided and then assigned to unoccluded projectors for shadow removal, the assigned pixel values affect the projected image quality (e.g., spatial resolution) and influence the likelihood of the emergence of a new shadow caused by the occluder’s movement. However, previous studies have not focused on this issue.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Overview of the proposed technique</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>This paper presents two basic technologies to achieve shadow removal in a multiple overlapping projection system by solving the issues described above. The first is an occluder shape reconstruction technique, and the second is a dividing technique to achieve the desired appearance that should be displayed in the shadow region.</p><p>To ensure that the shadows are completely removed, the whole shape of the occluder should be measured. This measurement can be approximate as long as the measured shape contains the occluder. Therefore, we propose to apply a visual hull technique (Laurentini <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Laurentini A (1994) The visual hull concept for silhouette-based image understanding. IEEE Trans Pattern Anal Mach Intell 16(2):150–162" href="/article/10.1007/s10055-014-0250-4#ref-CR11" id="ref-link-section-d36541e362">1994</a>) using multiple distributed cameras. For extracting the occluder’s silhouette, we estimate its background (i.e., projection surface) on the basis of a synthetic aperture capturing technique (Vaish et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Vaish V, Levoy M, Szeliski R, Zitnick CL, Kang SB (2006) Reconstructing occluded surfaces using synthetic apertures: stereo, focus and robust measures. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 2331–2338" href="/article/10.1007/s10055-014-0250-4#ref-CR19" id="ref-link-section-d36541e365">2006</a>). Once the shape is estimated, we can remove the shadow from the projection surface by dividing the desired appearance (or pixel value) for the shadow area and then assigning the divided pixel values to unoccluded projectors. However, in the next frame, a new shadow may emerge when the occluder moves into the view frustum of an unoccluded projector. Therefore, it is obviously reasonable to assign a large pixel value to an unoccluded projector whose view frustum is at some distance from the occluder, although it is possible that such a projector provides poor image quality. Note that, when we refer to the image quality, we consider the spatial resolution of the projected imagery. Therefore, we propose a new projector selection technique that considers both the likelihood of a new shadow emerging and the projected image quality. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig1">1</a> outlines the proposed technique.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related works</h2><div class="c-article-section__content" id="Sec2-content"><p>In the past decade, several approaches have been proposed for shadow removal in front projection display systems. All these methods employ multiple overlapping projectors, and single or multiple cameras to capture either a shadow on the projection surface or an occluder.</p><p>Sukthankar et al. proposed a method to remove shadows on a projection surface while displaying a still image (Sukthankar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Sukthankar R, Cham TJ, Sukthankar G (2001) Dynamic shadow elimination for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 151–157" href="/article/10.1007/s10055-014-0250-4#ref-CR17" id="ref-link-section-d36541e382">2001</a>). They applied a feedback process where each iteration compared the current appearance of the projection surface with a reference image captured in advance and then generated a projected image that minimized the residual. The extended version of the method realized the suppression of blinding light incident on a user (Cham et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Cham TJ, Rehg JM, Sukthankar R, Sukthankar G (2003) Shadow elimination and occluder light suppression for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 513–520" href="/article/10.1007/s10055-014-0250-4#ref-CR6" id="ref-link-section-d36541e385">2003</a>). Another approach focused on displaying shadowless video footage by predicting an unoccluded appearance on a projection surface by considering the geometric and radiometric properties of the surface (Jaynes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of IEEE visualization, pp 175–182" href="/article/10.1007/s10055-014-0250-4#ref-CR7" id="ref-link-section-d36541e388">2001</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Jaynes C, Webb S, Steele RM (2004) Camera-based detection and removal of shadows from interactive multiprojector displays. IEEE Trans Vis Comput Graph 10(3):290–301" href="/article/10.1007/s10055-014-0250-4#ref-CR8" id="ref-link-section-d36541e391">2004</a>). It compared the captured scene with the predicted appearance to find a shadow region that was then illuminated by another unoccluded projector. Sugaya et al. also proposed a method that required only a single-shot image of the projection surface (Sugaya et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Sugaya Y, Miyagawa I, Koike H (2010) Contrasting shadow for occluder light suppression from one-shot image. In: Proceedings of international workshop on projector–camera systems, pp 13–18" href="/article/10.1007/s10055-014-0250-4#ref-CR16" id="ref-link-section-d36541e394">2010</a>). They assigned different intensity values to each projector to identify occluded projectors from the shadows. The techniques described above assume that the cameras observing the projection surface are placed between the surface and the occluders.</p><p>Summet et al. relaxed this constraint (Summet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Summet J, Flagg M, Cham TJ, Rehg JM, Sukthankar R (2007) Shadow elimination and blinding light suppression for interactive projected displays. IEEE Trans Vis Comput Graph 13(3):508–517" href="/article/10.1007/s10055-014-0250-4#ref-CR18" id="ref-link-section-d36541e400">2007</a>). They detected an occluder instead of a shadow by illuminating the scene with infrared (IR) lights and capturing the projection surface with an IR camera. Then, the captured backlit silhouette of an occluder was warped to align with the projection surface. The camera does not have to be placed behind the occluder in this method. However, it is assumed that IR light sources are placed between the surface and occluder for robust silhouette extraction.</p><p>Audet and Cooperstock proposed recovering 3D occluder information with two cameras (Audet and Cooperstock <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Audet S, Cooperstock JR (2007) Shadow removal in front projection environments using object tracking. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1–8" href="/article/10.1007/s10055-014-0250-4#ref-CR1" id="ref-link-section-d36541e406">2007</a>). Once the 3D information was obtained, the position of the shadows of the occluder on the projection surface could be deduced from the geometric relationships among the projectors, surface, and occluder. There is no limitation on the placement of cameras in their method. As they focused on the compensation of shadows caused by walking people, the current dedicated system is designed to deal with humans standing vertically on the ground.</p><p>Most of the previous studies assumed that equipment, such as cameras or IR light sources, is placed between the occluder and the projection screen, thus constraining the user from being close to the projection surface. The central aim of this research is to relax this constraint. To this end, we reconstruct the shape of the occluder by applying a visual hull technique in order to address the shape reconstruction of various kinds of occluders, such as the user’s body or a handheld tool. In addition, none of the previous techniques explicitly considered how to divide the desired appearance for a shadow area among unoccluded projectors. This paper also presents a solution to this issue.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Shadow removal principle</h2><div class="c-article-section__content" id="Sec3-content"><p>The proposed shadow removal technique employs multiple overlapping projectors and cameras. This section describes the assumptions of the proposed technique, followed by the principle of each technical component of the proposal.</p><h3 class="c-article__sub-heading" id="Sec4">Assumptions</h3><p>We assume that multiple projectors and cameras are distributed in such a way that they face toward a projection surface in our system. This assumption is reasonable as recent projection-based MR/AR applications employ the same setup (Bimber et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Wetzstein G, Emmerling A, Nitschke C (2005) Enabling view-dependent stereoscopic projection in real environments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 14–23" href="/article/10.1007/s10055-014-0250-4#ref-CR4" id="ref-link-section-d36541e428">2005</a>).</p><p>Geometric correction and radiometric compensation of projected images are required to display the desired images from the projectors. Because various techniques to solve these issues already exist, as summarized in (Bimber et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector–camera systems. Comput Graph Forum 27(8):2219–2254" href="/article/10.1007/s10055-014-0250-4#ref-CR5" id="ref-link-section-d36541e434">2008</a>), we apply these existing techniques, in particular that in (Abdel-Aziz and Karara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Abdel-Aziz YI, Karara HM (1971) Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry. In: Proceedings of the symposium on close-range photogrammetry (American Society of Photogrammetry), pp 1–18&#xA;" href="/article/10.1007/s10055-014-0250-4#ref-CR21" id="ref-link-section-d36541e437">1971</a>) for the geometric correction and that in (Yoshida et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia, pp 161–168" href="/article/10.1007/s10055-014-0250-4#ref-CR22" id="ref-link-section-d36541e440">2003</a>) for radiometric compensation. To this end, we measure the shape of the projection surface as well as the poses and positions of the projectors and cameras in advance. However, the shape, position, and pose of the occluder remain unknown. We also assume that the reflectance property of the projection surface is Lambertian. Note that these assumptions are common in most projection-based MR/AR systems.</p><h3 class="c-article__sub-heading" id="Sec5">Projection surface estimation by synthetic aperture capturing</h3><p>To ensure that shadows from the projectors on the projection surface are completely removed, the whole shape of the occluder should be measured, but the measurement can be approximate as long as the measured shape contains the occluder. To meet this requirement, we apply a visual hull technique to reconstruct the occluder shape. Visual hull reconstruction obtains the shape of an object as an intersection of multiple silhouettes captured from different positions.</p><p>We need to estimate the background appearance for the silhouette extraction of the visual hull reconstruction. Note that in this study, the background is identical to the projection surface. A fixed background appearance captured in advance is not suitable in our context, because it is highly possible that while interacting with the system, the user’s movement affects the illumination from the environment lighting, and consequently, the background subtraction fails. Therefore, we employ a synthetic aperture capturing technique (Vaish et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Vaish V, Levoy M, Szeliski R, Zitnick CL, Kang SB (2006) Reconstructing occluded surfaces using synthetic apertures: stereo, focus and robust measures. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 2331–2338" href="/article/10.1007/s10055-014-0250-4#ref-CR19" id="ref-link-section-d36541e454">2006</a>) to estimate the projection surface for every frame. The synthetic aperture capturing technique realizes a virtual camera with an incredibly large aperture by digitally aligning multiple images taken from distributed cameras. Owing to the large aperture, the camera can “see-through” objects or occluders that are smaller than the aperture and placed between the cameras and the virtual focal plane.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Synthetic aperture capturing technique</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In particular, we independently estimate the appearance <span class="mathjax-tex">\(a_p\)</span> of each point <span class="mathjax-tex">\(p\ (=1,\ldots ,N_p)\)</span> on the projection surface. For each point <span class="mathjax-tex">\(p\)</span>, we select a color value from each camera <span class="mathjax-tex">\(c\ (=1,\ldots ,N_c)\)</span> at the corresponding pixel in its captured image <span class="mathjax-tex">\(I_c(u,v)\)</span>. The point-to-pixel correspondence can be acquired on the basis of geometric information, such as the shape of the surface and position/pose of the camera relative to the surface, which is calibrated offline. We represent <span class="mathjax-tex">\(i_p^c\)</span> as the color value of the corresponding pixel in the captured image of camera <span class="mathjax-tex">\(c\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig2">2</a>). We finally take the median of the color values of all cameras in the system to estimate the appearance <span class="mathjax-tex">\(a_p\)</span> of point <span class="mathjax-tex">\(p\)</span>.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} a_{p} = \text {median}(i_p^1,\ldots ,i_{p}^{N_c}). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>We can estimate the appearance of the entire surface by simply repeating the process described above for the other points on the surface. Then, the background image <span class="mathjax-tex">\(B_c(u,v)\)</span> of each camera <span class="mathjax-tex">\(c\)</span> is synthesized by aligning the estimated appearance of each point <span class="mathjax-tex">\(a_p\)</span> on the basis of the point-to-pixel correspondence described above.</p><p>The offline geometric calibration is done as follows. First, we place a physical calibration object, on which visual markers that define the world coordinate system are painted, in front of the projection surface. Second, the cameras capture the calibration object to acquire 2D (camera coordinates) and 3D (world coordinate) correspondences. From the correspondences, we calibrate the cameras’ intrinsic and extrinsic parameters using the direct linear transformation (DLT) method (Abdel-Aziz and Karara <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Abdel-Aziz YI, Karara HM (1971) Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry. In: Proceedings of the symposium on close-range photogrammetry (American Society of Photogrammetry), pp 1–18&#xA;" href="/article/10.1007/s10055-014-0250-4#ref-CR21" id="ref-link-section-d36541e929">1971</a>). Third, the projectors project structured light patterns, such as the gray code pattern (Sato and Inokuchi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision, pp 657–661" href="/article/10.1007/s10055-014-0250-4#ref-CR15" id="ref-link-section-d36541e932">1987</a>), onto the calibration object to acquire 2D (projector coordinates) and 3D (world coordinate) correspondences. The projectors’ intrinsic and extrinsic parameters are then calibrated from the correspondences using the DLT method. Finally, we measure the shape of the projection surface in the world coordinate system. We project structured light patterns from one of the calibrated projectors to the projection surface and capture the reflections by one of the cameras, to acquire pixel correspondences between the projector and the camera. From the correspondences and the calibration results of the projector and camera, we measure the 3D shape of the surface in the world coordinate system (Sato and Inokuchi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision, pp 657–661" href="/article/10.1007/s10055-014-0250-4#ref-CR15" id="ref-link-section-d36541e935">1987</a>).</p><h3 class="c-article__sub-heading" id="Sec6">Visual hull reconstruction</h3><p>Our visual hull reconstruction technique is based on the technique previously proposed by (Laurentini <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Laurentini A (1994) The visual hull concept for silhouette-based image understanding. IEEE Trans Pattern Anal Mach Intell 16(2):150–162" href="/article/10.1007/s10055-014-0250-4#ref-CR11" id="ref-link-section-d36541e946">1994</a>). The binary silhouette image <span class="mathjax-tex">\(S_c(u,v)\)</span> of camera <span class="mathjax-tex">\(c\)</span> is computed by thresholding the subtraction of the background image <span class="mathjax-tex">\(B_c\)</span> from the captured image <span class="mathjax-tex">\(I_c\)</span> as follows:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} S_{c}(u,v)= \left\{ \begin{array}{ll} 1, &amp;{} \text{ if } |I_{c}(u,v)-B_{c}(u,v)|&gt;t_{s},\\ 0, &amp;{} \text{ otherwise}.\\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(t_s\)</span> represents the predefined threshold.</p><p>After acquiring the silhouette images from all the cameras in the system, we reconstruct the shape from them. We assume a tessellation of the reconstruction space into discrete voxels <span class="mathjax-tex">\(V(x,y,z)\)</span>. The voxels are binarily labeled as either transparent or opaque, where the latter represents the element of the occluder’s volume.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Visual hull reconstruction technique</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>We project the voxels on camera image planes and carve <span class="mathjax-tex">\(V\)</span> by labeling them as transparent when at least one projection corresponds to the pixel value of 0 in a silhouette image <span class="mathjax-tex">\(S_c\)</span>. We denote the opaque and transparent voxels as <span class="mathjax-tex">\(V_\mathrm{o}\)</span> and <span class="mathjax-tex">\(V_\mathrm{t}\)</span>, respectively. All opaque voxels <span class="mathjax-tex">\(V_\mathrm{o}\)</span> belong to the visual hull that encloses the object (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig3">3</a>).</p><h3 class="c-article__sub-heading" id="Sec7">Shadow removal</h3><p>Once the 3D information of an occluder is recovered, we can compute which projector is visible from each point on the projection surface. Our technique can project compensation images onto the shadow region from unoccluded projectors to remove it. However, in some cases, this is not desirable if the target appearance is divided and assigned to the unoccluded projectors by considering only visibility. This is because a new shadow may emerge in the next frame when the occluder moves into the view frustum of the unoccluded projector. In such a case, it is obviously advisable to assign a large pixel value to the unoccluded projector whose view frustum is distant from the occluder in order to avoid the emergence of another shadow in the next frame. However, such a projector may provide poor image quality (hereafter, referred to as low spatial resolution) due to a steep grazing angle of incident light rays or a large distance from the projector to the surface.</p><p>Therefore, we propose a new dividing technique for the desired appearance of a shadow area, which considers the following three criteria: (1) the visibility of projectors, (2) the likelihood of new shadow emergence (LNSE), and (3) the spatial resolution of projected imagery. In particular, the target appearance <span class="mathjax-tex">\(a^t_p(l)\)</span> at point <span class="mathjax-tex">\(p\)</span> that a projector <span class="mathjax-tex">\(l\ (=1,\ldots ,N_l)\)</span> should display is decided by dividing the final target appearance <span class="mathjax-tex">\(a^t_p\)</span> according to the above-mentioned criteria. In other words, our technique decides the weights <span class="mathjax-tex">\(w_p(l)\)</span> (<span class="mathjax-tex">\(0\le w_p(l)\le 1\)</span>) to divide the target appearance:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} a^{t}_{p}(l)=a^{t}_{p}w_{p}(l), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{l=1}^{N_l}w_{p}(l)=1. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>We design the weights so that the second and third criteria can be balanced by each user. Therefore, the weights are decomposed as per the following equation:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w_{p}(l)=\alpha w_{p}^{L}(l)+(1-\alpha )w_{p}^{S}(l), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{l=1}^{N_l}w_{p}^{L}(l)=1,\ \sum _{l=1}^{N_{l}}w_{p}^{S}(l)=1,\ 0\le \alpha \le 1. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>Here, <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(w_p^L\)</span> and <span class="mathjax-tex">\(w_p^S\)</span> are the user-defined balance coefficient, the weight for the second criterion (i.e., the LNSE) and that for the third criterion (i.e., spatial resolution), respectively.</p><p>
                  <i>Visibility of projector.</i> For each point <span class="mathjax-tex">\(p\)</span> on the surface, we check the visibility of each projector <span class="mathjax-tex">\(l\ (=1,\ldots ,N_l)\)</span>. This is done by comparing the actual distance from the point to the projector and the depth value from the depth map computed by rendering the scene, including the projection surface and occluder, from the projector’s viewpoint. If the distance and depth values are different, the projector is regarded as invisible from the point. We define a binary function <span class="mathjax-tex">\(\text {vis}_p(l)\)</span> that takes the value 1 if the projector <span class="mathjax-tex">\(l\)</span> is visible from the point <span class="mathjax-tex">\(p\)</span>.</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \text {vis}_p(l)= \left\{ \begin{array}{ll} 1, &amp;{} \text{ if } l \text{ is } \text{ visible } \text{ from } p,\\ 0, &amp;{} \text{ otherwise}.\\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Parameters for shadow removal</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>
                  <i>Likelihood of new shadow emergence.</i> For each point <span class="mathjax-tex">\(p\)</span>, we calculate a distance <span class="mathjax-tex">\(d_p(l,V_\mathrm{o})\)</span> from each opaque voxel <span class="mathjax-tex">\(V_\mathrm{o}(x_\mathrm{o},y_\mathrm{o},z_\mathrm{o})\)</span> to the light ray emitted from each visible projector to the point. Then, we search for the minimum distance <span class="mathjax-tex">\(d^{\mathrm{min}}_p(l)\)</span> for each visible projector:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} d^{\mathrm{min}}_{p}(l) = \min _{V_{o}} d_{p}(l,V_o)vis_{p}(l). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>When we consider the LNSE, we assign weighting values to the visible projectors so that the weights are proportional to the minimum distances (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig4">4</a>). Thus,</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w_{p}^{L}(l)=\frac{d^{\mathrm{min}}_{p}(l)}{\sum _{l'=1}^{N_{l}}d^{\mathrm{min}}_{p}(l')}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>
                  <i>Spatial resolution.</i> The spatial resolution of the projected result depends on the point spread function (PSF) of the projected pixel, because the result can be represented as the convolution of the original image and the PSF. Researchers have proposed projector selection techniques for multi-projector systems in the context of extending the depth of field of the systems (Bimber and Emmerling <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bimber O, Emmerling A (2006) Multi-focal projection: a multi-projector technique for increasing focal depth. IEEE Trans Vis Comput Graph 12(4):658–667" href="/article/10.1007/s10055-014-0250-4#ref-CR3" id="ref-link-section-d36541e2974">2006</a>; Nagase et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Nagase M, Iwai D, Sato K (2011) Dynamic defocus and occlusion compensation of projected imagery by model-based optimal projector selection in multi-projection environment. Virtual Real 15(2):119–132" href="/article/10.1007/s10055-014-0250-4#ref-CR13" id="ref-link-section-d36541e2977">2011</a>). In these works, for each point on a projection surface, one of the projectors is selected to display a pixel on that point. For example, one of the previous works (Bimber and Emmerling <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bimber O, Emmerling A (2006) Multi-focal projection: a multi-projector technique for increasing focal depth. IEEE Trans Vis Comput Graph 12(4):658–667" href="/article/10.1007/s10055-014-0250-4#ref-CR3" id="ref-link-section-d36541e2981">2006</a>) selected the projector with the smallest PSF. An alternative technique was proposed by Nagase et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Nagase M, Iwai D, Sato K (2011) Dynamic defocus and occlusion compensation of projected imagery by model-based optimal projector selection in multi-projection environment. Virtual Real 15(2):119–132" href="/article/10.1007/s10055-014-0250-4#ref-CR13" id="ref-link-section-d36541e2984">2011</a>; it approximated the projector pixel PSF as an isotropic Gaussian function, and consequently, the shape of the PSF as an ellipse. Then, it selected a projector whose major axis of the PSF ellipse had the shortest length. Our method is based on the latter technique because it has been shown in (Nagase et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Nagase M, Iwai D, Sato K (2011) Dynamic defocus and occlusion compensation of projected imagery by model-based optimal projector selection in multi-projection environment. Virtual Real 15(2):119–132" href="/article/10.1007/s10055-014-0250-4#ref-CR13" id="ref-link-section-d36541e2987">2011</a>) that it provides better image quality. In particular, for each point <span class="mathjax-tex">\(p\)</span>, we calculate the major axis length <span class="mathjax-tex">\(r_p(l)\)</span> of the PSF of a pixel projected from each projector <span class="mathjax-tex">\(l\)</span> to the point (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig4">4</a>). Then, we compute the maximum length <span class="mathjax-tex">\(r_p^{\mathrm{max}}\)</span> of the major axes at the point <span class="mathjax-tex">\(p\)</span> among the projectors. Thus,</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} r_{p}^{\mathrm{max}}=\max _{l} r_{p}(l). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>We assign weighting values to the visible projectors so that the weight is high when the length is small. Thus,</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w_{p}^{S}(l)=\frac{\frac{1}{r_{p}(l)}\text {vis}_{p}(l)-\frac{1}{r_{p}^{\mathrm{max}}}}{\sum _{l'=1}^{N_{l}}\left( \frac{1}{r_{p}(l')}\text {vis}_{p}(l)-\frac{1}{r_{p}^{\mathrm{max}}}\right) }. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>If there is no occlusion, we assign the evenly divided appearance of the original image to the projectors. Thus,</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w_{p}(l)=\frac{1}{N_{l}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Overview of the system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Experiment</h2><div class="c-article-section__content" id="Sec8-content"><p>We conducted a proof-of-concept experiment to evaluate the proposed shadow removal technique on a real projection system prototype. The prototype projector-camera system was implemented as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig5">5</a>. The system consisted of four projectors (Acer K10, 100 ANSI Lumen, <span class="mathjax-tex">\(800\times 600\)</span> pixel) and five cameras (Point Grey Research Chameleon, <span class="mathjax-tex">\(1,290\times 960\)</span> pixel), which were connected to and controlled by a single PC. We assigned a square region on a planar surface as the projection surface of the experiment. The projectors and cameras were placed so that they were facing the projection surface. We calibrated the projectors and cameras in advance as described in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0250-4#Sec4">3.1</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0250-4#Sec5">3.2</a>. We prepared a sphere as an occluder. Because our system estimated the background in every frame, we could stably extract the silhouette of the occluder without having to assign a large value to the threshold <span class="mathjax-tex">\(t_s\)</span>. We particularly used <span class="mathjax-tex">\(t_s=20\)</span> in the range of <span class="mathjax-tex">\(0\le t_s\le 255\)</span>. In a general shape reconstruction context, the size of the voxel tessellation in the visual hull reconstruction must be small enough to measure an object’s shape as precisely as possible. However, in the case of our system, the voxel does not need to be so small because the measurement can be approximate as long as the measured shape contains the occluder. Therefore, we set the size of the voxel tessellation as 5 mm in the following experiment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Captured scene of each frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Target image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The experiment involved four successive frames; we moved the occluder in every frame. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig6">6</a> shows the scene of each frame captured by a camera in the system. We projected an image of two parrots (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig7">7</a>) onto the rectangle region, with shadows caused by the occluder in every frame. From the first frame to the last, we moved the occluder from the left to the right of the scene, occluding the projection lights from the first frame. Note that there was no occluder before the first frame. The movement of the occluder could be repeated so that we could compare the shadow removal results under different conditions. The experiment was conducted under two conditions where the desired appearance of the shadow area was divided and assigned to unoccluded projectors on the basis of either (1) the likelihood of new shadow emergence (i.e., <span class="mathjax-tex">\(\alpha =1.0\)</span> in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0250-4#Equ5">5</a>) or (2) the spatial resolution of the projected result (i.e., <span class="mathjax-tex">\(\alpha =0.0\)</span>). We denote the former condition as the <i>LNSE condition</i> and the latter as the <i>SR condition</i>. Under both conditions, the movement of the occluder was the same as that shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Results of visual hull reconstruction (<i>gray quadrilateral</i>: the projection surface, <i>black dots</i>: reconstructed voxels)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Shadow removal results obtained by synthetic aperture capturing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Magnified view around the face of the right parrot in the fourth frame. The SR condition provides a better image quality than the LNSE condition, that displays unclear smeared pixels</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 SSIM evaluation results</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0250-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig8">8</a> shows the results of visual hull reconstruction. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig9">9</a> shows the experimental results for shadow removal under the two conditions. The figures were obtained by the synthetic aperture capturing technique described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0250-4#Sec5">3.2</a>. The projected results under the SR condition showed that the new occluder shadows could not be removed in all the frames. On the other hand, the results under the LNSE condition showed that the shadows could be removed in the second and fourth frames more effectively than for the SR condition; furthermore, they were completely removed in the third frame. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig10">10</a> shows two magnified views around the face of the right parrot in the fourth frame. The SR condition decreases the weights of projectors that project a pixel over a wider area to achieve the best image quality in terms of spatial resolution. Therefore, at a place in the projection surface on which no shadow is cast, the SR condition can provide a better image quality than the LNSE condition, which does not explicitly consider the spatial resolution. Consequently, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig10">10</a>, the LNSE condition degrades the image quality of the displayed result that displays unclear smeared pixels. We objectively evaluated the image quality using the structural similarity index (SSIM), which is a method for assessing the perceptual quality of a distorted image when compared to the original (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig7">7</a> in this experiment) (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Perceptual image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process 13(4):600–612" href="/article/10.1007/s10055-014-0250-4#ref-CR20" id="ref-link-section-d36541e3992">2004</a>). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0250-4#Tab1">1</a> shows the result. In summary, we confirmed that the newly emerged shadows could be removed more effectively, and the overall image quality was better under the LNSE condition, while the spatial resolution of the projected result was better under the SR condition.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0250-4/MediaObjects/10055_2014_250_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>
                        <b>a</b> Weights <span class="mathjax-tex">\(w_p(l)\)</span> for each projector in each frame. The four weight maps in each frame correspond to the four projectors of the system. Higher intensity indicates a higher weight. <b>b</b> Projection images in the fourth frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0250-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig11">11</a>a shows the weights <span class="mathjax-tex">\(w_p(l)\)</span> (see Eqs. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0250-4#Equ3">3</a> and <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0250-4#Equ4">4</a>) for each projector in each frame. The projected images based on the weights are also shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig11">11</a>b. We confirmed that the weights were dynamically changed according to the movement of the occluder. In addition, we confirmed that the weights were evenly distributed among the projectors in the first frame because there was no occluder before the first frame. Therefore, in the first frame, the same number of shadows emerged in the projected results under both conditions, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0250-4#Fig9">9</a>. The running time was 8.1 seconds for each frame in the experiment.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Discussion</h2><div class="c-article-section__content" id="Sec9-content"><p>We compared our method with the most relevantly related work (Audet and Cooperstock <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Audet S, Cooperstock JR (2007) Shadow removal in front projection environments using object tracking. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1–8" href="/article/10.1007/s10055-014-0250-4#ref-CR1" id="ref-link-section-d36541e4126">2007</a>). The authors of the work developed their 3D tracking model by assuming that the occluder would stand vertically on the floor, which is usually the case with people. They also applied prediction of the occluders movement based on a Kalman filter. Therefore, in the context of virtual rear projection screen for presentations, their method may work better than our method. On the other hand, their model had a special geometrical process that only worked for the assumed occluders, i.e., people. Because our method does not make any assumption about the occluder, it should work better in other contexts than this related work.</p><p>Nevertheless, there is a problem with measuring the 3D shape of a moving occluder at the current frame, which is then used to update the weights of the projectors for the next frame. There will always be a latency between the actual new position of the occluder and the estimated positions. In case the occluder moves too fast, the shadow cannot be perfectly removed, particularly on the shadow edge, even under the LNSE condition. To reduce the errors on the shadow edge, shadow prediction can be used. We will apply this method to improve our system in the future.</p><p>The running time of the experiment showed that the proposed approach did not run in real time (30 frames per second). However, the current implementation was not well optimized (i.e., all processes ran on a CPU). The most time-consuming part was the 2D image warping process in synthetic aperture capturing. We can make this process significantly faster using GPU parallel architecture. Another improvement can be done by applying GPU optimization, such as in the work of (Ladikos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: Proceedings of IEEE workshop on visual computer vision on GPU" href="/article/10.1007/s10055-014-0250-4#ref-CR10" id="ref-link-section-d36541e4135">2008</a>), to our visual hull reconstruction. In the future, we will improve the implementation to realize real-time processing.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Conclusion</h2><div class="c-article-section__content" id="Sec10-content"><p>In this study, we proposed a shadow removal technique using a multiple overlapping projection system. In particular, we focused on the case where cameras cannot be placed between an occluder and a projection surface. We applied a synthetic aperture capturing technique to estimate the appearance of the projection surface, and a visual hull reconstruction technique to measure the occluder shape. Once the shape was acquired, shadow regions on the surface could be estimated. Our proposed shadow removal technique allows users to balance between the following two criteria: the LNSE and the spatial resolution of projected results. Through a real projection experiment, we confirmed that the technique that considers the LNSE (LNSE) could remove the shadows well, while that considering the other criterion (SR) could display clear images on the surface.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Microsoft, Kinect: <a href="http://www.xbox.com/kinect/">http://www.xbox.com/kinect/.</a>
                  </p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Abdel-Aziz YI, Karara HM (1971) Direct linear transformation from comparator coordinates into object space coo" /><p class="c-article-references__text" id="ref-CR21">Abdel-Aziz YI, Karara HM (1971) Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry. In: Proceedings of the symposium on close-range photogrammetry (American Society of Photogrammetry), pp 1–18
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Audet S, Cooperstock JR (2007) Shadow removal in front projection environments using object tracking. In: Proc" /><p class="c-article-references__text" id="ref-CR1">Audet S, Cooperstock JR (2007) Shadow removal in front projection environments using object tracking. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR2">Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of IEEE/ACM international symposium on augmented teality, pp 207–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Bimber, A. Emmerling, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Bimber O, Emmerling A (2006) Multi-focal projection: a multi-projector technique for increasing focal depth. I" /><p class="c-article-references__text" id="ref-CR3">Bimber O, Emmerling A (2006) Multi-focal projection: a multi-projector technique for increasing focal depth. IEEE Trans Vis Comput Graph 12(4):658–667</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2006.75" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-focal%20projection%3A%20a%20multi-projector%20technique%20for%20increasing%20focal%20depth&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=12&amp;issue=4&amp;pages=658-667&amp;publication_year=2006&amp;author=Bimber%2CO&amp;author=Emmerling%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bimber O, Wetzstein G, Emmerling A, Nitschke C (2005) Enabling view-dependent stereoscopic projection in real " /><p class="c-article-references__text" id="ref-CR4">Bimber O, Wetzstein G, Emmerling A, Nitschke C (2005) Enabling view-dependent stereoscopic projection in real environments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 14–23</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Bimber, D. Iwai, G. Wetzstein, A. Grundhöfer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector–camera systems. Comput Gr" /><p class="c-article-references__text" id="ref-CR5">Bimber O, Iwai D, Wetzstein G, Grundhöfer A (2008) The visual computing of projector–camera systems. Comput Graph Forum 27(8):2219–2254</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2008.01175.x" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20computing%20of%20projector%E2%80%93camera%20systems&amp;journal=Comput%20Graph%20Forum&amp;volume=27&amp;issue=8&amp;pages=2219-2254&amp;publication_year=2008&amp;author=Bimber%2CO&amp;author=Iwai%2CD&amp;author=Wetzstein%2CG&amp;author=Grundh%C3%B6fer%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cham TJ, Rehg JM, Sukthankar R, Sukthankar G (2003) Shadow elimination and occluder light suppression for mult" /><p class="c-article-references__text" id="ref-CR6">Cham TJ, Rehg JM, Sukthankar R, Sukthankar G (2003) Shadow elimination and occluder light suppression for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 513–520</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. " /><p class="c-article-references__text" id="ref-CR7">Jaynes C, Webb S, Steele RM, Brown M, Seales WB (2001) Dynamic shadow removal from front projection displays. In: Proceedings of IEEE visualization, pp 175–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Jaynes, S. Webb, RM. Steele, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Jaynes C, Webb S, Steele RM (2004) Camera-based detection and removal of shadows from interactive multiproject" /><p class="c-article-references__text" id="ref-CR8">Jaynes C, Webb S, Steele RM (2004) Camera-based detection and removal of shadows from interactive multiprojector displays. IEEE Trans Vis Comput Graph 10(3):290–301</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2004.1272728" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Camera-based%20detection%20and%20removal%20of%20shadows%20from%20interactive%20multiprojector%20displays&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=10&amp;issue=3&amp;pages=290-301&amp;publication_year=2004&amp;author=Jaynes%2CC&amp;author=Webb%2CS&amp;author=Steele%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jones BR, Sodhi R, Campbell RH, Garnett G, Bailey BP (2010) Build your world and play in it: interacting with " /><p class="c-article-references__text" id="ref-CR9">Jones BR, Sodhi R, Campbell RH, Garnett G, Bailey BP (2010) Build your world and play in it: interacting with surface particles on complex objects. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 165–174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using" /><p class="c-article-references__text" id="ref-CR10">Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: Proceedings of IEEE workshop on visual computer vision on GPU</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Laurentini, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Laurentini A (1994) The visual hull concept for silhouette-based image understanding. IEEE Trans Pattern Anal " /><p class="c-article-references__text" id="ref-CR11">Laurentini A (1994) The visual hull concept for silhouette-based image understanding. IEEE Trans Pattern Anal Mach Intell 16(2):150–162</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.273735" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20hull%20concept%20for%20silhouette-based%20image%20understanding&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=16&amp;issue=2&amp;pages=150-162&amp;publication_year=1994&amp;author=Laurentini%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Low KL, Welch G, Lastra A, Fuchs H (2001) Life-sized projector-based dioramas. In: Proceedings of ACM symposiu" /><p class="c-article-references__text" id="ref-CR12">Low KL, Welch G, Lastra A, Fuchs H (2001) Life-sized projector-based dioramas. In: Proceedings of ACM symposium on virtual reality software and technology, pp 93–101</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Nagase, D. Iwai, K. Sato, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Nagase M, Iwai D, Sato K (2011) Dynamic defocus and occlusion compensation of projected imagery by model-based" /><p class="c-article-references__text" id="ref-CR13">Nagase M, Iwai D, Sato K (2011) Dynamic defocus and occlusion compensation of projected imagery by model-based optimal projector selection in multi-projection environment. Virtual Real 15(2):119–132</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-010-0168-4" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20defocus%20and%20occlusion%20compensation%20of%20projected%20imagery%20by%20model-based%20optimal%20projector%20selection%20in%20multi-projection%20environment&amp;journal=Virtual%20Real&amp;volume=15&amp;issue=2&amp;pages=119-132&amp;publication_year=2011&amp;author=Nagase%2CM&amp;author=Iwai%2CD&amp;author=Sato%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumi" /><p class="c-article-references__text" id="ref-CR14">Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of eurographics workshop on rendering, pp 89–102</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE " /><p class="c-article-references__text" id="ref-CR15">Sato K, Inokuchi S (1987) Range-imaging system utilizing nematic liquid crystal mask. In: Proceedings of IEEE international conference on computer vision, pp 657–661</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sugaya Y, Miyagawa I, Koike H (2010) Contrasting shadow for occluder light suppression from one-shot image. In" /><p class="c-article-references__text" id="ref-CR16">Sugaya Y, Miyagawa I, Koike H (2010) Contrasting shadow for occluder light suppression from one-shot image. In: Proceedings of international workshop on projector–camera systems, pp 13–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sukthankar R, Cham TJ, Sukthankar G (2001) Dynamic shadow elimination for multi-projector displays. In: Procee" /><p class="c-article-references__text" id="ref-CR17">Sukthankar R, Cham TJ, Sukthankar G (2001) Dynamic shadow elimination for multi-projector displays. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 151–157</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Summet, M. Flagg, TJ. Cham, JM. Rehg, R. Sukthankar, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Summet J, Flagg M, Cham TJ, Rehg JM, Sukthankar R (2007) Shadow elimination and blinding light suppression for" /><p class="c-article-references__text" id="ref-CR18">Summet J, Flagg M, Cham TJ, Rehg JM, Sukthankar R (2007) Shadow elimination and blinding light suppression for interactive projected displays. IEEE Trans Vis Comput Graph 13(3):508–517</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2007.1007" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Shadow%20elimination%20and%20blinding%20light%20suppression%20for%20interactive%20projected%20displays&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=13&amp;issue=3&amp;pages=508-517&amp;publication_year=2007&amp;author=Summet%2CJ&amp;author=Flagg%2CM&amp;author=Cham%2CTJ&amp;author=Rehg%2CJM&amp;author=Sukthankar%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vaish V, Levoy M, Szeliski R, Zitnick CL, Kang SB (2006) Reconstructing occluded surfaces using synthetic aper" /><p class="c-article-references__text" id="ref-CR19">Vaish V, Levoy M, Szeliski R, Zitnick CL, Kang SB (2006) Reconstructing occluded surfaces using synthetic apertures: stereo, focus and robust measures. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 2331–2338</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Wang, AC. Bovik, HR. Sheikh, EP. Simoncelli, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Perceptual image quality assessment: from error visibility t" /><p class="c-article-references__text" id="ref-CR20">Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Perceptual image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process 13(4):600–612</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIP.2003.819861" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=13&amp;issue=4&amp;pages=600-612&amp;publication_year=2004&amp;author=Wang%2CZ&amp;author=Bovik%2CAC&amp;author=Sheikh%2CHR&amp;author=Simoncelli%2CEP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projectio" /><p class="c-article-references__text" id="ref-CR22">Yoshida T, Horii C, Sato K (2003) A virtual color reconstruction system for real heritage with light projection. In: Proceedings of international conference on virtual systems and multimedia, pp 161–168</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0250-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Graduate School of Engineering Science, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan</p><p class="c-article-author-affiliation__authors-list">Daisuke Iwai, Momoyo Nagase &amp; Kosuke Sato</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Daisuke-Iwai"><span class="c-article-authors-search__title u-h3 js-search-name">Daisuke Iwai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daisuke+Iwai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daisuke+Iwai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daisuke+Iwai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Momoyo-Nagase"><span class="c-article-authors-search__title u-h3 js-search-name">Momoyo Nagase</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Momoyo+Nagase&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Momoyo+Nagase" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Momoyo+Nagase%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kosuke-Sato"><span class="c-article-authors-search__title u-h3 js-search-name">Kosuke Sato</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kosuke+Sato&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kosuke+Sato" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kosuke+Sato%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0250-4/email/correspondent/c1/new">Daisuke Iwai</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Shadow%20removal%20of%20projected%20imagery%20by%20occluder%20shape%20measurement%20in%20a%20multiple%20overlapping%20projection%20system&amp;author=Daisuke%20Iwai%20et%20al&amp;contentID=10.1007%2Fs10055-014-0250-4&amp;publication=1359-4338&amp;publicationDate=2014-08-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0250-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0250-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Iwai, D., Nagase, M. &amp; Sato, K. Shadow removal of projected imagery by occluder shape measurement in a multiple overlapping projection system.
                    <i>Virtual Reality</i> <b>18, </b>245–254 (2014). https://doi.org/10.1007/s10055-014-0250-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0250-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-16">16 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08-08">08 August 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08-22">22 August 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11">November 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0250-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0250-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Shadow removal</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multiple overlapping projection</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Synthetic aperture capturing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual hull</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0250-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=250;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

