<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An augmented reality interface for visualizing and interacting with vi"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtual information and processing of tracking..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/11/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An augmented reality interface for visualizing and interacting with virtual content"/>

    <meta name="dc.source" content="Virtual Reality 2006 11:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-11-09"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtual information and processing of tracking data for natural interaction. Participants within the system can experience a real-time mixture of 3D objects, static video, images, textual information and 3D sound with the real environment. The user-friendly AR interface can achieve maximum interaction using simple but effective forms of collaboration based on the combinations of human&#8211;computer interaction techniques. To prove the feasibility of the interface, the use of indoor AR techniques are employed to construct innovative applications and demonstrate examples from heritage to learning systems. Finally, an initial evaluation of the AR interface including some initial results is presented."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-11-09"/>

    <meta name="prism.volume" content="11"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="23"/>

    <meta name="prism.endingPage" content="43"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0055-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0055-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0055-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0055-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An augmented reality interface for visualizing and interacting with virtual content"/>

    <meta name="citation_volume" content="11"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2007/03"/>

    <meta name="citation_online_date" content="2006/11/09"/>

    <meta name="citation_firstpage" content="23"/>

    <meta name="citation_lastpage" content="43"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0055-1"/>

    <meta name="DOI" content="10.1007/s10055-006-0055-1"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0055-1"/>

    <meta name="description" content="In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtu"/>

    <meta name="dc.creator" content="Fotis Liarokapis"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Angel E (2003) Interactive computer graphics: a top-down approach using OpenGL,3rd edn. Addison&#8211;Wesley, Reading, pp 17&#8211;18, 69, 107, 322&#8211;349, 472"/>

    <meta name="citation_reference" content="citation_journal_title=Teleoper Virtual Environ; citation_title=A survey of augmented reality; citation_author=R Azuma; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=355-385; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph November/December; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-47; citation_doi=10.1109/38.963459; citation_id=CR3"/>

    <meta name="citation_reference" content="Begault DR (1994) 3D Sound for virtual reality and multimedia, Academic, New York, 1, 17&#8211;18"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=The magicbook: a traditional AR interface; citation_author=M Billinghurst, H Kato, I Poupyrev; citation_volume=25; citation_publication_date=2001; citation_pages=745-753; citation_doi=10.1016/S0097-8493(01)00117-0; citation_id=CR5"/>

    <meta name="citation_reference" content="Butz A, H&#246;llerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: Proceedings of the 2nd IEEE and ACM international workshop on augmented reality &#8216;99. San Francisco, October 20&#8211;21 "/>

    <meta name="citation_reference" content="Camera calibration toolbox for Matlab, available at: [
                    http://www.vision.caltech.edu/bouguetj/calib_doc/
                    
                  ], Accessed at 14/01/2003"/>

    <meta name="citation_reference" content="Dobler D, Haller M, Stampfl P (2002) ASR&#8212;augmented sound reality, ACM SIGGRAPH 2002 conference abstracts and applications, San Antonio, p 148"/>

    <meta name="citation_reference" content="Feiner S, MacIntyre B et al (1993) Windows on the world: 2D Windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, Atlanta, November 3&#8211;5, Association for Computing Machinery, pp 145&#8211;155"/>

    <meta name="citation_reference" content="Fernandes B, Miranda JC (2003) Learning how computer works with augmented reality. In: Proceedings of the 2nd international conference on multimedia and information and communication technologies in education, Badajoz, December 3&#8211;6"/>

    <meta name="citation_reference" content="Fuhrmann A, Schmalstieg D (1999) Concept and implementation of a collaborative workspace for augmented reality, GRAPHICS &#8216;99, 18(3)"/>

    <meta name="citation_reference" content="Gatermann H (2000) From VRML to augmented reality via panorama-integration and EAI-Java, in constructing the digital space. In: Proceeding of the SiGraDi, September, 254&#8211;256"/>

    <meta name="citation_reference" content="Grasset R, Gascuel J-D (2002) MARE: multiuser augmented reality environment on table setup. ACM SIGGRAPH conference abstracts and applications"/>

    <meta name="citation_reference" content="Hall T, Ciolfi L et al (2001) The visitor as virtual archaeologist: using mixed reality technology to enhance education and social interaction in the museum. In: Spencer S (ed) Proceedings of the virtual reality, archaeology, and cultural heritage (VAST 2001), New York, ACM SIGGRAPH, Glyfada, Nr Athens, November, pp 91&#8211;96"/>

    <meta name="citation_reference" content="Haller M, Hartmann W et al (2002) Combining ARToolKit with scene graph libraries. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt, Germany, 29 September"/>

    <meta name="citation_reference" content="citation_journal_title=J Three Dimens Images; citation_title=Categorizing augmented reality systems; citation_author=D Haniff, C Baber, W Edmondson; citation_volume=14; citation_issue=4; citation_publication_date=2000; citation_pages=105-109; citation_id=CR16"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality 2000, Munich,  5&#8211;6 Oct, pp  111&#8211;119"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington "/>

    <meta name="citation_reference" content="citation_journal_title=August; citation_title=Confluence of computer vision and interactive graphics for augmented reality, PRESENCE: teleoperations and virtual environments. special issue on augmented reality; citation_author=G Klinker, KH Ahlers; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=433-451; citation_id=CR19"/>

    <meta name="citation_reference" content="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761&#8211;767"/>

    <meta name="citation_reference" content="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47&#8211;56"/>

    <meta name="citation_reference" content="Liarokapis F (2005) Augmented reality interfaces&#8212;architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer"/>

    <meta name="citation_reference" content="citation_journal_title=World Trans Eng Technol Educ; citation_title=Multimedia augmented reality interface for E-learning (MARIE); citation_author=null Liarokapis, P Petridis, PF Lister, M White; citation_volume=1; citation_issue=2; citation_publication_date=2002; citation_pages=173-176; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph (TOG); citation_title=DART: a toolkit for rapid design exploration of augmented reality experiences; citation_author=B MacIntyre, M Gandy, S Dow, JD Bolter; citation_volume=24; citation_issue=3; citation_publication_date=2005; citation_pages=932; citation_doi=10.1145/1073204.1073288; citation_id=CR24"/>

    <meta name="citation_reference" content="Mahoney D (1999b) Better than real, computer graphics world, pp 32&#8211;40"/>

    <meta name="citation_reference" content="Malbezin P, Piekarski W and Thomas B (2002) Measuring ARToolKit accuracy in long distance tracking experiments. In: Proceedings of the 1st international augmented reality toolkit workshop, Germany, Darmstadt, September 29"/>

    <meta name="citation_reference" content="Milgram P, Colquhoun H (1999) A Taxonomy of real and virtual world display integration, mixed reality merging real and virtual worlds. Ohta Y, Tamura H (eds) Ohmsha Ltd, Chapter 1, pp 5&#8211;30"/>

    <meta name="citation_reference" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays, IEICE Trans Inf Syst E77-D(12):1321&#8211;1329"/>

    <meta name="citation_reference" content="Moller T (1999) Real-time rendering. AK Peters Ltd, Natick, 23&#8211;38, 171"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Developing a generic augmented reality interface; citation_author=I Poupyrev, D Tan; citation_volume=35; citation_issue=3; citation_publication_date=2002; citation_pages=44-50; citation_doi=10.1109/2.989929; citation_id=CR30"/>

    <meta name="citation_reference" content="Reitmayr G, Schmalstieg D (2001) A wearable 3D augmented reality workspace. In: Proceedings of the 5th international symposium on wearable computers, October 8&#8211;9"/>

    <meta name="citation_reference" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Myers BA (ed) Proceedings of UIST &#8216;95. ACM, Pennsylvania, pp 29&#8211;36"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track, IEEE conference on computer vision and pattern recognition, Seattle, June, pp 593&#8211;600"/>

    <meta name="citation_reference" content="Sinclair P, Martinez K (2001) Adaptive hypermedia in augmented reality. In: Proceedings of the third workshop on adaptive hypertext and hypermedia at the twelfth ACM conference on hypertext and hypermedia, Denmark, August 2001, pp  217&#8211;219"/>

    <meta name="citation_reference" content="Slay H, Phillips M et al (2001) Interaction modes for augmented reality visualization, Australian symposium on information visualization, Sydney, December"/>

    <meta name="citation_reference" content="Smith GC (1994) The art of interaction. In: MacDonald L, Vince J (eds) Interacting with virtual environments. Wiley, New York,  pp 79&#8211;94"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Evaluating visualizations: do expert reviews work?; citation_author=null Tory; citation_volume=25; citation_issue=5; citation_publication_date=2005; citation_pages=8-11; citation_doi=10.1109/MCG.2005.102; citation_id=CR37"/>

    <meta name="citation_reference" content="Vallino J (1998) Interactive augmented reality. PhD thesis, Department of Computer Science, University of Rochester, pp 1&#8211;25"/>

    <meta name="citation_reference" content="Weng J, Cohen P, Herniou M (1992) Camera calibration with distortion models and accuracy evaluation, IEEE transactions on pattern analysis and machine intelligence, 14(10)"/>

    <meta name="citation_reference" content="Woo M, Neider J, Davis T (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2, Addison&#8211;Wesley, Reading "/>

    <meta name="citation_reference" content="Yewdall D (1999) Practical art of motion picture sound. Focal Press, Boston"/>

    <meta name="citation_author" content="Fotis Liarokapis"/>

    <meta name="citation_author_email" content="fotisl@soi.city.ac.uk"/>

    <meta name="citation_author_institution" content="Department of Informatics, University of Sussex, Brighton, UK"/>

    <meta name="citation_author_institution" content="Department of Information Science, City University, London, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0055-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2007/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0055-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An augmented reality interface for visualizing and interacting with virtual content"/>
        <meta property="og:description" content="In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtual information and processing of tracking data for natural interaction. Participants within the system can experience a real-time mixture of 3D objects, static video, images, textual information and 3D sound with the real environment. The user-friendly AR interface can achieve maximum interaction using simple but effective forms of collaboration based on the combinations of human–computer interaction techniques. To prove the feasibility of the interface, the use of indoor AR techniques are employed to construct innovative applications and demonstrate examples from heritage to learning systems. Finally, an initial evaluation of the AR interface including some initial results is presented."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An augmented reality interface for visualizing and interacting with virtual content | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0055-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Human–computer interaction, Tangible interfaces, Virtual heritage, Learning systems","kwrd":["Augmented_reality","Human–computer_interaction","Tangible_interfaces","Virtual_heritage","Learning_systems"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0055-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0055-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=55;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0055-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An augmented reality interface for visualizing and interacting with virtual content
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0055-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0055-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-11-09" itemprop="datePublished">09 November 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An augmented reality interface for visualizing and interacting with virtual content</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Fotis-Liarokapis" data-author-popup="auth-Fotis-Liarokapis" data-corresp-id="c1">Fotis Liarokapis<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Sussex" /><meta itemprop="address" content="grid.12082.39, 0000000419367590, Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QT, UK" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="City University" /><meta itemprop="address" content="grid.28577.3f, 0000000123539090, Department of Information Science, City University, London, EC1V 0HB, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 11</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">23</span>–<span itemprop="pageEnd">43</span>(<span data-test="article-publication-year">2007</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">720 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">19 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0055-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtual information and processing of tracking data for natural interaction. Participants within the system can experience a real-time mixture of 3D objects, static video, images, textual information and 3D sound with the real environment. The user-friendly AR interface can achieve maximum interaction using simple but effective forms of collaboration based on the combinations of human–computer interaction techniques. To prove the feasibility of the interface, the use of indoor AR techniques are employed to construct innovative applications and demonstrate examples from heritage to learning systems. Finally, an initial evaluation of the AR interface including some initial results is presented.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Augmented reality (AR) is an increasingly important and promising area of mixed reality (MR) and user interface design. In technical terms, it is not a single technology but a collection of different technologies that operate in conjunction, with the aim of enhancing the user’s perception of the real world through virtual information (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Teleoper Virtual Environ 6(4):355–385" href="/article/10.1007/s10055-006-0055-1#ref-CR2" id="ref-link-section-d64763e309">1997</a>). This sort of information is usually referred to as virtual, digital or synthetic information. The real world must be matched with the virtual in position and context in order to provide an understandable and meaningful view (Mahoney <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Mahoney D (1999b) Better than real, computer graphics world, pp 32–40" href="/article/10.1007/s10055-006-0055-1#ref-CR25" id="ref-link-section-d64763e312">1999</a>). Participants can work individually or collectively, experiment with virtual information and interact with a mixed environment in a natural way (Klinker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Klinker G, Ahlers KH et al (1997) Confluence of computer vision and interactive graphics for augmented reality, PRESENCE: teleoperations and virtual environments. special issue on augmented reality, August 6(4):433–451" href="/article/10.1007/s10055-006-0055-1#ref-CR19" id="ref-link-section-d64763e315">1997</a>). In an ideal AR visualisation scenario, the virtual information must be mixed with the real world in real-time in such a way that the user can either understand or not, the difference (Vallino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Vallino J (1998) Interactive augmented reality. PhD thesis, Department of Computer Science, University of Rochester, pp 1–25" href="/article/10.1007/s10055-006-0055-1#ref-CR38" id="ref-link-section-d64763e318">1998</a>). In case where virtual information looks alike the real environment, the AR visualisation is considered as the ultimate immersive system where participants cannot become more immersed in the real environment (RE).</p><p>The term AR usually refers to one of the following definitions (Milgram and Colquhoun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Milgram P, Colquhoun H (1999) A Taxonomy of real and virtual world display integration, mixed reality merging real and virtual worlds. Ohta Y, Tamura H (eds) Ohmsha Ltd, Chapter 1, pp 5–30" href="/article/10.1007/s10055-006-0055-1#ref-CR27" id="ref-link-section-d64763e324">1999</a>). A class of display systems that consist of a type of head mounted display (HMD) (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Teleoper Virtual Environ 6(4):355–385" href="/article/10.1007/s10055-006-0055-1#ref-CR2" id="ref-link-section-d64763e327">1997</a>); those systems that utilize an equivalent of an HMD belong to the second class, encompassing both large screen and monitor-based displays (Milgram and Kishino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays, IEICE Trans Inf Syst E77-D(12):1321–1329" href="/article/10.1007/s10055-006-0055-1#ref-CR28" id="ref-link-section-d64763e330">1994</a>). A third classification refers to the cases that include any type of mixture of real and virtual environments. Overall, the majority of AR systems rely on electronic sensors or video input in order to gain knowledge of the environment (Haniff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Haniff D, Baber C, Edmondson W (2000) Categorizing augmented reality systems. J Three Dimens Images 14(4):105–109" href="/article/10.1007/s10055-006-0055-1#ref-CR16" id="ref-link-section-d64763e333">2000</a>). All these variables make these systems more complex than systems that do not rely on sensors. Vision based systems on the other hand, often use markers as feature points so they can estimate the camera pose (position and orientation).</p><p>In the upcoming years, AR systems will be able to include a complete set of augmentation applied and exploiting all people’s senses (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y et al (2001) Recent advances in augmented reality. IEEE Comput Graph November/December 21(6):34–47" href="/article/10.1007/s10055-006-0055-1#ref-CR3" id="ref-link-section-d64763e339">2001</a>). However, although there are many examples of AR systems where users can interact with and manipulate virtual content and even create virtual content within some AR environments, one of their major constraints is the lack of ability to allow participants control multiple forms of virtual information in a number of different ways. To a great extend, this deficiency derives mainly from the lack of robustness of currently existing AR interface systems. At this stage, this can be dealt by using a user-friendly interface to allow users position audio–visual information anywhere inside the physical world. Since the pose can be easily estimated through an existing vision based tracking system such as the well-known ARToolKit (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000a" title="Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality 2000, Munich,  5–6 Oct, pp  111–119" href="/article/10.1007/s10055-006-0055-1#ref-CR17" id="ref-link-section-d64763e342">2000a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington " href="/article/10.1007/s10055-006-0055-1#ref-CR18" id="ref-link-section-d64763e345">b</a>), the focus of this research is to provide effective solutions for interactive indoor AR environments.</p><p>Vision-based AR interface environments highly depend on four key elements. The first two relate to marker implementation and calibration techniques. The latter are interrelated with the construction of software user interfaces that will allow the effective visualisation and manipulation of the virtual information. The integration of such interfaces into AR systems can reduce the complexity of the human–computer interaction using implicit contextual input information (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Myers BA (ed) Proceedings of UIST ‘95. ACM, Pennsylvania, pp 29–36" href="/article/10.1007/s10055-006-0055-1#ref-CR32" id="ref-link-section-d64763e351">1995</a>). Human computer interaction techniques can offer greater autonomy when compared with traditional windows style interfaces. Although some work has been performed into, the integration of such interfaces into AR systems (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Feiner S, MacIntyre B et al (1993) Windows on the world: 2D Windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, Atlanta, November 3–5, Association for Computing Machinery, pp 145–155" href="/article/10.1007/s10055-006-0055-1#ref-CR9" id="ref-link-section-d64763e354">1993</a>; Haller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Haller M, Hartmann W et al (2002) Combining ARToolKit with scene graph libraries. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt, Germany, 29 September" href="/article/10.1007/s10055-006-0055-1#ref-CR15" id="ref-link-section-d64763e357">2002</a>; MacIntyre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="MacIntyre B, Gandy M, Dow S, Bolter JD (2005) DART: a toolkit for rapid design exploration of augmented reality experiences. ACM Trans Graph (TOG), 24(3):932" href="/article/10.1007/s10055-006-0055-1#ref-CR24" id="ref-link-section-d64763e360">2005</a>) the design and implementation of an effective AR system that can deliver realistically audio–visual information in a user-friendly manner is a difficult task and an area of continuous research. However, it is very difficult even for technologists to create AR experiences to eliminate these barriers (MacIntyre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="MacIntyre B, Gandy M, Dow S, Bolter JD (2005) DART: a toolkit for rapid design exploration of augmented reality experiences. ACM Trans Graph (TOG), 24(3):932" href="/article/10.1007/s10055-006-0055-1#ref-CR24" id="ref-link-section-d64763e363">2005</a>) that prevent users to create new AR applications. To address the above issues, a prototype AR interface for assisting users that have some virtual reality experience to create fast and effective AR applications is proposed. The main novel contributions of this paper include the following:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Simultaneous and realistic 3D audio–visual augmentation in real-time performance;</p>
                  </li>
                  <li>
                    <p>Implementation and combination of five different ways for interacting with the virtual content;</p>
                  </li>
                  <li>
                    <p>Design and implementation of a high-level user centred interface that provides accurate and reliable control of the AR scene;</p>
                  </li>
                  <li>
                    <p>Two innovative applications: one for cultural heritage and one for higher education and</p>
                  </li>
                  <li>
                    <p>Initial evaluation regarding the overall effectiveness of the system;</p>
                  </li>
                </ul><p>In the remainder of this paper, we describe our system starting with Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec2">2</a> that gives a historical overview of the AR interfaces. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec3">3</a>, the architecture of the prototype AR interface is presented in detail. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec6">4</a>, presents various calibration approaches followed to calibrate our camera sub-system accurately. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec9">5</a> presents realistic augmentation techniques that can be applied in real-time performance. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec15">6</a> proposes five different ways of interacting with the AR scene while in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec21">7</a> two application scenarios are presented. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec24">8</a>, the results from an initial evaluation are presented whereas Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec27">9</a> summarises the key findings and the current status of research and suggests future work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Historical overview of AR interfaces</h2><div class="c-article-section__content" id="Sec2-content"><p>One of the earliest applications involved an experimental AR system that supports a full X11 server on a see-through HMD. The display overlays a selected portion of the X bitmap, on the user’s view of the world, creating an X-based AR. Three different types of windows were developed: surround-fixed windows, display-fixed windows and world-fixed windows. The performance of the system was in the range of 6–20 frames-per-second (FPS). A fast display server was developed supporting multiple overlaid bitmaps having the ability to index into a display a selected portion of a larger bitmap (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Feiner S, MacIntyre B et al (1993) Windows on the world: 2D Windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, Atlanta, November 3–5, Association for Computing Machinery, pp 145–155" href="/article/10.1007/s10055-006-0055-1#ref-CR9" id="ref-link-section-d64763e428">1993</a>). EMMIE (Butz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Butz A, Höllerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: Proceedings of the 2nd IEEE and ACM international workshop on augmented reality ‘99. San Francisco, October 20–21 " href="/article/10.1007/s10055-006-0055-1#ref-CR6" id="ref-link-section-d64763e431">1999</a>) is another experimental hybrid user interface designed for a collaborative augmented environment that combines various different technologies and techniques such as virtual components (i.e. 3D widgets) and physical objects (tracked displays, input devices). The objects in the system can be moved among various types of displays, ranging from see-through HMDs to additional 2D and 3D displays. These vary from palm-sized to wall-sized depending on the nature of the task.</p><p>The MagicBook (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753" href="/article/10.1007/s10055-006-0055-1#ref-CR5" id="ref-link-section-d64763e437">2001</a>) and the Tiles system (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan D et al (2002) Developing a generic augmented reality interface. Computer 35(3):44–50" href="/article/10.1007/s10055-006-0055-1#ref-CR30" id="ref-link-section-d64763e440">2002</a>) are two of the most well known AR interfaces based on the ARToolKit. The Tiles system proposes a way of creating an AR workspace blending together virtual and physical objects. The interface combines the advantages (power and flexibility) of computing environments with the comfort and awareness of the traditional workplace (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan D et al (2002) Developing a generic augmented reality interface. Computer 35(3):44–50" href="/article/10.1007/s10055-006-0055-1#ref-CR30" id="ref-link-section-d64763e443">2002</a>). On the other hand, the MagicBook uses a real book to transfer users from reality to virtuality. Virtual objects are superimposed on the pages of the book and users can interact with the augmented scene (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753" href="/article/10.1007/s10055-006-0055-1#ref-CR5" id="ref-link-section-d64763e446">2001</a>). Another example of an AR tangible interface is a tabletop system designed for virtual interior design (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000a" title="Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality 2000, Munich,  5–6 Oct, pp  111–119" href="/article/10.1007/s10055-006-0055-1#ref-CR17" id="ref-link-section-d64763e449">2000a</a>). One or multiple users can interact with the augmented scene, which consists of virtual furniture and manipulates the virtual objects.</p><p>MARE (Grasset and Gascuel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Grasset R, Gascuel J-D (2002) MARE: multiuser augmented reality environment on table setup. ACM SIGGRAPH conference abstracts and applications" href="/article/10.1007/s10055-006-0055-1#ref-CR13" id="ref-link-section-d64763e455">2002</a>) is a collaborative system that mixes together AR techniques with human-computer interaction techniques, in order to provide a combination of natural metaphors of communication (voice, gesture, expression) with virtual information (simulation, animation, persistent data). The architecture of the system is based on OpenGL Performer and XML configuration files and it can be easily adapted to many application domains. Another interesting workspace is a wearable AR generic platform that supports true stereoscopic 3D graphics (Reitmayr and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Reitmayr G, Schmalstieg D (2001) A wearable 3D augmented reality workspace. In: Proceedings of the 5th international symposium on wearable computers, October 8–9" href="/article/10.1007/s10055-006-0055-1#ref-CR31" id="ref-link-section-d64763e458">2001</a>). The system supports six degrees-of-freedom (DOF) manipulations of virtual objects in the near field using a pen and a pad interface. Slay et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Slay H, Phillips M et al (2001) Interaction modes for augmented reality visualization, Australian symposium on information visualization, Sydney, December" href="/article/10.1007/s10055-006-0055-1#ref-CR35" id="ref-link-section-d64763e461">2001</a>) developed an AR system that extends interactions from a traditional desktop interaction paradigm to a tangible AR paradigm. A range of issues related to the rapid assembly and deployment of adaptive visualisation systems was investigated. Three different techniques, for the task of switching the attributes of the virtual information in AR views, were presented.</p><p>Furthermore, the AMIRE project (Haller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Haller M, Hartmann W et al (2002) Combining ARToolKit with scene graph libraries. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt, Germany, 29 September" href="/article/10.1007/s10055-006-0055-1#ref-CR15" id="ref-link-section-d64763e467">2002</a>) aims at developing fast rapid prototyping through vision-based AR for users without detailed knowledge of the underlying base technologies of computer graphics and AR skills. AMIRE uses a component-oriented technology consisting of a reusable GEM collection, a visual authoring tool and object tracking system based on the ARToolKit library. Another system that allows users to create AR experiences is the designer’s augmented reality toolkit (DART) (MacIntyre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="MacIntyre B, Gandy M, Dow S, Bolter JD (2005) DART: a toolkit for rapid design exploration of augmented reality experiences. ACM Trans Graph (TOG), 24(3):932" href="/article/10.1007/s10055-006-0055-1#ref-CR24" id="ref-link-section-d64763e470">2005</a>). The system is based on the Macromedia Director multimedia-programming environment to allow a user to visually create complex AR applications as well as providing support for the trackers, sensors and camera.</p><p>Although most of the above systems describe generic frameworks that allow for AR and/or MR applications, they have not focused on designing a high-level user-focused interface that can deliver audio–visual information. The DART system is the most similar to this approach but it is based on a commercial multimedia package and thus it is addressed to designers and not general purpose developers. However, this sometimes limits the capabilities of the generated applications because they will be limited to the specific package (i.e. Director). On the contrary, this work is targeting developers who want to develop AR applications and use higher level tools than currently exist (i.e. ARToolKit).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Architecture of the system</h2><div class="c-article-section__content" id="Sec3-content"><p>The scope of the AR interface is to provide all the necessary tools for developers to generate user-specific AR applications (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec21">7</a>). They will select which sort of functionality is useful, and either use it as it is or extend it to fit the needs of the application. Based on previous prototypes (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767" href="/article/10.1007/s10055-006-0055-1#ref-CR20" id="ref-link-section-d64763e487">2004a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47–56" href="/article/10.1007/s10055-006-0055-1#ref-CR21" id="ref-link-section-d64763e490">b</a>) a tangible AR interface focused on superimposing five different types of virtual information and allowing users to interact using a combination of five different interaction techniques was designed and implemented. The system allows for the natural arrangement of virtual information anywhere inside the interior of a building or any other type of indoor environment. A diagrammatic overview of the operation of the system is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig1">1</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Overview of operation of the system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In the simplest configuration, a laptop computer with a USB web-camera and a set of trained marker cards are employed. The most complex configuration performed for the purpose of this research included two cy-visor HMDs, four LCD monitors, an 18 in. iiyama touch screen and a 42 in. plasma screen (Sony PFM-42V1N). Depending on the capabilities of the splitter different configurations can be supported depending on the level of immersion and collaboration required. For example, for some applications (i.e. museum environments) the plasma screen could provide an idealistic cognitive environment for collaborative while the touch screen could be preferred as an effective means for user-centred interaction. All displays have been used to present the capabilities of the system in various demonstrations and other dissemination events and the plasma screen found to be the most appealing one. To further increase the level of interaction, a 3D mouse is integrated into the system allowing users to manipulate the virtual information in a natural way in six DOF (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec20">6.5</a>).</p><p>Audio–visual augmentation techniques have been also been implemented (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec9">5</a>) in order to achieve a realistic visualisation such as, matching virtual lighting to real lighting, texture mapping techniques, shading and clipping. To further improve the quality of the visualisation, planar shadows and reflections are generated in real-time so that the user can get a more realistic perception of the augmented information in respect to the real world. It is worth-mentioning that the software and hardware infrastructure of the prototype AR interface developed in this research is based on off-the-self hardware components and low-priced software resources. The hierarchy of the software architecture is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig2">2</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Software technologies</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The blue boxes represent the off-line tools used and which form the basis of the implementation. The technologies in the orange boxes show the software components implemented for the creation of the AR interface. A brief overview of how each technology was used is presented in the following sections.</p><h3 class="c-article__sub-heading" id="Sec4">Off-line technologies</h3><p>The off-line software technologies include a number of commercial tools that must be used before the execution of the AR interface to prepare the content used in the augmentation (i.e. virtual information) as well as the AR environment. Specifically, the ARToolKit’s tracking libraries were used for the calibration of the camera (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec8">4.2</a>) as well as for the training of new markers designed for the needs of our research. Image processing (Adobe Photoshop) was appropriate for creating appropriate 2D images that were used as part of the visualisation process (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec11">5.2</a>) and for generating textures for the 3D models.</p><p>To create professional-quality 3D models, 3ds max employed to digitise the models and export them into 3ds format. Next, deep exploration utilised to convert 3ds models into a number of formats including VRML and ASCII. CoolEdit Pro served as a useful off-line tool to record and processes all the necessary wave samples required for the augmentation. WinHex was helpful to analyse the robustness of the markers existing in the AR environment. Finally, the Calibration Toolbox for Matlab was used to improve the camera parameters calculated from ARToolKit (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec8">4.2</a>).</p><h3 class="c-article__sub-heading" id="Sec5">Real-time technologies</h3><p>Real time software technologies consist of all the software libraries that have been integrated into a single application that comprise the AR interface. The Microsoft vision software development kit (SDK) was used as a basic platform to develop an interface between the video input (from and video or web cameras) and the rest of the AR application. Based on this, only ARToolKit’s (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000b" title="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington " href="/article/10.1007/s10055-006-0055-1#ref-CR18" id="ref-link-section-d64763e577">2000b</a>) tracking library (AR32.lib) was integrated to calculate the camera pose in real-time. On top of the tracking library a high-level computer graphics rendering engine was implemented based on C++ that can perform mathematical operations between 3D vectors and matrices. Standard graphics functionalities like shading, lighting and colouring were based on the OpenGL API (Woo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Woo M, Neider J, Davis T (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2, Addison–Wesley, Reading " href="/article/10.1007/s10055-006-0055-1#ref-CR40" id="ref-link-section-d64763e580">1999</a>) while more advanced functions like shading and reflection were implemented in the rendering engine to provide a platform for the rapid development of AR applications (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec21">7</a>).</p><p>GLUT (OpenGL utility toolkit) (Angel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Angel E (2003) Interactive computer graphics: a top-down approach using OpenGL,3rd edn. Addison–Wesley, Reading, pp 17–18, 69, 107, 322–349, 472" href="/article/10.1007/s10055-006-0055-1#ref-CR1" id="ref-link-section-d64763e589">2003</a>) was initially used to create a user-interface and to control the visualisation window of the AR interface. In addition, it was used for the textual augmentations (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec13">5.4</a>) because it provides sufficient support for bitmap and stroke fonts. However, GLUT provides only a minimum set of functions for the user to control the visualisation and therefore a more advanced solution was implemented based on MFC (Microsoft foundation classes). The advantage of implementing a windows-based interface is that it allows users to familiarise quickly with the GUI (graphical user interface) as well as it provides menus and toolbars to implement any type of user interaction. Finally, OpenAL (open audio library) API was employed to generate audio in a simulated 3D space (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec14">5.5</a>) because it is similar to OpenGL coding style and it can be considered as an extension of it.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Tracking</h2><div class="c-article-section__content" id="Sec6-content"><p>A key objective of this research was to provide a robust platform for developing innovative AR interfaces. However, to achieve the best tracking (with commercial web-cameras) accurate calculation of the camera parameters is required. As mentioned before, ARToolKit’s tracking library was preferred because it seems to provide accurate results with regards to the estimation of the location of the object especially at small distances and in cases where the camera is not moving fast. However, the major flaw of this approach is that all fiducials must be visible continuously. Also in un-calibrated environments, with poor lighting condition, tracking might not work at all. In this section, the results obtained from measuring ARToolKit’s error and the algorithms used for calibrating the camera (calculating the camera parameters) are briefly analysed.</p><h3 class="c-article__sub-heading" id="Sec7">Measuring ARToolKit’s error</h3><p>ARToolKit was originally designed for small applications working on a limited range of operation, usually around one meter. In these applications the distance between the marker and the user is often small so most of the errors occurred are not easily detectable. But in wide area applications, its positioning accuracy is not very robust. In distances between 1 and 2.5 m the error in the<i> x</i> and<i> y</i> values increases proportionally with the distance from the marker (Malbezin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Malbezin P, Piekarski W and Thomas B (2002) Measuring ARToolKit accuracy in long distance tracking experiments. In: Proceedings of the 1st international augmented reality toolkit workshop, Germany, Darmstadt, September 29" href="/article/10.1007/s10055-006-0055-1#ref-CR26" id="ref-link-section-d64763e619">2002</a>). Because this research is focused on indoor environments, it is very important to work accurately in small distances ranging between 1 m and reasonably well for up to 3 m. For this reason an experimental measurement of the accuracy of ARToolKit’s tracking libraries was performed in the laboratory environment. The aim of the experiment was to evaluate the error in distances ranging between 20 and 80 cm under normal lighting conditions. The experimental apparatus of this procedure is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig3">3</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Experimental setup for the measurement of ARToolKit’s error</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The optimal area, which contains the least error, is the one that is perpendicular to the marker card. To allow placing the camera on specific points with high precision a grid is positioned on the ground (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig3">3</a>). Besides, a rigid path was designed so that the camera cannot loose its direction while moving backwards. For each point on the grid, numerous measurements of the location of the web camera in a local co-ordinate system were taken. The camera is setup in the shortest operating distance (20 cm) and after completing measurements on its position it moves backwards on a step of 1 cm. When the camera moves 60 cm (60 different positions) the program exits. For each position 20 measurements were taken and they were averaged. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig4">4</a> illustrates the results of this experiment (purple line) showing that the error is proportional to the distance. In very small distances the error in the detection of the marker is small while in larger distances the error becomes considerably bigger. It increases proportionally to the angle of rotation when the camera does not change position, but it is rotated around the <i>Y</i>-axis.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Comparison of measured values</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To verify that the best location is on the perpendicular axis of the camera and the marker, another set of measurements were recorded with the camera facing the marker at variable angle (yaw) having the other two (pitch, roll) stable. In this case, the camera was setup again in the same plane (ground plane) but the measurements were taken when the <i>x</i> and <i>y</i> values tended to zero values. It was measured that the angle in the initial position (20 cm from the wall, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig3">3</a>) is approximately 12° while for the final position the angle is approximately 4°. It is worth mentioning, that the second sets of measurements were not done automatically, so on each step the camera had to be manually adjusted to provide values as close as possible to values of <i>x</i> and <i>y</i> to zero. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig4">4</a> shows the results from the second experiment and illustrates that the measured error, when the camera is not pointing directly to the marker, is proportional to the distance. However, the difference is of minimal significance. This means that when the camera lies with a certain area and does not change its orientation the error is quite small. In contrast, if the camera changes direction the error increases considerably. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig4">4</a> illustrates differences in the errors produced from the experiments compared with the actual value (top dark line).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig4">4</a> shows that the best results can be obtained when the camera is oriented to point at the centre of the marker. Even if the camera has a small offset, then the error increases linearly with the distance. Nevertheless, the tracking results are acceptable in the area of less than 1 m since the error is hardly noticed.</p><h3 class="c-article__sub-heading" id="Sec8">Camera calibration</h3><p>This section describes the procedures used in order to calculate the intrinsic and extrinsic camera parameters. The purpose for this was to define an accurate camera model that can be effectively applied into indoor AR environments. Although there are a few camera calibration techniques available (Weng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Weng J, Cohen P, Herniou M (1992) Camera calibration with distortion models and accuracy evaluation, IEEE transactions on pattern analysis and machine intelligence, 14(10)" href="/article/10.1007/s10055-006-0055-1#ref-CR39" id="ref-link-section-d64763e712">1992</a>; Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track, IEEE conference on computer vision and pattern recognition, Seattle, June, pp 593–600" href="/article/10.1007/s10055-006-0055-1#ref-CR33" id="ref-link-section-d64763e715">1994</a>) for calculating the intrinsic camera parameters, ARToolKit’s calibration library (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000b" title="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington " href="/article/10.1007/s10055-006-0055-1#ref-CR18" id="ref-link-section-d64763e718">2000b</a>) was preferred since it works reasonable good in small distances and in cases where the camera is not moving fast. This method was originally applied to measure the camera model properties such as: the <i>center point</i> of the camera image; the <i>lens distortion</i>; and the camera’s <i>focal length</i>. ARToolKit provides two software tools that can be used to calculate these camera properties, one to measure the lens distortion and the image center point, while the second to compute the focal length of the camera (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000b" title="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington " href="/article/10.1007/s10055-006-0055-1#ref-CR18" id="ref-link-section-d64763e731">2000b</a>). Based on this, the initial calibration was performed since it produces reasonable results for the calculation of the intrinsic camera parameters.</p><p>However, the greatest limitations of this vision solution include the tracking accuracy and the range of operation (Malbezin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Malbezin P, Piekarski W and Thomas B (2002) Measuring ARToolKit accuracy in long distance tracking experiments. In: Proceedings of the 1st international augmented reality toolkit workshop, Germany, Darmstadt, September 29" href="/article/10.1007/s10055-006-0055-1#ref-CR26" id="ref-link-section-d64763e737">2002</a>). To minimise some of the errors produced in the tracking of the markers, the extrinsic camera used had to be accurately estimated. The virtual objects will only appear when the tracking marks are in view. The size of the predefined patterns influences the effectiveness of the tracking algorithms. For instance, if the pattern is large then the pattern is detected further away. To calculate the extrinsic camera parameters the camera calibration toolbox (Camera Calibration Toolbox for Matlab <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Camera calibration toolbox for Matlab, available at: [&#xA;                    http://www.vision.caltech.edu/bouguetj/calib_doc/&#xA;                    &#xA;                  ], Accessed at 14/01/2003" href="/article/10.1007/s10055-006-0055-1#ref-CR7" id="ref-link-section-d64763e740">2003</a>) was used which provides a user-friendly interface and it is very convenient when working with a large number of images. Another advantage over the previous method is that it provides very accurate results.</p><p>Before the camera calibration begins two steps need to be initially followed. In the first step the calibration rig must be generated while in the second all the calibration images must be collected. When done, the grid corners are easily extracted. The ToolKit offers an automatic mechanism for counting the number of squares in each grid and all calibration images used are searched and focal and distortion factors are automatically estimated. However, similarly to ARToolKit method, in most occasions the algorithm may not predict the right number of squares and thus provides a poor result. This can be clearer by observing the results of the calculation of the re-projection error. As it is clearly observed from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig5">5</a>a, the re-projection error is quite big compared to the scale. The reason behind this is because the extraction of the corners is not acceptable on some highly distorted images. However, the advantage of this technique is that it allows the user to improve the calibration. Specifically, the whole procedure can be repeated until the error is minimised up to a certain point.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Calculation of camera error <b>a</b> re-projection error <b>b</b> minimisation of error</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>After repeating the procedure for five times the error is reduced from a scale of five to a scale of one as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig5">5</a>b. Furthermore, because ARToolKit accepts only binary data format for the calibration, a simple way to do this is to estimate the extrinsic parameters and then save the computed parameters in the data structure replacing the old values. The old data structure that holds the calculated camera parameters (ARParam struct) is shown below:<img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Figa_HTML.gif" alt="" />In this structure the <i>xsize</i>, <i>ysize</i> and <i>dist_factor</i> have been experimentally replaced with the new values calculated from the above. Specifically, the camera parameters including the focal length (fc), the principal point (cc), the skew (<i>s</i>
                           <sub>
                    <i>k</i>
                  </sub>) and the distortion (<i>k</i>
                           <sub>
                    <i>c</i>
                  </sub>) have been computed and based on these values the intrinsic matrix can be defined as shown in Eq. (1): </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ {\left( {\begin{array}{*{20}c} {{{\text{fc}}_{0} }} &amp; {{s_{k} {\text{fc}}_{0} }} &amp; {{{\text{cc}}_{0} }} \\ {0} &amp; {{{\text{fc}}_{1} }} &amp; {{{\text{cc}}_{1} }} \\ {0} &amp; {0} &amp; {1} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                        <p>Since ARToolKit does not take into account the skew factor and makes use of the following matrix: </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ {\left( {\begin{array}{*{20}c} {{s_{x} f}} &amp; {0} &amp; {{x_{0} }} \\ {0} &amp; {{s_{y} f}} &amp; {{y_{0} }} \\ {0} &amp; {0} &amp; {1} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                        <p>To match the outputted camera matrix from Matlab and fit it into ARToolKit’s matrix, the following matrix can be derived: </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ {\left( {\begin{array}{*{20}c} {{{\text{fc}}_{0} }} &amp; {0} &amp; {{{\text{cc}}_{0} }} \\ {0} &amp; {{{\text{fc}}_{1} }} &amp; {{{\text{cc}}_{1} }} \\ {0} &amp; {0} &amp; {1} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                        <p>After testing the new camera model a small improvement was succeeded in the distortion in the magnitude of 3–4%. As a further improvement, it was decided to add the skew parameters to the matrix, thus the skew parameter was used instead of zero in the matrix as shown below: </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ {\left( {\begin{array}{*{20}c} {{{\text{fc}}_{0} }} &amp; {{s_{k} {\text{fc}}_{0} }} &amp; {{{\text{cc}}_{0} }} \\ {{s_{k} }} &amp; {{{\text{fc}}_{1} }} &amp; {{{\text{cc}}_{1} }} \\ {{s_{k} }} &amp; {{s_{k} }} &amp; {1} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                        <p>Although the last modification provided us with a more correct camera model with an estimated improvement of about 1%, the effectiveness of the tracking system was not significantly improved. This is due to the fact that the optics used in the camera system (web camera) is really poor compared to professional video cameras. Other environmental issues that influence tracking include lighting conditions and range of operation.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Audio–visual augmentations</h2><div class="c-article-section__content" id="Sec9-content"><p>Each type of virtual media information is designed for specific purposes and as a result produces different outcomes. For instance, textual explanation can be utilized much more effectively than auditory description when communicating verbal information. On the other hand, pictures work better than text, for recalling or explaining diagrammatically a procedure. To describe a sequence of events video seems to be one of the most efficient techniques. In this section, the methodology used for the simultaneously multimedia visualisation of virtual information into an AR indoor environment is presented.</p><h3 class="c-article__sub-heading" id="Sec10">Object augmentation</h3><p>An ideal AR system must be able to mix the virtual information with the real in a physical way. The participants should not realize the difference between the real and the augmented visualisation. The focus of this research is to present and implement methods of realistically rendering 3D representations of real objects in an easy and interactive manner. The selection of the most appropriate 3D format is a crucial task in order to achieve a high level of realism in the system. In this research, both 3ds and VRML file formats have been used as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0055-1#Tab1">1</a>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Categorisation of 3D file formats</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0055-1/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In any case, one of the first problems derived when displaying a 3D representation of a real model is the correct alignment on the required position. Virtual objects may appear to float on the marker and the user will be easily confused. This usually occurs because the 3D model is not registered correctly into the scene. For example, when a 3D object is transformed into the real scene it may appear below the origin as illustrated in the left image of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig6">6</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Object augmentation <b>a</b> misalignment of object <b>b</b> correct registration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To correct the problem of misalignment, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig6">6</a>a, a sorting algorithm for registering 3D objects precisely onto the top of the markers was implemented. To achieve a correct registration the virtual information need to be first sorted and then initialised to exactly the same level, as the marker is located in the <i>Z</i>-axis. An efficient way to align objects is by using a two-stage process. In the first part, the vertices of the object are sorted by the <i>Z</i>-axis. Upon completion, the vertices are translated to the minimum value, which is the origin of the marker cards, resulting in a proper object registration.</p><p>Next, to improve the realism of the AR scene a fast algorithm for planar shadows and reflections was implemented. The location of the shadow can be calculated by projecting all the vertices of the AR object to the direction of the light source. To generate augmented shadows an algorithm that creates a 4 × 4 projection matrix (<i>P</i>
                           <sub>
                    <i>s</i>
                  </sub>) in homogeneous coordinates must be calculated based only on the plane equation coefficients and the position of the light (Moller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Moller T (1999) Real-time rendering. AK Peters Ltd, Natick, 23–38, 171" href="/article/10.1007/s10055-006-0055-1#ref-CR29" id="ref-link-section-d64763e1072">1999</a>). Say that <i>L</i> is the position of the point light source; <i>P</i> the position of a vertex of the AR object where the shadow is cast; and <i>n</i> the normal vector of the plane. The projection matrix of the shadow can be calculated by solving the system, which consists of the equation of the plane and a straight that passes from the plane point in the direction of the light source (see Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0055-1#Equ5">5</a>). </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$p_{s}  = {\left( {\begin{array}{*{20}c}
   {{Lp \bullet Pc - Lp_{0}  \times Pc_{0} }} &amp; {{0 - Lp_{1}  \times Pc_{0} }} &amp; {{0 - Lp_{2}  \times Pc_{0} }} &amp; {{0 - Lp_{3}  \times Pc_{0} }}  \\
   {{0 - Lp_{0}  \times Pc_{1} }} &amp; {{Lp \bullet Pc - Lp_{1}  \times Pc_{1} }} &amp; {{0 - Lp_{2}  \times Pc_{1} }} &amp; {{0 - Lp_{3}  \times Pc_{1} }}  \\
   {{0 - Lp_{0}  \times Pc_{2} }} &amp; {{0 - Lp_{1}  \times Pc_{2} }} &amp; {{Lp \bullet Pc - Lp_{2}  \times Pc_{2} }} &amp; {{0 - Lp_{3}  \times Pc_{2} }}  \\
   {{0 - Lp_{0}  \times Pc_{3} }} &amp; {{0 - Lp_{1}  \times Pc_{3} }} &amp; {{0 - Lp_{2}  \times Pc_{3} }} &amp; {{Lp \bullet Pc - Lp_{3}  \times Pc_{3} }}  \\\end{array}} \right)}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                        <p>where <i>L</i>
                           <sub>
                    <i>p</i>
                  </sub>
                           <i>·P</i>
                           <sub>
                    <i>c</i>
                  </sub> is the dot product of plane and light position. The projection matrix has a number of advantages compared with other methods (i.e. fake shadows) but the most important is that it works fast and it is generic so that it can generate hard shadows in real-time for any type of objects independently of their complexity (Liarokapis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer" href="/article/10.1007/s10055-006-0055-1#ref-CR22" id="ref-link-section-d64763e1123">2005</a>). An example screenshot that illustrates planar shadows is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig7">7</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Illustration of planar shadows</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The main disadvantage of this algorithm is that it renders the virtual information twice for each frame: once for the virtual object and another one for its shadow. Another obvious flaw is that it can cast shadows only into planar surfaces but with some modifications, it can be extended to be applied to specific cases such as curved surfaces (Liarokapis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer" href="/article/10.1007/s10055-006-0055-1#ref-CR22" id="ref-link-section-d64763e1151">2005</a>).</p><p>To realistically model reflections in AR environments, many issues must be taken into account. Although in reality the light is scattered uniformly in all directions depending on the material of the object in this work, the effect of mirror reflections has been implemented. An example screenshot of a virtual object casting a shadow and a reflection on a virtual plane is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig8">8</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Planar shadows and reflections</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Based on the OpenGL’s stencil buffer a reflection of the object is performed onto a user-defined virtual ground. The stencil buffer is initially set to sixteen bits in the pixel format function. Then, the buffer is emptied and finally the stencil test is enabled.</p><h3 class="c-article__sub-heading" id="Sec11">Image augmentation</h3><p>Images are widely used as a means to increase realism and in the past, they have been used with success for educating purposes. The augmentation of images is a highly cost effective means to present simple 2D information in the real world. The use of their operation may be performed in a number of different ways depending on the learning scenario applied. The digital image augmentation can be either static or dynamic. Dynamic image augmentation is widely used for achieving video augmentation (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec12">5.3</a>). With static augmentation, a single image only is rendered into the scene. Based on the theoretical framework provided by Smith (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Smith GC (1994) The art of interaction. In: MacDonald L, Vince J (eds) Interacting with virtual environments. Wiley, New York,  pp 79–94" href="/article/10.1007/s10055-006-0055-1#ref-CR36" id="ref-link-section-d64763e1191">1994</a>), images used for AR environments have been categorised into description, symbolic, iconic and functional as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0055-1#Tab2">2</a>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Categorisation of images augmentation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0055-1/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The algorithm used is simple but very efficient and can be applied into two types of image formats (BMP and TGA). First, it loads an image file and checks if it is a valid image format. In the next step, textures are generated using data from the image file. Following this, the texture is created and the parameters are set based on the OpenGL API. Finally, the texture is bound to the target texture, which is a quadrilateral.</p><h3 class="c-article__sub-heading" id="Sec12">Video augmentation</h3><p>The mode of operation within the video AR system is to read an AVI file, decompose it into 256 × 256 × 24 bit images, mix it with the dynamic video (coming from the camera) and finally display it on the selected visualisation display (Liarokapis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer" href="/article/10.1007/s10055-006-0055-1#ref-CR22" id="ref-link-section-d64763e1403">2005</a>). When the video file is loaded, the program automatically counts the number of frames so that its size is known. Then all frames are decomposed into 2D images and each image is applied to a square quad, exactly in the same way as textures are wrapped to objects. It is worth mentioning here that because each animation has a specific length (in seconds) and its own frame rate, the time required for each frame is calculated.</p><p>Moreover, the augmented video starts automatically when two things occur: a marker is detected and user has loaded a particular file from the filling system using the interface menu. When the animation is completed it repeats itself until the user decides to stop it (by pressing a keyboard key or using the interface menu). To increase the feasibility of the system, if the camera is not in line of view with the marker card, then the video augmentation will continue playing until the video sequence is finished. This was designed on purpose to prevent cases where the user changes position or orientation rapidly and thus looses the perceived visualisation. The augmented animation can be controlled in a number of different means including the cease of the animation, resize the animation window or even manipulate the augmented video animation into six DOF (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec15">6</a>).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig9">9</a> shows four frames of a video sequence superimposed into a marker card, describing a complex concept in electronics (i.e. Moore Diagram). In terms of efficiency, the video augmentation results range between 20 and 35 FPS depending on the resolution of the videos. However, the drawback of this method is that when the animation is augmented the overall performance of the system is significantly reduced. In particular, the performance was experimentally measured to be reduced by approximately 20–50% the FPS of the system. For instance, if the performance of the system is in real-time (i.e. 25 FPS) then the AR video algorithm would drop the performance to 12–20 FPS. Another limitation of the proposed video augmentation is that it can currently decompose videos into only 256 × 256 × 24 bit images.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Video augmentation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec13">Textual augmentation</h3><p>Textual annotations are the simplest form of information that can be easily augmented in any type of AR environments. This can be either presented as a <i>label</i> or as a <i>description</i>. Label text has been used in the past (Klinker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Klinker G, Ahlers KH et al (1997) Confluence of computer vision and interactive graphics for augmented reality, PRESENCE: teleoperations and virtual environments. special issue on augmented reality, August 6(4):433–451" href="/article/10.1007/s10055-006-0055-1#ref-CR19" id="ref-link-section-d64763e1451">1997</a>; Sinclair and Martinez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Sinclair P, Martinez K (2001) Adaptive hypermedia in augmented reality. In: Proceedings of the third workshop on adaptive hypertext and hypermedia at the twelfth ACM conference on hypertext and hypermedia, Denmark, August 2001, pp  217–219" href="/article/10.1007/s10055-006-0055-1#ref-CR34" id="ref-link-section-d64763e1454">2001</a>), to point out specific parts of a complex system using the minimum textual information. In this case, the most important aspect is to ensure that the augmented labels do not obscure each other and that the information is clearly presented to the user. Description text requires a much more demanding process because it needs to provide complete information about an object or about a virtual operation. The problems begin in cases where the magnitude of textual information needs to be augmented on a display is large.</p><p>In this research, label and descriptive textual information was performed by dynamically loading ASCII text files. Each file contained a very different level of information depending on the reasons for utilising it. For example, label text files were defined to specify the type of visualisation (i.e. image augmentation) or the name of an object. The main advantage of this method is that the textual information, which will be augmented on the real environment, is stored on a txt file. Text files are widely used and can be easily transferred over all types of networks. Users of the system can position textual augmentations anywhere in the real environment using standard transformations. In addition, they can change their appearance in terms of colour, size and font type (Bitmap, Times Roman and Helvetica).</p><h3 class="c-article__sub-heading" id="Sec14">Audio augmentation</h3><p>In the real world, audio is a process that is heard spatially and thus it is a very important aspect for any simulation scenario. The most important issue when designing 3D sound is to “see” the sound source (Yewdall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Yewdall D (1999) Practical art of motion picture sound. Focal Press, Boston" href="/article/10.1007/s10055-006-0055-1#ref-CR41" id="ref-link-section-d64763e1467">1999</a>). However, most AR applications have not incorporated 3D sound component even if it can contribute to the sense of immersivity. The augmented sound methodology followed in this work, has some similarities with the ASR approach (Dobler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Dobler D, Haller M, Stampfl P (2002) ASR—augmented sound reality, ACM SIGGRAPH 2002 conference abstracts and applications, San Antonio, p 148" href="/article/10.1007/s10055-006-0055-1#ref-CR8" id="ref-link-section-d64763e1470">2002</a>) in the way virtual sounds are augmented in the real environment. Unlike this experimental approach, which is based on a Creative EAX API, the implemented 3D audio system is based on OpenAL, which has many similarities with OpenGL and was originally designed for generating 3D sounds around a listener. The recording of speech sounds was done in mono format, using a standard microphone and the mono samples were converted into stereo format. Furthermore, each sound source in the system has been specified to have the following three properties: <i>position</i>, <i>orientation</i> and <i>velocity</i>. The spatial audio system can handle multiple sound sources and mix them together. The user can move the sources in 3D space using the keyboard and menu interaction techniques illustrated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec15">6</a>.</p><p>The spatial sound algorithm first initialises all the necessary OpenAL variables (position, orientation and velocity) and then loads them into the appropriate buffers (format, length and frequency) for further processing. Next, sound sources and buffers are initialised and the sources are assigned to the buffers. The picth and gain are set to one and the sources are set into a continuous loop unless stopped by the user. Each time the camera detects the marker the transformation matrix is inverted to estimate the position of the camera. In the context of this research, the distance model experimentally applied used to simulate the distance followed the linear equation as illustrated below:
</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ y = \alpha x + \beta $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where α represents distance between the camera and the marker and β the offset position of the marker card. Although this cannot accurately represent the distribution of sound in 3D space it provides very good results. To provide more freedom to the listener the values of the linear function may change depending on the requirements of the visualisation. If the sound source is positioned in the origin then the above equation may be re-written as shown below:
</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ Listener = \frac{{camera\_position}} {{distance\_factor}} $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where <i>camera_position</i> refers to the inverse transformation of the camera and <i>distance_factor</i> to a constant number. To achieve a realistic simulation of the sound different values have been tried to simulate the distance_factor. However, this constant value may change off-line depending on the requirements of the visualisation. For example, some users may prefer to perceive the auditory information louder than others do. In addition, the system is capable of loading and mixing music sound files. This option can be extremely useful for simulating surround music audio. The sound files may be overlaid into the same marker or onto a different marker depending on the needs of the application.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Human–computer interactions</h2><div class="c-article-section__content" id="Sec15-content"><p>Human–computer interactions are one of the most important issues when designing a robust real-time system. They have to be performed in a natural way so that inexperienced participants familiarise quickly in the AR environment (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767" href="/article/10.1007/s10055-006-0055-1#ref-CR20" id="ref-link-section-d64763e1531">2004a</a>). The proposed interface allows users tangible interaction with various types of multimedia information such as 3D models, images, textual information and 3D sound, using a number of interaction techniques. Interactions controlled by the user-computer can be distinguished into five different categories including physical manipulation, interface menu interaction, standard I/O, Touch Screen and SpaceMouse interaction as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig10">10</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Interactions within the system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Although, some types of interactions proposed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig10">10</a> are not novel (i.e. physical manipulation), the novelty comes in the way they are used by the participants. Participants can combine two or more types and experience a novel form of interaction with great flexibility. For example, the most significant combination of human–computer interactions is the use of intuitive methods like the physical manipulation with sophisticated devices such as the SpaceMouse. Users can hold in one hand a marker card with a virtual object superimposed and on the other hand use the SpaceMouse to perform graphics operations like virtual lighting. In the following sections, all the types of interactions are explained in detail.</p><h3 class="c-article__sub-heading" id="Sec16">Standard interactions</h3><p>The first method is addressed to users with some computer experience and is based on standard interaction input devices like the keyboard and the mouse. For example, by pressing buttons (hot keys) the visual parameters of the virtual objects can be changed faster instead of using the menu dialogues. Some of the most characteristic are described in (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767" href="/article/10.1007/s10055-006-0055-1#ref-CR20" id="ref-link-section-d64763e1568">2004a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47–56" href="/article/10.1007/s10055-006-0055-1#ref-CR21" id="ref-link-section-d64763e1571">b</a>; Liarokapis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer" href="/article/10.1007/s10055-006-0055-1#ref-CR22" id="ref-link-section-d64763e1574">2005</a>) and include the change of lighting conditions (ambient, diffuse, specular and shiness); the texturing information (standard and environmental); the switch from solid mode to wireframe mode; and others (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec17">6.2</a>). Moreover, the keyboard is also employed for changing the position (translation), orientation (rotation) and scaling of the virtual information in six DOF. Initially, the above transformations were implemented based on the OpenGL functionality but soon it became obvious that OpenGL could not meet the requirements of this research because it provides only the minimum functionality to rotate an object around <i>X</i>, <i>Y</i> or <i>Z</i>-axis. However, in a tabletop AR environment this is constraining the user when rotating the virtual information as well as it restricts the use of simultaneous interactions. To tackle this problem a generic rotational matrix that takes as input three angles and rotates the object around an arbitrary axis is specified in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0055-1#Equ8">8</a>: </p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ {\left[ {\begin{array}{*{20}c} {{\cos \vartheta \cos \psi }} &amp; {{ - \cos \vartheta \sin \psi }} &amp; {{\sin \vartheta }} \\ {{\sin \varphi \sin \vartheta \cos \psi + \cos \varphi \sin \psi }} &amp; {{ - \sin \varphi \sin \vartheta \sin \psi + \cos \varphi \cos \psi }} &amp; {{ - \sin \varphi \cos \vartheta }} \\ {{ - \cos \varphi \sin \vartheta \cos \psi + \sin \varphi \sin \psi }} &amp; {{\cos \varphi \sin \vartheta \sin \psi + \sin \varphi \cos \psi }} &amp; {{\cos \varphi \cos \vartheta }} \\ \end{array} } \right]} $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                        <p>Based on the above rotational matrix, it became possible for users to rotate virtual information around an arbitral-defined axis. The above matrix was also implemented to the standard mouse providing a quick way to perform intuitive rotations. Although, it provides the means to perform a rotation around all three axes simultaneously if one interaction device is used, problems occur when more than one device is used (i.e. keyboard and mouse). An alternative way of performing transformations is by using quaternions. To specify multiple rotations, many intermediate control points are required where a quaternion interpolation depends only on the relation between the initial and final rotations. The easiest way to prove the link between a rotation matrix and a quaternion is by linking them in three dimensions. Say that<i> q</i> = <i>s</i> + <i>v</i>·<i>I</i> a unit quaternion and defined<i> Q</i>, where<i> v</i> = (<i>u</i>
                           <sub>x</sub>, <i>u</i>
                           <sub>y</sub>, <i>u</i>
                           <sub>z</sub>)<sup>T</sup>, it can be shown that there is a 3 × 3 matrix that represents a rotation matrix of the form (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0055-1#Equ9">9</a>): </p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ vv^{{\text{T}}} + (sI_{{3 \times 3}} + C_{u} )^{2} = {\left( {\begin{array}{*{20}c} {{s^{2} + u^{2}_{x} - u^{2}_{y} - u^{2}_{z} }} &amp; {{2(u_{x} u_{y} - su_{z} )}} &amp; {{2(u_{x} u_{z} + su_{y} )}} \\ {{2(u_{x} u_{y} + su_{z} )}} &amp; {{s^{2} + u^{2}_{x} + u^{2}_{y} - u^{2}_{z} }} &amp; {{2(u_{y} u_{z} - su_{x} )}} \\ {{2(u_{x} u_{z} \, - \,su_{y} )}} &amp; {{2(u_{y} u_{z} + su_{x} )}} &amp; {{s^{2} - u^{2}_{x} - u^{2}_{y} + u^{2}_{z} }} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
                        <p>To obtain a quaternion corresponding to a given rotation matrix we first define an arbitrary rotation matrix R and then the corresponding quaternion q = <i>s</i> + <i>u</i>
                           <sub>xi</sub> + <i>u</i>
                           <sub>yj</sub> + <i>u</i>
                           <sub>zk</sub> to the rotation matrix. Using the above equation it is easy to solve the equation and derive the values for <i>u</i>
                           <sub>x</sub>, <i>u</i>
                           <sub>y</sub> and <i>u</i>
                           <sub>z</sub>, respectively. In OpenGL, rotations are specified as matrices since homogeneous matrices are the standard 3D representations. By combining the property of unit quaternion with the above rotation quaternion matrix we can deduce the following equation (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-006-0055-1#Equ10">10</a>): </p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ vv^{{\text{T}}} + (sI_{{3 \times 3}} + C_{u} )^{2} = {\left( {\begin{array}{*{20}c} {{1 - 2(u^{2}_{y} - u^{2}_{z} )}} &amp; {{2(u_{x} u_{y} - su_{z} )}} &amp; {{2(u_{x} u_{z} + su_{y} )}} \\ {{2(u_{x} u_{y} + su_{z} )}} &amp; {{1 - 2(u^{2}_{x} + u^{2}_{z} )}} &amp; {{2(u_{y} u_{z} - su_{x} )}} \\ {{2(u_{x} u_{z} - su_{y} )}} &amp; {{2(u_{y} u_{z} + su_{x} )}} &amp; {{1 - 2(u^{2}_{x} - u^{2}_{y} )}} \\ \end{array} } \right)} $$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div>
                        <p>Other functions that were integrated to the mouse include translations and scaling. On the other hand, using the mouse users can access the carefully designed GUI. This allows users to have full access to the superimposed virtual information. An example is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig11">11</a>, where users can select the information that is going to be augmented on the real environment.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>GUI functionality</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec17">GUI Interactions</h3><p>On the other hand, using the mouse or the Touch Screen users can access the functionality that has been carefully integrated into a novel GUI. The GUI consists of a menu, a toolbar, a status bar and a number of dialog boxes. This allows participants to have the same access to the augmented virtual information as if they were using standard interaction techniques. Four example screenshots that illustrate some of the functionalities of the GUI is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig11">11</a>.</p><p>The greatest advantage of the proposed GUI is that it allows participants to perform complex operations very accurately. Specifically, sometimes it is of crucial importance to transform a virtual object in a specific location in the real environment. Using other methods it could take a great amount of time and effort (depending on the experience of the user) to achieve this and it will definitely not be very accurate. However, the GUI interaction techniques offer the solution to the problem using double point precision.</p><p>Next, the “Edit” category consists of three basic operations including video (start or stop), a zoom dialog box and a scale dialog box. The “View” category consists of two sets of operations. Firstly, a Toolbar and a Status bar, which is commonly found in windows based applications. It is worth-mentioning here that the GUI has been built on top of the windows API so that full compatibility with windows based operating systems is ensured. As far as this research is concerned this is the only true windows based interface that can superimpose five different types of multimedia content into the real environment.</p><p>The second set of operations consists of three functions called axis (to insert a Cartesian set of axis indicating the origin of the AR environment), debug (to threshold the live video sequence and thus check whether a marker is detectable) and clip (to clip the graphics geometry). The rest of the menu categories (graphics and augment) are used to control visualisation properties of the augmented information. Functions that have been implemented include shadows, fog, lighting, material, texturing, colouring, transparency and shading. Finally, the “help” category provides some information about the release version of the AR interface as well as the date and the author name.</p><h3 class="c-article__sub-heading" id="Sec18">Physical manipulation</h3><p>Physical manipulations were specifically designed for users with no computer experience and refers to a physical manipulation of the marker cards (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000a" title="Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality 2000, Munich,  5–6 Oct, pp  111–119" href="/article/10.1007/s10055-006-0055-1#ref-CR17" id="ref-link-section-d64763e1766">2000a</a>; Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753" href="/article/10.1007/s10055-006-0055-1#ref-CR5" id="ref-link-section-d64763e1769">2001</a>). As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig12">12</a>, users can manipulate freely the marker cards in six DOF to receive a different perception of the superimposed information.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Natural manipulation of virtual object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Another benefit of natural interactions is that they can be used with the other types of interactions described in this section. This allows producing unique combinations (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig14">14</a>) that can provide solutions for specific AR applications that require a high-level of interaction. In addition, apart from using the marker cards for just superimposing virtual information, they have been used to perform some basic operations such as: assign an object into a marker, de-assign an object from a marker, scale, rotate and translate. The advantage of this method is that users can use only physical objects (marker cards) to visualise and interact with the virtual information. However, the disadvantage is that when multiple markers are used the overall efficiency of the system is reduced. Specifically, the template matching algorithm used operates very effectively in real-time performance with one marker but it starts to decrease drastically as more markers are added. The reason behind this is because for each marker the algorithm is aware; it creates four templates, one at each orientation. Each marker has to be compared to all known templates until the best match is detected. To calculate the number of comparisons performed by the algorithm the following equation is illustrated:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ N_{c} = 4 \times N_{m} \times N_{t} $$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <i>N</i>
                           <sub>c</sub> is the number of comparisons, <i>N</i>
                           <sub>m</sub> is the number of known markers and <i>N</i>
                           <sub>t</sub> corresponds to the number of known templates. If in the scene there are 10–20 markers and the application knows about 250 markers then the system performs around 10,000–20,000 comparisons. This makes any system to run much slower and makes the application operate in less than 25 FPS. Thus, to achieve a fast AR application in the final system it was preferred to use as few markers as possible having as limit ten markers.</p><h3 class="c-article__sub-heading" id="Sec19">Touch screen interactions</h3><p>An alternative way of interacting with the virtual information is to make use of interaction devices such as Touch Screens. This is ideal for some application scenarios where, the use of other interaction devices is not possible. For example, in museum environments, Touch Screens are the most appropriate means of interacting with the virtual exhibitions. Besides, although it was easy to integrate the Touch Screen to the AR interface, many problems arose when users tried to interact with the GUI menu. The reason for this is because the menus in the GUI were too small and it was difficult for some users to select. To tackle the problem, large toolbar buttons and dialog boxes were designed and associated with appropriate functionality. The main advantage of using the Touch Screen is that it can serve both the visualisation and interaction all in one device. However, the major drawback is that the effectiveness of the interactions is dependent on the effectiveness of the GUI. If the GUI is not user-friendly, it will affect the usefulness of the Touch Screen interactions.</p><h3 class="c-article__sub-heading" id="Sec20">SpaceMouse interactions</h3><p>Finally, users can manipulate virtual information using sophisticated VR sensor devices such as SpaceMouse (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004b" title="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47–56" href="/article/10.1007/s10055-006-0055-1#ref-CR21" id="ref-link-section-d64763e1843">2004b</a>) and InertiaCube. SpaceMouse allows the programmer to assign functionality to provide a customised nine button-menu interface. This method has the advantage manipulating virtual information in six DOF in a natural way using only one hand. A combination of C++ functions, SpaceMouse commands and OpenGL allowed the integration of the 3D mouse into the system. Important functionalities that have been implemented and assigned to the menu buttons include either standard graphics transformations for easier manipulation, or more advanced graphics operations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig13">13</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Pseudo code for SpaceMouse</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig13">13</a>, <i>S</i> represents the scaling operations, <i>T</i>
                           <sub>
                    <i>x</i>
                  </sub>, <i>T</i>
                           <sub>
                    <i>y</i>
                  </sub> and <i>T</i>
                           <sub>
                    <i>z</i>
                  </sub> represent the translations and <i>R</i>
                           <sub>
                    <i>x</i>
                  </sub>, <i>R</i>
                           <sub>
                    <i>y</i>
                  </sub>, and <i>R</i>
                           <sub>
                    <i>z</i>
                  </sub> the rotations. To perform one of the above operations the user has to press one of the buttons (the translation button for example) and then use the bar to translate the object in 3D space. Depending on which direction force is applied, the object will move respectively. Furthermore, the ambient lighting, the clipping of superimposed geometry through an infinite plane and the augmentation of a virtual plane can be switched on and off using the remaining SpaceMouse buttons. Four example screenshots of a user interacting with 3D information using the SpaceMouse is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig14">14</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>SpaceMouse interactions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>It illustrates how a user can adapt the MagicBook approach (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753" href="/article/10.1007/s10055-006-0055-1#ref-CR5" id="ref-link-section-d64763e1956">2001</a>) in conjunction with the SpaceMouse to visualise and interact with the virtual artefacts. On the top left image, the user is only visualizing the virtual artefact (marker B) while on the top right image, the user translates the artefact using the SpaceMouse. On the bottom left image, the user interacts (rotates) with another artefact (marker A) and on the bottom right image the user visualises another artefact (belonging on marker E). The most important limitation of this tangible interface is the use of a single marker for tracking by the computer vision based tracking system.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Application scenarios</h2><div class="c-article-section__content" id="Sec21-content"><p>To test the functionality of the proposed AR interface system two application scenarios have been designed. The first section (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec22">7.1</a>) presents an educational application used to support and simplify teaching and learning techniques currently applied in the higher education sector. The second section (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec23">7.2</a>) illustrates a museum application with the aim of facilitating access to museums and other cultural heritage galleries. In the following sections, each application is briefly analysed and the most important findings of the research are presented.</p><h3 class="c-article__sub-heading" id="Sec22">Educational application</h3><p>Most educational AR applications operate in indoor environments (Begault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Begault DR (1994) 3D Sound for virtual reality and multimedia, Academic, New York, 1, 17–18" href="/article/10.1007/s10055-006-0055-1#ref-CR4" id="ref-link-section-d64763e1981">1994</a>; Fuhrmann and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fuhrmann A, Schmalstieg D (1999) Concept and implementation of a collaborative workspace for augmented reality, GRAPHICS ‘99, 18(3)" href="/article/10.1007/s10055-006-0055-1#ref-CR11" id="ref-link-section-d64763e1984">1999</a>) and the scenarios proposed in this section are focused on enhancing the teaching and learning process for higher education institutions like colleges and universities. With this purpose in mind, AR educational scenarios have been designed to assist teachers to transfer knowledge to the students in other ways than traditionally has been the case (Liarokapis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer" href="/article/10.1007/s10055-006-0055-1#ref-CR22" id="ref-link-section-d64763e1987">2005</a>). The aim is to provide a rewarding learning experience that is otherwise difficult or impossible to obtain by offering the ability to achieve better user interaction (with teaching material and complex tools) while the provision of an interactive augmented presentation provides students a high degree of flexibility and understanding of the teaching material. All scenarios are specifically engaged with the improvement of learning and teaching techniques in the fields of engineering and informatics at the University of Sussex.</p><p>Based on the functionality of the AR interface described in the above sections, a lecture was prepared introducing students on how computers work. This application has in practice some similarities with the experimental application proposed by Fernandes and Miranda (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Fernandes B, Miranda JC (2003) Learning how computer works with augmented reality. In: Proceedings of the 2nd international conference on multimedia and information and communication technologies in education, Badajoz, December 3–6" href="/article/10.1007/s10055-006-0055-1#ref-CR10" id="ref-link-section-d64763e1993">2003</a>). However, the higher education application offers a very powerful user interface that allows audio–visual augmentation as well as simultaneous interactions. From a visualisation point of view, the system displays the data in a single window and the lecturer can describe basic IT principles with the use of AR technology in a number of different ways. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig15">15</a>, a PowerPoint slide presentation that describes the characteristics of a computer system as well as relative textual information is augmented onto the appropriate marker card.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Teaching IT using AR</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Learners can zoom into the diagram in two ways. Firstly, by using the predefined functionality (scale and translate) existing in the keyboard, the menu and the SpaceMouse interfaces. Alternatively, learners can either move the marker card intuitively closer to the camera and vice versa. In both ways, potential users can clearly observe and understand the theoretical operation of a computer. The textual information describes the diagram in detail providing a more complete learning presentation simultaneously. Learners can now get simultaneously appropriate audio–visual information that helps them to acquire a deeper knowledge about the characteristics of a computer. In the same way, to increase the level of understanding of the teaching material presented to the students, 3D information can be presented to deepen the level of knowledge transfer. Along these lines, learners can have a more rounded idea of what are the main characteristics of a computer, what are the main parts and how they look like in reality.</p><p>The main advantage of the educational application over the traditional teaching methods is that learners can actually “see” and “listen” the virtual information superimposed in the real world (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Liarokapis, Petridis P, Lister PF, White M (2002) Multimedia augmented reality interface for E-learning (MARIE). World Trans  Eng Technol Educ 1(2):173–176" href="/article/10.1007/s10055-006-0055-1#ref-CR23" id="ref-link-section-d64763e2022">2002</a>). Students can naturally manipulate the virtual information using standard or sophisticated VR devices and they can repeat a specific part of the augmentation as many times as they want. Another benefit of the system is that it does not require students to have any previous experience to operate it. Finally, even AR has been experimentally applied for teaching engineering and information technology (IT) courses it has been designed in such a way that it can be easily adapted and applied very easily to other educational courses.</p><h3 class="c-article__sub-heading" id="Sec23">Cultural heritage application</h3><p>The concept of virtual exhibitions in museums has been around for many years and researchers have designed and developed several applications (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767" href="/article/10.1007/s10055-006-0055-1#ref-CR20" id="ref-link-section-d64763e2033">2004a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47–56" href="/article/10.1007/s10055-006-0055-1#ref-CR21" id="ref-link-section-d64763e2036">b</a>; Hall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hall T, Ciolfi L et al (2001) The visitor as virtual archaeologist: using mixed reality technology to enhance education and social interaction in the museum. In: Spencer S (ed) Proceedings of the virtual reality, archaeology, and cultural heritage (VAST 2001), New York, ACM SIGGRAPH, Glyfada, Nr Athens, November, pp 91–96" href="/article/10.1007/s10055-006-0055-1#ref-CR14" id="ref-link-section-d64763e2039">2001</a>; Gatermann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Gatermann H (2000) From VRML to augmented reality via panorama-integration and EAI-Java, in constructing the digital space. In: Proceeding of the SiGraDi, September, 254–256" href="/article/10.1007/s10055-006-0055-1#ref-CR12" id="ref-link-section-d64763e2042">2000</a>). In addition, a number of museums hold innumerable archives or collections of artefacts, which they cannot exhibit in a low cost and efficient way. Museums simply do not have the space to exhibit all the artefacts in an educational and learning manner. Augmented Representations of Cultural Objects (ARCO) was an EU-funded research project (completed in September 2004) in order to analyse and provide innovative but simple to use technical solutions for virtual cultural object creation and visualisation. In short, ARCO provides museums with a set of tools that allow them to digitize, manage and present artefacts in virtual exhibitions. To evaluate the usability of the system properly ARCO collaborated with Victoria and Albert Museum and the Sussex Archaeological Society.</p><p>The work illustrated in the previous sections has been applied in ARCO to explore the potential of AR in a museum environment by mixing virtual information in an environment comprised of real objects. The success of an AR exhibition is highly related to the level of realism achieved. In general, there are a few AR applications that do not require a high level of realism, but within the cultural heritage field realistic visualisation is an important issue (Liarokapis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767" href="/article/10.1007/s10055-006-0055-1#ref-CR20" id="ref-link-section-d64763e2048">2004a</a>). The scenarios illustrate how virtual museum visitors can visualise archaeological information such as virtual artefacts or even whole virtual museum galleries providing an educational narration for the preservation of cultural heritage. An example screenshot of four different virtual galleries from Victoria and Albert Museum are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig16">16</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Virtual museum gallery visualisation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In theory, this technique can be extended to as many markers as long as the camera can detect them within the field-of-view. The major drawback of this method is that the frame rate drops analogous to the number of markers used (as illustrated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0055-1#Sec18">6.3</a>) but the overall effectiveness in all galleries was between 25–30 FPS. Furthermore, the realism of the system highly depends on the 3D modelling procedure and for this reason the 3D models used in this scenario are the very high resolution models. The visualisation of an expedition to large groups of people can be considered as a collaborative activity. By looking at and interacting with the artefact visualisation visitors can communicate with each other by expressing their thoughts about any aspects that relate to the history of the artefact. This results in an exchange of opinions amongst the visitors in an implicit and explicit way. By zooming into the artefact more contained arguments can be made about the nature of the material used for its construction. On the other hand, in a perspective view more verbal communication is possible. By using the configuration setting of the collaboration in the AR interface, visitors can use HMDs and obtain a completely immersed view.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Preliminary evaluation</h2><div class="c-article-section__content" id="Sec24-content"><p>The knowledge gained from reviewing the literature and the experimental results, enabled an initial dissemination of the prototype AR interface. Even if this work is still on an experimental status, the results can be taken into consideration to improve the effectiveness of the presented system as well as to design future high-level AR interfaces. An expert-based evaluation approach was followed that argues that formal laboratory user studies can effectively evaluate visualisation when a small sample of expert users is used (Tory and Möller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Tory M, Möller T (2005) Evaluating visualizations: do expert reviews work? IEEE Comput Graph Appl 25(5):8–11" href="/article/10.1007/s10055-006-0055-1#ref-CR37" id="ref-link-section-d64763e2087">2005</a>). In terms of evaluating the system, some initial empirical research was conducted based on a two stage human-centred questionnaire. The first part is generic but aims at evaluating the usability of the system in the learning process while the second part is more technical and refers to the effectiveness of the visualisation and interaction techniques of the interface. An educator would design the questionnaire in a different way taking primarily into consideration educational aspects whereas in this case, the purpose was to obtain a number of useful conclusions regarding the technicalities and practicalities of the system. This pilot study was disseminated to a five research staff from Sussex University that had experience in working with VR applications. Four were men and one was woman. Subjects were between the ages of 24 and 28 and the average time of the evaluation was 30 min.</p><h3 class="c-article__sub-heading" id="Sec25">General questions</h3><p>The feedback received following the completion of the evaluation process varied but in general lines was encouraging. As far as the first part of the questionnaire is concerned, all the users thought that the system has the potential to be used as a learning tool in the future although it currently lacks from interoperability issues. Specifically, they argued that the application scenarios were really interesting and exciting but for teaching purposes more comprehensive learning scenarios have to be implemented. The findings from this study are summarised in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0055-1#Tab3">3</a>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 General questions about the system</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0055-1/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Results illustrate that 92% of the users believe that the system has the potential to be used as a basic platform to create AR scenarios and applications whereas 80% rate the quality of the system good. On the other hand, 72% of the users liked the overall usage of the system and 64% feel that educational applications could benefit from this technology. Moreover, two users mentioned that the system would be much more useful if a multimedia database system with a content management system is used to increase interoperability issues. Another one stated that a print function would help to capture and store into images the different views of the AR environment.</p><h3 class="c-article__sub-heading" id="Sec26">Technical questions</h3><p>Regarding the second part, all users agreed that the system is very easy to use and that the visualisation process is more than satisfactory. Surprisingly, most of the users preferred the HMD-based visualisation versus the monitor-based visualisation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig17">17</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Monitor versus HMD user response</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig17">17</a> shows the user-response in comparing the monitor-based AR (mean = 6.8, SD = 2.77489, SE = 1.24097) versus video see through HMD-based AR (mean = 8.2, SD = 2.48998, SE = 1.11355). Similar studies have shown the exact opposite result but in this study all users were computer literature and all had previously used VR prototypes that make use of HMDs. Moreover, many difficulties were observed when participants tried to move around with the camera mounted on the HMD because they could not keep it in line with the sight of view. Also because the resolution of the HMD is limited to 800 × 600 and the quality of the overlaid graphics into the optics system is not very good, two participants felt nausea and motion sickness after a 10 min usage. However, even if these problems seem to restrict the use of HMDs, participants appreciated the level of immersion provided and thus preferred it. As far as the interaction techniques are concerned, the natural interaction techniques based on the marker cards were found to be very effective and intuitive to use compared to the other interaction techniques. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0055-1#Fig18">18</a> illustrates a comparison based on the user-response between the most important interaction techniques implemented.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0055-1/MediaObjects/10055_2006_55_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Interaction techniques user response</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0055-1/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The I/O interaction techniques got the second highest score (mean = 6.2, SD = 2.16795, SE = 0.96954) since they are the standard way for interacting with computers and the end-users feel more familiar with. Surprisingly, the SpaceMouse interactions (mean = 5.4, SD = 2.50998, SE = 1.1225) received the most variable responses. Some participants argued that it is extremely useful to manipulate the virtual information using only one hand but others recorded that a lot of time is required to fully familiarise with the device and even then it is not as easy to use other means such as the I/O devices and the marker cards.</p><p>Moreover, the GUI interactions (mean = 4.4, SD = 2.19089, SE = 0.9798) got the worst score from all other types of interaction. One of the end-users argued that it is difficult to understand how to alter the orientation of the virtual objects since it was specified as yaw, pitch and roll. Other users stated that it takes the most time to perform a single rotation compared to the rest of the methods. For example, the keyboard keys replicate the functionality and as soon as the user becomes familiar with the “shortcuts” it is much faster. On the contrary, the marker cards interaction (mean = 7.8, SD = 2.58844, SE = 1.15758) received the most positive feedback of all other types of interaction and although it was pretty much expected, an initial comparison between different techniques has been made. All participants agreed that it very easy and intuitive to manipulate the virtual information in 3D space using any type of physical interface but they also proposed to use in the future a physical interface that consists of a handle. Overall, the preliminary evaluation was a profitable experience to complete the first cycle of this research but more user-studies need to be performed in the future.</p></div></div></section><section aria-labelledby="Sec27"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Conclusions and future work</h2><div class="c-article-section__content" id="Sec27-content"><p>In this paper the design and implementation of effective AR interfaces for indoor-environments was presented and analysed. The proposed framework can be used as a generic tool to create high-level AR applications. The final visualisation can be performed either on a variety of display technologies ranging from monitor based to video see-through display technologies. A series of visualisation and interaction techniques were investigated in order to create the illusion that the virtual information coexists with the real world. In addition, two innovative AR case studies have been implemented: one for higher education purposes (university environments) and the second for archaeological and cultural heritage purposes (museum environments). Finally, an initial evaluation was performed to obtain useful critique concerning the overall technicalities and practicalities of the system.</p><p>The main advantages of the AR architecture are the low cost and the multimedia augmentation in real-time. The structure of the architectures is based on the philosophy that the most appropriate tool/device must be used for the task ones seeking to achieve. This, however, does not imply that the best tool/device is the most expensive one. The two different experimental setups successfully tested for this research clearly demonstrate this. One cost effective setup has been constructed comprising of off-the-self hardware and a second one based on state of the art expensive hardware components (i.e. Spacemouse, Touch Screen).</p><p>Although the system is designed for indoor environments it can be easily extended to operate in outdoor environments. The current status of the research is focused in various mobile devices such as personal digital assistants (PDAs) and third-generation (3G) phones as well as positioning technologies (such as GPS). This will create a robust mobile AR environment that will be integrated with the rest of the interface framework to provide prototype applications for outdoor environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Angel E (2003) Interactive computer graphics: a top-down approach using OpenGL,3rd edn. Addison–Wesley, Readin" /><p class="c-article-references__text" id="ref-CR1">Angel E (2003) Interactive computer graphics: a top-down approach using OpenGL,3rd edn. Addison–Wesley, Reading, pp 17–18, 69, 107, 322–349, 472</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma R (1997) A survey of augmented reality. Teleoper Virtual Environ 6(4):355–385" /><p class="c-article-references__text" id="ref-CR2">Azuma R (1997) A survey of augmented reality. Teleoper Virtual Environ 6(4):355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Teleoper%20Virtual%20Environ&amp;volume=6&amp;issue=4&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y et al (2001) Recent advances in augmented reality. IEEE Comput Graph November/December 21(6" /><p class="c-article-references__text" id="ref-CR3">Azuma R, Baillot Y et al (2001) Recent advances in augmented reality. IEEE Comput Graph November/December 21(6):34–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20November%2FDecember&amp;volume=21&amp;issue=6&amp;pages=34-47&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Begault DR (1994) 3D Sound for virtual reality and multimedia, Academic, New York, 1, 17–18" /><p class="c-article-references__text" id="ref-CR4">Begault DR (1994) 3D Sound for virtual reality and multimedia, Academic, New York, 1, 17–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, H. Kato, I. Poupyrev, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753" /><p class="c-article-references__text" id="ref-CR5">Billinghurst M, Kato H, Poupyrev I (2001) The magicbook: a traditional AR interface. Comput Graph 25:745–753</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2801%2900117-0" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20magicbook%3A%20a%20traditional%20AR%20interface&amp;journal=Comput%20Graph&amp;volume=25&amp;pages=745-753&amp;publication_year=2001&amp;author=Billinghurst%2CM&amp;author=Kato%2CH&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Butz A, Höllerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: Pr" /><p class="c-article-references__text" id="ref-CR6">Butz A, Höllerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: Proceedings of the 2nd IEEE and ACM international workshop on augmented reality ‘99. San Francisco, October 20–21 </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Camera calibration toolbox for Matlab, available at: [http://www.vision.caltech.edu/bouguetj/calib_doc/], Acce" /><p class="c-article-references__text" id="ref-CR7">Camera calibration toolbox for Matlab, available at: [<a href="http://www.vision.caltech.edu/bouguetj/calib_doc/">http://www.vision.caltech.edu/bouguetj/calib_doc/</a>], Accessed at 14/01/2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dobler D, Haller M, Stampfl P (2002) ASR—augmented sound reality, ACM SIGGRAPH 2002 conference abstracts and a" /><p class="c-article-references__text" id="ref-CR8">Dobler D, Haller M, Stampfl P (2002) ASR—augmented sound reality, ACM SIGGRAPH 2002 conference abstracts and applications, San Antonio, p 148</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feiner S, MacIntyre B et al (1993) Windows on the world: 2D Windows for 3D augmented reality. In: Proceedings " /><p class="c-article-references__text" id="ref-CR9">Feiner S, MacIntyre B et al (1993) Windows on the world: 2D Windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, Atlanta, November 3–5, Association for Computing Machinery, pp 145–155</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fernandes B, Miranda JC (2003) Learning how computer works with augmented reality. In: Proceedings of the 2nd " /><p class="c-article-references__text" id="ref-CR10">Fernandes B, Miranda JC (2003) Learning how computer works with augmented reality. In: Proceedings of the 2nd international conference on multimedia and information and communication technologies in education, Badajoz, December 3–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuhrmann A, Schmalstieg D (1999) Concept and implementation of a collaborative workspace for augmented reality" /><p class="c-article-references__text" id="ref-CR11">Fuhrmann A, Schmalstieg D (1999) Concept and implementation of a collaborative workspace for augmented reality, GRAPHICS ‘99, 18(3)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gatermann H (2000) From VRML to augmented reality via panorama-integration and EAI-Java, in constructing the d" /><p class="c-article-references__text" id="ref-CR12">Gatermann H (2000) From VRML to augmented reality via panorama-integration and EAI-Java, in constructing the digital space. In: Proceeding of the SiGraDi, September, 254–256</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R, Gascuel J-D (2002) MARE: multiuser augmented reality environment on table setup. ACM SIGGRAPH confe" /><p class="c-article-references__text" id="ref-CR13">Grasset R, Gascuel J-D (2002) MARE: multiuser augmented reality environment on table setup. ACM SIGGRAPH conference abstracts and applications</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hall T, Ciolfi L et al (2001) The visitor as virtual archaeologist: using mixed reality technology to enhance " /><p class="c-article-references__text" id="ref-CR14">Hall T, Ciolfi L et al (2001) The visitor as virtual archaeologist: using mixed reality technology to enhance education and social interaction in the museum. In: Spencer S (ed) Proceedings of the virtual reality, archaeology, and cultural heritage (VAST 2001), New York, ACM SIGGRAPH, Glyfada, Nr Athens, November, pp 91–96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haller M, Hartmann W et al (2002) Combining ARToolKit with scene graph libraries. In: Proceedings of the first" /><p class="c-article-references__text" id="ref-CR15">Haller M, Hartmann W et al (2002) Combining ARToolKit with scene graph libraries. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt, Germany, 29 September</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Haniff, C. Baber, W. Edmondson, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Haniff D, Baber C, Edmondson W (2000) Categorizing augmented reality systems. J Three Dimens Images 14(4):105–" /><p class="c-article-references__text" id="ref-CR16">Haniff D, Baber C, Edmondson W (2000) Categorizing augmented reality systems. J Three Dimens Images 14(4):105–109</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Categorizing%20augmented%20reality%20systems&amp;journal=J%20Three%20Dimens%20Images&amp;volume=14&amp;issue=4&amp;pages=105-109&amp;publication_year=2000&amp;author=Haniff%2CD&amp;author=Baber%2CC&amp;author=Edmondson%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedin" /><p class="c-article-references__text" id="ref-CR17">Kato H, Billinghurst M, et al (2000a) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality 2000, Munich,  5–6 Oct, pp  111–119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, Universit" /><p class="c-article-references__text" id="ref-CR18">Kato H, Billinghurst M, Poupyrev I (2000b) ARToolkit user manual, version 2.33, Human Interface Lab, University of Washington </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Klinker, KH. Ahlers, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Klinker G, Ahlers KH et al (1997) Confluence of computer vision and interactive graphics for augmented reality" /><p class="c-article-references__text" id="ref-CR19">Klinker G, Ahlers KH et al (1997) Confluence of computer vision and interactive graphics for augmented reality, PRESENCE: teleoperations and virtual environments. special issue on augmented reality, August 6(4):433–451</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Confluence%20of%20computer%20vision%20and%20interactive%20graphics%20for%20augmented%20reality%2C%20PRESENCE%3A%20teleoperations%20and%20virtual%20environments.%20special%20issue%20on%20augmented%20reality&amp;journal=August&amp;volume=6&amp;issue=4&amp;pages=433-451&amp;publication_year=1997&amp;author=Klinker%2CG&amp;author=Ahlers%2CKH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the internati" /><p class="c-article-references__text" id="ref-CR20">Liarokapis F, White M, Lister PF (2004a) Augmented reality interface toolkit. In: Proceedings of the international symposium on augmented and virtual reality, London, pp 761–767</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedi" /><p class="c-article-references__text" id="ref-CR21">Liarokapis F, Sylaiou S, et al (2004b) An interactive visualisation interface for virtual museum. In: Proceedings of the 5th international symposium on virtual reality, Archaeology  Cultural Heritage, pp 47–56</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual in" /><p class="c-article-references__text" id="ref-CR22">Liarokapis F (2005) Augmented reality interfaces—architectures for visualising and interacting with virtual information. PhD thesis. University of Sussex, Falmer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Liarokapis, P. Petridis, PF. Lister, M. White, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Liarokapis, Petridis P, Lister PF, White M (2002) Multimedia augmented reality interface for E-learning (MARIE" /><p class="c-article-references__text" id="ref-CR23">Liarokapis, Petridis P, Lister PF, White M (2002) Multimedia augmented reality interface for E-learning (MARIE). World Trans  Eng Technol Educ 1(2):173–176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimedia%20augmented%20reality%20interface%20for%20E-learning%20%28MARIE%29&amp;journal=World%20Trans%20Eng%20Technol%20Educ&amp;volume=1&amp;issue=2&amp;pages=173-176&amp;publication_year=2002&amp;author=Liarokapis%2C&amp;author=Petridis%2CP&amp;author=Lister%2CPF&amp;author=White%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. MacIntyre, M. Gandy, S. Dow, JD. Bolter, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="MacIntyre B, Gandy M, Dow S, Bolter JD (2005) DART: a toolkit for rapid design exploration of augmented realit" /><p class="c-article-references__text" id="ref-CR24">MacIntyre B, Gandy M, Dow S, Bolter JD (2005) DART: a toolkit for rapid design exploration of augmented reality experiences. ACM Trans Graph (TOG), 24(3):932</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1073204.1073288" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=DART%3A%20a%20toolkit%20for%20rapid%20design%20exploration%20of%20augmented%20reality%20experiences&amp;journal=ACM%20Trans%20Graph%20%28TOG%29&amp;volume=24&amp;issue=3&amp;publication_year=2005&amp;author=MacIntyre%2CB&amp;author=Gandy%2CM&amp;author=Dow%2CS&amp;author=Bolter%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mahoney D (1999b) Better than real, computer graphics world, pp 32–40" /><p class="c-article-references__text" id="ref-CR25">Mahoney D (1999b) Better than real, computer graphics world, pp 32–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Malbezin P, Piekarski W and Thomas B (2002) Measuring ARToolKit accuracy in long distance tracking experiments" /><p class="c-article-references__text" id="ref-CR26">Malbezin P, Piekarski W and Thomas B (2002) Measuring ARToolKit accuracy in long distance tracking experiments. In: Proceedings of the 1st international augmented reality toolkit workshop, Germany, Darmstadt, September 29</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Milgram P, Colquhoun H (1999) A Taxonomy of real and virtual world display integration, mixed reality merging " /><p class="c-article-references__text" id="ref-CR27">Milgram P, Colquhoun H (1999) A Taxonomy of real and virtual world display integration, mixed reality merging real and virtual worlds. Ohta Y, Tamura H (eds) Ohmsha Ltd, Chapter 1, pp 5–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays, IEICE Trans Inf Syst E77-D(12):1321–1" /><p class="c-article-references__text" id="ref-CR28">Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays, IEICE Trans Inf Syst E77-D(12):1321–1329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moller T (1999) Real-time rendering. AK Peters Ltd, Natick, 23–38, 171" /><p class="c-article-references__text" id="ref-CR29">Moller T (1999) Real-time rendering. AK Peters Ltd, Natick, 23–38, 171</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, D. Tan, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Poupyrev I, Tan D et al (2002) Developing a generic augmented reality interface. Computer 35(3):44–50" /><p class="c-article-references__text" id="ref-CR30">Poupyrev I, Tan D et al (2002) Developing a generic augmented reality interface. Computer 35(3):44–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F2.989929" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20a%20generic%20augmented%20reality%20interface&amp;journal=Computer&amp;volume=35&amp;issue=3&amp;pages=44-50&amp;publication_year=2002&amp;author=Poupyrev%2CI&amp;author=Tan%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reitmayr G, Schmalstieg D (2001) A wearable 3D augmented reality workspace. In: Proceedings of the 5th interna" /><p class="c-article-references__text" id="ref-CR31">Reitmayr G, Schmalstieg D (2001) A wearable 3D augmented reality workspace. In: Proceedings of the 5th international symposium on wearable computers, October 8–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world envi" /><p class="c-article-references__text" id="ref-CR32">Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: Myers BA (ed) Proceedings of UIST ‘95. ACM, Pennsylvania, pp 29–36</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track, IEEE conference on computer vision and pattern recognition, Sea" /><p class="c-article-references__text" id="ref-CR33">Shi J, Tomasi C (1994) Good features to track, IEEE conference on computer vision and pattern recognition, Seattle, June, pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sinclair P, Martinez K (2001) Adaptive hypermedia in augmented reality. In: Proceedings of the third workshop " /><p class="c-article-references__text" id="ref-CR34">Sinclair P, Martinez K (2001) Adaptive hypermedia in augmented reality. In: Proceedings of the third workshop on adaptive hypertext and hypermedia at the twelfth ACM conference on hypertext and hypermedia, Denmark, August 2001, pp  217–219</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Slay H, Phillips M et al (2001) Interaction modes for augmented reality visualization, Australian symposium on" /><p class="c-article-references__text" id="ref-CR35">Slay H, Phillips M et al (2001) Interaction modes for augmented reality visualization, Australian symposium on information visualization, Sydney, December</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smith GC (1994) The art of interaction. In: MacDonald L, Vince J (eds) Interacting with virtual environments. " /><p class="c-article-references__text" id="ref-CR36">Smith GC (1994) The art of interaction. In: MacDonald L, Vince J (eds) Interacting with virtual environments. Wiley, New York,  pp 79–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Tory, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Tory M, Möller T (2005) Evaluating visualizations: do expert reviews work? IEEE Comput Graph Appl 25(5):8–11" /><p class="c-article-references__text" id="ref-CR37">Tory M, Möller T (2005) Evaluating visualizations: do expert reviews work? IEEE Comput Graph Appl 25(5):8–11</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2005.102" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20visualizations%3A%20do%20expert%20reviews%20work%3F&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=25&amp;issue=5&amp;pages=8-11&amp;publication_year=2005&amp;author=Tory%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vallino J (1998) Interactive augmented reality. PhD thesis, Department of Computer Science, University of Roch" /><p class="c-article-references__text" id="ref-CR38">Vallino J (1998) Interactive augmented reality. PhD thesis, Department of Computer Science, University of Rochester, pp 1–25</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Weng J, Cohen P, Herniou M (1992) Camera calibration with distortion models and accuracy evaluation, IEEE tran" /><p class="c-article-references__text" id="ref-CR39">Weng J, Cohen P, Herniou M (1992) Camera calibration with distortion models and accuracy evaluation, IEEE transactions on pattern analysis and machine intelligence, 14(10)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Woo M, Neider J, Davis T (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2, " /><p class="c-article-references__text" id="ref-CR40">Woo M, Neider J, Davis T (1999) OpenGL programming guide: the official guide to learning OpenGL, Version 1.2, Addison–Wesley, Reading </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yewdall D (1999) Practical art of motion picture sound. Focal Press, Boston" /><p class="c-article-references__text" id="ref-CR41">Yewdall D (1999) Practical art of motion picture sound. Focal Press, Boston</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0055-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>Part of this research work was funded by the EU IST Framework V programme, Key Action III- Multimedia Content and Tools, Augmented Representation of Cultural Objects (ARCO) project IST-2000-28366.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QT, UK</p><p class="c-article-author-affiliation__authors-list">Fotis Liarokapis</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Information Science, City University, London, EC1V 0HB, UK</p><p class="c-article-author-affiliation__authors-list">Fotis Liarokapis</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Fotis-Liarokapis"><span class="c-article-authors-search__title u-h3 js-search-name">Fotis Liarokapis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Fotis+Liarokapis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Fotis+Liarokapis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Fotis+Liarokapis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0055-1/email/correspondent/c1/new">Fotis Liarokapis</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20augmented%20reality%20interface%20for%20visualizing%20and%20interacting%20with%20virtual%20content&amp;author=Fotis%20Liarokapis&amp;contentID=10.1007%2Fs10055-006-0055-1&amp;publication=1359-4338&amp;publicationDate=2006-11-09&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Liarokapis, F. An augmented reality interface for visualizing and interacting with virtual content.
                    <i>Virtual Reality</i> <b>11, </b>23–43 (2007). https://doi.org/10.1007/s10055-006-0055-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0055-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-12-15">15 December 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-10-19">19 October 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-11-09">09 November 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2007-03">March 2007</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0055-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0055-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tangible interfaces</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual heritage</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Learning systems</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0055-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=55;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

