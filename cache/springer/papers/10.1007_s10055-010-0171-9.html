<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An integrated head pose and eye gaze tracking approach to non-intrusiv"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Eye gaze tracking is very useful for quantitatively measuring visual attention in virtual environments. However, most eye trackers have a limited tracking range, e.g., &#177;35&#176; in the..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators"/>

    <meta name="dc.source" content="Virtual Reality 2010 16:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-09-03"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Eye gaze tracking is very useful for quantitatively measuring visual attention in virtual environments. However, most eye trackers have a limited tracking range, e.g., &#177;35&#176; in the horizontal direction. This paper proposed a method to combine head pose tracking and eye gaze tracking together to achieve a large range of tracking in virtual driving simulation environments. Multiple parallel multilayer perceptrons were used to reconstruct the relationship between head images and head poses. Head images were represented with the coefficients extracted from Principal Component Analysis. Eye gaze tracking provides precise results on the front view, while head pose tracking is more suitable for tracking areas of interest than for tracking points of interest on the side view."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-09-03"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="25"/>

    <meta name="prism.endingPage" content="32"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0171-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0171-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0171-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0171-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2012/03"/>

    <meta name="citation_online_date" content="2010/09/03"/>

    <meta name="citation_firstpage" content="25"/>

    <meta name="citation_lastpage" content="32"/>

    <meta name="citation_article_type" content="SI: Manufacturing and Construction"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0171-9"/>

    <meta name="DOI" content="10.1007/s10055-010-0171-9"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0171-9"/>

    <meta name="description" content="Eye gaze tracking is very useful for quantitatively measuring visual attention in virtual environments. However, most eye trackers have a limited tracking "/>

    <meta name="dc.creator" content="Hua Cai"/>

    <meta name="dc.creator" content="Yingzi Lin"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Syst and Comput; citation_title=Analysis and recognition of a human head&#8217;s movement from an image sequence; citation_author=Y Abe, M Hagiwara; citation_volume=32; citation_issue=5; citation_publication_date=2001; citation_pages=36-45; citation_doi=10.1002/scj.1024; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=HERA: learner tracking in a virtual environment; citation_author=K Amokrane, D Lourdeaux, J Burkhardt; citation_volume=7; citation_issue=3; citation_publication_date=2008; citation_pages=23-30; citation_id=CR2"/>

    <meta name="citation_reference" content="Beverina F, Palmas G, Anisetti M, Bellandi V (2006) Tracking based face identification: a way to manage occlusions, and illumination, posture and expression changes. 2nd IET international conference on intelligent environments, IE, pp 161&#8211;166"/>

    <meta name="citation_reference" content="Cai H, Lin Y, and Mourant RR (2007) Evaluation of driver visual behavior and road signs in virtual environment. In: Proceeding of HFES 51st annual meeting, Baltimore, vol 5. USA, pp 1645&#8211;1649"/>

    <meta name="citation_reference" content="citation_journal_title=Netw Comput Neural Syst; citation_title=Human eye-head co-ordination in natural exploration; citation_author=W Einhauser, F Schumann, S Bardins, K Bartl, G Boning, E Schneider, P Konig; citation_volume=18; citation_issue=3; citation_publication_date=2007; citation_pages=267-297; citation_doi=10.1080/09548980701671094; citation_id=CR5"/>

    <meta name="citation_reference" content="Ekman P, Friesen W, Hager J (2002) Facial action coding system. 2nd ed. salt lake city. ISBN 0-931835-01-1"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Netw; citation_title=Multilayer feedforward networks are universal approximators; citation_author=KM Hornik, M Stinchcombe, H White; citation_volume=2; citation_issue=5; citation_publication_date=1989; citation_pages=359-366; citation_doi=10.1016/0893-6080(89)90020-8; citation_id=CR7"/>

    <meta name="citation_reference" content="Kuratate T, Masuda S, Vatikiotis-Bateson E (2001) What perceptible information can be implemented in talking head animations? In: Proceedings of the IEEE international workshop on robot and human interactive communication (RO-MAN2001), Bordaux and Paris, France, pp 430&#8211;435"/>

    <meta name="citation_reference" content="citation_journal_title=Electron Lett; citation_title=Conformal snake algorithm for contour detection; citation_author=SY Lam, CS Tong; citation_volume=38; citation_issue=10; citation_publication_date=2002; citation_pages=452-453; citation_doi=10.1049/el:20020335; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Predictable eye-head coordination during driving; citation_author=MF Land; citation_volume=359; citation_issue=6393; citation_publication_date=1992; citation_pages=318-320; citation_doi=10.1038/359318a0; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Visual attention in driving: the effects of cognitive load and visual disruption; citation_author=YC Lee, JD Lee, L Boyle; citation_volume=49; citation_issue=4; citation_publication_date=2007; citation_pages=721-733; citation_doi=10.1518/001872007X215791; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum-Comput Stud; citation_title=Using eye movement parameters for evaluating human&#8211;machine interface frameworks under normal control operation and fault detection situation; citation_author=Y Lin, WJ Zhang, LG Watson; citation_volume=59; citation_issue=6; citation_publication_date=2003; citation_pages=837-873; citation_doi=10.1016/S1071-5819(03)00122-8; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=Realization of the virtual driving system rules based on the improved petri-net; citation_author=L Ma, Y Yu, Y Zhang; citation_volume=4; citation_issue=4; citation_publication_date=2000; citation_pages=18; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Displays; citation_title=Optic flow and geometric field of view in a driving simulator display; citation_author=RR Mourant, N Ahmad, BK Jaeger, Y Lin; citation_volume=28; citation_publication_date=2007; citation_pages=145-149; citation_doi=10.1016/j.displa.2007.04.011; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res; citation_title=The influence of future gaze orientation upon eye-head coupling during saccades; citation_author=BS Oommen, RM Smith, JS Stah; citation_volume=155; citation_issue=1; citation_publication_date=2004; citation_pages=9-18; citation_doi=10.1007/s00221-003-1694-z; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Image Process; citation_title=A faster converging snake algorithm to locate object boundaries; citation_author=M Sakalli, KM Lam, H Yan; citation_volume=15; citation_issue=5; citation_publication_date=2006; citation_pages=1182-1191; citation_doi=10.1109/TIP.2006.871401; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Fusion of perceptual cues for robust tracking of head pose and position; citation_author=J Sherrah, S Gong; citation_volume=34; citation_issue=8; citation_publication_date=2001; citation_pages=1565-1572; citation_doi=10.1016/S0031-3203(00)00091-1; citation_id=CR17"/>

    <meta name="citation_reference" content="Shlens J (2005) A tutorial on principal component analysis. University of California. 
                    http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=Virtual automobile driver training simulator; citation_author=F Xu, B Xu, H Ding, W Qian, D Zhang, W Wang, F Ge; citation_volume=4; citation_issue=4; citation_publication_date=2000; citation_pages=11; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Identification of real-time diagnostic measures of visual distraction with an automatic eye-tracking system; citation_author=H Zhang, M Smith, GJ Witt; citation_volume=48; citation_issue=4; citation_publication_date=2006; citation_pages=805-821; citation_doi=10.1518/001872006779166307; citation_id=CR20"/>

    <meta name="citation_author" content="Hua Cai"/>

    <meta name="citation_author_institution" content="Department of Mechanical and Industrial Engineering, Northeastern University, Boston, USA"/>

    <meta name="citation_author" content="Yingzi Lin"/>

    <meta name="citation_author_email" content="yilin@coe.neu.edu"/>

    <meta name="citation_author_institution" content="Department of Mechanical and Industrial Engineering, Northeastern University, Boston, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0171-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0171-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators"/>
        <meta property="og:description" content="Eye gaze tracking is very useful for quantitatively measuring visual attention in virtual environments. However, most eye trackers have a limited tracking range, e.g., ±35° in the horizontal direction. This paper proposed a method to combine head pose tracking and eye gaze tracking together to achieve a large range of tracking in virtual driving simulation environments. Multiple parallel multilayer perceptrons were used to reconstruct the relationship between head images and head poses. Head images were represented with the coefficients extracted from Principal Component Analysis. Eye gaze tracking provides precise results on the front view, while head pose tracking is more suitable for tracking areas of interest than for tracking points of interest on the side view."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0171-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Eye gaze tracking, Head pose tracking, Multilayer perceptron (MLP), Visual attention","kwrd":["Eye_gaze_tracking","Head_pose_tracking","Multilayer_perceptron_(MLP)","Visual_attention"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0171-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0171-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=171;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0171-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0171-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0171-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Manufacturing and Construction</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-09-03" itemprop="datePublished">03 September 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hua-Cai" data-author-popup="auth-Hua-Cai">Hua Cai</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Northeastern University" /><meta itemprop="address" content="grid.261112.7, 0000000121733359, Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, 02115, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yingzi-Lin" data-author-popup="auth-Yingzi-Lin" data-corresp-id="c1">Yingzi Lin<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Northeastern University" /><meta itemprop="address" content="grid.261112.7, 0000000121733359, Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, 02115, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">25</span>–<span itemprop="pageEnd">32</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">475 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0171-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Eye gaze tracking is very useful for quantitatively measuring visual attention in virtual environments. However, most eye trackers have a limited tracking range, e.g., ±35° in the horizontal direction. This paper proposed a method to combine head pose tracking and eye gaze tracking together to achieve a large range of tracking in virtual driving simulation environments. Multiple parallel multilayer perceptrons were used to reconstruct the relationship between head images and head poses. Head images were represented with the coefficients extracted from Principal Component Analysis. Eye gaze tracking provides precise results on the front view, while head pose tracking is more suitable for tracking areas of interest than for tracking points of interest on the side view.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><h3 class="c-article__sub-heading" id="Sec2">Challenges of visual attention tracking in virtual environments with wide field of view</h3><p>Visual attention is essential for most human–machine interactions. In safety–critical tasks, a short moment of losing visual attention may cause serious consequences, such as collisions in driving (Cai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Cai H, Lin Y, and Mourant RR (2007) Evaluation of driver visual behavior and road signs in virtual environment. In: Proceeding of HFES 51st annual meeting, Baltimore, vol 5. USA, pp 1645–1649" href="/article/10.1007/s10055-010-0171-9#ref-CR4" id="ref-link-section-d51414e339">2007</a>; Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lee YC, Lee JD, Boyle L (2007) Visual attention in driving: the effects of cognitive load and visual disruption. Hum Factors 49(4):721–733" href="/article/10.1007/s10055-010-0171-9#ref-CR11" id="ref-link-section-d51414e342">2007</a>). In human–machine interactive experiments, eye gaze tracking is a very efficient method to quantitatively measure users’ visual attention, including evaluating interface frameworks (Lin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Lin Y, Zhang WJ, Watson LG (2003) Using eye movement parameters for evaluating human–machine interface frameworks under normal control operation and fault detection situation. Int J Hum-Comput Stud 59(6):837–873" href="/article/10.1007/s10055-010-0171-9#ref-CR12" id="ref-link-section-d51414e345">2003</a>) and investigating web searching behaviors. Visual attention measures usually include eye gaze target, reaction time, accuracy, visual acuity, and subjective evaluation. In the virtual environment–based driving experiments, eye gaze tracking can precisely indicate drivers’ visual attention (Xu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Xu F, Xu B, Ding H, Qian W, Zhang D, Wang W, Ge F (2000) Virtual automobile driver training simulator. Int J Virtual Real 4(4):11" href="/article/10.1007/s10055-010-0171-9#ref-CR19" id="ref-link-section-d51414e348">2000</a>; Ma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Ma L, Yu Y, Zhang Y (2000) Realization of the virtual driving system rules based on the improved petri-net. Int J Virtual Real 4(4):18" href="/article/10.1007/s10055-010-0171-9#ref-CR13" id="ref-link-section-d51414e351">2000</a>).</p><p>Currently, a variety of eye tracking systems are available for visual attention measurement (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zhang H, Smith M, Witt GJ (2006) Identification of real-time diagnostic measures of visual distraction with an automatic eye-tracking system. Hum Factors 48(4):805–821" href="/article/10.1007/s10055-010-0171-9#ref-CR20" id="ref-link-section-d51414e357">2006</a>). They utilize red-eye effect in near-infrared reference light to measure the precise target of eye gaze. Non-wearable eye trackers, such as Tobii and SeeingMachine, are usually put in front of users without blocking their views. They are characterized as non-intrusive to participants, low sensitivity to head movement, and highly accurate (as high as 0.5°). More importantly, they output the coordinates with reference to the world coordination system. These characteristics are highly suitable for tasks carried out in a sitting posture in a virtual environment. Head-mounted eye trackers, e.g., EyeLink and iView X, require the users to wear them on the head. Since the camera is fixed on the head, it captures the head direction–dependent scenes and then tracks the scenes-dependent eye gaze locations. Head-mounted eye trackers are flexible on the field of view for many applications. However, their output is related to the moving scenes. They cannot provide the coordinates with reference to the world coordination system. Therefore, they are more suitable for post-experiment manual analysis rather than real-time automatic processing.</p><p>The non-wearable eye trackers that provide the coordinates suitable for automatic processing usually have a limited range of eye gaze tracking. For example, the tracking range of Tobii ×50 is ±35° in the horizontal direction and 40° in the vertical direction. Such a small tracking range is probably enough for the applications with a small field of view (FOV). It may not be wide enough for the applications with wide FOVs. To accommodate the applications with wide FOVs, a complex system of multiple cameras has to be deployed. For example, the Smart Eye Pro 5.0 system uses up to six cameras to adapt to natural head motion (translation and/or rotation). With the growing demand of high fidelity, more and more virtual environment facilities are built to have wide FOVs. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig1">1</a> shows a three-panel screen with a 135° FOV and a cylindrical screen with a 360° FOV for driving simulation (Mourant et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mourant RR, Ahmad N, Jaeger BK, Lin Y (2007) Optic flow and geometric field of view in a driving simulator display. Displays 28:145–149" href="/article/10.1007/s10055-010-0171-9#ref-CR14" id="ref-link-section-d51414e366">2007</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Process of head pose estimation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For the non-wearable eye trackers, it is difficult to track eye movements in the applications with wide FOVs. Due to the large head rotation in the wide FOV scenarios, the pupils may no longer be visible in the eye trackers’ view scope. In such situations, the single camera-based eye trackers are unable to continuously track eye gaze. It is possible to use multiple eye trackers to solve this problem but it could be costly for many applications.</p><h3 class="c-article__sub-heading" id="Sec3">Integrating eye gaze tracking with head pose tracking for visual attention measurement</h3><p>In this paper, a head pose tracking approach is proposed to extend the range of eye gaze tracking. In natural situations, eye and head movements are closely coupled, although not completely, and so head poses can, within limits, indicate the focus of visual fixation.</p><p>Many previous studies have shown that eye and head movements are coupled under certain conditions. Land (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Land MF (1992) Predictable eye-head coordination during driving. Nature 359(6393):318–320" href="/article/10.1007/s10055-010-0171-9#ref-CR10" id="ref-link-section-d51414e399">1992</a>) found that the pattern of eye and head movements is highly predictable when a participant is too busy to exert conscious control over head or eye movements. Einhauser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Einhauser W, Schumann F, Bardins S, Bartl K, Boning G, Schneider E, Konig P (2007) Human eye-head co-ordination in natural exploration. Netw Comput Neural Syst 18(3):267–297" href="/article/10.1007/s10055-010-0171-9#ref-CR5" id="ref-link-section-d51414e402">2007</a>) validated that a substantial fraction of eye movements is in the same direction of head movements in natural environment exploration. Eye–head coordination may dissociate in some circumstances, especially when future gaze shifts are expected or large gaze shifting happens (Oommen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Oommen BS, Smith RM, Stah JS (2004) The influence of future gaze orientation upon eye-head coupling during saccades. Exp Brain Res 155(1):9–18" href="/article/10.1007/s10055-010-0171-9#ref-CR15" id="ref-link-section-d51414e405">2004</a>). Particularly, Land observed that the eye–head discrepancy varied between 5° and 30° in very large horizontal gaze shifts (up to 100°) for drivers. Such discrepancies may reduce the final precision of integrated eye gaze tracking, although it may still be accurate enough for tracking areas of interest (AOI) rather than points of interest (POI).</p><p>Some previous studies have discussed how to estimate head poses with computer vision technologies. For example, Sherrah and Gong (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Sherrah J, Gong S (2001) Fusion of perceptual cues for robust tracking of head pose and position. Pattern Recognit 34(8):1565–1572" href="/article/10.1007/s10055-010-0171-9#ref-CR17" id="ref-link-section-d51414e411">2001</a>) estimated human head poses through second-order, similarity-based measures. They also used orientation-selective filtering for pre-processing facial images to improve computation speed. Abe and Haqiwara (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Abe Y, Hagiwara M (2001) Analysis and recognition of a human head’s movement from an image sequence. Syst and Comput 32(5):36–45" href="/article/10.1007/s10055-010-0171-9#ref-CR1" id="ref-link-section-d51414e414">2001</a>) estimated head poses from facial features, such as eye positions, facial region, and hair region. However, many facial features may no longer be observable for a front-mounted camera when head rotation is very large. The unreliable detection of such facial features significantly degrades the accuracy of head pose estimations. Kuratate et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kuratate T, Masuda S, Vatikiotis-Bateson E (2001) What perceptible information can be implemented in talking head animations? In: Proceedings of the IEEE international workshop on robot and human interactive communication (RO-MAN2001), Bordaux and Paris, France, pp 430–435" href="/article/10.1007/s10055-010-0171-9#ref-CR8" id="ref-link-section-d51414e417">2001</a>) designed a talking head animation system in which principal component analysis (PCA) was used to extract the deformation characteristics of the 3-D faces from multiple subjects. This study gave some hints on head image representation with PCA coefficients. Beverina et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Beverina F, Palmas G, Anisetti M, Bellandi V (2006) Tracking based face identification: a way to manage occlusions, and illumination, posture and expression changes. 2nd IET international conference on intelligent environments, IE, pp 161–166" href="/article/10.1007/s10055-010-0171-9#ref-CR3" id="ref-link-section-d51414e420">2006</a>) proposed using an eigen-face approach for face identification and then to reconstruct the frontal, neutral, and normalized images. This approach was focused on very limited poses and not applicable for the goal of visual performance measurement in wide field of views.</p><p>Computer vision–based computing approaches have advantages on showing the clear logic relationship between facial features and head poses. They are weak on detail features detection in the situations with large head rotation. Also, the general classification of head poses: up, down, left, and right, is not sufficient for visual performance measurement. Other practical problems, such as how to quickly setup the system and how to improve estimation accuracy, still remain unresolved.</p><h3 class="c-article__sub-heading" id="Sec4">Framework for head poses tracking</h3><p>In general, head pose can be described with three motions: yaw, pitch, and roll. Among them, yaw and pitch are directly related to the direction of sightline. Taking driving simulation as an example, the background of the test driver can be physically cleared out by putting up a whiteboard behind the driver. Simple background makes the head poses tracking relatively easy and robust.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig1">1</a> shows the process of head pose estimation. The offline switch is for calibration and neural network model training. The online switch is for real-time head pose tracking. The final result of sightline tracking comes from the spatial combination of head pose tracking and eye gaze tracking. Each individual participant requires an independent calibration.</p><p>Similar to the calibration process of general eye movement tracking devices, it is feasible to quickly create a calibration profile for the current user at the beginning of each experiment. The profile contains multiple head images that are captured at different head poses. Then, the head images are represented with the coefficients obtained from the PCA. Neural network models (multilayer perceptrons or MLP) are built to construct the relationship between the PCA coefficients and the corresponding head poses, which are highly close to eye-fixation poses. Multiple neural network models can be trained in advance and then perform parallel estimations in real time.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Methodology</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">Head pose calibration and head image capturing</h3><p>The head pose tracking system that collaborated with a non-wearable standalone eye tracker is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig2">2</a>. The eye tracker provided precise tracking results (fixations and saccades) in zone A (&lt;±35°), the shaded area in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig2">2</a>. The head tracker estimated eye fixations in zone B by estimating head poses. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig3">3</a> shows precise eye tracking in front view. The small window at the top simulates a back-view mirror in a virtual environment. The big window represents the precise eye tracking region covered by an eye tracker. In this window, saccades and fixations indicate a driver’s points of interest in real time.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Integrated head pose tracking (<i>Zone B</i>) and eye gaze tracking (<i>Zone A</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Precise eye gaze tracking in front view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig4">4</a> shows a calibration grid to cover the target tracking area. Each dot corresponding to an eye-fixation location associates with a head image. The calibration dots are not necessary to be equally spaced. It is preferable to have higher angular resolution on the side view than on the front view. The front view tracking was accomplished with the eye gaze tracker.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Calibration grid and current active dot</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The head images were converted to gray-scale intensity images. Due to the simple background of head images, the contour of head images was easier to be extracted with the commonly used snake algorithm. The details of this algorithm can be found in many studies (Sakalli et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sakalli M, Lam KM, Yan H (2006) A faster converging snake algorithm to locate object boundaries. IEEE Trans Image Process 15(5):1182–1191" href="/article/10.1007/s10055-010-0171-9#ref-CR16" id="ref-link-section-d51414e532">2006</a>; Lam and Tong <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lam SY, Tong CS (2002) Conformal snake algorithm for contour detection. Electron Lett 38(10):452–453" href="/article/10.1007/s10055-010-0171-9#ref-CR9" id="ref-link-section-d51414e535">2002</a>). Based on the extracted head image contour and the gray value of each pixel, the mass center of the head was calculated. Then, all head images were centered to their corresponding mass centers before cropping off to a proper size.</p><h3 class="c-article__sub-heading" id="Sec7">Representation of head images with PCA coefficients</h3><p>Each head image associated with a head pose that was represented with yaw angle and pitch angle. The roll movement of the head was ignored in this research. PCA is a good way to represent images with coefficient vectors. PCA is also easier to be implemented automatically than facial feature exaction, just as facial action coding system (FACS) (Ekman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ekman P, Friesen W, Hager J (2002) Facial action coding system. 2nd ed. salt lake city. ISBN 0-931835-01-1" href="/article/10.1007/s10055-010-0171-9#ref-CR6" id="ref-link-section-d51414e546">2002</a>) describing the movement of facial muscles.</p><p>The following introduces the method of PCA-based head image representation. For a detail introduction to PCA, refer to (Shlens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Shlens J (2005) A tutorial on principal component analysis. University of California. &#xA;                    http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-010-0171-9#ref-CR18" id="ref-link-section-d51414e552">2005</a>). PCA is an orthogonal decomposition. In PCA, a large number of partially correlated vectors are represented with a smaller number of uncorrelated vectors (principal components). It significantly reduces the amount of data to be processed through dimensional deduction.</p><p>Let each vector <i>x</i>
                  <sub>1</sub>, <i>x</i>
                  <sub>2</sub>, …, <i>x</i>
                  <sub>
                    <i>n</i>
                  </sub> denotes an individual head image. All head images are centered to their respective mass centers and tailored to the same size. Let <i>z</i>
                  <sub>1</sub>, <i>z</i>
                  <sub>2</sub>,…, <i>z</i>
                  <sub>
                    <i>n</i>
                  </sub> represents the corresponding head poses in yaw angle and pitch angle. The mean of matrix <i>X</i> is denoted by</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \mu_{X} = E\{ X\} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>X</i> = (<i>x</i>
                  <sub>1</sub>, <i>x</i>
                  <sub>2</sub>,…, <i>x</i>
                  <sub>
                    <i>n</i>
                  </sub>)<sup><i>T.</i></sup>
                </p><p>The covariance matrix of head images <i>x</i>
                  <sub>1</sub>, <i>x</i>
                  <sub>2</sub>,…, <i>x</i>
                  <sub>
                    <i>n</i>
                  </sub> is denoted by</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ C_{x} = E\{ \left( {X - \mu_{X} } \right)\left( {X - \mu_{X} } \right)^{T} \} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>The element of <i>C</i>
                  <sub>
                    <i>x</i>
                  </sub>, denoted by <i>C</i>
                  <sub>
                    <i>ij</i>
                  </sub>, represents the correlation between image <i>x</i>
                  <sub>
                    <i>i</i>
                  </sub> and <i>x</i>
                  <sub>
                    <i>j</i>
                  </sub>.</p><p>Let <i>U</i> be the matrix consisting of transposed eigenvectors <i>e</i>
                  <sub>
                    <i>i</i>
                  </sub> of the covariance matrix <i>C</i>
                  <sub>
                    <i>x</i>
                  </sub>. Then, <i>U</i> = (<i>e</i>
                  <sub>1</sub>, <i>e</i>
                  <sub>2</sub>,…, <i>e</i>
                  <sub>
                    <i>n</i>
                  </sub>)<sup><i>T</i></sup>, where <i>C</i>
                  <sub>
                    <i>x</i>
                  </sub>
                  <i>e</i>
                  <sub>
                    <i>i</i>
                  </sub> = <i>λ</i>
                  <sub>
                    <i>i</i>
                  </sub>
                  <i>e</i>
                  <sub>
                    <i>i</i>
                  </sub>, <i>i</i> = 1,…, <i>n</i>, and <i>λ</i>
                  <sub>
                    <i>i</i>
                  </sub> is the distinct eigenvalue.</p><p>
                  <i>U</i> can be understood as an orthogonal coordinate system. Then, <i>X</i> can be precisely represented by <i>U</i> with a coefficients matrix <i>P</i> = (<i>p</i>
                  <sub>1</sub>, <i>p</i>
                  <sub>2</sub>,…, <i>p</i>
                  <sub>
                    <i>n</i>
                  </sub>).</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P = U(X - \mu_{X} ) $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                  <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ X = U^{T} P + \mu_{X} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>The total number of head images acquired during calibration is usually small, e.g., &lt;50. When head images are too many to be processed in real time, the first <i>k</i> (<i>k</i> &lt; <i>n</i>) eigenvectors <i>e</i>
                  <sub>1</sub>, <i>e</i>
                  <sub>2</sub>,…, <i>e</i>
                  <sub>
                    <i>k</i>
                  </sub> can be used to construct a smaller <i>U</i>
                  <sub>
                    <i>k</i>
                  </sub> to reduce computation load. The last several eigenvectors can be ignored due to corresponding least variances</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ P_{k} = U_{k} \left( {X - \mu_{X} } \right) $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                  <div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ X = U_{k}^{\text{T}} P_{k} + \mu_{X}. $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                <p>The original images <i>x</i>
                  <sub>1</sub>, <i>x</i>
                  <sub>2</sub>,…, <i>x</i>
                  <sub>
                    <i>n</i>
                  </sub> in <i>X</i> can be compressed and represented by the coefficients <i>p</i>
                  <sub>1</sub>, <i>p</i>
                  <sub>2</sub>,…, <i>p</i>
                  <sub>
                    <i>n</i>
                  </sub> in <i>P</i>. Within the same coordinate system <i>U</i>, <i>P</i> still maintains the essential information to differentiate the head images at different eye-fixation poses.</p><h3 class="c-article__sub-heading" id="Sec8">Neural network modeling</h3><p>Let the coefficients <i>p</i>
                  <sub>1</sub>, <i>p</i>
                  <sub>2</sub>,…, <i>p</i>
                  <sub>
                    <i>n</i>
                  </sub> and the corresponding head poses <i>z</i>
                  <sub>1</sub>, <i>z</i>
                  <sub>2</sub>,…, <i>z</i>
                  <sub>
                    <i>n</i>
                  </sub> comprise a training set. Then, a multiple layer perceptor (MLP) can be built to construct the relationship between <i>p</i>
                  <sub>
                    <i>i</i>
                  </sub> and <i>z</i>
                  <sub>
                    <i>i</i>
                  </sub>, where <i>i</i> = 1,…, <i>n</i>, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Neural network modeling in MLP structure</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Previous research has pointed out that a MLP with biases, a sigmoid layer, and a linear output layer is capable of approximating any function with a finite number of discontinuities (Hornik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Hornik KM, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal approximators. Neural Netw 2(5):359–366" href="/article/10.1007/s10055-010-0171-9#ref-CR7" id="ref-link-section-d51414e1099">1989</a>). Hence, the transfer functions of the first layer and the second layer can be set to “tansig” and “purelin”, respectively. The performance measurement is mean squared error (MSE).</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ a^{1} = {\text{tansig}}\left( {W^{1} P + b^{1} } \right) $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                  <div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ a^{2} = {\text{purlin}}\left( {W^{2} a^{1} + b^{2} } \right) $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <i>W</i>
                  <sup>1</sup> and <i>W</i>
                  <sup>2</sup> are weights. <i>b</i>
                  <sup>1</sup> and <i>b</i>
                  <sup>2</sup> are biases.</p><p>With a well-trained MLP, the pose <i>z</i>
                  <sub>
                    <i>i</i>
                  </sub> can be estimated from the PCA coefficient <i>p</i>
                  <sub>
                    <i>i</i>
                  </sub> in real time. To enhance the robustness of the neural network model, multiple neural networks can be organized in parallel to emulate multiple experts cooperate together. The final output is the mean value of multiple estimations.</p><p>For this neural network model, the learning process works toward associative mapping rather than regularity detection. The head images with similar head poses have high similarities. These similarities are extracted with the PCA method and represented with PCA coefficients. Therefore, the output of the MLP is a similarity-based interpolation of head poses.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Experiments</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10">Apparatus</h3><p>The experiment was conducted in a driving simulator located in Northeastern University. A high-fidelity steering wheel, an accelerator pedal, and a brake pedal were connected to a desktop computer. The computer rendered the virtual scenarios according to the inputs from these devices. The curved screen had about 135° FOV. The driver seat in the simulator was adjustable. The head poses (or corresponding eye-fixation locations) were calculated based on the geometrical relationship between the drivers’ head positions and the calibration dots on the screen.</p><h3 class="c-article__sub-heading" id="Sec11">Head pose calibration and testing</h3><p>In the experimental system, the calibration grid was comprised of 42 dots in 6 rows and 7 columns. During the calibration process, the dots in the calibration grid were randomly permutated and then projected to the screen one by one. The active dots attracted the participant’s eye fixations by dynamically shrinking and enlarging for 3 s. Inactive dots were nearly invisible to avoid distraction to the participants. The randomized sequence and attractive change were helpful to avoid any large discrepancy between eye gaze and head rotation that could arise from the participant’s expectation of future gaze shifts (Oommen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Oommen BS, Smith RM, Stah JS (2004) The influence of future gaze orientation upon eye-head coupling during saccades. Exp Brain Res 155(1):9–18" href="/article/10.1007/s10055-010-0171-9#ref-CR15" id="ref-link-section-d51414e1192">2004</a>).</p><p>The participants were required to look at the calibration dots naturally. The front-mounted mini-camera was in charge of head image capturing. The head poses (yaw angle and pitch angle) were calculated based on the geometrical relationship, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig6">6</a>. The head rotation on the horizontal direction was from −63.5 to 63.5°. The head rotation on the vertical direction was from −16.3 to 45°.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Head pose (yaw, pitch) calculation based on calibration grid</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec12">Head images processing</h3><p>Clean white background made the edge of head images very sharp. The head images were easily cropped with the snake algorithm by detecting the deformable contour. The final images were in the size of 200 × 180 pixels.</p><p>The 42 head images were put together for Principal Component Analysis. The mean image and some representative principal components (PC) were shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig7">7</a>. Then, all original head images and new head images were represented with a linear combination of the mean image and 42 weighted principal components. The weights were the corresponding coefficients calculated with Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0171-9#Equ3">3</a>. According to the theory of PCA, the first principal component is corresponding to the greatest variance. The last principal component is corresponding to the least variance. The least variance indicates little change. Therefore, the last several principal components of head images, e.g., the 42th principal component in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig8">8</a>, were usually blank and contained little information. They were ignored to reduce the computation load, based on (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0171-9#Equ5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mean image and representative principal components (PC) of head images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Training results of MLP neural network</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec13">Neural network training</h3><p>In the multilayer perceptron (MLP) model, the input layer took the PCA coefficients as input. Since the calibration grid contained 42 positions, the maximum number of input neurons was 42. There were 2 neurons in the output layer to indicate the head pose (yaw angle and pitch angle). The number of neurons in the hidden layer was determined based on the balance between computation speed and training precision. Current experiment chose 100 neurons for testing. This number slightly doubled the number of input neurons.</p><p>The training target of mean square error (MSE) was set to 1.0, which was corresponding to about 1° angular error. For 30 training processes (corresponding to 30 neural networks) with randomly partitioned training sets, the average number of epochs was 147 and the standard deviation was 28. The computation platform was a Dell Optiplex Desktop 620 with an Intel Pentium 4 2.8 GHz CPU and 2 GB SDRAM. The average training time was 3.9 s., and the standard deviation was 0.7 s.</p><p>The training results (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig8">8</a>) well matched most targets in the calibration grid. Large errors of yaw angle and pitch angle were usually observed at the four corners of the calibration grid. When the number of hidden neurons and the training target of MSE were raised, the errors could be reduced. However, it was found that the neuron networks were constrained by the input patterns and did not show better performance on generalization. Using a well-trained neural network, the average calculation time in real-time head pose tracking was less than 0.1 s.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Results and discussion</h2><div class="c-article-section__content" id="Sec14-content"><h3 class="c-article__sub-heading" id="Sec15">Results of head pose tracking</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig9">9</a> shows 30 trials of single MLP-based estimation on one test participant. These 30 test dots were randomly plotted on the screen one by one. They were deliberately distanced from the calibration grid. In addition, most test dots were concentrated in the side areas that were not well covered by the Tobii eye tracker. The original point was also tested. The estimated yaw angles and pitch angles were generally close to the actual yaw angles and pitch angles. However, the distances between them were still easily observable. The arrows indicated the pairing relationship between the estimated pose and the actual pose. The head poses corresponding to the front view had much better accuracy than the head poses toward the side view.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Test results of head pose tracking</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To examine the tracking performance on the side view, especially on the areas outside of the eye tracker’s optimal tracking range, another experiment (with three test participants) was organized and the corresponding statistical analysis was conducted. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig10">10</a> shows the error distributions at four corners: (−55°, −10°), (−55°, +40°), (55°, −10°), and (55°, 40°). In the total 100 trials of randomized head poses tracking, each corner was tested 15 times. Other random locations were used to interfere with the test participant’s gaze shift expectation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Error distribution of head pose estimation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The means and standard deviations of the yaw angle error and the pitch angle error were plotted in pairs. For the four corner-oriented trackings, the average absolute error of the yaw angle was 2.1° and the average absolute error of the pitch angle was 5.9°. The standard deviations of both the yaw angle error and the pitch angle error were very close, 9.7° vs. 11.1°. Since the front middle view was covered by the eye tracker, the errors related to the four corners indicated the potential maximum errors related to head pose tracking on the side view.</p><h3 class="c-article__sub-heading" id="Sec16">Discussions</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Why PCA coefficients are useful to recognize head pose in wide FOV?</h4><p>Head images with similar head poses have high similarities. When the head images are represented with PCA coefficients, there is some correlation between their PCA coefficients. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig11">11</a> shows this correlation disclosed by a third experiment. The training bars (yellow/gray) and the testing bars (blue/dark) were alternatively placed with equal space (30 cm). The head poses of images 1–13 were close to the head poses of images 14–26, respectively, i.e., 1 vs. 14, 2 vs. 15, 3 vs. 16,…, 13 vs. 26. The correlation coefficients between 1 and 14, 2 and 15,…, 13 and 26, circled in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig11">11</a> were usually above 0.8, which was significantly larger than their neighbors. The peaks appeared at the diagonal direction were the self-correlation coefficients, which were always equal to 1.0. The high correlation meant that the PCA coefficients of the head images with similar head poses were tightly correlated. Compared with facial features extraction, such as the recognition of eye, eyebrow, and nose, the extraction of PCA coefficients is more robust in situations with large head rotation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0171-9/MediaObjects/10055_2010_171_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Correlation between the PCA coefficients at different head poses</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0171-9/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">What does the statistical information of estimation error indicate?</h4><p>According to the second experiment, the mean errors of the yaw angles were slightly smaller than those of the pitch angles but they were not significant at the confidence level of 95%. The standard deviations of the yaw errors and the pitch errors did not show significant difference. An interesting phenomenon was that all tracking results were center-toward. It indicated that eye gaze shift still existed to compensate the insufficient movement of head rotation. Although the average absolute errors of yaw angle and pitch angle were relatively small, they should not be overemphasized to indicate the real tracking performance. The standard deviations were more meaningful in representing the accuracy of head pose estimation. The relatively large standard deviations indicate that head pose tracking is more suitable for tracking areas of interest than for tracking points of interest.</p><p>In terms of the errors of head pose estimation, three factors made significant impact on it. One was because of the uncertainty of neural networks. The estimation results of neural networks were affected by their training sets and the target errors of training. The neural networks were not trained well at the corners. Multiple optimized neural networks may work in parallel to achieve better results. Another error source was the inconsistency between the head pose and the line of eyesight because of dissociated eye–head coordination. The third reason was due to the calibration grid not being dense enough. For the 7 × 6 calibration grid in the horizontal 127° span, the angular space was more than 20° between adjacent dots. A calibration grid with more rows and columns in the target tracking areas may help to reduce the mean estimation errors and the standard deviations but it may require longer calibration time. A differential system with two cameras may improve the accuracy of head pose tracking.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Limitations</h4><p>The current approach did not address what exact patterns were searched by the Principal Component Analysis and the neural networks. Overall, the Principal Component Analysis and the neural networks tried to recognize the similarities between head images corresponding to different head poses. Such similarities may include the layout of the facial organs, the contour of the head hair. In general, the similarity between adjacent head poses shows very strong correlation, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0171-9#Fig11">11</a>. Since the similarity is not based on detail features, such as mouth corner, eye brows, and nose, this approach is more robust than detail features-based approaches.</p><p>The current approach is useful in tracking the areas of interest rather than the points of interest. Due to the limited number of calibration grids, head poses–based tracking cannot produce tracking results with very high precision. That is why head pose tracking should be integrated with eye gaze tracking. Eye gaze tracking provides very precision results (points of interest) on the front view, while head pose tracking provides relatively coarse results (areas of interest) on the side view.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusion and future work</h2><div class="c-article-section__content" id="Sec20-content"><p>This paper proposed a new approach of extending eye tracking capabilities for visual performance measurement in wide FOV displays. It combines broad range head pose tracking and narrow range eye tracking together. The approach of head pose tracking includes PCA and neural network modeling. With a 7 × 6 calibration grid in a span of 127° × 61° FOV, the standard deviations of yaw angle errors and pitch angle errors at four corners were about 8.7°–11.7° for corners tracking. The average absolute errors of multiple estimations could be as low as 2.1° for yaw angle and 5.9° for pitch angle.</p><p>The current head pose tracking approach can be integrated with non-intrusive standalone eye tracking systems, which monitor eye movement in contactless ways, such as Tobii and SeeingMachine eye trackers. The proposed approach is not supposed to work with head-mounted eye tracking systems because they do not offer the coordinates with reference to the world coordination system. Since the tracking system is calibrated for each user before experiments, the tracking error caused by individual difference is not a concern. In terms of potential applications, it can be used for driving simulators with wide field of view that are becoming more and more affordable today. It can be also used to monitor the learners for safety learning in virtual environment to avoid risks and unwanted consequences (Amokrane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Amokrane K, Lourdeaux D, Burkhardt J (2008) HERA: learner tracking in a virtual environment. Int J Virtual Real 7(3):23–30 Sept" href="/article/10.1007/s10055-010-0171-9#ref-CR2" id="ref-link-section-d51414e1423">2008</a>). This approach is non-intrusive and easy to implement in virtual environments for wide FOV simulators to measure operators’ visual attention.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Abe, M. Hagiwara, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Abe Y, Hagiwara M (2001) Analysis and recognition of a human head’s movement from an image sequence. Syst and " /><p class="c-article-references__text" id="ref-CR1">Abe Y, Hagiwara M (2001) Analysis and recognition of a human head’s movement from an image sequence. Syst and Comput 32(5):36–45</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fscj.1024" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20and%20recognition%20of%20a%20human%20head%E2%80%99s%20movement%20from%20an%20image%20sequence&amp;journal=Syst%20and%20Comput&amp;volume=32&amp;issue=5&amp;pages=36-45&amp;publication_year=2001&amp;author=Abe%2CY&amp;author=Hagiwara%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Amokrane, D. Lourdeaux, J. Burkhardt, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Amokrane K, Lourdeaux D, Burkhardt J (2008) HERA: learner tracking in a virtual environment. Int J Virtual Rea" /><p class="c-article-references__text" id="ref-CR2">Amokrane K, Lourdeaux D, Burkhardt J (2008) HERA: learner tracking in a virtual environment. Int J Virtual Real 7(3):23–30 Sept</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=HERA%3A%20learner%20tracking%20in%20a%20virtual%20environment&amp;journal=Int%20J%20Virtual%20Real&amp;volume=7&amp;issue=3&amp;pages=23-30&amp;publication_year=2008&amp;author=Amokrane%2CK&amp;author=Lourdeaux%2CD&amp;author=Burkhardt%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Beverina F, Palmas G, Anisetti M, Bellandi V (2006) Tracking based face identification: a way to manage occlus" /><p class="c-article-references__text" id="ref-CR3">Beverina F, Palmas G, Anisetti M, Bellandi V (2006) Tracking based face identification: a way to manage occlusions, and illumination, posture and expression changes. 2nd IET international conference on intelligent environments, IE, pp 161–166</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cai H, Lin Y, and Mourant RR (2007) Evaluation of driver visual behavior and road signs in virtual environment" /><p class="c-article-references__text" id="ref-CR4">Cai H, Lin Y, and Mourant RR (2007) Evaluation of driver visual behavior and road signs in virtual environment. In: Proceeding of HFES 51st annual meeting, Baltimore, vol 5. USA, pp 1645–1649</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Einhauser, F. Schumann, S. Bardins, K. Bartl, G. Boning, E. Schneider, P. Konig, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Einhauser W, Schumann F, Bardins S, Bartl K, Boning G, Schneider E, Konig P (2007) Human eye-head co-ordinatio" /><p class="c-article-references__text" id="ref-CR5">Einhauser W, Schumann F, Bardins S, Bartl K, Boning G, Schneider E, Konig P (2007) Human eye-head co-ordination in natural exploration. Netw Comput Neural Syst 18(3):267–297</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F09548980701671094" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20eye-head%20co-ordination%20in%20natural%20exploration&amp;journal=Netw%20Comput%20Neural%20Syst&amp;volume=18&amp;issue=3&amp;pages=267-297&amp;publication_year=2007&amp;author=Einhauser%2CW&amp;author=Schumann%2CF&amp;author=Bardins%2CS&amp;author=Bartl%2CK&amp;author=Boning%2CG&amp;author=Schneider%2CE&amp;author=Konig%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Friesen W, Hager J (2002) Facial action coding system. 2nd ed. salt lake city. ISBN 0-931835-01-1" /><p class="c-article-references__text" id="ref-CR6">Ekman P, Friesen W, Hager J (2002) Facial action coding system. 2nd ed. salt lake city. ISBN 0-931835-01-1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Hornik, M. Stinchcombe, H. White, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Hornik KM, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal approximators. Neural N" /><p class="c-article-references__text" id="ref-CR7">Hornik KM, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal approximators. Neural Netw 2(5):359–366</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0893-6080%2889%2990020-8" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multilayer%20feedforward%20networks%20are%20universal%20approximators&amp;journal=Neural%20Netw&amp;volume=2&amp;issue=5&amp;pages=359-366&amp;publication_year=1989&amp;author=Hornik%2CKM&amp;author=Stinchcombe%2CM&amp;author=White%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kuratate T, Masuda S, Vatikiotis-Bateson E (2001) What perceptible information can be implemented in talking h" /><p class="c-article-references__text" id="ref-CR8">Kuratate T, Masuda S, Vatikiotis-Bateson E (2001) What perceptible information can be implemented in talking head animations? In: Proceedings of the IEEE international workshop on robot and human interactive communication (RO-MAN2001), Bordaux and Paris, France, pp 430–435</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SY. Lam, CS. Tong, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lam SY, Tong CS (2002) Conformal snake algorithm for contour detection. Electron Lett 38(10):452–453" /><p class="c-article-references__text" id="ref-CR9">Lam SY, Tong CS (2002) Conformal snake algorithm for contour detection. Electron Lett 38(10):452–453</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1049%2Fel%3A20020335" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Conformal%20snake%20algorithm%20for%20contour%20detection&amp;journal=Electron%20Lett&amp;volume=38&amp;issue=10&amp;pages=452-453&amp;publication_year=2002&amp;author=Lam%2CSY&amp;author=Tong%2CCS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MF. Land, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Land MF (1992) Predictable eye-head coordination during driving. Nature 359(6393):318–320" /><p class="c-article-references__text" id="ref-CR10">Land MF (1992) Predictable eye-head coordination during driving. Nature 359(6393):318–320</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F359318a0" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Predictable%20eye-head%20coordination%20during%20driving&amp;journal=Nature&amp;volume=359&amp;issue=6393&amp;pages=318-320&amp;publication_year=1992&amp;author=Land%2CMF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YC. Lee, JD. Lee, L. Boyle, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Lee YC, Lee JD, Boyle L (2007) Visual attention in driving: the effects of cognitive load and visual disruptio" /><p class="c-article-references__text" id="ref-CR11">Lee YC, Lee JD, Boyle L (2007) Visual attention in driving: the effects of cognitive load and visual disruption. Hum Factors 49(4):721–733</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872007X215791" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20attention%20in%20driving%3A%20the%20effects%20of%20cognitive%20load%20and%20visual%20disruption&amp;journal=Hum%20Factors&amp;volume=49&amp;issue=4&amp;pages=721-733&amp;publication_year=2007&amp;author=Lee%2CYC&amp;author=Lee%2CJD&amp;author=Boyle%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Lin, WJ. Zhang, LG. Watson, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Lin Y, Zhang WJ, Watson LG (2003) Using eye movement parameters for evaluating human–machine interface framewo" /><p class="c-article-references__text" id="ref-CR12">Lin Y, Zhang WJ, Watson LG (2003) Using eye movement parameters for evaluating human–machine interface frameworks under normal control operation and fault detection situation. Int J Hum-Comput Stud 59(6):837–873</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1071-5819%2803%2900122-8" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20eye%20movement%20parameters%20for%20evaluating%20human%E2%80%93machine%20interface%20frameworks%20under%20normal%20control%20operation%20and%20fault%20detection%20situation&amp;journal=Int%20J%20Hum-Comput%20Stud&amp;volume=59&amp;issue=6&amp;pages=837-873&amp;publication_year=2003&amp;author=Lin%2CY&amp;author=Zhang%2CWJ&amp;author=Watson%2CLG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Ma, Y. Yu, Y. Zhang, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Ma L, Yu Y, Zhang Y (2000) Realization of the virtual driving system rules based on the improved petri-net. In" /><p class="c-article-references__text" id="ref-CR13">Ma L, Yu Y, Zhang Y (2000) Realization of the virtual driving system rules based on the improved petri-net. Int J Virtual Real 4(4):18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Realization%20of%20the%20virtual%20driving%20system%20rules%20based%20on%20the%20improved%20petri-net&amp;journal=Int%20J%20Virtual%20Real&amp;volume=4&amp;issue=4&amp;publication_year=2000&amp;author=Ma%2CL&amp;author=Yu%2CY&amp;author=Zhang%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RR. Mourant, N. Ahmad, BK. Jaeger, Y. Lin, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Mourant RR, Ahmad N, Jaeger BK, Lin Y (2007) Optic flow and geometric field of view in a driving simulator dis" /><p class="c-article-references__text" id="ref-CR14">Mourant RR, Ahmad N, Jaeger BK, Lin Y (2007) Optic flow and geometric field of view in a driving simulator display. Displays 28:145–149</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.displa.2007.04.011" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optic%20flow%20and%20geometric%20field%20of%20view%20in%20a%20driving%20simulator%20display&amp;journal=Displays&amp;volume=28&amp;pages=145-149&amp;publication_year=2007&amp;author=Mourant%2CRR&amp;author=Ahmad%2CN&amp;author=Jaeger%2CBK&amp;author=Lin%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BS. Oommen, RM. Smith, JS. Stah, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Oommen BS, Smith RM, Stah JS (2004) The influence of future gaze orientation upon eye-head coupling during sac" /><p class="c-article-references__text" id="ref-CR15">Oommen BS, Smith RM, Stah JS (2004) The influence of future gaze orientation upon eye-head coupling during saccades. Exp Brain Res 155(1):9–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00221-003-1694-z" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20influence%20of%20future%20gaze%20orientation%20upon%20eye-head%20coupling%20during%20saccades&amp;journal=Exp%20Brain%20Res&amp;volume=155&amp;issue=1&amp;pages=9-18&amp;publication_year=2004&amp;author=Oommen%2CBS&amp;author=Smith%2CRM&amp;author=Stah%2CJS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Sakalli, KM. Lam, H. Yan, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Sakalli M, Lam KM, Yan H (2006) A faster converging snake algorithm to locate object boundaries. IEEE Trans Im" /><p class="c-article-references__text" id="ref-CR16">Sakalli M, Lam KM, Yan H (2006) A faster converging snake algorithm to locate object boundaries. IEEE Trans Image Process 15(5):1182–1191</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIP.2006.871401" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20faster%20converging%20snake%20algorithm%20to%20locate%20object%20boundaries&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=15&amp;issue=5&amp;pages=1182-1191&amp;publication_year=2006&amp;author=Sakalli%2CM&amp;author=Lam%2CKM&amp;author=Yan%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Sherrah, S. Gong, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Sherrah J, Gong S (2001) Fusion of perceptual cues for robust tracking of head pose and position. Pattern Reco" /><p class="c-article-references__text" id="ref-CR17">Sherrah J, Gong S (2001) Fusion of perceptual cues for robust tracking of head pose and position. Pattern Recognit 34(8):1565–1572</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0984.68707" aria-label="View reference 17 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2800%2900091-1" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fusion%20of%20perceptual%20cues%20for%20robust%20tracking%20of%20head%20pose%20and%20position&amp;journal=Pattern%20Recognit&amp;volume=34&amp;issue=8&amp;pages=1565-1572&amp;publication_year=2001&amp;author=Sherrah%2CJ&amp;author=Gong%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shlens J (2005) A tutorial on principal component analysis. University of California. http://www.snl.salk.edu/" /><p class="c-article-references__text" id="ref-CR18">Shlens J (2005) A tutorial on principal component analysis. University of California. <a href="http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf">http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Xu, B. Xu, H. Ding, W. Qian, D. Zhang, W. Wang, F. Ge, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Xu F, Xu B, Ding H, Qian W, Zhang D, Wang W, Ge F (2000) Virtual automobile driver training simulator. Int J V" /><p class="c-article-references__text" id="ref-CR19">Xu F, Xu B, Ding H, Qian W, Zhang D, Wang W, Ge F (2000) Virtual automobile driver training simulator. Int J Virtual Real 4(4):11</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20automobile%20driver%20training%20simulator&amp;journal=Int%20J%20Virtual%20Real&amp;volume=4&amp;issue=4&amp;publication_year=2000&amp;author=Xu%2CF&amp;author=Xu%2CB&amp;author=Ding%2CH&amp;author=Qian%2CW&amp;author=Zhang%2CD&amp;author=Wang%2CW&amp;author=Ge%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Zhang, M. Smith, GJ. Witt, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Zhang H, Smith M, Witt GJ (2006) Identification of real-time diagnostic measures of visual distraction with an" /><p class="c-article-references__text" id="ref-CR20">Zhang H, Smith M, Witt GJ (2006) Identification of real-time diagnostic measures of visual distraction with an automatic eye-tracking system. Hum Factors 48(4):805–821</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872006779166307" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Identification%20of%20real-time%20diagnostic%20measures%20of%20visual%20distraction%20with%20an%20automatic%20eye-tracking%20system&amp;journal=Hum%20Factors&amp;volume=48&amp;issue=4&amp;pages=805-821&amp;publication_year=2006&amp;author=Zhang%2CH&amp;author=Smith%2CM&amp;author=Witt%2CGJ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0171-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The research has been supported by the National Science Foundation (NSF) through a research grant awarded to the corresponding author (Grant # 0954579).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, 02115, USA</p><p class="c-article-author-affiliation__authors-list">Hua Cai &amp; Yingzi Lin</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Hua-Cai"><span class="c-article-authors-search__title u-h3 js-search-name">Hua Cai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hua+Cai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hua+Cai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hua+Cai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Yingzi-Lin"><span class="c-article-authors-search__title u-h3 js-search-name">Yingzi Lin</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yingzi+Lin&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yingzi+Lin" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yingzi+Lin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0171-9/email/correspondent/c1/new">Yingzi Lin</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20integrated%20head%20pose%20and%20eye%20gaze%20tracking%20approach%20to%20non-intrusive%20visual%20attention%20measurement%20for%20wide%20FOV%20simulators&amp;author=Hua%20Cai%20et%20al&amp;contentID=10.1007%2Fs10055-010-0171-9&amp;publication=1359-4338&amp;publicationDate=2010-09-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Cai, H., Lin, Y. An integrated head pose and eye gaze tracking approach to non-intrusive visual attention measurement for wide FOV simulators.
                    <i>Virtual Reality</i> <b>16, </b>25–32 (2012). https://doi.org/10.1007/s10055-010-0171-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0171-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-03-04">04 March 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-08-20">20 August 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-09-03">03 September 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-03">March 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0171-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0171-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Eye gaze tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Head pose tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multilayer perceptron (MLP)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual attention</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0171-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=171;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

