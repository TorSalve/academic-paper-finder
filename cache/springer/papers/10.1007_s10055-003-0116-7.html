<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Mediating the expression of emotion in educational collaborative virtu"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The use of avatars with emotionally expressive faces is potentially highly beneficial to communication in collaborative virtual environments (CVEs), especially when used in a distance learning..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/7/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Mediating the expression of emotion in educational collaborative virtual environments: an experimental study"/>

    <meta name="dc.source" content="Virtual Reality 2004 7:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-02-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The use of avatars with emotionally expressive faces is potentially highly beneficial to communication in collaborative virtual environments (CVEs), especially when used in a distance learning context. However, little is known about how, or indeed whether, emotions can effectively be transmitted through the medium of a CVE. Given this, an avatar head model with limited but human-like expressive abilities was built, designed to enrich CVE communication. Based on the facial action coding system (FACS), the head was designed to express, in a readily recognisable manner, the six universal emotions. An experiment was conducted to investigate the efficacy of the model. Results indicate that the approach of applying the FACS model to virtual face representations is not guaranteed to work for all expressions of a particular emotion category. However, given appropriate use of the model, emotions can effectively be visualised with a limited number of facial features. A set of exemplar facial expressions is presented."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-02-05"/>

    <meta name="prism.volume" content="7"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="66"/>

    <meta name="prism.endingPage" content="81"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-003-0116-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-003-0116-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-003-0116-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-003-0116-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Mediating the expression of emotion in educational collaborative virtual environments: an experimental study"/>

    <meta name="citation_volume" content="7"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2004/04"/>

    <meta name="citation_online_date" content="2004/02/05"/>

    <meta name="citation_firstpage" content="66"/>

    <meta name="citation_lastpage" content="81"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-003-0116-7"/>

    <meta name="DOI" content="10.1007/s10055-003-0116-7"/>

    <meta name="citation_doi" content="10.1007/s10055-003-0116-7"/>

    <meta name="description" content="The use of avatars with emotionally expressive faces is potentially highly beneficial to communication in collaborative virtual environments (CVEs), especi"/>

    <meta name="dc.creator" content="Marc Fabri"/>

    <meta name="dc.creator" content="David Moore"/>

    <meta name="dc.creator" content="Dave Hobbs"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Thalmann D (2001) The role of virtual humans in virtual environment technology and interfaces. In: Earnshaw R, Guedj R and Vince J (eds) Frontiers of human-centred computing, online communities and virtual environments. Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Fleming B, Dobbs D (1999) Animating facial features and expressions. Charles River Media, Boston, MA"/>

    <meta name="citation_reference" content="Dumas C, Saugis G, Chaillou C, Degrande S and Viaud M (1998) A 3-D interface for cooperative work. In: Proceedings of the Conference on Collaborative Virtual Environments, Manchester, UK, 17&#8211;19 June 1998"/>

    <meta name="citation_reference" content="Manninen T, Kujanp&#228;&#228; T (2002) Non-verbal communication forms in multi-player game sessions. In: Faulkner X, Finlay J, D&#233;tienne F (eds) People and computers XVI&#8212;memorable yet invisible. BCS Press, London, UK"/>

    <meta name="citation_reference" content="Atkins H, Moore D, Hobbs D and Sharpe S (2001) Learning style theory and computer mediated communication. In: Proceedings of ED-Media, Tampere, Finland, 25&#8211;30 June 2001"/>

    <meta name="citation_reference" content="Laurillard D (1993) Rethinking university teaching. Routledge, London, UK"/>

    <meta name="citation_reference" content="Moore M (1993) Three types of interaction. In: Harry K, John M and Keegan D (eds) Distance education: new perspectives. Routledge, London, UK"/>

    <meta name="citation_reference" content="Johnson D, Johnson R (1994) Cooperative learning in the culturally diverse classroom. In: DeVillar, Faltis, Cummins (eds) Cultural diversity in schools. State University of New York Press, Albany, NY"/>

    <meta name="citation_reference" content="citation_journal_title=Educ Psychol; citation_author=null Webb; citation_volume=87; citation_publication_date=1995; citation_pages=406; citation_doi=10.1037//0022-0663.87.3.406; citation_id=CR9"/>

    <meta name="citation_reference" content="Wu A, Farrell R and Singley M (2002) Scaffolding group learning in a collaborative networked environment. In: Proceedings of CSCL 2002, Boulder, CO, 7&#8211;11 January 2002"/>

    <meta name="citation_reference" content="Lisetti C, Douglas M and LeRouge C (2002) Intelligent affective interfaces: a user-modeling approach for telemedicine. In: Proceedings of the International Conference on Universal Access in HCI, New Orleans, LA, 5&#8211;10 August 2002"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum-Comp Stud; citation_title=Some advantages of video conferencing over high-quality audio conferencing: fluency and awareness of attentional focus; citation_author=O Daly-Jones, A Monk, L Watts; citation_volume=49; citation_issue=1; citation_publication_date=1998; citation_pages=21-58; citation_id=CR12"/>

    <meta name="citation_reference" content="McShea J, Jennings S and McShea H (1997) Characterising user control of video conferencing in distance education. In: Proceedings of CAL-97, Exeter, UK, 25&#8211;26 March 1997"/>

    <meta name="citation_reference" content="Fabri M, Gerhard M (2000) The virtual student: user embodiment in virtual learning environments. In: Orange G, Hobbs D (eds) International perspectives on tele-education and virtual learning environments, Ashgate, Aldershot, UK"/>

    <meta name="citation_reference" content="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY"/>

    <meta name="citation_reference" content="Morris D, Collett P, Marsh P and O&#8217;Shaughnessy M (1979) Gestures, their origin and distribution. Jonathan Cape, London, UK"/>

    <meta name="citation_reference" content="Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY"/>

    <meta name="citation_reference" content="Strongman K (1996) The psychology of emotion (fourth edition). Wiley, New York"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_author=null Dittrich; citation_volume=25; citation_publication_date=1996; citation_pages=727; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Soc Psychol; citation_author=null Keltner; citation_volume=68; citation_publication_date=1995; citation_pages=441; citation_doi=10.1037//0022-3514.68.3.441; citation_id=CR20"/>

    <meta name="citation_reference" content="Picard R (1997) Affective computing. MIT Press, Cambridge, MA"/>

    <meta name="citation_reference" content="citation_journal_title=Prag Cognit; citation_author=null Lisetti; citation_volume=8; citation_publication_date=2000; citation_pages=185; citation_id=CR22"/>

    <meta name="citation_reference" content="Dam&#225;sio A (1994) Descarte&#8217;s error: emotion, reason and the human brain. Avon, New York"/>

    <meta name="citation_reference" content="Cooper B, Brna P and Martins A (2000) Effective affective in intelligent systems&#8212;building on evidence of empathy in teaching and learning. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces. Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Johnson W (1998) Pedagogical agents. In: Computers in education proceedings, Beijing, China"/>

    <meta name="citation_reference" content="McGrath A, Prinz W (2001) All that Is solid melts into software. In: Churchill, Snowdon, Munro (eds) Collaborative virtual environments&#8212;digital places and spaces for interaction, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Durlach N, Slater M (2002) Meeting people virtually: experiments in shared virtual environments. In: Schroeder R (ed) The social life of avatars, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ"/>

    <meta name="citation_reference" content="New Oxford Dictionary of English. Oxford University Press, Oxford, UK"/>

    <meta name="citation_reference" content="Russell J, F&#233;rnandez-Dols J (1997) The psychology of facial expression. Cambridge University Press, Cambridge, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Psycholphys; citation_author=null Surakka; citation_volume=29; citation_publication_date=1998; citation_pages=23; citation_doi=10.1016/S0167-8760(97)00088-3; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Reading; citation_author=null Zebrowitz; citation_volume=faces; citation_publication_date=1997; citation_pages=window; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Emotion in the human; citation_author=null Ekman; citation_volume=face; citation_publication_date=1972; citation_pages=guidelines; citation_id=CR33"/>

    <meta name="citation_reference" content="Ekman P (1999) Facial expressions. In: Dalgleish T, Power M (eds) Handbook of cognition and emotion. Wiley, New York"/>

    <meta name="citation_reference" content="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA"/>

    <meta name="citation_reference" content="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, San Francisco, CA"/>

    <meta name="citation_reference" content="Bartlett M (1998) Face image analysis by unsupervised learning and redundancy reduction. Dissertation, University of California"/>

    <meta name="citation_reference" content="citation_journal_title=Cog Sci; citation_author=null P&#233;lachaud; citation_volume=20; citation_publication_date=1996; citation_pages=1; citation_doi=10.1016/S0364-0213(99)80001-9; citation_id=CR38"/>

    <meta name="citation_reference" content="Ekman P, Rosenzweig L (eds) What the face reveals: basic and applied studies of spontaneous expression using the facial action coding system. Oxford University Press, Oxford, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Patt Anal Mach Intell; citation_author=null Terzopoulos; citation_volume=15; citation_publication_date=1993; citation_pages=569; citation_doi=10.1109/34.216726; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGGRAPH; citation_author=null Platt; citation_volume=15; citation_publication_date=1981; citation_pages=245; citation_id=CR41"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comp Graph Appl; citation_author=null Parke; citation_volume=2; citation_publication_date=1982; citation_pages=61; citation_id=CR42"/>

    <meta name="citation_reference" content="Benford S, Bowers J, Fahl&#233;n L, Greenhalgh C and Snowdon D (1995) User embodiment in collaborative virtual environments. In: Proceedings of CHI 1995 Proceedings, Denver, CO, 7&#8211;11 May 1995"/>

    <meta name="citation_reference" content="Mori M (1982) The buddha in the robot. Tuttle, Boston, MA"/>

    <meta name="citation_reference" content="Reichardt J (1978) Robots: fact, fiction and prediction. Thames &amp; Hudson, London, UK"/>

    <meta name="citation_reference" content="Hindmarsh J, Fraser M, Heath C and Benford S (2001) Virtually missing the point: configuring CVEs for object-focused interaction. In: Churchill E, Snowdon D and Munro A (eds) Collaborative virtual environments: digital places and spaces for interaction, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Godenschweger F, Strothotte T and Wagener H (1997) Rendering gestures as line drawings. In: Proceedings of the Gesture Workshop 1997, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Donath J (2001) Mediated faces. In: Beynon M, Nehaniv C and Dautenhahn K (eds) Cognitive technology: instruments of mind, Warwick, UK"/>

    <meta name="citation_reference" content="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March&#8211;5 April 2001"/>

    <meta name="citation_reference" content="citation_journal_title=Bullet Brit Psychol Soc; citation_author=null Ellis; citation_volume=3; citation_publication_date=1990; citation_pages=114; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Beit; citation_title=Facial motion and the recognition of emotions; citation_author=W Dittrich; citation_volume=33; citation_issue=3/4; citation_publication_date=1991; citation_pages=366-377; citation_id=CR51"/>

    <meta name="citation_reference" content="H-Anim Working Group. Specification for a standard VRML humanoid. http://www.h-anim.org."/>

    <meta name="citation_reference" content="Yacoob Y, Davis L (1994) Computing spatio-temporal representations of human faces. In: Proceedings of the Computer Vision and Pattern Recognition Conference, Seattle, WA, June 1994"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Patt Anal Mach Intellig; citation_author=null Essa; citation_volume=19; citation_publication_date=1995; citation_pages=757; citation_doi=10.1109/34.598232; citation_id=CR54"/>

    <meta name="citation_reference" content="Neisser U (1976) Cognition and reality. Freeman, San Francisco, CA"/>

    <meta name="citation_reference" content="Poggi I, P&#233;lachaud C (2000) Emotional meaning and expression in animated faces. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Rutter D (1990) Non-verbal communication. In: Eysenck M (ed) The Blackwell dictionary of cognitive psychology, Blackwell, Oxford, UK"/>

    <meta name="citation_reference" content="Wehrle T, Kaiser S (2000) Emotion and facial expression. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="citation_journal_title=Cross-Cult Psychol; citation_title=Recognition of emotion by Chinese and Australian children; citation_author=R Markham, L Wang; citation_volume=27; citation_issue=5; citation_publication_date=1996; citation_pages=616-643; citation_id=CR59"/>

    <meta name="citation_reference" content="Spencer-Smith J, Innes-Ker A, Wild H and Townsend J (2002) Making faces with action unit morph targets. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3&#8211;5 April 2002"/>

    <meta name="citation_reference" content="Coulson M (2002) Expressing emotion through body movement: a component process approach. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3&#8211;5 April 2002"/>

    <meta name="citation_reference" content="Capin T, Pandzic I, Thalmann N and Thalmann D (1999) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: Earnshaw R, Vince J (eds) Virtual worlds on the Internet, IEEE Computer Science Press, Washington, DC"/>

    <meta name="citation_reference" content="Wing L (1996) Autism spectrum disorders. Constable Robinson, London, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Innov Educ Train Intl; citation_author=null Moore; citation_volume=37; citation_publication_date=2000; citation_pages=218; citation_doi=10.1080/13558000050138452; citation_id=CR64"/>

    <meta name="citation_reference" content="citation_journal_title=Educ Med; citation_author=null Moore; citation_volume=25; citation_publication_date=2001; citation_pages=169; citation_doi=10.1080/13581650020054361; citation_id=CR65"/>

    <meta name="citation_author" content="Marc Fabri"/>

    <meta name="citation_author_email" content="m.fabri@leedsmet.ac.uk"/>

    <meta name="citation_author_institution" content="ISLE Research Group, Leeds Metropolitan University, Leeds, UK"/>

    <meta name="citation_author" content="David Moore"/>

    <meta name="citation_author_institution" content="ISLE Research Group, Leeds Metropolitan University, Leeds, UK"/>

    <meta name="citation_author" content="Dave Hobbs"/>

    <meta name="citation_author_institution" content="School of Informatics, University of Bradford, Bradford, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-003-0116-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/04/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-003-0116-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Mediating the expression of emotion in educational collaborative virtual environments: an experimental study"/>
        <meta property="og:description" content="The use of avatars with emotionally expressive faces is potentially highly beneficial to communication in collaborative virtual environments (CVEs), especially when used in a distance learning context. However, little is known about how, or indeed whether, emotions can effectively be transmitted through the medium of a CVE. Given this, an avatar head model with limited but human-like expressive abilities was built, designed to enrich CVE communication. Based on the facial action coding system (FACS), the head was designed to express, in a readily recognisable manner, the six universal emotions. An experiment was conducted to investigate the efficacy of the model. Results indicate that the approach of applying the FACS model to virtual face representations is not guaranteed to work for all expressions of a particular emotion category. However, given appropriate use of the model, emotions can effectively be visualised with a limited number of facial features. A set of exemplar facial expressions is presented."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Mediating the expression of emotion in educational collaborative virtual environments: an experimental study | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-003-0116-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Avatar, Collaborative virtual environment, Emotion, Facial expression","kwrd":["Avatar","Collaborative_virtual_environment","Emotion","Facial_expression"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-003-0116-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-003-0116-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=116;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-003-0116-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Mediating the expression of emotion in educational collaborative virtual environments: an experimental study
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-003-0116-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-003-0116-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-02-05" itemprop="datePublished">05 February 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Mediating the expression of emotion in educational collaborative virtual environments: an experimental study</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marc-Fabri" data-author-popup="auth-Marc-Fabri" data-corresp-id="c1">Marc Fabri<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Leeds Metropolitan University" /><meta itemprop="address" content="grid.10346.30, 0000000107458880, ISLE Research Group, Leeds Metropolitan University, Leeds, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-David-Moore" data-author-popup="auth-David-Moore">David Moore</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Leeds Metropolitan University" /><meta itemprop="address" content="grid.10346.30, 0000000107458880, ISLE Research Group, Leeds Metropolitan University, Leeds, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dave-Hobbs" data-author-popup="auth-Dave-Hobbs">Dave Hobbs</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bradford" /><meta itemprop="address" content="grid.6268.a, 0000000403795283, School of Informatics, University of Bradford, Bradford, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 7</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">66</span>–<span itemprop="pageEnd">81</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">543 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">39 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-003-0116-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The use of avatars with emotionally expressive faces is potentially highly beneficial to communication in collaborative virtual environments (CVEs), especially when used in a distance learning context. However, little is known about how, or indeed whether, emotions can effectively be transmitted through the medium of a CVE. Given this, an avatar head model with limited but human-like expressive abilities was built, designed to enrich CVE communication. Based on the facial action coding system (FACS), the head was designed to express, in a readily recognisable manner, the six universal emotions. An experiment was conducted to investigate the efficacy of the model. Results indicate that the approach of applying the FACS model to virtual face representations is not guaranteed to work for all expressions of a particular emotion category. However, given appropriate use of the model, emotions<i> can</i> effectively be visualised with a limited number of facial features. A set of exemplar facial expressions is presented.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Introduction</h2><div class="c-article-section__content" id="Sec2-content"><p>This document outlines an experimental study to investigate the use of facial expressions for humanoid user representations as a means of non-verbal communication in collaborative virtual environments (CVEs). The intention is to establish detailed knowledge about how facial expressions can be effectively and efficiently visualised in CVEs.</p><p>We start by arguing for the insufficiency of existing distance communication media in terms of emotional context and means for emotional expression, and propose that this problem could be overcome by enabling people to meet<i> virtually</i> in a CVE and engage in quasi face-to-face communication via their avatars. We further argue that the use of avatars with emotionally expressive faces is potentially highly beneficial to communication in CVEs.</p><p>However, although research in the field of CVEs has been proceeding for some time now, the representation of user embodiments, or avatars, in most systems is still relatively simple and rudimentary [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Thalmann D (2001) The role of virtual humans in virtual environment technology and interfaces. In: Earnshaw R, Guedj R and Vince J (eds) Frontiers of human-centred computing, online communities and virtual environments. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR1" id="ref-link-section-d71542e323">1</a>]. In particular, virtual environments are often poor in terms of the emotional cues that they convey [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Fleming B, Dobbs D (1999) Animating facial features and expressions. Charles River Media, Boston, MA" href="/article/10.1007/s10055-003-0116-7#ref-CR2" id="ref-link-section-d71542e326">2</a>]. Accordingly, the need for sophisticated ways to reflect emotions in virtual embodiments has been pointed out repeatedly in recent investigations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Dumas C, Saugis G, Chaillou C, Degrande S and Viaud M (1998) A 3-D interface for cooperative work. In: Proceedings of the Conference on Collaborative Virtual Environments, Manchester, UK, 17–19 June 1998" href="/article/10.1007/s10055-003-0116-7#ref-CR3" id="ref-link-section-d71542e329">3</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Manninen T, Kujanpää T (2002) Non-verbal communication forms in multi-player game sessions. In: Faulkner X, Finlay J, Détienne F (eds) People and computers XVI—memorable yet invisible. BCS Press, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR4" id="ref-link-section-d71542e332">4</a>].</p><p>In the light of this, a controlled experiment was conducted to investigate the applicability of non-verbal means of expression, particularly the use of facial expressions, via avatars in CVE systems. It is the purpose of the experiments to establish whether and how emotions can effectively be transmitted through the medium of CVE.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">A case for a CVE for education</h2><div class="c-article-section__content" id="Sec3-content"><p>Today’s information society provides us with numerous technological options to facilitate human interaction over a distance, in real time or asynchronously: telephony, electronic mail, text-based chat, video-conferencing systems. These tools are useful, and indeed crucial, for people who cannot come together physically but need to discuss, collaborate on, or even dispute certain matters. Distance learning programmes make extensive use of such technologies to enable communication between spatially separated tutors and learners, and between learners and fellow learners [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Atkins H, Moore D, Hobbs D and Sharpe S (2001) Learning style theory and computer mediated communication. In: Proceedings of ED-Media, Tampere, Finland, 25–30 June 2001" href="/article/10.1007/s10055-003-0116-7#ref-CR5" id="ref-link-section-d71542e345">5</a>]. Extensive research [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Laurillard D (1993) Rethinking university teaching. Routledge, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR6" id="ref-link-section-d71542e348">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Moore M (1993) Three types of interaction. In: Harry K, John M and Keegan D (eds) Distance education: new perspectives. Routledge, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR7" id="ref-link-section-d71542e351">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Johnson D, Johnson R (1994) Cooperative learning in the culturally diverse classroom. In: DeVillar, Faltis, Cummins (eds) Cultural diversity in schools. State University of New York Press, Albany, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR8" id="ref-link-section-d71542e354">8</a>] has shown that such interaction is crucial for the learning process, for the purpose of mutual reflection on actions and problem solutions, for motivation and stimulation as well as assessment and control of progress, and has given rise to a growing body of literature in computer supported collaborative learning, cf [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Webb N (1995) Constructive activity and learning in collaborative small groups. Educ Psychol 87(3):406–423" href="/article/10.1007/s10055-003-0116-7#ref-CR9" id="ref-link-section-d71542e357">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Wu A, Farrell R and Singley M (2002) Scaffolding group learning in a collaborative networked environment. In: Proceedings of CSCL 2002, Boulder, CO, 7–11 January 2002" href="/article/10.1007/s10055-003-0116-7#ref-CR10" id="ref-link-section-d71542e361">10</a>].</p><p>However, when communicating over a distance through media tools, the emotional context is often lost, as well as the ability to express emotional states in the way one is accustomed to in face-to-face conversations. When using text-based tools, important indicators like accentuation, emotion and change of emotion or intonation are difficult to mediate [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Lisetti C, Douglas M and LeRouge C (2002) Intelligent affective interfaces: a user-modeling approach for telemedicine. In: Proceedings of the International Conference on Universal Access in HCI, New Orleans, LA, 5–10 August 2002" href="/article/10.1007/s10055-003-0116-7#ref-CR11" id="ref-link-section-d71542e367">11</a>]. Audio conferencing tools can alleviate some of these difficulties but lack ways to mediate non-verbal means of communication such as facial expressions, posture or gesture. These channels, however, play an important role in human interaction and it has been argued that the<i> socio-emotional content</i> they convey is vital for building relationships that need to go beyond purely factual and task-oriented communication [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Lisetti C, Douglas M and LeRouge C (2002) Intelligent affective interfaces: a user-modeling approach for telemedicine. In: Proceedings of the International Conference on Universal Access in HCI, New Orleans, LA, 5–10 August 2002" href="/article/10.1007/s10055-003-0116-7#ref-CR11" id="ref-link-section-d71542e373">11</a>].</p><p>Video conferencing can alleviate some of the shortcomings concerning body language and the visual expression of a participant’s emotional state. Daly-Jones et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Daly-Jones O, Monk A and Watts L (1998) Some advantages of video conferencing over high-quality audio conferencing: fluency and awareness of attentional focus. Int J Hum-Comp Stud 49(1):21–58" href="/article/10.1007/s10055-003-0116-7#ref-CR12" id="ref-link-section-d71542e379">12</a>] identify several advantages of video conferencing over high quality audio conferencing, in particular the vague awareness of an interlocutor’s attentional focus. However, because of the non-immersive character of typical video-based interfaces, conversational threads during meetings can easily break down when people are distracted by external influences or have to change the active window, for example, to handle electronically shared data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="McShea J, Jennings S and McShea H (1997) Characterising user control of video conferencing in distance education. In: Proceedings of CAL-97, Exeter, UK, 25–26 March 1997" href="/article/10.1007/s10055-003-0116-7#ref-CR13" id="ref-link-section-d71542e382">13</a>].</p><p>CVEs are a potential alternative to these communication tools, aiming to overcome the lack of emotional and social context whilst at the same time offering a stimulating and integrated framework for conversation and collaboration. Indeed, it can be argued that CVEs represent a communication technology in their own right due to the highly visual and interactive character of the interfaces that allow communication and the representation of information in new, innovative ways. Users are likely to be actively engaged in interaction with the virtual world and with other inhabitants. In the distance learning discipline in particular, this high-level interactivity, in which the users’ senses are engaged in the action and they “feel” they are participating in it, is seen as an essential factor for effective and efficient learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Fabri M, Gerhard M (2000) The virtual student: user embodiment in virtual learning environments. In: Orange G, Hobbs D (eds) International perspectives on tele-education and virtual learning environments, Ashgate, Aldershot, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR14" id="ref-link-section-d71542e388">14</a>].</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">The need for emotionally expressive avatars</h2><div class="c-article-section__content" id="Sec4-content"><p>The term “non-verbal communication” is commonly used to describe all human communication events which transcend the spoken or written word [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e399">15</a>]. Non-verbal communication plays a substantial role in human interpersonal behaviour. Social psychologists argue that more than 65% of the information exchanged during a person-to-person conversation is carried on the non-verbal band [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Morris D, Collett P, Marsh P and O’Shaughnessy M (1979) Gestures, their origin and distribution. Jonathan Cape, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR16" id="ref-link-section-d71542e402">16</a>]. Argyle [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR17" id="ref-link-section-d71542e405">17</a>] sees non-verbal behaviour taking place whenever one person influences another by means of facial expressions, gestures, body posture, bodily contact, gaze and pupil dilation, spatial behaviour, clothes, appearance, or non-verbal vocalisation (e.g. a murmur).</p><p>A particularly important aspect of non-verbal communication is its use to convey information concerning the emotional state of interlocutors. Wherever one interacts with another person, that other person’s emotional expressions are monitored and interpreted—and the other person is doing the same [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Strongman K (1996) The psychology of emotion (fourth edition). Wiley, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR18" id="ref-link-section-d71542e411">18</a>]. Indeed, the ability to judge the emotional state of others is considered an important goal in human perception [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Dittrich W, Troscianko T, Lea S and Morgan D (1996) Perception of emotion from dynamic point-light displays presented in dance. Perception 25:727–738" href="/article/10.1007/s10055-003-0116-7#ref-CR19" id="ref-link-section-d71542e414">19</a>], and it is argued that from an evolutionary point of view, it is probably the most significant function of interpersonal perception. Since different emotional states are likely to lead to different courses of action, it can be crucial for survival to be able to recognise emotional states, in particular anger or fear, in another person. Similarly, Argyle [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR17" id="ref-link-section-d71542e417">17</a>] argues that the expression of emotion, in the face or through the body, is part of a wider system of natural human communication that has evolved to facilitate social life. Keltner [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Keltner D (1995) Signs of appeasement: evidence for the distinct displays of embarrassment, amusement and shame. Pers Soc Psychol 68(3):441–454" href="/article/10.1007/s10055-003-0116-7#ref-CR20" id="ref-link-section-d71542e420">20</a>] showed that for example<i> embarrassment</i> is an appeasement signal that helps reconcile relations when they have gone awry, a way of apologising for making a social faux-pas. Again, recent findings in psychology and neurology suggest that emotions are also an important factor in decision-making, problem solving, cognition and intelligence in general [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Dittrich W, Troscianko T, Lea S and Morgan D (1996) Perception of emotion from dynamic point-light displays presented in dance. Perception 25:727–738" href="/article/10.1007/s10055-003-0116-7#ref-CR19" id="ref-link-section-d71542e427">19</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Picard R (1997) Affective computing. MIT Press, Cambridge, MA" href="/article/10.1007/s10055-003-0116-7#ref-CR21" id="ref-link-section-d71542e430">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Lisetti C, Schiano D (2000) Facial expression recognition: where human-computer interaction, artificial intelligence and cognitive science intersect. Prag Cognit 8(1):185–235" href="/article/10.1007/s10055-003-0116-7#ref-CR22" id="ref-link-section-d71542e433">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Damásio A (1994) Descarte’s error: emotion, reason and the human brain. Avon, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR23" id="ref-link-section-d71542e436">23</a>].</p><p>Of particular importance from the point of view of education, it has been argued that the ability to show emotions, empathy and understanding through facial expressions and body language is central to ensuring the quality of tutor-learner and learner-learner interaction [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Cooper B, Brna P and Martins A (2000) Effective affective in intelligent systems—building on evidence of empathy in teaching and learning. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR24" id="ref-link-section-d71542e442">24</a>]. Acceptance and understanding of ideas and feelings, encouraging and criticising, silence, questioning—all involve non-verbal elements of interaction [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e445">15</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Cooper B, Brna P and Martins A (2000) Effective affective in intelligent systems—building on evidence of empathy in teaching and learning. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR24" id="ref-link-section-d71542e448">24</a>].</p><p>Given this, it can be argued that CSCL technologies ought to provide for at least some degree of non-verbal, and in particular emotional, communication. For instance, the pedagogical agent STEVE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Johnson W (1998) Pedagogical agents. In: Computers in education proceedings, Beijing, China" href="/article/10.1007/s10055-003-0116-7#ref-CR25" id="ref-link-section-d71542e454">25</a>] is used in a virtual training environment for control panel operation. STEVE has the ability to give instant praise or express criticism via hand and head gestures depending on a student’s performance. Concerning CVE technology in particular, McGrath and Prinz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="McGrath A, Prinz W (2001) All that Is solid melts into software. In: Churchill, Snowdon, Munro (eds) Collaborative virtual environments—digital places and spaces for interaction, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR26" id="ref-link-section-d71542e457">26</a>] call for appropriate ways to express presence and awareness in order to aid communication between inhabitants, be it full verbal communication or non-verbal<i> presence in silence</i>.</p><p>Thalmann [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Thalmann D (2001) The role of virtual humans in virtual environment technology and interfaces. In: Earnshaw R, Guedj R and Vince J (eds) Frontiers of human-centred computing, online communities and virtual environments. Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR1" id="ref-link-section-d71542e467">1</a>] sees a direct relation between the quality of a user’s representation and his ability to interact with the environment and with other users. Even avatars with rather primitive expressive abilities can potentially cause strong emotional responses in people using a CVE system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Durlach N, Slater M (2002) Meeting people virtually: experiments in shared virtual environments. In: Schroeder R (ed) The social life of avatars, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR27" id="ref-link-section-d71542e470">27</a>]. It appears, then, that the avatar can readily take on a personal role, thereby increasing the sense of togetherness—the community feeling. The avatar potentially becomes a genuine representation of the underlying individual, not only visually, but also within a social context.</p><p>It is argued, then, that people’s naturally developed skill to “read” emotional expressions is potentially highly beneficial to communication in CVEs in general, and educational CVEs in particular. The emotionally expressive nature of an interlocutor’s avatar may be able to aid the communication process and provide information that would otherwise be difficult to mediate.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Modelling an emotionally expressive avatar</h2><div class="c-article-section__content" id="Sec5-content"><p>Given that emotional expressiveness would be a desirable attribute of a CVE, the issue becomes one of how such emotional expressions can be mediated. Whilst all of the different channels for non-verbal communication—face, gaze, gesture, posture—can in principle be mediated in CVEs to a certain degree, our current work focuses on the face. In the real world, it is the face that is the most immediate indicator of the emotional state of a person [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e483">28</a>].</p><p>While<i> physiology</i> looks beneath the skin,<i> physiognomy</i> stays on the surface studying facial features and lineaments; it is the art of judging character or the emotional state of an individual from the features of the face [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="New Oxford Dictionary of English. Oxford University Press, Oxford, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR29" id="ref-link-section-d71542e495">29</a>]. The face reflects interpersonal attitudes, provides feedback on the comments of others and is regarded as the primary source of information after human speech [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e498">15</a>]. Production (encoding) and recognition (decoding) of distinct facial expressions constitute a signalling system between humans [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Russell J, Férnandez-Dols J (1997) The psychology of facial expression. Cambridge University Press, Cambridge, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR30" id="ref-link-section-d71542e501">30</a>]. Surakka and Hietanen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Surakka V, Hietanen J (1998) Facial and emotional reactions to Duchénne and non-Duchénne smiles. Int J Psychophys 29(1):23–33" href="/article/10.1007/s10055-003-0116-7#ref-CR31" id="ref-link-section-d71542e505">31</a>] see facial expressions of emotion clearly dominating over vocal expressions of emotion; Knapp [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e508">15</a>] generally considers facial expressions as the primary site for communication of emotional states.</p><p>Indeed, most researchers even suggest that the ability to classify facial expressions of an interlocutor is a necessary prerequisite for the inference of emotion. It appears that there are certain key stimuli in the human face that support cognition. Zebrowitz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e514">32</a>] found that, for example, in the case of an infant’s appearance, these key stimuli can by themselves trigger favourable emotional responses. Strongman [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Strongman K (1996) The psychology of emotion (fourth edition). Wiley, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR18" id="ref-link-section-d71542e517">18</a>] points out that humans make such responses not only to the expression but also to what is believed to be the “meaning” behind the expression.</p><p>Our work therefore concentrates on the face. To model an emotionally expressive avatar face, the work of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ekman P, Friesen W and Ellsworth P (1972) Emotion in the human face: guidelines for research and an integration of findings. Pergamon, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR33" id="ref-link-section-d71542e523">33</a>] was followed. It was found that there are six universal facial expressions, corresponding to the following emotions:<i> surprise, anger, fear, happiness, disgust/Contempt,</i> and<i> sadness</i>. This categorisation is widely accepted, and considerable research has shown that these basic emotions can be accurately communicated by facial expressions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e532">32</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Ekman P (1999) Facial expressions. In: Dalgleish T, Power M (eds) Handbook of cognition and emotion. Wiley, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR34" id="ref-link-section-d71542e535">34</a>]. Indeed, it is held that the expression, and to an extent the recognition, of these six emotions has an innate basis. They can be found in all cultures, and correspond to distinctive patterns of physiognomic arousal. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig1">1</a> shows sample photographs depicting the six universal emotions, together with the neutral expression (from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR35" id="ref-link-section-d71542e542">35</a>], used with permission). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p> The six universal emotions and neutral expressions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec6">Describing facial expressions</h3><p>Great effort has gone into the development of scoring systems for facial movements. These systems attempt to objectively to describe and quantify all visually discriminating units of facial action seen in adults. For the purpose of analysis, the face is typically broken down into three areas: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>the brows and forehead</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>the eyes, eyelids and the root of the nose</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>the lower face with the mouth, nose, cheeks, and chin</p>
                      
                    </li>
                  </ol>
                        <p>These are the areas which appear to be capable of independent movement. In order to describe the visible muscle activity in the face comprehensively, FACS was developed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR36" id="ref-link-section-d71542e607">36</a>]. FACS is based on highly detailed anatomical studies of human faces and results from a major body of work. It has formed the basis for numerous series of experiments in social psychology, computer vision and computer animation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Bartlett M (1998) Face image analysis by unsupervised learning and redundancy reduction. Dissertation, University of California" href="/article/10.1007/s10055-003-0116-7#ref-CR37" id="ref-link-section-d71542e610">37</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Pélachaud C, Badler N and Steedman M (1996) Generating facial expressions for speech. Cog Sci 20(1):1–46" href="/article/10.1007/s10055-003-0116-7#ref-CR38" id="ref-link-section-d71542e613">38</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Ekman P, Rosenzweig L (eds) What the face reveals: basic and applied studies of spontaneous expression using the facial action coding system. Oxford University Press, Oxford, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR39" id="ref-link-section-d71542e616">39</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Terzopoulos D, Waters K (1993) Analysis and synthesis of facial image sequences using physical and anatomical models. Patt Anal Mach Intell 15(6):569–579" href="/article/10.1007/s10055-003-0116-7#ref-CR40" id="ref-link-section-d71542e619">40</a>].</p><p>A facial expression is a high level description of facial motions, which can be decomposed into certain muscular activities, i.e., relaxation or contraction, called “action units” (AUs). FACS identifies 58 action units, which separately or in various combinations are capable of characterising any human expression. An AU corresponds to an action produced by one or a group of related muscles. Action Unit 1, for example, is the<i>
inner-brow-raiser</i>, a contraction of the<i>
central frontalis</i> muscle. Action Unit 7 is the<i> lid-tightener</i>, tightening the eyelids and thereby narrowing the eye opening.</p><p>FACS is usually coded from video or photographs, and a trained human FACS coder decomposes an observed expression into the specific AUs that occurred, their duration, onset, and offset time [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Bartlett M (1998) Face image analysis by unsupervised learning and redundancy reduction. Dissertation, University of California" href="/article/10.1007/s10055-003-0116-7#ref-CR37" id="ref-link-section-d71542e637">37</a>]. From this system, some very specific details can be learnt about facial movement for different emotional expressions of humans in the real world. For instance, the brow seems capable of the fewest positions and the lower face the most [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e640">15</a>]. Certain emotions also seem to manifest themselves in particular areas of the face. The best predictors for<i> anger,</i> for example, are the lower face and the brows/forehead area, whereas<i> sadness</i> is most revealed in the area around the eyes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR15" id="ref-link-section-d71542e649">15</a>].</p><p>For our current modelling work, then, FACS was adapted to generate the expression of emotions in the virtual face, by applying a limited number of relevant action units to the animated head. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig2">2</a>
shows photographs of some alternative expressions for the<i> anger</i> emotion category, together with the corresponding virtual head expressions as modelled by our avatar. Equivalent representations exist for all remaining universal emotions (and the neutral expression). All photographs are taken from the<i>
Pictures of Facial Affect</i> databank [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR35" id="ref-link-section-d71542e665">35</a>]. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p> Photographs showing variations of anger, with corresponding virtual heads</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec7">Keeping it simple</h3><p>Interest in modelling the human face has been strong in the computer graphics community since the 1980s. The first muscle-based model of an animated face, using geometric deformation operators to control a large number of muscle units, was developed by Platt and Badler [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Platt S, Badler N (1981) Animating facial expression. ACM SIGGRAPH 15(3):245–252" href="/article/10.1007/s10055-003-0116-7#ref-CR41" id="ref-link-section-d71542e694">41</a>]. This was developed further by modelling the anatomical nature of facial muscles and the elastic nature of human skin, resulting in a dynamic muscle model [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Terzopoulos D, Waters K (1993) Analysis and synthesis of facial image sequences using physical and anatomical models. Patt Anal Mach Intell 15(6):569–579" href="/article/10.1007/s10055-003-0116-7#ref-CR40" id="ref-link-section-d71542e697">40</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Parke F (1982) Parameterized modeling for facial animation. IEEE Comp Graph Appl 2(9):61–68" href="/article/10.1007/s10055-003-0116-7#ref-CR42" id="ref-link-section-d71542e700">42</a>].</p><p>The approach adopted in this study, however, is feature-based and therefore less complex than a realistic simulation of real-life physiology. It is argued that it is not necessary, and indeed may be counter-productive, to assume that a “good” avatar has to be a realistic and very accurate representation of the real world physiognomy. We argue this partly on the ground that early evidence suggested that approaches aiming to reproduce the human physics in detail may in fact be wasteful [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Benford S, Bowers J, Fahlén L, Greenhalgh C and Snowdon D (1995) User embodiment in collaborative virtual environments. In: Proceedings of CHI 1995 Proceedings, Denver, CO, 7–11 May 1995" href="/article/10.1007/s10055-003-0116-7#ref-CR43" id="ref-link-section-d71542e706">43</a>].</p><p>Indeed, this has been described as the<i> Uncanny Valley</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Mori M (1982) The buddha in the robot. Tuttle, Boston, MA" href="/article/10.1007/s10055-003-0116-7#ref-CR44" id="ref-link-section-d71542e715">44</a>]<i>,</i>
originally created to predict human psychological reaction to humanoid robots (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig3">3</a>, adapted from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Reichardt J (1978) Robots: fact, fiction and prediction. Thames &amp; Hudson, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR45" id="ref-link-section-d71542e724">45</a>]). When plotting human reaction against robot movement, the curve initially shows a steady upward trend. That trend continues until the robot reaches a reasonably human quality. The curve then plunges down dramatically, even evoking a negative emotional response. A<i> nearly</i> human robot is considered irritating and repulsive. The curve only rises again once the robot eventually reaches a complete resemblance to humans. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p> The “uncanny valley”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>It is postulated that human reaction to avatars is similarly characterised by an<i> uncanny valley</i>. An avatar designed to suspend disbelief that is only<i> nearly</i> realistic may be equally confusing and not be accepted, even considered repulsive. In any event, Hindmarsh et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Hindmarsh J, Fraser M, Heath C and Benford S (2001) Virtually missing the point: configuring CVEs for object-focused interaction. In: Churchill E, Snowdon D and Munro A (eds) Collaborative virtual environments: digital places and spaces for interaction, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR46" id="ref-link-section-d71542e758">46</a>] suggest that even with full realism and full perceptual capabilities of physical human bodies in virtual space, opportunities for employing more inventive and evocative ways of expression would probably be lost if the focus is merely on simulating the real world—with its rules, habits and limitations.</p><p>It may be more appropriate, and indeed more supportive to perception and cognition, to represent issues in simple or unusual ways. Godenschweger et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Godenschweger F, Strothotte T and Wagener H (1997) Rendering gestures as line drawings. In: Proceedings of the Gesture Workshop 1997, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR47" id="ref-link-section-d71542e765">47</a>] found that minimalist drawings of body parts, showing gestures, were generally easier to recognise than more complex representations. Further, Donath [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Donath J (2001) Mediated faces. In: Beynon M, Nehaniv C and Dautenhahn K (eds) Cognitive technology: instruments of mind, Warwick, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR48" id="ref-link-section-d71542e768">48</a>] warns that because the face is so highly expressive and humans are so adept in reading (into) it, any level of detail in 3D facial rendering could potentially provoke the interpretation of various social messages. If these messages are unintentional, the face will arguably be hindering rather than helping communication.</p><p>Again, there is evidence that particularly distinctive faces can convey emotions more efficiently than normal faces [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e774">32</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 April 2001" href="/article/10.1007/s10055-003-0116-7#ref-CR49" id="ref-link-section-d71542e777">49</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Ellis H (1990) Developmental trends in face recognition. Bullet Brit Psychol Soc 3:114–119" href="/article/10.1007/s10055-003-0116-7#ref-CR50" id="ref-link-section-d71542e780">50</a>], a detail regularly employed by caricaturists. The human perception system can recognise physiognomic clues, in particular facial expressions, from very few visual stimuli [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Dittrich W (1991) Facial motion and the recognition of emotions. Psychol Beit 33(3/4):366–377" href="/article/10.1007/s10055-003-0116-7#ref-CR51" id="ref-link-section-d71542e783">51</a>].</p><p>To summarise, rather than simulating the real world accurately, we aim to take advantage of humans’ innate cognitive abilities to perceive, recognise and interpret distinctive physiognomic clues. With regard to avatar expressiveness and the<i> uncanny valley</i>, we are targeting the first summit of the curve (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig3">3</a>) where human emotional response is maximised while employing a relatively simple avatar model.</p><h3 class="c-article__sub-heading" id="Sec8">Modelling facial expressions</h3><p>In order to realise such an approach in our avatar work, we developed an animated virtual head with a limited number of controllable features. It is loosely based on the H-Anim specification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="H-Anim Working Group. Specification for a standard VRML humanoid. http://www.h-anim.org." href="/article/10.1007/s10055-003-0116-7#ref-CR52" id="ref-link-section-d71542e803">52</a>] developed by the international panel that develops the virtual reality modeling language (VRML). H-Anim specifies seven control parameters: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>the left eyeball</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>the right eyeball</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>the left eyebrow</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>the right eyebrow</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>the left upper eyelid</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">6.</span>
                      
                        <p>the right upper eyelid</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">7.</span>
                      
                        <p>the temporomandibular (for moving the jaw)</p>
                      
                    </li>
                  </ol>
                        <p>Early in the investigation it became evident, however, that eyeball movement was not necessary as the virtual head was always in direct eye contact with the observer. We also found that although we were aiming at a simple model, a single parameter for moving and animating the mouth area (the temporomandibular) was insufficient for the variety of expressions required in the lower face area.</p><p>Consequently, the H-Anim basis was developed further and additional features were derived from, and closely mapped to, FACS action units. This allowed for greater freedom, especially in the mouth area. It has to be noted that while FACS describes muscle movement, our animated head was not designed to necessarily emulate such muscle movement faithfully, but to achieve a visual effect very similar to the result of muscle activity in the human face.</p><p>It turned out that it is not necessary for the entire set of action units to be reproduced in order to achieve the level of detail envisaged for the current face model. In fact, reducing the number of relevant action units is not an uncommon practice for simple facial animation models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Yacoob Y, Davis L (1994) Computing spatio-temporal representations of human faces. In: Proceedings of the Computer Vision and Pattern Recognition Conference, Seattle, WA, June 1994" href="/article/10.1007/s10055-003-0116-7#ref-CR53" id="ref-link-section-d71542e887">53</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Essa I, Pentland A (1995) Coding, analysis, interpretation, and recognition of facial expressions. IEEE Trans Patt Anal Mach Intellig 19(7):757–763" href="/article/10.1007/s10055-003-0116-7#ref-CR54" id="ref-link-section-d71542e890">54</a>], and this study used a subset of 11 action units (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab1">1</a>). </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1.  Reduced set of action units</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-003-0116-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The relevant animation control parameters required to model facial features that correspond to these 11 action units are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig4">4</a>. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p> Controllable features of the virtual head</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>As an example, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig5">5</a> shows four variations of the<i> sadness</i> emotion, as used in the experiment. Note the wider eye opening in 1, and the change of angle and position of the eyebrows. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p> Variations within emotion category sadness</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Certain facial features have deliberately been omitted to keep the number of control parameters, and action units, low. For example, AU12 (the lip corner puller) normally involves a change in cheek appearance. The virtual head however shows AU12 only in the mouth corners. Also, the virtual head showing AU26 (the jaw drop) does not involve jawbone movement but is characterised solely by the relaxation of the mentalis muscle, resulting in a characteristic opening of the mouth. These omissions were considered tolerable, as they did not appear to change the visual appearance of the expression significantly. Accordingly, neither the statistical analysis nor feedback from participants indicated a disadvantage of doing so.</p><p>In summary, then, we argue that the virtual face model introduced above is a potentially effective and efficient means for conveying emotion in CVEs. By reducing the facial animation to a minimal set of features believed to display the most distinctive area segments of the six universal expressions of emotion (according to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e1198">28</a>]), we take into account findings from cognitive and social psychology. These findings suggest that there are internal, probably innate, physiognomic schemata that support face perception and emotion recognition in the face [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Neisser U (1976) Cognition and reality. Freeman, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR55" id="ref-link-section-d71542e1201">55</a>]. This recognition process works with even a very limited set of simple but distinctive visual clues [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY" href="/article/10.1007/s10055-003-0116-7#ref-CR17" id="ref-link-section-d71542e1204">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Dittrich W (1991) Facial motion and the recognition of emotions. Psychol Beit 33(3/4):366–377" href="/article/10.1007/s10055-003-0116-7#ref-CR51" id="ref-link-section-d71542e1207">51</a>].</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Experimental investigation</h2><div class="c-article-section__content" id="Sec9-content"><p>We argue, then, that there is a strong prima facie case that the proposed virtual head, with its limited, but human-like expressive abilities, is a potentially effective and efficient means to convey emotions in virtual environments, and that the reduced set of action units and the resulting facial animation control parameters are sufficient to express, in a readily recognisable manner, the six universal emotions.</p><p>We have experimentally investigated this prima facie argument, comparing recognition rates of virtual head expressions with recognition rates based on photographs of faces for which FACS action unit coding, as well as recognition rates from human participants, was available. These photographs were taken from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR35" id="ref-link-section-d71542e1221">35</a>]. A detailed description of the experimental setup is presented in this section. The aims of the experiment were (a) to investigate the use of simple but distinctive visual clues to mediate the emotional and social state of a CVE user, and (b) to establish the most distinctive and essential features of an avatar facial expression.</p><p>Given these aims, the experiment was designed to address the following working hypothesis: “For a well-defined subset that includes at least one expression per emotion category, recognition rates of the virtual head model and of the corresponding photographs are comparable”.</p><h3 class="c-article__sub-heading" id="Sec10">Design</h3><p>The independent variable (IV) in this study is the stimulus material presented to the participants. The facial expressions of emotion are presented in two different ways, as FACS training photographs or in emotions displayed by the animated virtual head. Within each of these two factors, there are seven sub-levels (the six universal expressions of emotion and<i> neutral</i>). The dependent variable (DV) is the success rate achieved when assigning the presented expressions of emotion to their respective categories.</p><p>Two control variables (CVs) can be identified: the cultural background of participants and their previous experience in similar psychological experiments. Since the cultural background of participants potentially may affect their ability to recognise certain emotions in the face [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e1239">32</a>], this factor was neutralised by ensuring that all participants had broadly the same ability concerning the recognition of emotion. In the same manner, it was checked that none of the participants had previous experience with FACS coding or related psychological experiments, as this may influence the perception abilities due to specifically developed skills.</p><p>We adopted a<i> one-factor, within subjects</i> design (also known as<i> repeated measures</i> design) for the experiment. The factor comprises two levels, photograph or virtual face, and each participant performs under both conditions: </p><ul class="u-list-style-dash">
                    <li>
                      <p>Condition A: emotions depicted by the virtual head</p>
                    </li>
                    <li>
                      <p>Condition B: emotions shown by persons on FACS photographs</p>
                    </li>
                  </ul>
                        <p>29 participants took part in the experiment, 17 female and 12 male, with an age range of 22 to 51 years old. All participants were volunteers. None had classified facial expressions or used FACS before. None of the participants worked in facial animation, although some were familiar with 3D modelling techniques in general.</p><h3 class="c-article__sub-heading" id="Sec11">Procedure</h3><p>The experiment involved three phases: a pre-test questionnaire, a recognition exercise and a post-test questionnaire. Each participant was welcomed by the researcher and seated at the workstation where the experiment would be conducted. The researcher then gave the participant an overview of what was expected of him/her and what to expect during the experiment. Care was taken not to give out information that might bias the user. The participants were assured that they themselves were not under evaluation and that they could leave the experiment at any point if they felt uncomfortable. Participants were then presented with the pre-test questionnaire, which led into the recognition exercise. From this moment, the experiment ran automatically, via a software application, and no further experimenter intervention was required.</p><p>The actual experiment was preceded by a pilot test with a single participant. This participant was not part of the participant group in the later experiment. The pilot run confirmed that the software designed to present the stimulus material and collect the data was functioning correctly and also that a 20 minutes duration time per participant was realistic. Furthermore, it gave indications that questionnaire items possess the desired qualities of measurement and discriminability.</p><p>The pre-test questionnaire (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig6">6</a>) collected information about the participant in relation to their applicability for the experiment. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p> The pre-test questionnaire</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The<i> Cancel</i> button allowed the abortion of the experiment at any stage, in which case all data collected so far was deleted.<i> Back</i>
and<i> Next</i> buttons were displayed depending on the current context. A screen collecting further data about the participant’s background on FACS as well as possible involvement in similar experiments followed the pre-test questionnaire.</p><p>Before the recognition task started, a “practice” screen illustrating the actual recognition screen and giving information about the choice of emotion categories and the functionality of buttons and screen element, was shown to the participant.</p><p>During the recognition task, each participant was shown 28 photographs and 28 corresponding virtual head images, mixed together in a randomly generated order that was the same for all participants. Each of the six emotion categories was represented in 4 variations, and 4 variations of the neutral face were also shown. The variations were defined not by intensity, but by differences in expression of the same emotion. The controllable parameters of the virtual head were adjusted so that they corresponded with the photographs.</p><p>All material was presented in digitised form, i.e., as virtual head screenshots and scanned photographs, respectively. Each of the six emotion categories was represented in four variations. In addition, there were four variations of the neutral face. Each participant was therefore asked to classify 56 expressions (two conditions x seven emotion categories x four variations per category).</p><p>All virtual head images depicted the same male model throughout, whereas the photographs showed several people, expressing a varying number of emotions (21 images showing male persons, eight female).</p><p>The order of expressions in terms of categories and variations was randomised but the same for all participants. Where the facial atlas did not provide four distinctive variations of a particular emotion category, or the virtual head could not show the variation because of the limited set of animation parameters, a similar face was repeated.</p><p>The face images used in the task were cropped to display the full face, including hair. Photographs were scaled to 320 × 480 pixels, whereas virtual head images were slightly smaller at 320 × 440 pixels. The data collected for each facial expression of emotion consisted of: </p><ul class="u-list-style-dash">
                    <li>
                      <p>the type of stimulus material</p>
                    </li>
                    <li>
                      <p>the expression depicted by each of the facial areas</p>
                    </li>
                    <li>
                      <p>the emotion category expected</p>
                    </li>
                    <li>
                      <p>the emotion category picked by the participant</p>
                    </li>
                  </ul>
                        <p>A “recognition screen” (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig7">7</a>) displayed the images and provided buttons for participants to select an emotion category. In addition to the aforementioned seven categories, two more choices were offered. The “Other...” choice allowed entry of a term that, according to the participant, described the shown emotion best but was not part of the categories offered. If none of the emotions offered appeared to apply, and no other emotion could be named, the participant was able to choose “Don’t know”. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb7.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb7.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p> The recognition screen</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>On completion of the recognition task, the software presented the post-test questionnaire (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig8">8</a>) to the participant. This collected various quantitative and qualitative data, with a view to complementing the data collected during the recognition task. The Next button was enabled only on completion of all rows. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p> The post-test questionnaire</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Results</h2><div class="c-article-section__content" id="Sec12-content"><p>The overall number of pictures shown was 1624 (29 participants x 56 pictures per participant). On average, a participant took 11 minutes to complete the experiment including the pre-test and the post-test questionnaire. Results show that recognition rates varied across emotion categories, as well as between the two conditions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig9">9</a> summarises the results. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p> A summary of recognition rates</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>
                        <i>Surprise</i>,<i> fear</i>,<i> happiness</i>
and<i> neutral</i> show slightly higher recognition rates for the photographs, while in categories<i>
anger</i> and<i> sadness</i> the virtual faces are more easily recognised than their counterparts.<i> Disgust</i> stands out as it shows a very low scoring for virtual faces (around 20%) in contrast to the result for photographs of disgust which is over 70%.</p><p>Overall, results clearly suggest that recognition rates for photographs (78.6% overall) are significantly higher than those for virtual heads (62.2% overall). The Mann-Whitney test confirms this, even at a significance level of 1%. However, a closer look at the recognition rates of particular emotions reveals that all but one emotion category have at least one photograph-virtual head pair with comparable results, demonstrating that recognition was as successful with the virtual head as it was with the directly corresponding photographs. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig10">10</a>
shows recognition rates for these “top” virtual heads in each category.<i> Disgust</i> still stands out as a category with “poor” results for the virtual head. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p> A summary of recognition rates for selected images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Results also indicate that recognition rates vary significantly between participants. The lowest scoring individual recognised 30 out of 56 emotions correctly (54%), and the highest score was 48 (86%). Those who achieved better results did so homogeneously between virtual heads and photographs. Lower scoring participants were more likely to fail recognising virtual heads rather than the photographs.</p><p>The expressions of emotion identified as being most distinctive are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig11">11</a>. Each expression is coded according to FACS with corresponding action units. Some action units are binary, i.e., they are applied or not, while other action units have an associated intensity scoring. Intensity can vary from A (weakest) to E (strongest). The study results would recommend use of these particular expressions, or “exemplars”, for models with a similarly limited number of animation control parameters:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p> The most distinctive expressions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Surprise (AUs 1C 2C 5C 26) is a very brief emotion, shown mostly around the eyes. Our “exemplary surprise face” features high raised eyebrows and raised upper lids. The lower eyelids remain in the relaxed position. The open mouth is relaxed, not tense. Unlike the typical human<i> surprise</i> expression, the virtual head does not actually drop the jaw bone. The evidence is that this does not have an adverse effect however, considering that 80% of all participants classified the expression correctly.</p><p>Fear (AUs 1B 5C L10A 15A 25) usually has a distinctive appearance in all three areas of the face [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e1517">28</a>]. The variation which proved to be most successful in our study is characterised by raised, slightly arched eyebrows. The eyes are wide open as in surprise and the lips are parted and tense. This is in contrast to the open but relaxed “surprise” mouth. There is an asymmetry in that the left upper lip is slightly raised.</p><p>Disgust (AUs 4C 7C 10A) is typically shown in the mouth and nose area [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e1523">28</a>]. The variation with the best results is characterised mainly by the raised upper lip (AU10) together with tightened eyelids. It has to be stressed that disgust was the least successful category with only 30% of the participants assigning this expression correctly.</p><p>Our Anger face (AUs 2A 4B 7C 17B) features lowered brows that are drawn together. Accordingly, the eyelids are tightened which makes the eyes appear to be staring out in a penetrating fashion. Lips are pressed firmly together with the corners straight, a result of the chin raiser AU17.</p><p>Happiness (AUs 12C, 25) turned out to be easy to recognise—in most cases a cheek raiser (AU12) is sufficient. In our exemplary face, the eyes are relaxed and mouth corners are being pulled up. The virtual head does not allow a change to the cheek appearance, neither does it allow for wrinkles to appear underneath the eyes. Such smiles without cheek or eye involvement are sometimes referred to as non-enjoyment smiles, or “Duchénne smiles” after the 19<sup>th</sup> century French neurologist Duchénne de Boulogne [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Surakka V, Hietanen J (1998) Facial and emotional reactions to Duchénne and non-Duchénne smiles. Int J Psychophys 29(1):23–33" href="/article/10.1007/s10055-003-0116-7#ref-CR31" id="ref-link-section-d71542e1534">31</a>].</p><p>The sadness expression (AUs 1D 4D 15A 25) that was most successful has characteristic brow and eye features. The brows are raised in the middle while the outer corners are lowered. This affects the eyes which are triangulated with the inner corner of the upper lids raised. The slightly raised lower eyelid is not necessarily typical [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ekman P, Friesen W and Ellsworth P (1972) Emotion in the human face: guidelines for research and an integration of findings. Pergamon, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR33" id="ref-link-section-d71542e1540">33</a>] but, in this case, increases the sadness expression. The corners of the lips are down.</p><h3 class="c-article__sub-heading" id="Sec13">Recognition errors</h3><p>The errors made by participants when assigning expressions to categories are presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab2">2</a>. The matrix shows which categories have been confused, and compares virtual heads with photographs. Rows give per cent occurrence of each response. Confusion values above 10% are shaded light grey, above 20% dark grey, above 30% black. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2.  Error matrix for emotion categorisation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-003-0116-7/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Disgust and anger</h4><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab2">2</a> shows that the majority of confusion errors were made in the category<i> disgust</i>, an emotion frequently confused with<i> anger.</i> When examining results for virtual heads only, anger (39%) was picked almost twice as often as disgust (22%). Further, with faces showing disgust, participants often felt unable to select any given category and instead picked “Don’t know”, or suggested an alternative emotion. These alternatives were for example<i>
aggressiveness</i>,<i> hatred</i>,<i> irritation</i>, or<i>
self-righteousness</i>.</p><p>Ekman and Friesen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2036">28</a>] describe disgust (or contempt) as an emotion that often carries an element of condescension toward the object of contempt. People feeling disgusted by other people, or their behaviour, tend to feel morally superior to them. Our observations confirm this tendency, for where “other” was selected instead of the expected “disgust”, the suggested alternative was often in line with the interpretation found by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2039">28</a>].</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Fear and surprise</h4><p>The error matrix (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab2">2</a>) further reveals that<i>
fear</i> was often mistaken for<i>
surprise</i>, a tendency that was also observed in several other studies (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Ekman P (1999) Facial expressions. In: Dalgleish T, Power M (eds) Handbook of cognition and emotion. Wiley, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR34" id="ref-link-section-d71542e2059">34</a>]). It is stated that a distinction between the two emotions can be observed with high certainty only in “literate” cultures, but not in “pre-literate”, visually isolated cultures. Social psychology states that experience and therefore expression of fear and surprise often happen simultaneously, such as when fear is felt suddenly due to an unexpected threat [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2062">28</a>]. The appearance of fear and surprise is also similar, with fear generally producing a more tense facial expression. However, fear differs from surprise in three ways: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Whilst surprise is not necessarily pleasant or unpleasant, even mild fear is unpleasant.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>One can be afraid of something familiar that is certainly going to happen (for example a visit to the dentist), whereas something familiar or expected can hardly be surprising.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Whilst surprise usually disappears as soon as it is clear what the surprising event was, fear can last much longer, even when the nature of the event is fully known.</p>
                        
                      </li>
                    </ol>
                           <p>These indicators allow for the differentiation of whether a person is afraid or surprised. All three have to do with the context and timing of the fear-inspiring event—factors that are not perceivable from a still image.</p><p>In accordance with this, Poggi and Pélachaud [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Poggi I, Pélachaud C (2000) Emotional meaning and expression in animated faces. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR56" id="ref-link-section-d71542e2104">56</a>] found that emotional information is not only contained in the facial expression itself, but also in the performatives of a communicative act: suggesting, warning, ordering, imploring, approving and praising. Similarly, Bartneck [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 April 2001" href="/article/10.1007/s10055-003-0116-7#ref-CR49" id="ref-link-section-d71542e2107">49</a>] observed significantly higher recognition rates when still images of facial expressions were shown in a dice game context, compared to a display without any context. In other words, the meaning and interpretation of an emotional expression can depend on the situation in which it is shown. This strongly suggests that in situations where the facial expression is animated or displayed in context, recognition rates can be expected to be higher.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Fear and anger</h4><p>The relationship between<i>
fear</i> and<i> anger</i> is similar to that between fear and surprise. Both can occur simultaneously, and their appearance often blends. What is striking is that all confusions were made with virtual faces, whilst not even one of the fear photographs was categorised as anger. This may suggest that the fear category contained some relatively unsuitable examples of modelled facial expressions. An examination of the results shows that there was one artefact in particular that was regularly mistaken for anger.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig12">12</a> shows an expression with the appearance of the eyes being characteristic of fear. The lower eyelid is visibly drawn up and appears to be very tensed. Both eyebrows are slightly raised and drawn together. The lower area of the face also shows clear characteristics of fear, such as the slightly opened mouth with stretched lips that are drawn together. In contrast, an angry mouth has the lips either pressed firmly together or open in a “squarish” shape, as if to shout. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p> Fear expression, variation A</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>However, 18 out of 29 times this expression was categorised as anger. In anger, as in fear, eyebrows can be drawn together. But unlike the fearful face which shows raised eyebrows, the angry face features a lowered brow. Generally, we have found that subtle changes to upper eyelids and brows had a significant effect on the expression overall, which is in line with findings for real-life photographs [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2151">28</a>].</p><p>The eyebrows in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig12">12</a> are only slightly raised from the relaxed position, but perhaps not enough to give the desired impression. Another confusing indicator is the furrowed shape of the eyebrows, since a straight line or arched brows are more typical for fear.</p><p>In contrast, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig13">13</a> the expression is identical to the expression in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-003-0116-7#Fig12">12</a> apart from the eyebrows, which are now raised and arched, thereby changing the facial expression significantly and making it less ambiguous and distinctively “fearful”. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb13.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-003-0116-7/MediaObjects/s10055-003-0116-7fhb13.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p> Fear expression, variation B</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-003-0116-7/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec17">Post-experiment questionnaire results</h3><p>After completing the recognition task participants were asked to complete a questionnaire and were invited to comment on any aspect of the experiment. Responses to the latter are discussed in the next section of this paper. The questionnaire comprised eleven questions, each one answered on a scale from 0–4 with 0 being total disagreement and 4 being total agreement. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab3">3</a> shows the average values per question. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3.  Post-experiments questionnaire results</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-003-0116-7/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Discussion and conclusions</h2><div class="c-article-section__content" id="Sec18-content"><p>The experiment followed the standard practice for expression recognition experiments by preparing the six universal emotions as pictures of avatar faces of photographs of real human faces and showing these pictures to participants who were asked to say what emotion they thought each photograph or picture portrayed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Rutter D (1990) Non-verbal communication. In: Eysenck M (ed) The Blackwell dictionary of cognitive psychology, Blackwell, Oxford, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR57" id="ref-link-section-d71542e2454">57</a>]. Photographs were selected from the databank “Pictures of Facial Affect” solely based on their high recognition rates. This was believed to be the most appropriate method, aiming to avoid the introduction of factors that would potentially disturb results, such as gender, age or ethnicity.</p><p>Furthermore, the photographs are considered standardised facial expressions of emotions and exact AU coding is available for them. This ensures concurrent validity, since performance in one test (virtual head) is related to another, well reputed test (FACS coding and recognition). Potential order effects induced by the study’s repeat measures design were neutralised by presenting the artefacts of the two conditions in a mixed random order.</p><p>Further confidence in the results derives from the fact that participants found the interface easy to use (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab3">3</a>, statement 1), implying that results were not distorted by extraneous user interface factors. Similarly, although participants tended to feel that the photographs looked posed (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab3">3</a>, statement 8), they nevertheless tended to see them as showing real emotion (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab3">3</a>, statement 4). Again, despite some ambivalence in the matter (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-003-0116-7#Tab3">3</a> statements 2 and 9), participants were on the whole happy with the number of categories of emotion offered in the experiment. This is not unexpected since the facial expressions were showing merely the offered range of emotions, and it supports the validity of our results. However, the slight agreement indicates more categories could potentially have produced more satisfaction in participants when making their choice. Two participants noted in their comments, explicitly, that they would have preferred a wider choice of categories.</p><p>Having established the validity of the experimental procedure and results, an important conclusion to be drawn is that the approach of applying the reduced FACS model to virtual face representations is not guaranteed to work for all expressions, or all variations of a particular emotion category. This is implied by the finding that recognition rates for the photographs were significantly higher than those for the virtual heads. Further evidence is supplied in the post-experiment questionnaire data. Two participants, for example, noted that on several occasions the virtual face expression was not distinctive enough, and two other participants noted that the virtual head showed no lines or wrinkles and that recognition might have been easier with these visual cues.</p><p>Nevertheless, our data also suggests that, when applying the FACS model to virtual face representations, emotions<i>
can</i> effectively be visualised with a very limited number of facial features and action units. For example, in respect of the “top scoring” virtual heads, emotion recognition rates are, with the exception of the “disgust” emotion, comparable to those of their corresponding real-life photographs. These top-scoring expressions are exemplar models for which detailed AU scoring is available. They potentially build a basis for emotionally expressive avatars in collaborative virtual environments and hence for the advantages of emotionally enriched CVEs argued for earlier.</p><p>No categorisation system can ever be complete. Although accepted categories exist, emotions can vary in intensity and inevitably there is a subjective element to recognition. When modelling and animating facial features, however, our results suggest that such ambiguity in interpretation can be minimised by focussing on, and emphasising, those visual clues that are particularly distinctive. Although it remains to be corroborated through further studies, it is believed that such simple, pure emotional expressions could fulfil a useful role in displaying explicit, intended communicative acts which can therefore help interaction in a CVE. They can provide a basis for emotionally enriched CVEs, and hence for the benefits of such technology being used, for example, within distance learning as argued for earlier.</p><p>It should perhaps be noted, however, that such pure forms of emotion are not generally seen in real life, as many expressions occurring in face-to-face communication between humans are unintended or automatic reactions. They are often caused by a complex interaction of several simultaneous emotions, vividly illustrated in Picard’s example of a marathon runner who, after winning a race, experiences a range of emotions: “tremendously<i> happy</i> for winning the race,<i> surprised</i> because she believed she would not win,<i> sad</i> that the race was over, and a bit<i> fearful</i> because during the race she had acute abdominal pain” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Picard R (1997) Affective computing. MIT Press, Cambridge, MA" href="/article/10.1007/s10055-003-0116-7#ref-CR21" id="ref-link-section-d71542e2500">21</a>].</p><p>With regards to our own work, such instinctive reactions could be captured and used to control an avatar directly, potentially allowing varying intensities and blends of facial expressions to be recognised and modelled onto avatar faces. However, this study has deliberately opted for an avatar that can express clearly, and unambiguously, what the controlling individual exactly wants it to express, since this is one way in which people may want to use CVE technology.</p><p>Another issue concerns consistency. Social psychology suggests, as do our own findings, that an emotion’s recognisability depends on how consistently it is shown on a face. Furthermore, most emotions, with the exception of sadness, become clearer and more distinctive when their intensity increases. There are indications that in cases where the emotion appeared to be ambiguous at first, the photographs contained subtle clues as to what emotion is displayed, enabling the viewer to assign the emotion after closer inspection. These clues appear to be missing in the virtual head artefacts, suggesting the need to either emphasise distinctive and unambiguous features, or to enhance the model by adding visual cues that help identify variations of emotion more clearly. For further work on emotions in real-time virtual environment interactions the authors aim to concentrate on the former.</p><p>Overall, it should be noted that many of the artefacts classified by participants as the “Other...” choice are actually close to the emotion category expected, confirming that the facial expressions in those cases were not necessarily badly depicted. This highlights the importance of having a well-defined vocabulary when investigating emotions—a problem that is not new to the research community and that has been discussed at length over the years (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ekman P, Friesen W and Ellsworth P (1972) Emotion in the human face: guidelines for research and an integration of findings. Pergamon, New York" href="/article/10.1007/s10055-003-0116-7#ref-CR33" id="ref-link-section-d71542e2510">33</a>] for an early comparison of emotion dimensions vs. categories, also [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e2513">32</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Wehrle T, Kaiser S (2000) Emotion and facial expression. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-003-0116-7#ref-CR58" id="ref-link-section-d71542e2516">58</a>])</p><p>The experimental work discussed in this paper provides strong evidence that creating avatar representations based on the FACS model, but using only a limited number of facial features, allows emotions to be effectively conveyed, giving rise to recognition rates that are comparable with those of the corresponding real-life photographs. Effectiveness has been demonstrated through good recognition rates for all but one of the emotion categories, and efficiency has been established since a reduced feature set was found to be sufficient to build a successfully recognised core set of avatar facial expressions.</p><p>In consequence, the top-scoring expressions illustrated earlier may be taken to provide a sound basis for building emotionally expressive avatars to represent users (which may in fact be agents or human users), in CVEs. When modelling and animating facial features, potential ambiguity in interpretation can be minimised by focussing on, and emphasising, particularly distinctive visual clues of a particular emotion. We have proposed a set of expressions that fulfil this. These are not necessarily<i> the</i> most distinctive clues for a particular emotion as a whole, but those that we found to be very distinctive for that emotion category.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Further work</h2><div class="c-article-section__content" id="Sec19-content"><p>It is planned to extend the work in a variety of ways. The data reveals that certain emotions were confused more often than others, most notably<i> disgust</i> and<i>
anger</i>. This was particularly the case for the virtual head expressions. Markham and Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Markham R, Wang L (1996) Recognition of emotion by Chinese and Australian children. Cross-Cult Psychol 27(5):616–643" href="/article/10.1007/s10055-003-0116-7#ref-CR59" id="ref-link-section-d71542e2542">59</a>] observed a similar link between these two emotions when showing photographs of faces to children. Younger children (aged 4–6) in particular tended to group certain emotions together, while older children (aged 10+) were typically found to have the ability to differentiate correctly. In view of the findings from the current study, this may indicate that although adults can differentiate emotions well in day-to-day social interactions, the limited clues provided by the virtual head make observers revert back to a less experience-based, but more instinct-based manner when categorising them. However, more work will be necessary to investigate this possibility.</p><p>Two other studies also found<i>
disgust</i> often confused with<i> anger</i>
[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 April 2001" href="/article/10.1007/s10055-003-0116-7#ref-CR49" id="ref-link-section-d71542e2554">49</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Spencer-Smith J, Innes-Ker A, Wild H and Townsend J (2002) Making faces with action unit morph targets. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3–5 April 2002" href="/article/10.1007/s10055-003-0116-7#ref-CR60" id="ref-link-section-d71542e2557">60</a>] and concluded that the lack of morph targets, or visual clues, around the nose was a likely cause. In humans,<i> Disgust</i> is typically shown around the mouth and nose [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2564">28</a>] and although our model features a slightly raised lip (AU10), there is no movement of the nose. This strongly suggests that to improve distinctiveness of the<i> disgust</i> expression in a real-time animated model, the nose should be included in the animation, as should the relevant action unit AU9 which is responsible for “nose wrinkling”. Given this, we have now developed an animated model of the virtual head that is capable of lifting and wrinkling the nose to express<i>
disgust</i>.</p><p>The experimental results, in particular the relatively high number of “Other” and “Don’t know” responses, indicate that limiting the number of categories of emotion might have had a negative effect on the recognition success rates. It might be that allowing more categories, and/or offering a range of suitable descriptions for an emotion category (such as<i> joy</i>,<i> cheerfulness</i>
and<i> delight,</i> to complement<i> happiness</i>), would yield still higher recognition rates, and future experiments will address this.</p><p>Similarly, although concentrating on the face as the primary channel for conveying emotions, the work must be seen in a wider context in which the entire humanoid representation of a user can in principle act as the communication device in CVEs. The experiments discussed here set the foundation for further work on emotional postures and the expression of attitude through such a virtual embodiment, drawing for example on the work of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Coulson M (2002) Expressing emotion through body movement: a component process approach. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3–5 April 2002" href="/article/10.1007/s10055-003-0116-7#ref-CR61" id="ref-link-section-d71542e2591">61</a>] on posture, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Capin T, Pandzic I, Thalmann N and Thalmann D (1999) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: Earnshaw R, Vince J (eds) Virtual worlds on the Internet, IEEE Computer Science Press, Washington, DC" href="/article/10.1007/s10055-003-0116-7#ref-CR62" id="ref-link-section-d71542e2594">62</a>] on gestures, or [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Manninen T, Kujanpää T (2002) Non-verbal communication forms in multi-player game sessions. In: Faulkner X, Finlay J, Détienne F (eds) People and computers XVI—memorable yet invisible. BCS Press, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR4" id="ref-link-section-d71542e2597">4</a>] on spatial behaviour and gestures.</p><p>A further contextual aspect of emotional recognition concerns the conversational milieu within which emotions are expressed and recognised. Context plays a crucial role in emotion expression and recognition—the effective, accurate mediation of emotion is closely linked with the situation and other, related, communicative signals. A reliable interpretation of facial expressions, which fails to take cognisance of the context in which they are displayed, is often not possible. One would expect, therefore, that recognition of avatar representations of emotion will be higher when contextualised. This assumption requires empirical investigation, however, and future experiments are planned to address this.</p><p>Bartneck [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 April 2001" href="/article/10.1007/s10055-003-0116-7#ref-CR49" id="ref-link-section-d71542e2606">49</a>] distinguishes between the recognisability of a facial expression of emotion, and its “convincingness”, seeing the latter as more important, and further experimental work will enable study of how this distinction plays itself out in a virtual world. It is predicted that timing will affect “convincingness” in a virtual world. For example, showing surprise over a period of, say, a minute would—at the very least—send confusing or contradictory signals. It will also be possible to investigate this and, more generally, what impact the mediation of emotions has on the conversational interchanges.</p><p>A further contextual issue concerns culture. Although emotions exist universally, there can be cultural differences concerning when emotions are displayed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e2612">32</a>]. It appears that people in various cultures differ in what they have been taught about managing or controlling their facial expressions of emotions. Ekman and Friesen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-003-0116-7#ref-CR28" id="ref-link-section-d71542e2615">28</a>] call these cultural norms “display rules”. Display rules prescribe whether, and if so when, an emotion is supposed to be fully expressed, masked, lowered or intensified. For instance, it has been observed that male Japanese are often reluctant to show unpleasant emotions in the physical presence of others. Interestingly, these cultural differences can also affect the recognition of emotions. In particular, Japanese people reportedly have more difficulty than others recognising negative expressions of emotions, an effect that may reflect a lack of perceptual experience with such expression because of the cultural proscriptions against displaying them [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" href="/article/10.1007/s10055-003-0116-7#ref-CR32" id="ref-link-section-d71542e2618">32</a>]. How such cultural differences might play themselves out in a virtual world is an important open question.</p><p>Finally, the authors wish to explore how the results concerning the mediation of emotions via avatars might be beneficially used to help people with autism. A commonly, if not universally, held view of the nature of autism is that it involves a “triad of impairments” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Wing L (1996) Autism spectrum disorders. Constable Robinson, London, UK" href="/article/10.1007/s10055-003-0116-7#ref-CR63" id="ref-link-section-d71542e2624">63</a>]. There is a social impairment: the person with autism finds it hard to relate to, and empathise with, other people. Secondly, there is a communication impairment: the person with autism finds it hard to understand and use verbal and non-verbal communication. Finally, there is a tendency to rigidity and inflexibility in thinking, language and behaviour. Much current thinking is that this triad is underpinned by a “theory of mind deficit”—people with autism may have a difficulty in understanding mental states and in ascribing them to themselves or to others.</p><p>CVE technology of the sort discussed in this paper could potentially provide a means by which people with autism might communicate with others (autistic or non-autistic) and thus circumvent their social and communication impairment and sense of isolation. Further, as well as this prosthetic role, the technology can also be used for purposes of practice and rehearsal. For this to help combat any theory of mind problem, users would need to be able to recognise the emotions being displayed via the avatars. The findings reported in the current paper give grounds for confidence that the technology will be useful in such a role, but this needs to be investigated in practice [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Moore D, McGrath P and Thorpe J (2000) Computer aided learning for people with autism—a framework for research and development. Innov Educ Train Intl 37(3): 218–228" href="/article/10.1007/s10055-003-0116-7#ref-CR64" id="ref-link-section-d71542e2630">64</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Moore D, Taylor J (2001) Interactive multimedia systems for people with autism. Educ Med 25(3):169–177" href="/article/10.1007/s10055-003-0116-7#ref-CR65" id="ref-link-section-d71542e2633">65</a>].</p><p>Much remains to be investigated, therefore, concerning the educational use of the emerging CVE technology. It is hoped that the work reported in this paper will help set the foundation for further work on the mediation of emotions in virtual worlds.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thalmann D (2001) The role of virtual humans in virtual environment technology and interfaces. In: Earnshaw R," /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Thalmann D (2001) The role of virtual humans in virtual environment technology and interfaces. In: Earnshaw R, Guedj R and Vince J (eds) Frontiers of human-centred computing, online communities and virtual environments. Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fleming B, Dobbs D (1999) Animating facial features and expressions. Charles River Media, Boston, MA" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Fleming B, Dobbs D (1999) Animating facial features and expressions. Charles River Media, Boston, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dumas C, Saugis G, Chaillou C, Degrande S and Viaud M (1998) A 3-D interface for cooperative work. In: Proceed" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Dumas C, Saugis G, Chaillou C, Degrande S and Viaud M (1998) A 3-D interface for cooperative work. In: Proceedings of the Conference on Collaborative Virtual Environments, Manchester, UK, 17–19 June 1998</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Manninen T, Kujanpää T (2002) Non-verbal communication forms in multi-player game sessions. In: Faulkner X, Fi" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Manninen T, Kujanpää T (2002) Non-verbal communication forms in multi-player game sessions. In: Faulkner X, Finlay J, Détienne F (eds) People and computers XVI—memorable yet invisible. BCS Press, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Atkins H, Moore D, Hobbs D and Sharpe S (2001) Learning style theory and computer mediated communication. In: " /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Atkins H, Moore D, Hobbs D and Sharpe S (2001) Learning style theory and computer mediated communication. In: Proceedings of ED-Media, Tampere, Finland, 25–30 June 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Laurillard D (1993) Rethinking university teaching. Routledge, London, UK" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Laurillard D (1993) Rethinking university teaching. Routledge, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moore M (1993) Three types of interaction. In: Harry K, John M and Keegan D (eds) Distance education: new pers" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Moore M (1993) Three types of interaction. In: Harry K, John M and Keegan D (eds) Distance education: new perspectives. Routledge, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson D, Johnson R (1994) Cooperative learning in the culturally diverse classroom. In: DeVillar, Faltis, Cu" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Johnson D, Johnson R (1994) Cooperative learning in the culturally diverse classroom. In: DeVillar, Faltis, Cummins (eds) Cultural diversity in schools. State University of New York Press, Albany, NY</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Webb, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Webb N (1995) Constructive activity and learning in collaborative small groups. Educ Psychol 87(3):406–423" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Webb N (1995) Constructive activity and learning in collaborative small groups. Educ Psychol 87(3):406–423</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F0022-0663.87.3.406" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Educ%20Psychol&amp;volume=87&amp;publication_year=1995&amp;author=Webb%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu A, Farrell R and Singley M (2002) Scaffolding group learning in a collaborative networked environment. In: " /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Wu A, Farrell R and Singley M (2002) Scaffolding group learning in a collaborative networked environment. In: Proceedings of CSCL 2002, Boulder, CO, 7–11 January 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lisetti C, Douglas M and LeRouge C (2002) Intelligent affective interfaces: a user-modeling approach for telem" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Lisetti C, Douglas M and LeRouge C (2002) Intelligent affective interfaces: a user-modeling approach for telemedicine. In: Proceedings of the International Conference on Universal Access in HCI, New Orleans, LA, 5–10 August 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Daly-Jones, A. Monk, L. Watts, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Daly-Jones O, Monk A and Watts L (1998) Some advantages of video conferencing over high-quality audio conferen" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Daly-Jones O, Monk A and Watts L (1998) Some advantages of video conferencing over high-quality audio conferencing: fluency and awareness of attentional focus. Int J Hum-Comp Stud 49(1):21–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Some%20advantages%20of%20video%20conferencing%20over%20high-quality%20audio%20conferencing%3A%20fluency%20and%20awareness%20of%20attentional%20focus&amp;journal=Int%20J%20Hum-Comp%20Stud&amp;volume=49&amp;issue=1&amp;pages=21-58&amp;publication_year=1998&amp;author=Daly-Jones%2CO&amp;author=Monk%2CA&amp;author=Watts%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McShea J, Jennings S and McShea H (1997) Characterising user control of video conferencing in distance educati" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">McShea J, Jennings S and McShea H (1997) Characterising user control of video conferencing in distance education. In: Proceedings of CAL-97, Exeter, UK, 25–26 March 1997</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fabri M, Gerhard M (2000) The virtual student: user embodiment in virtual learning environments. In: Orange G," /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Fabri M, Gerhard M (2000) The virtual student: user embodiment in virtual learning environments. In: Orange G, Hobbs D (eds) International perspectives on tele-education and virtual learning environments, Ashgate, Aldershot, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Knapp M (1978) Nonverbal communication in human interaction. Holt Rinehart Winston, New York, NY</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morris D, Collett P, Marsh P and O’Shaughnessy M (1979) Gestures, their origin and distribution. Jonathan Cape" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Morris D, Collett P, Marsh P and O’Shaughnessy M (1979) Gestures, their origin and distribution. Jonathan Cape, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Argyle M (1988) Bodily communication (second edition). Methuen, New York, NY</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Strongman K (1996) The psychology of emotion (fourth edition). Wiley, New York" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Strongman K (1996) The psychology of emotion (fourth edition). Wiley, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Dittrich, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Dittrich W, Troscianko T, Lea S and Morgan D (1996) Perception of emotion from dynamic point-light displays pr" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Dittrich W, Troscianko T, Lea S and Morgan D (1996) Perception of emotion from dynamic point-light displays presented in dance. Perception 25:727–738</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Perception&amp;volume=25&amp;publication_year=1996&amp;author=Dittrich%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Keltner, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Keltner D (1995) Signs of appeasement: evidence for the distinct displays of embarrassment, amusement and sham" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Keltner D (1995) Signs of appeasement: evidence for the distinct displays of embarrassment, amusement and shame. Pers Soc Psychol 68(3):441–454</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F0022-3514.68.3.441" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Pers%20Soc%20Psychol&amp;volume=68&amp;publication_year=1995&amp;author=Keltner%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Picard R (1997) Affective computing. MIT Press, Cambridge, MA" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Picard R (1997) Affective computing. MIT Press, Cambridge, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Lisetti, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Lisetti C, Schiano D (2000) Facial expression recognition: where human-computer interaction, artificial intell" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Lisetti C, Schiano D (2000) Facial expression recognition: where human-computer interaction, artificial intelligence and cognitive science intersect. Prag Cognit 8(1):185–235</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Prag%20Cognit&amp;volume=8&amp;publication_year=2000&amp;author=Lisetti%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Damásio A (1994) Descarte’s error: emotion, reason and the human brain. Avon, New York" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Damásio A (1994) Descarte’s error: emotion, reason and the human brain. Avon, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cooper B, Brna P and Martins A (2000) Effective affective in intelligent systems—building on evidence of empat" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Cooper B, Brna P and Martins A (2000) Effective affective in intelligent systems—building on evidence of empathy in teaching and learning. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces. Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson W (1998) Pedagogical agents. In: Computers in education proceedings, Beijing, China" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Johnson W (1998) Pedagogical agents. In: Computers in education proceedings, Beijing, China</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McGrath A, Prinz W (2001) All that Is solid melts into software. In: Churchill, Snowdon, Munro (eds) Collabora" /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">McGrath A, Prinz W (2001) All that Is solid melts into software. In: Churchill, Snowdon, Munro (eds) Collaborative virtual environments—digital places and spaces for interaction, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Durlach N, Slater M (2002) Meeting people virtually: experiments in shared virtual environments. In: Schroeder" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Durlach N, Slater M (2002) Meeting people virtually: experiments in shared virtual environments. In: Schroeder R (ed) The social life of avatars, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Ekman P, Friesen W (1975) Unmasking the face. Prentice Hall, Englewood Cliffs, NJ</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="New Oxford Dictionary of English. Oxford University Press, Oxford, UK" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">New Oxford Dictionary of English. Oxford University Press, Oxford, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Russell J, Férnandez-Dols J (1997) The psychology of facial expression. Cambridge University Press, Cambridge," /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Russell J, Férnandez-Dols J (1997) The psychology of facial expression. Cambridge University Press, Cambridge, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Surakka, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Surakka V, Hietanen J (1998) Facial and emotional reactions to Duchénne and non-Duchénne smiles. Int J Psychop" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Surakka V, Hietanen J (1998) Facial and emotional reactions to Duchénne and non-Duchénne smiles. Int J Psychophys 29(1):23–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0167-8760%2897%2900088-3" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Int%20J%20Psycholphys&amp;volume=29&amp;publication_year=1998&amp;author=Surakka%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Zebrowitz, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Zebrowitz L (1997) Reading faces: window to the soul? Westview, Boulder, CO</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Reading&amp;volume=faces&amp;publication_year=1997&amp;author=Zebrowitz%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Ekman, " /><meta itemprop="datePublished" content="1972" /><meta itemprop="headline" content="Ekman P, Friesen W and Ellsworth P (1972) Emotion in the human face: guidelines for research and an integratio" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Ekman P, Friesen W and Ellsworth P (1972) Emotion in the human face: guidelines for research and an integration of findings. Pergamon, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Emotion%20in%20the%20human&amp;volume=face&amp;publication_year=1972&amp;author=Ekman%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P (1999) Facial expressions. In: Dalgleish T, Power M (eds) Handbook of cognition and emotion. Wiley, Ne" /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Ekman P (1999) Facial expressions. In: Dalgleish T, Power M (eds) Handbook of cognition and emotion. Wiley, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, San Francisco, CA" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, San Francisco, CA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bartlett M (1998) Face image analysis by unsupervised learning and redundancy reduction. Dissertation, Univers" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Bartlett M (1998) Face image analysis by unsupervised learning and redundancy reduction. Dissertation, University of California</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Pélachaud, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Pélachaud C, Badler N and Steedman M (1996) Generating facial expressions for speech. Cog Sci 20(1):1–46" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">Pélachaud C, Badler N and Steedman M (1996) Generating facial expressions for speech. Cog Sci 20(1):1–46</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0364-0213%2899%2980001-9" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cog%20Sci&amp;volume=20&amp;publication_year=1996&amp;author=P%C3%A9lachaud%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P, Rosenzweig L (eds) What the face reveals: basic and applied studies of spontaneous expression using t" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">Ekman P, Rosenzweig L (eds) What the face reveals: basic and applied studies of spontaneous expression using the facial action coding system. Oxford University Press, Oxford, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Terzopoulos, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Terzopoulos D, Waters K (1993) Analysis and synthesis of facial image sequences using physical and anatomical " /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Terzopoulos D, Waters K (1993) Analysis and synthesis of facial image sequences using physical and anatomical models. Patt Anal Mach Intell 15(6):569–579</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.216726" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Patt%20Anal%20Mach%20Intell&amp;volume=15&amp;publication_year=1993&amp;author=Terzopoulos%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Platt, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Platt S, Badler N (1981) Animating facial expression. ACM SIGGRAPH 15(3):245–252" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Platt S, Badler N (1981) Animating facial expression. ACM SIGGRAPH 15(3):245–252</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=ACM%20SIGGRAPH&amp;volume=15&amp;publication_year=1981&amp;author=Platt%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Parke, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Parke F (1982) Parameterized modeling for facial animation. IEEE Comp Graph Appl 2(9):61–68" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">Parke F (1982) Parameterized modeling for facial animation. IEEE Comp Graph Appl 2(9):61–68</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=IEEE%20Comp%20Graph%20Appl&amp;volume=2&amp;publication_year=1982&amp;author=Parke%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benford S, Bowers J, Fahlén L, Greenhalgh C and Snowdon D (1995) User embodiment in collaborative virtual envi" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Benford S, Bowers J, Fahlén L, Greenhalgh C and Snowdon D (1995) User embodiment in collaborative virtual environments. In: Proceedings of CHI 1995 Proceedings, Denver, CO, 7–11 May 1995</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mori M (1982) The buddha in the robot. Tuttle, Boston, MA" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">Mori M (1982) The buddha in the robot. Tuttle, Boston, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reichardt J (1978) Robots: fact, fiction and prediction. Thames &amp; Hudson, London, UK" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Reichardt J (1978) Robots: fact, fiction and prediction. Thames &amp; Hudson, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hindmarsh J, Fraser M, Heath C and Benford S (2001) Virtually missing the point: configuring CVEs for object-f" /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Hindmarsh J, Fraser M, Heath C and Benford S (2001) Virtually missing the point: configuring CVEs for object-focused interaction. In: Churchill E, Snowdon D and Munro A (eds) Collaborative virtual environments: digital places and spaces for interaction, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Godenschweger F, Strothotte T and Wagener H (1997) Rendering gestures as line drawings. In: Proceedings of the" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">Godenschweger F, Strothotte T and Wagener H (1997) Rendering gestures as line drawings. In: Proceedings of the Gesture Workshop 1997, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Donath J (2001) Mediated faces. In: Beynon M, Nehaniv C and Dautenhahn K (eds) Cognitive technology: instrumen" /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Donath J (2001) Mediated faces. In: Beynon M, Nehaniv C and Dautenhahn K (eds) Cognitive technology: instruments of mind, Warwick, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 Apri" /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">Bartneck C (2001) Affective expressions of machines. In: Proceedings of CHI 2001, Seattle, WA, 31 March–5 April 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Ellis, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Ellis H (1990) Developmental trends in face recognition. Bullet Brit Psychol Soc 3:114–119" /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">Ellis H (1990) Developmental trends in face recognition. Bullet Brit Psychol Soc 3:114–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Bullet%20Brit%20Psychol%20Soc&amp;volume=3&amp;publication_year=1990&amp;author=Ellis%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Dittrich, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Dittrich W (1991) Facial motion and the recognition of emotions. Psychol Beit 33(3/4):366–377" /><span class="c-article-references__counter">51.</span><p class="c-article-references__text" id="ref-CR51">Dittrich W (1991) Facial motion and the recognition of emotions. Psychol Beit 33(3/4):366–377</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20motion%20and%20the%20recognition%20of%20emotions&amp;journal=Psychol%20Beit&amp;volume=33&amp;issue=3%2F4&amp;pages=366-377&amp;publication_year=1991&amp;author=Dittrich%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="H-Anim Working Group. Specification for a standard VRML humanoid. http://www.h-anim.org." /><span class="c-article-references__counter">52.</span><p class="c-article-references__text" id="ref-CR52">H-Anim Working Group. Specification for a standard VRML humanoid. http://www.h-anim.org.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yacoob Y, Davis L (1994) Computing spatio-temporal representations of human faces. In: Proceedings of the Comp" /><span class="c-article-references__counter">53.</span><p class="c-article-references__text" id="ref-CR53">Yacoob Y, Davis L (1994) Computing spatio-temporal representations of human faces. In: Proceedings of the Computer Vision and Pattern Recognition Conference, Seattle, WA, June 1994</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Essa, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Essa I, Pentland A (1995) Coding, analysis, interpretation, and recognition of facial expressions. IEEE Trans " /><span class="c-article-references__counter">54.</span><p class="c-article-references__text" id="ref-CR54">Essa I, Pentland A (1995) Coding, analysis, interpretation, and recognition of facial expressions. IEEE Trans Patt Anal Mach Intellig 19(7):757–763</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.598232" aria-label="View reference 54">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20Intellig&amp;volume=19&amp;publication_year=1995&amp;author=Essa%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Neisser U (1976) Cognition and reality. Freeman, San Francisco, CA" /><span class="c-article-references__counter">55.</span><p class="c-article-references__text" id="ref-CR55">Neisser U (1976) Cognition and reality. Freeman, San Francisco, CA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poggi I, Pélachaud C (2000) Emotional meaning and expression in animated faces. In: Paiva A (ed) Affective int" /><span class="c-article-references__counter">56.</span><p class="c-article-references__text" id="ref-CR56">Poggi I, Pélachaud C (2000) Emotional meaning and expression in animated faces. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rutter D (1990) Non-verbal communication. In: Eysenck M (ed) The Blackwell dictionary of cognitive psychology," /><span class="c-article-references__counter">57.</span><p class="c-article-references__text" id="ref-CR57">Rutter D (1990) Non-verbal communication. In: Eysenck M (ed) The Blackwell dictionary of cognitive psychology, Blackwell, Oxford, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wehrle T, Kaiser S (2000) Emotion and facial expression. In: Paiva A (ed) Affective interactions: towards a ne" /><span class="c-article-references__counter">58.</span><p class="c-article-references__text" id="ref-CR58">Wehrle T, Kaiser S (2000) Emotion and facial expression. In: Paiva A (ed) Affective interactions: towards a new generation of computer interfaces, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Markham, L. Wang, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Markham R, Wang L (1996) Recognition of emotion by Chinese and Australian children. Cross-Cult Psychol 27(5):6" /><span class="c-article-references__counter">59.</span><p class="c-article-references__text" id="ref-CR59">Markham R, Wang L (1996) Recognition of emotion by Chinese and Australian children. Cross-Cult Psychol 27(5):616–643</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20emotion%20by%20Chinese%20and%20Australian%20children&amp;journal=Cross-Cult%20Psychol&amp;volume=27&amp;issue=5&amp;pages=616-643&amp;publication_year=1996&amp;author=Markham%2CR&amp;author=Wang%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spencer-Smith J, Innes-Ker A, Wild H and Townsend J (2002) Making faces with action unit morph targets. In: Pr" /><span class="c-article-references__counter">60.</span><p class="c-article-references__text" id="ref-CR60">Spencer-Smith J, Innes-Ker A, Wild H and Townsend J (2002) Making faces with action unit morph targets. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3–5 April 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coulson M (2002) Expressing emotion through body movement: a component process approach. In: Proceedings of th" /><span class="c-article-references__counter">61.</span><p class="c-article-references__text" id="ref-CR61">Coulson M (2002) Expressing emotion through body movement: a component process approach. In: Proceedings of the Artificial Intelligence and Simulated Behaviour Conference, London, UK, 3–5 April 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Capin T, Pandzic I, Thalmann N and Thalmann D (1999) Realistic avatars and autonomous virtual humans in VLNET " /><span class="c-article-references__counter">62.</span><p class="c-article-references__text" id="ref-CR62">Capin T, Pandzic I, Thalmann N and Thalmann D (1999) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: Earnshaw R, Vince J (eds) Virtual worlds on the Internet, IEEE Computer Science Press, Washington, DC</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wing L (1996) Autism spectrum disorders. Constable Robinson, London, UK" /><span class="c-article-references__counter">63.</span><p class="c-article-references__text" id="ref-CR63">Wing L (1996) Autism spectrum disorders. Constable Robinson, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Moore, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Moore D, McGrath P and Thorpe J (2000) Computer aided learning for people with autism—a framework for research" /><span class="c-article-references__counter">64.</span><p class="c-article-references__text" id="ref-CR64">Moore D, McGrath P and Thorpe J (2000) Computer aided learning for people with autism—a framework for research and development. Innov Educ Train Intl 37(3): 218–228</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F13558000050138452" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Innov%20Educ%20Train%20Intl&amp;volume=37&amp;publication_year=2000&amp;author=Moore%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Moore, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Moore D, Taylor J (2001) Interactive multimedia systems for people with autism. Educ Med 25(3):169–177" /><span class="c-article-references__counter">65.</span><p class="c-article-references__text" id="ref-CR65">Moore D, Taylor J (2001) Interactive multimedia systems for people with autism. Educ Med 25(3):169–177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F13581650020054361" aria-label="View reference 65">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 65 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Educ%20Med&amp;volume=25&amp;publication_year=2001&amp;author=Moore%2C">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-003-0116-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>Photographs from the CD-Rom<i>
Pictures of Facial Affect</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Ekman P, Friesen W (1975) Pictures of facial affect. University of California Press, San Francisco, CA" href="/article/10.1007/s10055-003-0116-7#ref-CR35" id="ref-link-section-d71542e2652">35</a>] are used with permission. Original virtual head geometry by Geometrek. Detailed results of this study as well as the virtual head prototypes are available online at http://www.leedsmet.ac.uk/ies/comp/staff/mfabri/emotion.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">ISLE Research Group, Leeds Metropolitan University, Leeds, UK</p><p class="c-article-author-affiliation__authors-list">Marc Fabri &amp; David Moore</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">School of Informatics, University of Bradford, Bradford, UK</p><p class="c-article-author-affiliation__authors-list">Dave Hobbs</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Marc-Fabri"><span class="c-article-authors-search__title u-h3 js-search-name">Marc Fabri</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marc+Fabri&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marc+Fabri" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marc+Fabri%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-David-Moore"><span class="c-article-authors-search__title u-h3 js-search-name">David Moore</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;David+Moore&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David+Moore" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David+Moore%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Dave-Hobbs"><span class="c-article-authors-search__title u-h3 js-search-name">Dave Hobbs</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Dave+Hobbs&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dave+Hobbs" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dave+Hobbs%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-003-0116-7/email/correspondent/c1/new">Marc Fabri</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Mediating%20the%20expression%20of%20emotion%20in%20educational%20collaborative%20virtual%20environments%3A%20an%20experimental%20study&amp;author=Marc%20Fabri%20et%20al&amp;contentID=10.1007%2Fs10055-003-0116-7&amp;publication=1359-4338&amp;publicationDate=2004-02-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Fabri, M., Moore, D. &amp; Hobbs, D. Mediating the expression of emotion in educational collaborative virtual environments: an experimental study.
                    <i>Virtual Reality</i> <b>7, </b>66–81 (2004). https://doi.org/10.1007/s10055-003-0116-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-003-0116-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2002-09-03">03 September 2002</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-10-02">02 October 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-02-05">05 February 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-04">April 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-003-0116-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-003-0116-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Avatar</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Emotion</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Facial expression</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-003-0116-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=116;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

